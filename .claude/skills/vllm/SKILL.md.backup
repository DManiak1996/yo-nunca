---
name: vllm
description: vLLM high-performance LLM inference engine from GitHub README and docs
---

# vllm

vLLM high-performance LLM inference engine from GitHub README and docs

## Description

A high-throughput and memory-efficient inference and serving engine for LLMs

**Repository:** [vllm-project/vllm](https://github.com/vllm-project/vllm)
**Language:** Python
**Stars:** 62,275
**License:** Apache License 2.0

## When to Use This Skill

Use this skill when you need to:
- Understand how to use vllm
- Look up API documentation
- Find usage examples
- Check for known issues or recent changes
- Review release history

## Quick Reference

### Repository Info
- **Homepage:** https://docs.vllm.ai
- **Topics:** gpt, llm, pytorch, model-serving, transformer, llm-serving, inference, llama, amd, cuda, tpu, deepseek, qwen, blackwell, deepseek-v3, gpt-oss, kimi, moe, openai, qwen3
- **Open Issues:** 3144
- **Last Updated:** 2025-11-06

### Languages
- **Python:** 86.2%
- **Cuda:** 8.0%
- **C++:** 4.1%
- **Shell:** 0.7%
- **C:** 0.4%
- **CMake:** 0.4%
- **Dockerfile:** 0.1%
- **Jinja:** 0.0%

### Recent Releases
- **v0.11.0** (2025-10-02): v0.11.0
- **v0.10.2** (2025-09-13): v0.10.2
- **v0.10.1.1** (2025-08-20): v0.10.1.1

## Available References

- `references/README.md` - Complete README documentation
- `references/CHANGELOG.md` - Version history and changes
- `references/issues.md` - Recent GitHub issues
- `references/releases.md` - Release notes
- `references/file_structure.md` - Repository structure

## Usage

See README.md for complete usage instructions and examples.

---

**Generated by Skill Seeker** | GitHub Repository Scraper
