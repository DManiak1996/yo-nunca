# Releases

Version history for this repository (75 releases).

## v0.11.0: v0.11.0
**Published:** 2025-10-02

## Highlights
This release features 538 commits, 207 contributors (65 new contributors)!

* This release completes the removal of V0 engine. V0 engine code including AsyncLLMEngine, LLMEngine, MQLLMEngine, all attention backends, and related components have been removed. **V1 is the only engine in the codebase now.**
* This releases turns on **FULL_AND_PIECEWISE as the CUDA graph mode default**. This should provide better out of the box performance for most models, particularly fine-grained MoEs, while preserving compatibility with existing models supporting only PIECEWISE mode.

Note: In v0.11.0 (and v0.10.2), `--async-scheduling` will produce gibberish output in some cases such as preemption and others. This functionality is correct in v0.10.1. We are actively fixing it for the next version. 

### Model Support
* New architectures: DeepSeek-V3.2-Exp (#25896), Qwen3-VL series (#24727), Qwen3-Next (#24526), OLMo3 (#24534), LongCat-Flash (#23991), Dots OCR (#24645), Ling2.0 (#24627), CWM (#25611).
* Encoders: RADIO encoder support (#24595), Transformers backend support for encoder-only models (#25174).
* Task expansion: BERT token classification/NER (#24872), multimodal models for pooling tasks (#24451).
* Data parallel for vision encoders: InternVL (#23909), Qwen2-VL (#25445), Qwen3-VL (#24955).
* Speculative decoding: EAGLE3 for MiniCPM3 (#24243) and GPT-OSS (#25246).
* Features: Qwen3-VL text-only mode (#26000), EVS video token pruning (#22980), Mamba2 TP+quantization (#24593), MRoPE + YaRN (#25384), Whisper on XPU (#25123), LongCat-Flash-Chat tool calling (#24083).
* Performance: GLM-4.1V 916ms TTFT reduction via fused RMSNorm (#24733), GLM-4 MoE SharedFusedMoE optimization (#24849), Qwen2.5-VL CUDA sync removal (#24741), Qwen3-VL Triton MRoPE kernel (#25055), FP8 checkpoints for Qwen3-Next (#25079).
* Reasoning: SeedOSS reason parser (#24263).

### Engine Core
* KV cache offloading: CPU offloading with LRU management (#19848, #20075, #21448, #22595, #24251).
* V1 features: Prompt embeddings (#24278), sharded state loading (#25308), FlexAttention sliding window (#24089), LLM.apply_model (#18465).
* Hybrid allocator: Pipeline parallel (#23974), varying hidden sizes (#25101).
* Async scheduling: Uniprocessor executor support (#24219).
* Architecture: Tokenizer group removal (#24078), shared memory multimodal caching (#20452).
* Attention: Hybrid SSM/Attention in Triton (#21197), FlashAttention 3 for ViT (#24347).
* Performance: FlashInfer RoPE 2x speedup (#21126), fused Q/K RoPE 11% improvement (#24511, #25005), 8x spec decode overhead reduction (#24986), FlashInfer spec decode with 1.14x speedup (#25196), model info caching (#23558), inputs_embeds copy avoidance (#25739).
* LoRA: Optimized weight loading (#25403).
* Defaults: CUDA graph mode FULL_AND_PIECEWISE (#25444), Inductor standalone compile disabled (#25391).
* torch.compile: CUDA graph Inductor partition integration (#24281).

### Hardware & Performance
* NVIDIA: FP8 FlashInfer MLA decode (#24705), BF16 fused MoE for Hopper/Blackwell expert parallel (#25503).
* DeepGEMM: Enabled by default (#24462), 5.5% throughput improvement (#24783).
* New architectures: RISC-V 64-bit (#22112), ARM non-x86 CPU (#25166), ARM 4-bit fused MoE (#23809).
* AMD: ROCm 7.0 (#25178), GLM-4.5 MI300X tuning (#25703).
* Intel XPU: MoE DP accuracy fix (#25465).

### Large Scale Serving & Performance
* Dual-Batch Overlap (DBO): Overlapping computation mechanism (#23693), DeepEP high throughput + prefill (#24845).
* Data Parallelism: torchrun launcher (#24899), Ray placement groups (#25026), Triton DP/EP kernels (#24588).
* EPLB: Hunyuan V1 (#23078), Mixtral (#22842), static placement (#23745), reduced overhead (#24573).
* Disaggregated serving: KV transfer metrics (#22188), NIXL MLA latent dimension (#25902).
* MoE: Shared expert overlap optimization (#24254), SiLU kernel for DeepSeek-R1 (#24054), Enable Allgather/ReduceScatter backend for NaiveAllToAll (#23964).
* Distributed: NCCL symmetric memory with 3-4% throughput improvement (#24532), enabled by default for TP (#25070).

### Quantization
* FP8: Per-token-group quantization (#24342), hardware-accelerated instructions (#24757), torch.compile KV cache (#22758), paged attention update (#22222).
* FP4: NVFP4 for dense models (#25609), Gemma3 (#22771), Llama 3.1 405B (#25135).
* W4A8: Faster preprocessing (#23972).
* Compressed tensors: Blocked FP8 for MoE (#25219).

### API & Frontend
* OpenAI: Prompt logprobs for all tokens (#24956), logprobs=-1 for full vocab (#25031), reasoning streaming events (#24938), Responses API MCP tools (#24628, #24985), health 503 on dead engine (#24897).
* Multimodal: Media UUID caching (#23950), image path format (#25081).
* Tool calling: XML parser for Qwen3-Coder (#25028), Hermes-style tokens (#25281).
* CLI: --enable-logging (#25610), improved --help (#24903).
* Config: Speculative model engine args (#25250), env validation (#24761), NVTX profiling (#25501), guided decoding backward compatibility (#25615, #25422).
* Metrics: V1 TPOT histogram (#24015), hidden deprecated gpu_ metrics (#24245), KV cache GiB units (#25204, #25479).
* UX: Removed misleading quantization warning (#25012).

### Security
* https://github.com/vllm-project/vllm/security/advisories/GHSA-wr9h-g72x-mwhm

### Dependencies
* PyTorch 2.8 for CPU (#25652), FlashInfer 0.3.1 (#24470), CUDA 13 (#24599), ROCm 7.0 (#25178).
* **Build requirements**: C++17 now enforced globally (#24823).
* **TPU**: Deprecated `xm.mark_step` in favor of `torch_xla.sync` (#25254).

### V0 Deprecation
* Engines: AsyncLLMEngine (#25025), LLMEngine (#25033), MQLLMEngine (#25019), core (#25321), model runner (#25328), MP executor (#25329).
* Components: Attention backends (#25351), encoder-decoder (#24907), output processor (#25320), sampling metadata (#25345), Sequence/Sampler (#25332).
* Interfaces: LoRA (#25686), async output processor (#25334), MultiModalPlaceholderMap (#25366), seq group methods (#25330), placeholder attention (#25510), input embeddings (#25242), multimodal registry (#25362), max_seq_len_to_capture (#25543), attention classes (#25541), hybrid models (#25400), backend suffixes (#25489), compilation fallbacks (#25675), default args (#25409).

## What's Changed
* [Qwen3-Next] MoE configs for H20 TP=1,2,4,8 by @jeejeelee in https://github.com/vllm-project/vllm/pull/24707
* [DOCs] Update ROCm installation docs section by @gshtras in https://github.com/vllm-project/vllm/pull/24691
* Enable conversion of multimodal models to pooling tasks by @maxdebayser in https://github.com/vllm-project/vllm/pull/24451
* Fix implementation divergence for BLOOM models between vLLM and HuggingFace when using prompt embeds by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/24686
* [Bugfix] Fix MRoPE dispatch on CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/24712
* [BugFix] Fix Qwen3-Next PP by @njhill in https://github.com/vllm-project/vllm/pull/24709
* [CI] Fix flaky test  v1/worker/test_gpu_model_runner.py::test_kv_cache_stride_order          by @heheda12345 in https://github.com/vllm-project/vllm/pull/24640
* [CI] Add ci_envs for convenient local testing by @noooop in https://github.com/vllm-project/vllm/pull/24630
* [CI/Build] Skip prompt embeddings tests on V1-only CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/24721
* [Misc][gpt-oss] Add gpt-oss label to PRs that mention harmony or related to builtin tool call by @heheda12345 in https://github.com/vllm-project/vllm/pull/24717
* [Bugfix] Fix BNB name match by @jeejeelee in https://github.com/vllm-project/vllm/pull/24735
* [Kernel] [CPU] refactor `cpu_attn.py:_run_sdpa_forward` for better memory access by @ignaciosica in https://github.com/vllm-project/vllm/pull/24701
* [sleep mode] save memory for on-the-fly quantization by @youkaichao in https://github.com/vllm-project/vllm/pull/24731
* [Multi Modal] Add FA3 in VIT by @wwl2755 in https://github.com/vllm-project/vllm/pull/24347
* [Multimodal] Remove legacy multimodal fields in favor of MultiModalFeatureSpec  by @sfeng33 in https://github.com/vllm-project/vllm/pull/24548
* [Doc]: fix typos in various files by @didier-durand in https://github.com/vllm-project/vllm/pull/24726
* [Docs] Fix warnings in mkdocs build (continued) by @Zerohertz in https://github.com/vllm-project/vllm/pull/24740
* [Bugfix] Fix MRoPE dispatch on XPU by @yma11 in https://github.com/vllm-project/vllm/pull/24724
* [Qwen3-Next] MoE configs for H100 TP=1,2 and TP2/EP by @elvircrn in https://github.com/vllm-project/vllm/pull/24739
* [Core] Shared memory based object store for Multimodal data caching and IPC by @dongluw in https://github.com/vllm-project/vllm/pull/20452
* [Bugfix][Frontend] Fix `--enable-log-outputs` does not match the documentation by @kebe7jun in https://github.com/vllm-project/vllm/pull/24626
* [Models] Optimise and simplify `_validate_and_reshape_mm_tensor` by @lgeiger in https://github.com/vllm-project/vllm/pull/24742
* [Models] Prevent CUDA sync in Qwen2.5-VL by @lgeiger in https://github.com/vllm-project/vllm/pull/24741
* [Model] Switch to Fused RMSNorm in GLM-4.1V model by @SamitHuang in https://github.com/vllm-project/vllm/pull/24733
* [UX] Remove AsyncLLM torch profiler disabled log by @mgoin in https://github.com/vllm-project/vllm/pull/24609
* [CI] Speed up model unit tests in CI by @afeldman-nm in https://github.com/vllm-project/vllm/pull/24253
* [Bugfix] Fix incompatibility between #20452 and #24548 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/24754
* [CI] Trigger BC Linter when labels are added/removed by @zhewenl in https://github.com/vllm-project/vllm/pull/24767
* [Benchmark] Allow arbitrary headers to be passed to benchmarked endpoints by @smarterclayton in https://github.com/vllm-project/vllm/pull/23937
* [Compilation Bug] Fix Inductor Graph Output with Shape Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/24772
* Invert pattern order to make sure that out_proj layers are identified by @anmarques in https://github.com/vllm-project/vllm/pull/24781
* [Attention][FlashInfer] Enable FP8 FlashInfer (TRTLLM) MLA decode by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/24705
* Add FLASHINFER_MLA to backend selector test by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/24753
* [Qwen3Next] Fixes the cuda graph capture conditions under large batch sizes (#24660) by @sighingnow in https://github.com/vllm-project/vllm/pull/24667
* [Core] Support async scheduling with uniproc executor  by @njhill in https://github.com/vllm-project/vllm/pull/24219
* [Frontend][Multimodal] Allow skipping media data when UUIDs are provided.  by @huachenheli in https://github.com/vllm-project/vllm/pull/23950
* [Model] Add Olmo3 model implementation by @2015aroras in https://github.com/vllm-project/vllm/pull/24534
* [Bugfix] Fix GPUModelRunner has no attribute lora_manager by @jeejeelee in https://github.com/vllm-project/vllm/pull/24762
* [Chore] Remove unused batched RoPE op & kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24789
* [Docs] Fix warnings in mkdocs build (continued) by @Zerohertz in https://github.com/vllm-project/vllm/pull/24791
* [Docs] Remove Neuron install doc as backend no longer exists by @hmellor in https://github.com/vllm-project/vllm/pull/24396
* [Doc]: Remove 404 hyperlinks by @rozeappletree in https://github.com/vllm-project/vllm/pull/24785
* [Perf] Use NVIDIA hardware-accelerated instruction for float to fp8_e4m3 quantization by @elvischenv in https://github.com/vllm-project/vllm/pull/24757
* [Kernels][DP/EP] Optimize Silu Kernel for R1 by @elvircrn in https://github.com/vllm-project/vllm/pull/24054
* [Core][Multimodal] Cache `supports_kw` by @lgeiger in https://github.com/vllm-project/vllm/pull/24773
* [CI Failure] Fix test_flashinfer_cutlass_mxfp4_mxfp8_fused_moe by @mgoin in https://github.com/vllm-project/vllm/pull/24750
* [Misc] Correct an outdated comment. by @russellb in https://github.com/vllm-project/vllm/pull/24765
* [Doc]: fix typos in various files by @didier-durand in https://github.com/vllm-project/vllm/pull/24798
* [CI][Spec Decode] Adjust threshold for flaky ngram spec decoding test again by @wwl2755 in https://github.com/vllm-project/vllm/pull/24771
* Remove redundant assignment in xfer_buffers, This is a little fix by @ChenTaoyu-SJTU in https://github.com/vllm-project/vllm/pull/24732
* [Minor] Simplify duplicative device check for cuda by @ziliangpeng in https://github.com/vllm-project/vllm/pull/24793
* [Chore] Minor simplification for non-PP path by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24810
* [Multi Modal][Performance] Fused Q,K's apply_rope into one by @wwl2755 in https://github.com/vllm-project/vllm/pull/24511
* [Misc] Improve `s3_utils` type hints with `BaseClient` by @Zerohertz in https://github.com/vllm-project/vllm/pull/24825
* [Perf] Fix DeepGEMM Contiguous Layout Issue, 5.5% Throughput Improvement by @yewentao256 in https://github.com/vllm-project/vllm/pull/24783
* fix type of sampling rate for encode_base64 by @co63oc in https://github.com/vllm-project/vllm/pull/24826
* [Benchmarks] Throw usage error when using dataset-name random and dataset-path together by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24819
* Force use C++17 globally to avoid compilation error by @chenfengjin in https://github.com/vllm-project/vllm/pull/24823
* [Chore] Remove ipex_ops warning by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/24835
* [Spec Decoding]Support Spec Decoding Metrics in DP Mode by @wuhang2014 in https://github.com/vllm-project/vllm/pull/24049
* [Hybrid Allocator] Support Pipeline Parallel by @heheda12345 in https://github.com/vllm-project/vllm/pull/23974
* [Docs] Have a try to improve frameworks/streamlit.md by @windsonsea in https://github.com/vllm-project/vllm/pull/24841
* [kv cache] update num_free_blocks in the end by @andyxning in https://github.com/vllm-project/vllm/pull/24228
* [Frontend] Skip `stop` in reasoning content by @gaocegege in https://github.com/vllm-project/vllm/pull/14550
* [Bugfix] MiDashengLM model contact error under concurrent testing by @bingchen-mi in https://github.com/vllm-project/vllm/pull/24738
* [Doc]: fix typos in various files by @didier-durand in https://github.com/vllm-project/vllm/pull/24821
* [Misc] rename interval to max_recent_requests by @andyxning in https://github.com/vllm-project/vllm/pull/24229
* [Misc] Own KVConnectors installation by @NickLucche in https://github.com/vllm-project/vllm/pull/24867
* [P/D]`kv_output_aggregator` support heterogeneous by @LCAIZJ in https://github.com/vllm-project/vllm/pull/23917
* [UT] enhance free kv cache block queue popleft_n by @andyxning in https://github.com/vllm-project/vllm/pull/24220
* [XPU] Set consistent default KV cache layout by @NickLucche in https://github.com/vllm-project/vllm/pull/24745
* [Misc] Fix examples openai_pooling_client.py  by @noooop in https://github.com/vllm-project/vllm/pull/24853
* [Model]: support Ling2.0 by @ant-yy in https://github.com/vllm-project/vllm/pull/24627
* [Bugfix] Fix GLM4.1V multimodal processor with compatability for Transformers v4.56 by @Isotr0py in https://github.com/vllm-project/vllm/pull/24822
* Fp8 paged attention update by @xiao-llm in https://github.com/vllm-project/vllm/pull/22222
* Reinstate existing torch script by @hmellor in https://github.com/vllm-project/vllm/pull/24729
* [USAGE] Improve error handling for weight initialization in Unquantizedâ€¦ by @koiker in https://github.com/vllm-project/vllm/pull/20321
* Move `MultiModalConfig` from `config/__init__.py` to `config/multimodal.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24659
* [Transform] Deterministic Hadacore Transforms by @kylesayrs in https://github.com/vllm-project/vllm/pull/24106
* Update num_tokens_across_dp to use nccl instead of gloo by @SageMoore in https://github.com/vllm-project/vllm/pull/24105
* Bump Flashinfer to 0.3.1 by @bbartels in https://github.com/vllm-project/vllm/pull/24868
* [gpt-oss] Add IncompleteDetails to ResponsesRepsonse by @qandrew in https://github.com/vllm-project/vllm/pull/24561
* [gpt-oss][1a] create_responses stream outputs BaseModel type, api server is SSE still by @qandrew in https://github.com/vllm-project/vllm/pull/24759
* [Performance] Remove redundant clone() calls in cutlass_mla by @alexm-redhat in https://github.com/vllm-project/vllm/pull/24891
* [Bug] Fix Cutlass Scaled MM Compilation Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/24887
* [ci] fix wheel names for arm wheels by @simon-mo in https://github.com/vllm-project/vllm/pull/24898
* [Tests] fix initialization of kv hash in tests by @mickaelseznec in https://github.com/vllm-project/vllm/pull/24273
* [Compile] Fix noop_elimination pass and add tests for noop_elimination by @ZJY0516 in https://github.com/vllm-project/vllm/pull/24880
* `HuggingFace` -> `Hugging Face` in `Integration with Hugging Face` docs by @sergiopaniego in https://github.com/vllm-project/vllm/pull/24889
* Updated CODEOWNERS for flashinfer, mla, fused_moe by @mgoin in https://github.com/vllm-project/vllm/pull/24906
* [Deprecation] Remove DeepGEMM Old Symbol Wrapper by @yewentao256 in https://github.com/vllm-project/vllm/pull/24902
* [ROCm][Bugfix] Fix the case where there's bias by @gshtras in https://github.com/vllm-project/vllm/pull/24895
* Add pytest-cov and .coveragerc by @rzabarazesh in https://github.com/vllm-project/vllm/pull/24778
* [Bug] Fix `is_flashmla_supported` Check Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/24774
* [CI] Small Accuracy Eval Test for Deepseek Model by @yewentao256 in https://github.com/vllm-project/vllm/pull/24259
* [Metrics] Hide deprecated metrics with gpu_ prefix by @markmc in https://github.com/vllm-project/vllm/pull/24245
* [Docs] Update instructions for how to using existing torch binary by @zou3519 in https://github.com/vllm-project/vllm/pull/24892
* Upgrade flashinfer to 0.3.1 by @houseroad in https://github.com/vllm-project/vllm/pull/24470
* [XPU] Fix circular import error.  by @jikunshang in https://github.com/vllm-project/vllm/pull/24927
* Remove V0 Encoder-Decoder Support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24907
* [Bugfix] Fix sequence parallelism bug when enable pipeline parallelism by @cascade812 in https://github.com/vllm-project/vllm/pull/24021
* [Bug] [Spec Dec]: Fix kv_cache dtype mismatch for Eagle3 drafter on FP8 target by @vllmellm in https://github.com/vllm-project/vllm/pull/24505
* [QWEN NEXT] Fused MoE kernels Optimization configs by @samanamp in https://github.com/vllm-project/vllm/pull/24924
* [benchmark] Add triton version in the moe tuned config by @jeejeelee in https://github.com/vllm-project/vllm/pull/24769
* [Bugfix] remove duplicate tokens streamed in required tool choice streaming by @Jason-CKY in https://github.com/vllm-project/vllm/pull/23312
* [Mamba] Support TP>1 with quantization for mamba2 mixer in case `n_groups % tp_size == 0` by @tomeras91 in https://github.com/vllm-project/vllm/pull/24593
* [Feat][EPLB] A novel static EPLB placement strategy for MoE models. by @cboss6 in https://github.com/vllm-project/vllm/pull/23745
* Move `SpeculativeConfig` from `config/__init__.py` to `config/speculative.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24904
* [Docs] move benchmarks README to contributing guides by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24820
* feat: Add Grafana and Perces monitoring dashboards for vLLM by @liangwen12year in https://github.com/vllm-project/vllm/pull/23498
* (doc): set cmake c++ compatible standard when building on MacOS CPU. by @teekenl in https://github.com/vllm-project/vllm/pull/23483
* [CI] Add Decode Context Parallelism (DCP) test to CI by @minosfuture in https://github.com/vllm-project/vllm/pull/24487
* [Model] Clean up and simplify Mamba2 Metadata Usage in both V0 and V1 by @cyang49 in https://github.com/vllm-project/vllm/pull/24331
* [Core][MultiModalHasher] Don't convert memoryviews to bytes during hashing by @lgeiger in https://github.com/vllm-project/vllm/pull/24925
* [Core/DBO][1/N] Add Dual-Batch Overlap mechanism to VLLM by @SageMoore in https://github.com/vllm-project/vllm/pull/23693
* [Bugfix] Fix unable to run encoder model when disable_hybrid_kv_cache_manager is true by @lianyiibo in https://github.com/vllm-project/vllm/pull/24571
* [Misc] Add removed encoder-decoder models to previously supported models list by @Isotr0py in https://github.com/vllm-project/vllm/pull/24961
* Directly get max encoder len from VLLM config in V1 by @Sugar-zsg in https://github.com/vllm-project/vllm/pull/24866
* [gpt-oss][1b] streaming add item id, content id by @qandrew in https://github.com/vllm-project/vllm/pull/24788
* [MISC] Add code owners of vllm/v1 to vllm/v1/core by @heheda12345 in https://github.com/vllm-project/vllm/pull/24928
* [ROCm] Add dependencies for ROCm by @Concurrensee in https://github.com/vllm-project/vllm/pull/24900
* [gpt-oss][1][bugfix] fix streaming final output by @qandrew in https://github.com/vllm-project/vllm/pull/24466
* Use kwargs for long lists of `EngineCoreRequest` arguments in tests and fix extra kwargs by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/24987
* fp8 kv cache support fix for torch.compile by @maleksan85 in https://github.com/vllm-project/vllm/pull/22758
* [Perf] Reuse workspace for FP8+FP4 Marlin MoE by @mgoin in https://github.com/vllm-project/vllm/pull/20500
* [CI][Bugfix] Fix failing Blackwell test by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/24993
* [CI] GPT-OSS GPQA eval test for Blackwell by @mgoin in https://github.com/vllm-project/vllm/pull/24920
* [FP8] Extend per-token-group quantization support to QuantFP8 by @tahsintunan in https://github.com/vllm-project/vllm/pull/24342
* Removes source compilation of nixl dependency by @bbartels in https://github.com/vllm-project/vllm/pull/24874
* [Doc] Add --force-overwrite option to generate_cmake_presets.py by @elvischenv in https://github.com/vllm-project/vllm/pull/24375
* [Core] Use `CpuGpuBuffer` for block table tensors by @njhill in https://github.com/vllm-project/vllm/pull/24795
* [Benchmarks] Add MMVU video dataset support and clean up deprecated datasets by @Isotr0py in https://github.com/vllm-project/vllm/pull/24719
* [UX] Enforce valid choices for envs like VLLM_ATTENTION_BACKEND, etc by @mgoin in https://github.com/vllm-project/vllm/pull/24761
* [Docs] fix invalid doc link by @yyzxw in https://github.com/vllm-project/vllm/pull/25017
* [UX] Remove "quantization is not fully optimized yet" log by @mgoin in https://github.com/vllm-project/vllm/pull/25012
* [misc] fix typo in value error by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/24995
* [Core] Get num_encoder_tokens from scheduler config by @russellb in https://github.com/vllm-project/vllm/pull/24989
* [V0 Deprecation] Remove MQLLMEngine by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25019
* [Model] Support Qwen3-VL Model Series by @ywang96 in https://github.com/vllm-project/vllm/pull/24727
* [Rocm] [quantization] Fix quark ptpc moe and add test case by @haoyangli-amd in https://github.com/vllm-project/vllm/pull/24649
* Add more documentation and improve usability of lognormal dist (benchmark_serving_multi_turn) by @pliops-daniels in https://github.com/vllm-project/vllm/pull/23255
* [XPU] Fix xpu model runner call torch.cuda APIs by @jikunshang in https://github.com/vllm-project/vllm/pull/25011
* [EPLB] Support EPLB for Mixtral Model by @rouchenzi in https://github.com/vllm-project/vllm/pull/22842
* [Core][MultiModalHasher] Hash images without converting image mode by @lgeiger in https://github.com/vllm-project/vllm/pull/24969
* [Model] Pass param prefix to LLMHead by @whx-sjtu in https://github.com/vllm-project/vllm/pull/24862
* [Model] Apply SharedFusedMoE to glm4_moe. by @whx-sjtu in https://github.com/vllm-project/vllm/pull/24849
* [Core] Remove tokenizer group in vLLM by @zhuohan123 in https://github.com/vllm-project/vllm/pull/24078
* [Docs] Fix griffe warning in base_static_graph.py by @windsonsea in https://github.com/vllm-project/vllm/pull/25018
* [DP] Create placement groups by ray_device_key by @xinyu-intel in https://github.com/vllm-project/vllm/pull/25026
* [Frontend] Support returning all prompt logprobs by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24956
* [BugFix] enable DOTALL to match multi-line tool_call parameters in extract_tool_call_required_streaming by @shijun-yin in https://github.com/vllm-project/vllm/pull/24668
* [Misc] Avoid use of deprecated `AutoModelForVision2Seq` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25065
* Add RADIO Vision Encoder Support to vLLM by @danielafrimi in https://github.com/vllm-project/vllm/pull/24595
* [Bugfix] Fix Stream usage in CPU model runner and OneDNN kernel check by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/25046
* Apply fixes for CUDA 13 by @Aidyn-A in https://github.com/vllm-project/vllm/pull/24599
* [fix] lora benchmarks pass no_lora_flag_cpu by @dolpm in https://github.com/vllm-project/vllm/pull/23774
* [Bugfix][Qwen3-Next] fixes the varlen issue in qwen3-next's MTP implementation. by @sighingnow in https://github.com/vllm-project/vllm/pull/24957
* [Docs] improve code formatting and comments for eliminate griffe build warning. by @samzong in https://github.com/vllm-project/vllm/pull/25010
* Remove old cutlass mla by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/23961
* [Docs] vllm/benchmarks/datasets.py fix docstring param format. by @samzong in https://github.com/vllm-project/vllm/pull/24970
* [CI Bugfix] Fix failing test_invalid_env by @mgoin in https://github.com/vllm-project/vllm/pull/25078
* [V0 Deprecation] Remove V0 Core tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25082
* cleanup: remove adapter commons  by @simon-mo in https://github.com/vllm-project/vllm/pull/25045
* Remove unused find_cuda_init helper script by @simon-mo in https://github.com/vllm-project/vllm/pull/25044
* [V0 Deprecation] Remove unused output processor util by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25023
* Change log level from info to debug for IOProcessor by @mgoin in https://github.com/vllm-project/vllm/pull/24999
* [CI] Revert back prepare_prompts and check_answers by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25087
* [V0 Deprecation] Remove V0 tests in test_sequence.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25088
* [CI Bugfix] Fix failing test_model_load_with_params tests due to tokenizer refactor by @mgoin in https://github.com/vllm-project/vllm/pull/25086
* [V1] Logits processor docs by @afeldman-nm in https://github.com/vllm-project/vllm/pull/22919
* [Misc] Update owners for KV connector and V1 offloading by @ApostaC in https://github.com/vllm-project/vllm/pull/25041
* [Bugfix] Update import path for bc_linter_include by @mmangkad in https://github.com/vllm-project/vllm/pull/24766
* [BUG] Exclude .pth files when pulling remote files  by @ahao-anyscale in https://github.com/vllm-project/vllm/pull/25092
* [Kernel] Faster pre-processing time for W4A8 by @czhu-cohere in https://github.com/vllm-project/vllm/pull/23972
* [gpt-oss][2] fix types for streaming by @qandrew in https://github.com/vllm-project/vllm/pull/24556
* [Bugfix][B200] Fix `cutlass_mla` hang by @alexm-redhat in https://github.com/vllm-project/vllm/pull/24966
* [ROCm][Bugfix] Aiter mha fp8 fix by @dllehr-amd in https://github.com/vllm-project/vllm/pull/24991
* Disable failing GPT-OSS Eval (Blackwell) for now by @mgoin in https://github.com/vllm-project/vllm/pull/25107
* [Bugfix] Refactor Flashinfer TRTLLM attention kernel selection logic by @elvischenv in https://github.com/vllm-project/vllm/pull/24600
* Add a batched auto tune script by @karan in https://github.com/vllm-project/vllm/pull/25076
* [Bugfix] Fix accuracy issue for silu_mul + nvfp4 quant fusion kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/24833
* [Kernel] Delegate construction of FusedMoEQuantConfig to FusedMoEMethodBase subclasses by @bnellnm in https://github.com/vllm-project/vllm/pull/22537
* [V0 Deprecation] Remove V0 Engine tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25114
* [V0 Deprecation] Remove V0 Tracing & Metrics tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25115
* [V0 Deprecation] Remove misc V0 tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25118
* [V0 Deprecation] Skip PP test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25128
* [Kernels] Enable DeepGEMM by default by @bnellnm in https://github.com/vllm-project/vllm/pull/24462
* [MM Encoder] Apply DP ViT for Qwen3-VL model series by @ywang96 in https://github.com/vllm-project/vllm/pull/24955
* [Docs] Clean up the contributing README by @hmellor in https://github.com/vllm-project/vllm/pull/25099
* [Core][MM] Cleanup `MultiModalCache` by @lgeiger in https://github.com/vllm-project/vllm/pull/25006
* [Bugfix][Qwen3-Next] add prefixes to shared_expert in qwen3-next and mlp in qwen2moe to successfully load ignored params in quantized models by @toncao in https://github.com/vllm-project/vllm/pull/24960
* [Kernels] Overlap shared experts with combine instead of dispatch by @bnellnm in https://github.com/vllm-project/vllm/pull/24254
* [Model] enable data parallel for InternVL vision encoder by @666even666 in https://github.com/vllm-project/vllm/pull/23909
* Mark prompt logprobs as incompatible with prompt embeds at API level by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/25077
* [XPU] Whisper model support on XPU Platform by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/25123
* [EPLB] Add EPLB support for hunyuan_v1 by @666even666 in https://github.com/vllm-project/vllm/pull/23078
* [V0 Deprecation] Remove more V0 tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25117
* [Spec Decode] Efficient padded speculation by @benchislett in https://github.com/vllm-project/vllm/pull/24539
* [benchmark] add peak throughput metrics and plot by @simon-mo in https://github.com/vllm-project/vllm/pull/23867
* [CLI] Use streaming in CLI chat and completion commands by @simon-mo in https://github.com/vllm-project/vllm/pull/23769
* [Kernel] Better inf handling for grouped topk cu by @lumina37 in https://github.com/vllm-project/vllm/pull/24886
* [Docs] Fix API Reference by @hmellor in https://github.com/vllm-project/vllm/pull/25140
* Retrieve `sliding_window` from text config in Gemma3 MM by @hmellor in https://github.com/vllm-project/vllm/pull/25085
* [Bugfix] when use s3 model cannot use default load_format by @lengrongfu in https://github.com/vllm-project/vllm/pull/24435
* [Qwen] Add fp8 checkpoint support for qwen3-next. by @sighingnow in https://github.com/vllm-project/vllm/pull/25079
* Add 'path' option to ImagePrompt data_format by @gfinol in https://github.com/vllm-project/vllm/pull/25081
* [Doc] Fix cross-reference warnings by @punitvara in https://github.com/vllm-project/vllm/pull/25058
* [Chore] Cleanup guided namespace, move to structured outputs config by @aarnphm in https://github.com/vllm-project/vllm/pull/22772
* Fix: Add explicit #include <omp.h> for OpenMP compatibility on certain toolchains  by @ihb2032 in https://github.com/vllm-project/vllm/pull/24951
* silu-v1: Fix EPS not being used during max-reduction by @elvircrn in https://github.com/vllm-project/vllm/pull/25069
* [Frontend] Support setting logprobs to -1 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/25031
* [Model] Improve Pooling Model by @jeejeelee in https://github.com/vllm-project/vllm/pull/25149
* Move `StructuredOutputsConfig` from `config/__init__.py` to `config/structured_outputs.py` by @hmellor in https://github.com/vllm-project/vllm/pull/25153
* [Docs] Fix pooling-params doc references in openai_compatible_server.md by @yankay in https://github.com/vllm-project/vllm/pull/24939
* [Docs] add the parallel sampling usage in LLMEngine and AsyncLLM by @gigit0000 in https://github.com/vllm-project/vllm/pull/24222
* Fix forward reference warning in documentation by @hmellor in https://github.com/vllm-project/vllm/pull/25150
* Fix `validate-config` pre-commit check by @hmellor in https://github.com/vllm-project/vllm/pull/25157
* [Bugfix][Mamba] - Fix Conv State Kernel FP32 Support by @Josephasafg in https://github.com/vllm-project/vllm/pull/24883
* [Misc] Clean up flags in `vllm bench serve` by @ywang96 in https://github.com/vllm-project/vllm/pull/25138
* [Structured Output][Refactor] Move `apply_grammar_bitmask()` method from `ModelRunner` to structured output utils by @shen-shanshan in https://github.com/vllm-project/vllm/pull/21999
* Refactor dense FP8 tensor/channel/block utils and add CT FP8 block by @mgoin in https://github.com/vllm-project/vllm/pull/21404
* [Misc] Add kv-connector label by @NickLucche in https://github.com/vllm-project/vllm/pull/25156
* [Kernel] Enable Hybrid Model Support in Triton Unified Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/21197
* [PERF] Add `conv1d` metadata to GDN attn by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/25105
* feat(api): Return 503 on /health when engine is dead by @dongbo910220 in https://github.com/vllm-project/vllm/pull/24897
* [New Model] Support BertForTokenClassification / Named Entity Recognition (NER) task by @noooop in https://github.com/vllm-project/vllm/pull/24872
* [Docs] Fix warnings in mkdocs build (continued) by @Zerohertz in https://github.com/vllm-project/vllm/pull/25163
* Enable Allgather/ReduceScatter backend for NaiveAllToAll by @wenscarl in https://github.com/vllm-project/vllm/pull/23964
* [Misc] Add codeowner for Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/25180
* [spec decode] Fix MTP inference path for MiMo-7B model by @zixi-qi in https://github.com/vllm-project/vllm/pull/25136
* [ROCm][CI/Build] Use ROCm7.0 as the base by @gshtras in https://github.com/vllm-project/vllm/pull/25178
* [ROCm][AITER][Bugfix] Switch AITER to use PIECEWISE_AND_FULL compilation by @Rohan138 in https://github.com/vllm-project/vllm/pull/25104
* [KV offload][1/N] Introduce an offloading component by @orozery in https://github.com/vllm-project/vllm/pull/19848
* [V0 Deprecation] Remove AsyncLLMEngine by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25025
* [fix]: remove data type hardcoding from gptoss model implementation by @nikhil-arm in https://github.com/vllm-project/vllm/pull/23807
* [feat]: Create interface for model-specific M-RoPE by @AzizCode92 in https://github.com/vllm-project/vllm/pull/24194
* [Bug] Fix `returned_lse` not Defined issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/25106
* [Bug] Fix torch Compilation Cache Hit Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/25093
* [V0 Deprecation] Remove unused async_timeout.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25190
* [KV offload][1b/N] rename offloading to kv_offload by @orozery in https://github.com/vllm-project/vllm/pull/25191
* [BugFix] Fix DeepGEMM warmup, no m.weight_scale_inv by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25206
* [CORE] Prompt Embeddings Support for v1 Engine by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/24278
* [KV offload][2/N] Introduce LRU-based CPU offloading management by @orozery in https://github.com/vllm-project/vllm/pull/20075
* [gpt-oss] Add ResponseReasoningPartAddedEvent, ResponseReasoningPartDoneEvent for streaming by @qandrew in https://github.com/vllm-project/vllm/pull/24938
* [Perf] Optimize memory peak during EAGLE model loading. by @candyzone in https://github.com/vllm-project/vllm/pull/24585
* [Misc] Clean up MM profiling warnings by @ywang96 in https://github.com/vllm-project/vllm/pull/25222
* [Docs] Fix griffe warnings in vllm/multimodal by @windsonsea in https://github.com/vllm-project/vllm/pull/25216
* [OOT] Support sync_model_loading for OOT by @xuechendi in https://github.com/vllm-project/vllm/pull/25126
* [Build] Update Xgrammar to 0.1.24 to get a CVE fix by @russellb in https://github.com/vllm-project/vllm/pull/25188
* [CPU] Disable oneDNN linear on non-x86 platforms by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/25166
* [Bugfix][CPU] Add placeholder to avoid import errors when using fused_moe ops on platforms without triton by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/25137
* [Misc] Cleanup test conftest for deprecated encoder-decoder models by @Isotr0py in https://github.com/vllm-project/vllm/pull/25231
* [bugfix] fix MHA for models like OpenGVLab/InternVL3_5-38B by @yma11 in https://github.com/vllm-project/vllm/pull/25146
* [Kernel][Performance] Add Triton kernel for Qwen3-VL interleaved MRoPE by @Isotr0py in https://github.com/vllm-project/vllm/pull/25055
* [Bugfix][Perf] Misc fixes for Qwen3 VL by @ywang96 in https://github.com/vllm-project/vllm/pull/25238
* Move `PoolerConfig` from `config/__init__.py` to `config/pooler.py` by @hmellor in https://github.com/vllm-project/vllm/pull/25181
* [P/D][Nixl] Introduce `KVTransferMetrics` and aggregation strategy by @NickLucche in https://github.com/vllm-project/vllm/pull/22188
* [V0 Deprecation] Remove V0 logic from `get_input_embeddings` interface by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25242
* [Qwen] Remove cuda hard-code in qwen3 next by @wxsIcey in https://github.com/vllm-project/vllm/pull/25243
* Update CODEOWNERS by @hmellor in https://github.com/vllm-project/vllm/pull/25269
* Move `ModelConfig` from `config/__init__.py` to `config/model.py` by @hmellor in https://github.com/vllm-project/vllm/pull/25252
* refactor(benchmarks): add type annotations to wait_for_endpoint parameters by @samzong in https://github.com/vllm-project/vllm/pull/25218
* [KV offload][3/N] Add worker-side CPU support by @orozery in https://github.com/vllm-project/vllm/pull/21448
* [Frontend] Pass API server count to each process by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23717
* [Core] Modify the initialization parameters of the lora manager by @jeejeelee in https://github.com/vllm-project/vllm/pull/25249
* Remove Redundant Assignment in Qwen3_VisionPatchMerger by @LJH-LBJ in https://github.com/vllm-project/vllm/pull/25224
* Encoder model support for the Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/25174
* [CI/Build] fix test function_calling by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/25072
* [Core][Prefix Hash] Fix prefix hash metrics sliding window maintainance by @Jialin in https://github.com/vllm-project/vllm/pull/24990
* [Docs] add __init__.py to vllm/model_executor/layers/quantization/compressed_tensors/transform by @samzong in https://github.com/vllm-project/vllm/pull/24974
* [bugfix] fix structured outputs key missing issue from #24929 by @luccafong in https://github.com/vllm-project/vllm/pull/25195
* [KV offload][4/N] Offloading KV connector by @orozery in https://github.com/vllm-project/vllm/pull/22595
* Optimize triton unified attention performance for sliding window attention by @zixi-qi in https://github.com/vllm-project/vllm/pull/24390
* [Bugfix] GPT OSS Attritbute error on H100 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/25228
* [Bugfix] Fix chunked a2_scales in modular kernels by @bnellnm in https://github.com/vllm-project/vllm/pull/25264
* Specify platform in `pip-compile` `pre-commit` hook so it runs on MacOS by @hmellor in https://github.com/vllm-project/vllm/pull/25273
* [Perf] Use FlashInfer RoPE for RotaryEmbedding.forward_cuda when available by @mgoin in https://github.com/vllm-project/vllm/pull/21126
* [BugFix] Make FlashInferMetadataBuilder non-blocking by @nvjullin in https://github.com/vllm-project/vllm/pull/25040
* Fix: Correct FusedMoE layer reference in auto_round quantization by @David-Wen2025 in https://github.com/vllm-project/vllm/pull/24818
* [Frontend] Responses API messages out, just harmony for now by @alecsolder in https://github.com/vllm-project/vllm/pull/24985
* [Compile] Fix Compile Warning for Ignoring `MIN_BLOCK_PER_SM` by @yewentao256 in https://github.com/vllm-project/vllm/pull/25193
* Enable modelopt gemma3 nvfp4/fp8, make workflow more robust by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/22771
* allow disable flashinfer prefill by @luccafong in https://github.com/vllm-project/vllm/pull/25276
* [BugFix] Fix async scheduling CPU tensor race take 2 by @njhill in https://github.com/vllm-project/vllm/pull/25279
* [Bugfix] Remove VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE #2969 by @Lucaskabela in https://github.com/vllm-project/vllm/pull/25090
* Don't skip special tokens with hermes-style tool calling by @maxdebayser in https://github.com/vllm-project/vllm/pull/25281
* test: Remove vestigial skip for prompt embeds tests after landing v1 Prompt Embeds support by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/25291
* [docs] Prompt Embedding feature support by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/25288
* [torch.compile] CUDAGraph Inductor partition integration by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/24281
* [BugFix] Ensure appropriate guards in destructors by @njhill in https://github.com/vllm-project/vllm/pull/25284
* [Misc] Support more collective_rpc return types by @njhill in https://github.com/vllm-project/vllm/pull/25294
* Improve weight loading for encoder models in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/25289
* [BUGFIX] GPTQ quantization compatibility for Qwen3 Next MOE models (AutoGPTQ and AutoRound-GPTQ) by @JartX in https://github.com/vllm-project/vllm/pull/25268
* [BugFix] Exclude self when checking for port collision by @njhill in https://github.com/vllm-project/vllm/pull/25286
* [BUG FIX][NON-CUDA]quick fix to avoid call cudagraph_unsafe in attention by @xuechendi in https://github.com/vllm-project/vllm/pull/25298
* [Bugfix] fix tool call arguments is empty by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/25223
* [Optimization] Avoid repeated model architecture conversion for pooling models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25261
* [Hybrid Allocator] Support full attention with different hidden size  by @heheda12345 in https://github.com/vllm-project/vllm/pull/25101
* [Bugfix] Fix Qwen3-VL-MoE weight loading for EP by @ywang96 in https://github.com/vllm-project/vllm/pull/25300
* [V1] Support `LLM.apply_model` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18465
* [CI Failure] Disable FlashInfer RoPE to unblock CI by @mgoin in https://github.com/vllm-project/vllm/pull/25299
* [Docs] Fix warnings in mkdocs build (continued)  by @wwl2755 in https://github.com/vllm-project/vllm/pull/25042
* Generate _ModelInfo properties file when loading to improve loading speed by @manoelmarques in https://github.com/vllm-project/vllm/pull/23558
* [Model] Cleanup InternViT's data parallel implementation  by @Isotr0py in https://github.com/vllm-project/vllm/pull/25306
* [Core] Enable sharded state loader for V1 engine and enhance test coverage by @lirong-lirong in https://github.com/vllm-project/vllm/pull/25308
* [V0 Deprecation] Enable the remaining multimodal tests in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25307
* [Docs] Fix warnings in vllm/profiler and vllm/transformers_utils by @windsonsea in https://github.com/vllm-project/vllm/pull/25220
* [V0 Deprecation] Remove LLMEngine by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25033
* [V0 Deprecation] Remove V0 Output Processor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25320
* [Chore] Remove unused sampler in models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25324
* [CI] Skip tests failing on main by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25326
* [V0 Deprecation] Remove V0 core by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25321
* [Doc] improve test-pipeline.yaml documentation by @hl475 in https://github.com/vllm-project/vllm/pull/25305
* [V0 Deprecation] Remove V0 model runner base & simplify worker base by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25328
* [Multi Modal][Performance] Fused Q,K's apply_rope in more models by @wwl2755 in https://github.com/vllm-project/vllm/pull/25005
* [V0 Deprecation] Remove from_seq_group methods by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25330
* [V0 Deprecation] Remove V0 MP executor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25329
* [V1] Add sliding window support to Flex Attention backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/24089
* [MM][Perf] Minor Optimization on Qwen3-VL `fast_pos_embed_interpolate` by @ywang96 in https://github.com/vllm-project/vllm/pull/25337
* [Bugfix] Typos in error message for missing model config file by @simondanielsson in https://github.com/vllm-project/vllm/pull/25339
* [Optimization] Cache chat template result when processor fails to be loaded by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25341
* [V0 Deprecation] Remove V0 Sequence class & Sampler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25332
* [V0 Deprecation] Remove async_output_proc, preemption mode, delay factor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25334
* feat: Enable engine-level arguments with speculators models by @rahul-tuli in https://github.com/vllm-project/vllm/pull/25250
* [V0 Deprecation] Remove V0 sampling metadata by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25345
* [Perf] Further optimization for Qwen3-VL `fast_pos_embed_interpolate` by @Isotr0py in https://github.com/vllm-project/vllm/pull/25347
* Remove V0 attention backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25351
* [Bugfix][V0 Deprecation][CI] use async mock and await for async method by @KKSK-DON in https://github.com/vllm-project/vllm/pull/25325
* Multimodal - audio tests by @debroy-rh in https://github.com/vllm-project/vllm/pull/25285
* [Model] Support Dots OCR by @ywang96 in https://github.com/vllm-project/vllm/pull/24645
* [Docs] GSM8K Accuracy Evaluation doc update by @david6666666 in https://github.com/vllm-project/vllm/pull/25360
* [Bugfix] Fix hermes tool parser handling of non-string argument types by @david6666666 in https://github.com/vllm-project/vllm/pull/22002
* [V0 Deprecation] Remove V0-only methods in multi-modal registry by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25362
* [V0 Deprecation] Remove `MultiModalPlaceholderMap` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25366
* Enable Eagle3 speculative decoding for GPT-OSS model by @eldarkurtic in https://github.com/vllm-project/vllm/pull/25246
* [TPU][Bugfix][CI] Fix broken tests/build dependency by @NickLucche in https://github.com/vllm-project/vllm/pull/25255
* [TPU] Deprecate `xm.mark_step` in favor of ``torch_xla.sync`  by @NickLucche in https://github.com/vllm-project/vllm/pull/25254
* refactor: abstract graph mode support into platform interface by @yiz-liu in https://github.com/vllm-project/vllm/pull/25161
* [Misc] Remove unused encoder-decoder error strings by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25374
* Make pickle import check fast by @hmellor in https://github.com/vllm-project/vllm/pull/25379
* Make `mypy` behave like a proper pre-commit hook by @hmellor in https://github.com/vllm-project/vllm/pull/25313
* MI-300X triton moe configs by @Sara-KS in https://github.com/vllm-project/vllm/pull/23445
* [Bugfix] Fix several issues with p2p xPyD in GET type by @Csrayz in https://github.com/vllm-project/vllm/pull/23993
* [V1][Attention] Split triton_attn in triton-only and rocm specific backends  by @bringlein in https://github.com/vllm-project/vllm/pull/24648
* [EPLB] Reduce EPLB Inference Overhead by @abmfy in https://github.com/vllm-project/vllm/pull/24573
* [CLI env var] Add VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH in env variables by @Daisy-Ma-coder in https://github.com/vllm-project/vllm/pull/25274
* [Compiler] Disable Inductor standalone compile by default by @ElizaWszola in https://github.com/vllm-project/vllm/pull/25391
* [CI Failure] Fix fp8 kv cache on <SM90 by @mgoin in https://github.com/vllm-project/vllm/pull/25396
* [DP] support torchrun external launcher with Data Parallelism by @luccafong in https://github.com/vllm-project/vllm/pull/24899
* Remove RFC review hours reference by @simon-mo in https://github.com/vllm-project/vllm/pull/25416
* [torch.compile] Cleanup compilation tests and custom passes, add debug utils, fix DCE bug (#23091), fix test (#24376), and prep for custom op matching (#24604) by @ProExpertProg in https://github.com/vllm-project/vllm/pull/24542
* [KV offload][5/N] Add `CPUOffloadingSpec` by @orozery in https://github.com/vllm-project/vllm/pull/24251
* [CI/Build] Skip Qwen3-VL initialization tests until models are actually released by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25394
* [TPU] update torch_xla dependency for PyPI compatibility by @jcyang43 in https://github.com/vllm-project/vllm/pull/25278
* [Frontend] Responses API MCP tools for built in tools and to pass through headers by @alecsolder in https://github.com/vllm-project/vllm/pull/24628
* [Bugfix] fix custom op test by @ProExpertProg in https://github.com/vllm-project/vllm/pull/25429
* [Core] Drop overly aggressive whisper assertion by @russellb in https://github.com/vllm-project/vllm/pull/25408
* [Bugfix] Fix missing `clear_connector_metadata` by @NickLucche in https://github.com/vllm-project/vllm/pull/25397
* [BugFix] [DP/EP] Fix slow execution when BS <= DP by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/25407
* [Performance] Remove input pads in cutlass_mla and optimize v_proj output handling by @alexm-redhat in https://github.com/vllm-project/vllm/pull/25184
* [Perf] Apply torch.compile for `per_block_cast_to_fp8` by @yewentao256 in https://github.com/vllm-project/vllm/pull/24611
* [V0 deprecation] Remove platform v1 controling interface by @Isotr0py in https://github.com/vllm-project/vllm/pull/25410
* [V0 deprecation] Remove `_set_default_args_v0` function by @Isotr0py in https://github.com/vllm-project/vllm/pull/25409
* [Bug] Fix Long Context OOM Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/25290
* [feat] Support MRoPE +  YaRN by @JJJYmmm in https://github.com/vllm-project/vllm/pull/25384
* [XPU] Fix `compile_size` is `None` case. by @jikunshang in https://github.com/vllm-project/vllm/pull/25433
* [benchmarks]allow skip ready check for bench serve by @luccafong in https://github.com/vllm-project/vllm/pull/25420
* [Bugfix] Remove contiguous output req for context parallel MLA by @mgoin in https://github.com/vllm-project/vllm/pull/25414
* [Docs] Fix griffe warnings in vllm/lora/ops by @windsonsea in https://github.com/vllm-project/vllm/pull/25369
* [DP/EP][GPTOSS] Use triton matmul-ogs kernels for GPTOSS DP/EP by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/24588
* [NIXL][OOT platform] support nixl_connector with oot platform and other nixl_backend by @xuechendi in https://github.com/vllm-project/vllm/pull/25121
* [Model] Enable DP for ViT in Qwen2-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25445
* Handle triton kernel import exception by @minosfuture in https://github.com/vllm-project/vllm/pull/25319
* [Frontend] Add a new xml-based tool parser for qwen3-coder by @Zhikaiiii in https://github.com/vllm-project/vllm/pull/25028
* [Misc] Move DP for ViT code inside model executor dir by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25459
* [Test]: Hermes tool parser stream output error in Qwen3 case by @ahartel in https://github.com/vllm-project/vllm/pull/25203
* [Bugfix] Fix idefics3 `tie_word_embeddings` by @Isotr0py in https://github.com/vllm-project/vllm/pull/25454
* [Core] Optimize LoRA weight loading by @jeejeelee in https://github.com/vllm-project/vllm/pull/25403
* [docs] Benchmark Serving Incorrect Arg by @vllmellm in https://github.com/vllm-project/vllm/pull/25474
* [CI/Build] Fix disabled v1 attention backend selection test by @Isotr0py in https://github.com/vllm-project/vllm/pull/25471
* [BugFix] Register expert_map as named buffer for wake_up and sleep by @wuxibin89 in https://github.com/vllm-project/vllm/pull/25458
* [P/D] Support NIXL connector to disconnect during a clean shutdown by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24423
* [test/doc] make NixlConnector example more clear by @panpan0000 in https://github.com/vllm-project/vllm/pull/24249
* [XPU] Fix MOE DP accuracy issue on XPU by @faaany in https://github.com/vllm-project/vllm/pull/25465
* [UX] Change kv-cache-memory log level to debug by @mgoin in https://github.com/vllm-project/vllm/pull/25479
* [V1] Remove V0 code paths for Hybrid models by @tdoublep in https://github.com/vllm-project/vllm/pull/25400
* [Core/DBO][2/N] Dual-Batch Overlap add DeepEP High Throughput support and Prefill support by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/24845
* Add backward compatibility for `GuidedDecodingParams` by @hmellor in https://github.com/vllm-project/vllm/pull/25422
* [Kernels] Support blocked fp8 quantization for compressed tensors MoE by @bnellnm in https://github.com/vllm-project/vllm/pull/25219
* [BugFix] Fix UB in per_token_group_quant.cu by @rivos-shreeasish in https://github.com/vllm-project/vllm/pull/24913
* [Log] Optimize kv cache memory log from Bytes to GiB by @yewentao256 in https://github.com/vllm-project/vllm/pull/25204
* Use macro guard CUDA functions for back compatibility in grouped_topk_kernel.cu by @minosfuture in https://github.com/vllm-project/vllm/pull/25346
* [V1][Kernel] Add triton implementation for `reshape_and_cache_flash` by @bringlein in https://github.com/vllm-project/vllm/pull/24503
* [Misc] Reduce initialization time of auto_tune by @wdhongtw in https://github.com/vllm-project/vllm/pull/23682
* [Spec Decode][CI] Add e2e test for `examples/spec_decode.py` and prevent breaking Acceptance Length by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/24531
* [Core] Ensure LoRA linear respect the base_layer's tp_size and tp_rank by @jeejeelee in https://github.com/vllm-project/vllm/pull/25487
* [ROCm] Add skinny gemm bias support for dtypes fp16,bf16,fp8 by @amd-hhashemi in https://github.com/vllm-project/vllm/pull/24988
* [core] add nccl symmetric memory for all reduce by @Amir-19 in https://github.com/vllm-project/vllm/pull/24532
* [Performance] Move apply_w8a8_block_fp8_linear to an op class by @ElizaWszola in https://github.com/vllm-project/vllm/pull/24666
* [Perf] Change default CUDAGraphMode from PIECEWISE to FULL_AND_PIECEWISE by @mgoin in https://github.com/vllm-project/vllm/pull/25444
* [Speculators][Speculative Decoding] Fix gpt-oss eagle3 accuracy issue by @jiahanc in https://github.com/vllm-project/vllm/pull/25406
* [Bugfix] Lower gpt-oss max cudagraph size to 992 to be compatible with FA3 by @mgoin in https://github.com/vllm-project/vllm/pull/25508
* Enable symmetric memory all reduce by default only enabling for TP by @ilmarkov in https://github.com/vllm-project/vllm/pull/25070
* [CI] Fix Pre-commit Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/25497
* [Bugfix] gpt-oss container tool output bug by @alecsolder in https://github.com/vllm-project/vllm/pull/25485
* [Build] Update Xgrammar to 0.1.25 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/25467
* [Bugfix] Fix for the import error from #24588 by @gshtras in https://github.com/vllm-project/vllm/pull/25481
* [CI/Build] Fix and re-enable v1 PP test on CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/25496
* [Core] Use KVCacheBlock as much as possible instead of dict[block_id, KVCacheBlock] by @Jialin in https://github.com/vllm-project/vllm/pull/24830
* [V0 Deprecation] Remove placeholder attn by @tdoublep in https://github.com/vllm-project/vllm/pull/25510
* Add VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE & VLLM_ENABLE_INDUCTOR_COORDINAâ€¦ by @rouchenzi in https://github.com/vllm-project/vllm/pull/25493
* Fix triton_reshape_and_cache_flash.py triton import by @mgoin in https://github.com/vllm-project/vllm/pull/25522
* [gpt-oss][bugfix] remove logic to require resp_ in ResponseAPI by @qandrew in https://github.com/vllm-project/vllm/pull/25428
* Remove redundant mutates_args and dispatch_key for direct_register_custom_op by @mgoin in https://github.com/vllm-project/vllm/pull/25512
* [BugFix] Fix OOM in vLLM replicas by ensuring consistent NCCL memory accounting by @kouroshHakha in https://github.com/vllm-project/vllm/pull/25359
* Add `VLLM_NVTX_SCOPES_FOR_PROFILING=1` to enable `nvtx.annotate` scopes by @coreylowman in https://github.com/vllm-project/vllm/pull/25501
* [Kernel] [Mamba] Remove BLOCK_H=1 from list of tuneable configurations for `_chunk_cumsum_fwd_kernel` by @tdoublep in https://github.com/vllm-project/vllm/pull/25197
* [ROCm] Small functional changes for gptoss by @jpvillam-amd in https://github.com/vllm-project/vllm/pull/25201
* [Perf] Increase default max splits for FA3 full cudagraphs by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25495
* [Bugfix] [B200] cutlass_mla - ensure kv_split == 1 for batch size > 1 by @alexm-redhat in https://github.com/vllm-project/vllm/pull/25509
* [BugFix] AssertionError: Do not capture num_reqs > max_num_reqs for uniform batch by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25505
* Improve output when failing json.loads() on structured output test by @dougbtv in https://github.com/vllm-project/vllm/pull/25483
* Add CUTLASS FP8 MOE benchmark scripts and kernel config by @chenxi-yang in https://github.com/vllm-project/vllm/pull/25302
* [Bug] Fix AttributeError: 'FusedMoE' object has no attribute 'w13_weight_scale'. Did you mean: 'w13_weight_scale_inv' by @yewentao256 in https://github.com/vllm-project/vllm/pull/25519
* [BUG] Allows for RunAI Streamer and Torch.compile cache to be used together by @ahao-anyscale in https://github.com/vllm-project/vllm/pull/24922
* [Model] Support SeedOss Reason Parser by @LuYanFCP in https://github.com/vllm-project/vllm/pull/24263
* [V1][Metrics] Add per-request TPOT histogram by @baxingpiaochong in https://github.com/vllm-project/vllm/pull/24015
* [Bugfix] Use a separate FlashInfer workspace buffer for trtllm-gen by @benchislett in https://github.com/vllm-project/vllm/pull/25520
* [Core] Support weight_loader_v2 for `UnquantizedLinearMethod` by @kylesayrs in https://github.com/vllm-project/vllm/pull/23036
* [Compile] Fix AMD Compile Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/25518
* [BugFix] Fix MLA assert with CUTLASS MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25478
* [fix]: add Arm 4bit fused moe support by @nikhil-arm in https://github.com/vllm-project/vllm/pull/23809
* [KV sharing] Re-land Gemma3n model changes from #22628 by @sarckk in https://github.com/vllm-project/vllm/pull/24357
* [Spec Decode] Enable FlashInfer Spec Decoding by @benchislett in https://github.com/vllm-project/vllm/pull/25196
* [Perf] Fix jit compiles at runtime of fla gated delta rule by @coreylowman in https://github.com/vllm-project/vllm/pull/25432
* [Bugfix] [Frontend] Cleanup gpt-oss non-streaming chat tool calls by @bbrowning in https://github.com/vllm-project/vllm/pull/25514
* [TPU][Bugfix] fix the missing apply_model in tpu worker by @yaochengji in https://github.com/vllm-project/vllm/pull/25526
* [Misc] Retry HF processing if "Already borrowed" error occurs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25535
* [Bugfix][CPU] Skip unsupported custom op register on CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/25534
* [CI/Build] Fix v1 OOT registration test by @Isotr0py in https://github.com/vllm-project/vllm/pull/25547
* [Misc]] Move processing context to multimodal directory by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25548
* [CI/Build] add nightly prime-rl integration tests by @Jackmin801 in https://github.com/vllm-project/vllm/pull/25207
* [V0 Deprecation] Remove max_seq_len_to_capture by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25543
* [BugFix] Potential Fix for FA3 full-cudagraph IMA  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25490
* [misc] update the warning message by @youkaichao in https://github.com/vllm-project/vllm/pull/25566
* [Bugfix] Fix dummy video number of frames calculation by @ywang96 in https://github.com/vllm-project/vllm/pull/25553
* [Bug] fix import and unit test by @jmkuebler in https://github.com/vllm-project/vllm/pull/25558
* [Benchmark] Fix regression in structured output benchmark by @russellb in https://github.com/vllm-project/vllm/pull/25500
* [docs] fix nixl kv_connector_extra_config.backends key by @panpan0000 in https://github.com/vllm-project/vllm/pull/25565
* [Bugfix] Fix DeepSeekV31ToolParser to correctly parse multiple tools in non-streaming output by @taohui in https://github.com/vllm-project/vllm/pull/25405
* Move `DeviceConfig`, `ObservabilityConfig`, `SpeechToTextConfig` to their own files by @hmellor in https://github.com/vllm-project/vllm/pull/25564
* [Misc] Improve type annotations for jsontree by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25577
* [ROCm][Bugfix] Only enable +rms_norm based on aiter if not explicitly disabled by @gshtras in https://github.com/vllm-project/vllm/pull/25275
* [ROCm][Build][Bugfix] Fix ROCm base docker whls installation order by @gshtras in https://github.com/vllm-project/vllm/pull/25415
* Fixes and updates to bench_per_token_quant_fp8 by @mgoin in https://github.com/vllm-project/vllm/pull/25591
* [Bugfix] add cache model when from object storage get model by @lengrongfu in https://github.com/vllm-project/vllm/pull/24764
* Support mnnvl all2allv from Flashinfer by @wenscarl in https://github.com/vllm-project/vllm/pull/21003
* Suppress benign cuBLAS warning when capturing cudagraphs with DBO by @SageMoore in https://github.com/vllm-project/vllm/pull/25596
* [Docs] Enable `fail_on_warning` for the docs build in CI by @hmellor in https://github.com/vllm-project/vllm/pull/25580
* [V0 Deprecation] Remove unused classes in attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/25541
* [Logging] Improve log for when DeepEP HT disables CUDA Graphs by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/25531
* feat: BF16 FlashInfer Fused Cutlass MOE for Hopper and Blackwell Expert Parallel by @djmmoss in https://github.com/vllm-project/vllm/pull/25503
* [Refactor] Use DeepGEMM Col Major TMA Aligned Tensor by @yewentao256 in https://github.com/vllm-project/vllm/pull/25517
* Improve `--help` for enhanced user experience by @hmellor in https://github.com/vllm-project/vllm/pull/24903
* [MISC] replace c10::optional with std::optional by @842974287 in https://github.com/vllm-project/vllm/pull/25602
* [Model] Improve DotsOCRForCausalLM by @jeejeelee in https://github.com/vllm-project/vllm/pull/25466
* [Kernel] Support DCP for Triton backend  by @frank-wei in https://github.com/vllm-project/vllm/pull/25132
* [Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super() by @yewentao256 in https://github.com/vllm-project/vllm/pull/25613
* Enable Fbgemm NVFP4 on Dense models by @samanamp in https://github.com/vllm-project/vllm/pull/25609
* [Model] Add LongCat-Flash  by @OftenDream in https://github.com/vllm-project/vllm/pull/23991
* optimize: eliminate duplicate split_enc_dec_inputs calls by @nicole-lihui in https://github.com/vllm-project/vllm/pull/25573
* [Bugfix] fix apply_temperature to avoid nan in probs by @courage17340 in https://github.com/vllm-project/vllm/pull/24734
* [Misc] Simplify PoolerOutput and move to `v1/outputs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25629
* Map CwmForCausalLM to llama and LlamaForCausalLM by @jacobkahn in https://github.com/vllm-project/vllm/pull/25611
* typo: remove duplicate `is` by @nicole-lihui in https://github.com/vllm-project/vllm/pull/25641
* Revert "[Performance] Move apply_w8a8_block_fp8_linear to an op classâ€¦ by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/25607
* [fix] Update torch version in cpu-build.txt for AArch64/ppc64le and Darwin by @fadara01 in https://github.com/vllm-project/vllm/pull/25579
* [Misc] Fix Qwen3-VL `video_grid_thw` typing by @ywang96 in https://github.com/vllm-project/vllm/pull/25646
* [Bugfix] Add triton.language.tensor placeholder by @adobrzyn in https://github.com/vllm-project/vllm/pull/25649
* [Bugfix] Fix Qwen3-VL max_num_video_tokens calculation for video profiling by @Isotr0py in https://github.com/vllm-project/vllm/pull/25648
* [mypy] Further improve MM type annotations by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25654
* [Bugfix] Parse SpeculativeConfig Error by @yyzxw in https://github.com/vllm-project/vllm/pull/25142
* [V0 deprecation] Remove unreachable model_config.supported_tasks by @noooop in https://github.com/vllm-project/vllm/pull/25642
* Add backward compatibility for `guided_...` API by @hmellor in https://github.com/vllm-project/vllm/pull/25615
* [CI/Build] Fix flaky entrypoints test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25663
* [XPU][Triton]add xpu config in triton_reshape_and_cache_flash by @jikunshang in https://github.com/vllm-project/vllm/pull/25643
* [Hardware][RISC-V] Add riscv64 support for vLLM with scalar by @langc23 in https://github.com/vllm-project/vllm/pull/22112
* [mypy] Fix wrong type annotations related to tuple by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25660
* [misc] warning by default for hanging / busy / idle by @youkaichao in https://github.com/vllm-project/vllm/pull/25627
* [torch.compile] Make Query Quantization Fusable by @jmkuebler in https://github.com/vllm-project/vllm/pull/24914
* [CPU] update torch 2.8 and fix missing fields in TorchSDPAMetadata by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/25652
* [ux] Switch a warning to debug about a pytorch fallback by @russellb in https://github.com/vllm-project/vllm/pull/23750
* [Bugfix] Fix InternS1 video processing after Transformers v4.56 by @Isotr0py in https://github.com/vllm-project/vllm/pull/25644
* [Misc] Remove cruft file in repo by @NickLucche in https://github.com/vllm-project/vllm/pull/25678
* [Logging] Remove TORCH_NCCL_AVOID_RECORD_STREAMS to squash a warning by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/25532
* [BUGFIX] Fix crash in Eagle Speculative Decoding models when exceedinâ€¦ by @AlonKejzman in https://github.com/vllm-project/vllm/pull/24662
* Revert "[Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super()" by @mgoin in https://github.com/vllm-project/vllm/pull/25681
* [BugFix] Fix DBO hang by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25625
* [Model] Add optional parameter to reasoning parser constructor by @taohui in https://github.com/vllm-project/vllm/pull/25554
* [Model] Define `merge_by_field_config` MM interface by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25676
* [V0 deprecation] Clean up V0 fallback in compilation config by @Isotr0py in https://github.com/vllm-project/vllm/pull/25675
* [V0 deprecation] Remove _VLLM_V1 suffixes from attention backend names by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/25489
* [V0 deprecation] Clean up LoRA  by @jeejeelee in https://github.com/vllm-project/vllm/pull/25686
* [Misc] Simplify `test_argsort_mm_positions` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25690
* [Optimization] Streamline `InputPreprocessor` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25702
* [Optimization] Use a cheaper cache key in `get_model_architecture` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25682
* [Spec Decode] Add Batch Parallel Ngram. Upto 8x lower overhead. by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/24986
* [Core] Enable command line logging for LLMEngine by @zhuohan123 in https://github.com/vllm-project/vllm/pull/25610
* [Model] rename NemotronH_Nano_VL -> NemotronH_Nano_VL_V2 by @tomeras91 in https://github.com/vllm-project/vllm/pull/25708
* Fix routing_bias dtype  by @wenscarl in https://github.com/vllm-project/vllm/pull/25711
* [Refactor] Remove DeepGEMM OP Register by @yewentao256 in https://github.com/vllm-project/vllm/pull/25710
* [Misc] Don't log shm dequeue delay warning on worker side by @njhill in https://github.com/vllm-project/vllm/pull/25720
* Llamas 3.1 405B fp4 changes upstreaming from 355_wip by @maleksan85 in https://github.com/vllm-project/vllm/pull/25135
* [Core] Force PIECEWISE CUDAGraph mode for encoder-decoder by @russellb in https://github.com/vllm-project/vllm/pull/25701
* [Misc] Remove unnecessary memoryviews in shm_broadcast.py by @njhill in https://github.com/vllm-project/vllm/pull/25721
* EVS Support (Video tokens pruning) by @BloodAxe in https://github.com/vllm-project/vllm/pull/22980
* [CI/Build] fix doc build warning: Failed to get 'name: description' pair by @yitingdc in https://github.com/vllm-project/vllm/pull/25733
* fix: revert cast to cpu in `MsgpackEncoder._encode_tensor` to avoid hidden performance regressions by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/25738
* perf: Avoid copying inputs_embeds tensors to GPU unless prompt_embeds is enabled by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/25739
* [Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI300X by @xaguilar-amd in https://github.com/vllm-project/vllm/pull/25703
* fix: print outputt offline_inference/base/chat.py example by @Iceber in https://github.com/vllm-project/vllm/pull/25744
* [Qwen3-Next][GDN] fixes cuda graph capturing bug in GDN metadata and a stride bug in causal_conv_1d. by @sighingnow in https://github.com/vllm-project/vllm/pull/25743
* Remove cuda hard-code in compute_causal_conv1d_metadata by @wxsIcey in https://github.com/vllm-project/vllm/pull/25555
* [misc] refactor speculative config by @yyzxw in https://github.com/vllm-project/vllm/pull/25657
* [Bugfix] Fix Shared Expert/Zero expert code in FusedMoE.process_chunk by @SageMoore in https://github.com/vllm-project/vllm/pull/25698
* Support LongCat-Flash-Chat tool call by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/24083
* [Doc] Update Batch-level DP docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25757
* [Model] Mamba2 varlen and metadata refactor  by @cyang49 in https://github.com/vllm-project/vllm/pull/21467
* [CI] Fix test_shared_storage_connector_hashes by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/25748
* [Bugfix] Properly abort pooling request. by @noooop in https://github.com/vllm-project/vllm/pull/25734
* [CI/Build] Split up Distributed Tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25572
* [CI/Build] Fix some V1 tests not being run by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/25569
* [Quantization] Add field to skip unquantized modules for GPTQ config by @Isotr0py in https://github.com/vllm-project/vllm/pull/25455
* [BugFix] Fix using `dbo_decode_token_threshold` always (and ignoring `dbo_prefill_token_threshold`) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/25622
* [ray][metrics] Replace ':' with '_' for OpenTelemetry compatibility in Ray by @eicherseiji in https://github.com/vllm-project/vllm/pull/25439
* [Fix][torch.compile] fix unique_filepath by @ZJY0516 in https://github.com/vllm-project/vllm/pull/25732
* Eagle3 that supports the Minicpm3 model by @LDLINGLINGLING in https://github.com/vllm-project/vllm/pull/24243
* [Doc]: improve CPU(x86) build-wheel-from-source section by @brokedba in https://github.com/vllm-project/vllm/pull/25617

## New Contributors
* @SamitHuang made their first contribution in https://github.com/vllm-project/vllm/pull/24733
* @rozeappletree made their first contribution in https://github.com/vllm-project/vllm/pull/24785
* @ChenTaoyu-SJTU made their first contribution in https://github.com/vllm-project/vllm/pull/24732
* @ziliangpeng made their first contribution in https://github.com/vllm-project/vllm/pull/24793
* @chenfengjin made their first contribution in https://github.com/vllm-project/vllm/pull/24823
* @LCAIZJ made their first contribution in https://github.com/vllm-project/vllm/pull/23917
* @xiao-llm made their first contribution in https://github.com/vllm-project/vllm/pull/22222
* @koiker made their first contribution in https://github.com/vllm-project/vllm/pull/20321
* @cboss6 made their first contribution in https://github.com/vllm-project/vllm/pull/23745
* @liangwen12year made their first contribution in https://github.com/vllm-project/vllm/pull/23498
* @lianyiibo made their first contribution in https://github.com/vllm-project/vllm/pull/24571
* @tahsintunan made their first contribution in https://github.com/vllm-project/vllm/pull/24342
* @haoyangli-amd made their first contribution in https://github.com/vllm-project/vllm/pull/24649
* @rouchenzi made their first contribution in https://github.com/vllm-project/vllm/pull/22842
* @xinyu-intel made their first contribution in https://github.com/vllm-project/vllm/pull/25026
* @shijun-yin made their first contribution in https://github.com/vllm-project/vllm/pull/24668
* @Aidyn-A made their first contribution in https://github.com/vllm-project/vllm/pull/24599
* @dolpm made their first contribution in https://github.com/vllm-project/vllm/pull/23774
* @samzong made their first contribution in https://github.com/vllm-project/vllm/pull/25010
* @mmangkad made their first contribution in https://github.com/vllm-project/vllm/pull/24766
* @karan made their first contribution in https://github.com/vllm-project/vllm/pull/25076
* @toncao made their first contribution in https://github.com/vllm-project/vllm/pull/24960
* @666even666 made their first contribution in https://github.com/vllm-project/vllm/pull/23909
* @lumina37 made their first contribution in https://github.com/vllm-project/vllm/pull/24886
* @gfinol made their first contribution in https://github.com/vllm-project/vllm/pull/25081
* @punitvara made their first contribution in https://github.com/vllm-project/vllm/pull/25058
* @gigit0000 made their first contribution in https://github.com/vllm-project/vllm/pull/24222
* @Rohan138 made their first contribution in https://github.com/vllm-project/vllm/pull/25104
* @candyzone made their first contribution in https://github.com/vllm-project/vllm/pull/24585
* @wxsIcey made their first contribution in https://github.com/vllm-project/vllm/pull/25243
* @LJH-LBJ made their first contribution in https://github.com/vllm-project/vllm/pull/25224
* @David-Wen2025 made their first contribution in https://github.com/vllm-project/vllm/pull/24818
* @alecsolder made their first contribution in https://github.com/vllm-project/vllm/pull/24985
* @Lucaskabela made their first contribution in https://github.com/vllm-project/vllm/pull/25090
* @manoelmarques made their first contribution in https://github.com/vllm-project/vllm/pull/23558
* @lirong-lirong made their first contribution in https://github.com/vllm-project/vllm/pull/25308
* @debroy-rh made their first contribution in https://github.com/vllm-project/vllm/pull/25285
* @Sara-KS made their first contribution in https://github.com/vllm-project/vllm/pull/23445
* @Daisy-Ma-coder made their first contribution in https://github.com/vllm-project/vllm/pull/25274
* @jcyang43 made their first contribution in https://github.com/vllm-project/vllm/pull/25278
* @Zhikaiiii made their first contribution in https://github.com/vllm-project/vllm/pull/25028
* @ahartel made their first contribution in https://github.com/vllm-project/vllm/pull/25203
* @wuxibin89 made their first contribution in https://github.com/vllm-project/vllm/pull/25458
* @rivos-shreeasish made their first contribution in https://github.com/vllm-project/vllm/pull/24913
* @Amir-19 made their first contribution in https://github.com/vllm-project/vllm/pull/24532
* @LuYanFCP made their first contribution in https://github.com/vllm-project/vllm/pull/24263
* @baxingpiaochong made their first contribution in https://github.com/vllm-project/vllm/pull/24015
* @Jackmin801 made their first contribution in https://github.com/vllm-project/vllm/pull/25207
* @taohui made their first contribution in https://github.com/vllm-project/vllm/pull/25405
* @OftenDream made their first contribution in https://github.com/vllm-project/vllm/pull/23991
* @nicole-lihui made their first contribution in https://github.com/vllm-project/vllm/pull/25573
* @jacobkahn made their first contribution in https://github.com/vllm-project/vllm/pull/25611
* @fadara01 made their first contribution in https://github.com/vllm-project/vllm/pull/25579
* @langc23 made their first contribution in https://github.com/vllm-project/vllm/pull/22112
* @AlonKejzman made their first contribution in https://github.com/vllm-project/vllm/pull/24662
* @BloodAxe made their first contribution in https://github.com/vllm-project/vllm/pull/22980
* @yitingdc made their first contribution in https://github.com/vllm-project/vllm/pull/25733
* @xaguilar-amd made their first contribution in https://github.com/vllm-project/vllm/pull/25703
* @Iceber made their first contribution in https://github.com/vllm-project/vllm/pull/25744
* @LDLINGLINGLING made their first contribution in https://github.com/vllm-project/vllm/pull/24243
* @brokedba made their first contribution in https://github.com/vllm-project/vllm/pull/25617

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.2...v0.11.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.11.0)

---

## v0.10.2: v0.10.2
**Published:** 2025-09-13

## Highlights
This release contains 740 commits from 266 contributors (97 new)!

**Breaking Changes**: This release includes PyTorch 2.8.0 upgrade, V0 deprecations, and API changes - please review the changelog carefully.

**aarch64 support**: This release features native support for aarch64 allowing usage of vLLM on GB200 platform. The docker image `vllm/vllm-openai` should already be multiplatform. To install the wheels, you can download the wheels from this release artifact or install via 
```
uv pip install vllm==0.10.2 --extra-index-url https://wheels.vllm.ai/0.10.2/ --torch-backend=auto
```

### Model Support
* **New model families and enhancements**: Apertus (#23068), LFM2 (#22845), MiDashengLM (#23652), Motif-1-Tiny (#23414), Seed-Oss (#23241), Google EmbeddingGemma-300m (#24318), GTE sequence classification (#23524), Donut OCR model (#23229), KeyeVL-1.5-8B (#23838), R-4B vision model (#23246), Ernie4.5 VL (#22514), MiniCPM-V 4.5 (#23586), Ovis2.5 (#23084), Qwen3-Next with hybrid attention (#24526), InternVL3.5 with video support (#23658), Qwen2Audio embeddings (#23625), NemotronH Nano VLM (#23644), BLOOM V1 engine support (#23488), and Whisper encoder-decoder for V1 (#21088).
* **Pipeline parallelism expansion**: Added PP support for Hunyuan (#24212), Ovis2.5 (#23405), GPT-OSS (#23680), and Kimi-VL-A3B-Thinking-2506 (#23114).
* **Data parallelism for vision models**: Enabled DP for ViT across Qwen2.5VL (#22742), MiniCPM-V (#23948, #23327), Kimi-VL (#23817), and GLM-4.5V (#23168).
* **LoRA ecosystem expansion**: Added LoRA support to Voxtral (#24517), Qwen-2.5-Omni (#24231), and DeepSeek models V2/V3/R1-0528 (#23971), with significantly faster LoRA startup performance (#23777).
* **Classification and pooling enhancements**: Multi-label classification support (#23173), logit bias and sigmoid normalization (#24031), and FP32 precision heads for pooling models (#23810).
* **Performance optimizations**: Removed unnecessary CUDA sync from GLM-4.1V (#24332) and Qwen2VL (#24334) preprocessing, eliminated redundant all-reduce in Qwen3 MoE (#23169), optimized InternVL CPU threading (#24519), and GLM4.5-V video frame decoding (#24161).

### Engine Core
* **V1 engine maturation**: Extended V1 support to compute capability < 8.0 (#23614, #24022), added cross-attention KV cache for encoder-decoder models (#23664), request-level logits processor integration (#23656), and KV events from connectors (#19737).
* **Backend expansion**: Terratorch backend integration (#23513) enabling non-language model tasks like semantic segmentation and geospatial applications with `--model-impl terratorch` support.
* **Hybrid and Mamba model improvements**: Enabled full CUDA graphs by default for hybrid models (#22594), disabled prefix caching for hybrid/Mamba models (#23716), added FP32 SSM kernel support (#23506), full CUDA graph support for Mamba1 (#23035), and V1 as default for Mamba models (#23650).
* **Performance core improvements**: `--safetensors-load-strategy` for NFS based file loading acceleration (#24469), critical CUDA graph capture throughput fix (#24128), scheduler optimization for single completions (#21917), multi-threaded model weight loading (#23928), and tensor core usage enforcement for FlashInfer decode (#23214).
* **Multimodal enhancements**: Multimodal cache tracking with mm_hash (#22711), UUID-based multimodal identifiers (#23394), improved V1 video embedding estimation (#24312), and simplified multimodal UUID handling (#24271).
* **Sampling and structured outputs**: Support for all prompt logprobs (#23868), final logprobs (#22387), grammar bitmask optimization (#23361), and user-configurable KV cache memory size (#21489).
* **Distributed**: Support Decode Context Parallel (DCP) for MLA (#23734)

### Hardware & Performance
* **NVIDIA Blackwell/SM100 generation**: FP8 MLA support with CUTLASS backend (#23289), DeepGEMM Linear with 1.5% E2E throughput improvement (#23351), Hopper DeepGEMM E8M0 for DeepSeekV3.1 (#23666), SM100 FlashInfer CUTLASS MoE FP8 backend (#22357), MXFP4 fused CUTLASS MoE (#23696), default MXFP4 MoE on Blackwell (#23008), and GPT-OSS DP/EP support with 52,003 tokens/s throughput (#23608).
* **Breaking change**: FlashMLA disabled on Blackwell GPUs due to compatibility issues (#24521).
* **Kernel and attention optimizations**: FlashAttention MLA with CUDA graph support (#14258, #23958), V1 cross-attention support (#23297), FP8 support for FlashMLA (#22668), fused grouped TopK for MoE (#23274), Flash Linear Attention kernels (#24518), and W4A8 support on Hopper (#23198).
* **Performance improvements**: 13.7x speedup for token conversion (#20413), TTIT/TTFT improvements for disaggregated serving (#22760), symmetric memory all-reduce by default (#24111), FlashInfer warmup during startup (#23439), V1 model execution overlap (#23569), and various Triton configuration tuning (#23748, #23939).
* **Platform expansion**: Apple Silicon bfloat16 support for M2+ (#24129), IBM Z V1 engine support (#22725), Intel XPU torch.compile (#22609), XPU MoE data parallelism (#22887), XPU Triton attention (#24149), XPU FP8 quantization (#23148), and ROCm pipeline parallelism with Ray (#24275).
* **Model-specific optimizations**: Hardware-tuned MoE configurations for Qwen3-Next on B200/H200/H100 (#24698, #24688, #24699, #24695), GLM-4.5-Air-FP8 B200 configs (#23695), Kimi K2 optimization (#24597), and QWEN3 Coder/Thinking configs (#24266, #24330).

### Quantization
* **New quantization capabilities**: Per-layer quantization routing (#23556), GGUF quantization with layer skipping (#23188), NFP4+FP8 MoE support (#22674), W4A8 channel scales (#23570), and AMD CDNA2/CDNA3 FP4 support (#22527).
* **Advanced quantization infrastructure**: Compressed tensors transforms for linear operations (#22486) enabling techniques like SpinQuantR1R2R4 and QuIP quantization methods.
* **FlashInfer quantization integration**: FP8 KV cache for TRTLLM prefill attention (#24197), FP8-qkv attention kernels (#23647), and FP8 per-tensor GEMMs (#22895).
* **Platform-specific quantization**: ROCm TorchAO quantization enablement (#24400) and TorchAO module swap configuration (#21982).
* **Performance optimizations**: MXFP4 MoE loading cache optimization (#24154) and compressed tensors version updates (#23202).
* **Breaking change**: Removed original Marlin quantization format (#23204).

### API & Frontend
* **OpenAI API enhancements**: Gemma3n audio transcription/translation endpoints (#23735), transcription response usage statistics (#23576), and return_token_ids parameter (#22587).
* **Response API improvements**: Streaming support for non-harmony responses (#23741), non-streaming logprobs (#23319), MCP tool background mode (#23494), MCP streaming+background support (#23927), and tool output token reporting (#24285).
* **Frontend optimizations**: Error stack traces with --log-error-stack (#22960), collective RPC endpoint (#23075), beam search concurrency optimization (#23599), unnecessary detokenization skipping (#24236), and custom media UUIDs (#23449).
* **Configuration enhancements**: Formalized --mm-encoder-tp-mode flag (#23190), VLLM_DISABLE_PAD_FOR_CUDAGRAPH environment variable (#23595), EPLB configuration parameter (#20562), embedding endpoint chat request support (#23931), and LM Format Enforcer V1 integration (#22564).

### Dependencies
* **Major updates**: PyTorch 2.8.0 upgrade (#20358) - breaking change requiring environment updates, FlashInfer v0.3.0 upgrade (#24086), and FlashInfer 0.2.14.post1 maintenance update (#23537).
* **Supporting updates**: XGrammar 0.1.23 (#22988), TPU core dump fix with tpu_info 0.4.0 (#23135), and compressed tensors version bump (#23202).
* **Deployment improvements**: FlashInfer cubin directory environment variable (#22675) for offline environments and pre-cached CUDA binaries.

### V0 Deprecation
* **Backend removals**: V0 Neuron backend deprecation (#21159), V0 pooling model support removal (#23434), V0 FlashInfer attention backend removal (#22776), and V0 test cleanup (#23418, #23862).
* **API breaking changes**: prompt_token_ids fallback removal from LLM.generate and LLM.embed (#18800), LoRA extra vocab size deprecation warning (#23635), LoRA bias parameter deprecation (#24339), and metrics naming change from TPOT to ITL (#24110).

### Breaking Changes
1. **PyTorch 2.8.0 upgrade** - Environment dependency change requiring updated CUDA versions
2. **FlashMLA Blackwell restriction** - FlashMLA disabled on Blackwell GPUs due to compatibility issues
3. **V0 feature removals** - Neuron backend, pooling models, FlashInfer attention backend
5. **Quantizations** - Removed quantized Mixtral hack implementation, and original Marlin format. 
6. **Metrics renaming** - TPOT deprecated in favor of ITL

## What's Changed
* [Misc] Minor code cleanup for _get_prompt_logprobs_dict by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23064
* [Misc] enhance static type hint by @andyxning in https://github.com/vllm-project/vllm/pull/23059
* [Bugfix] fix Qwen2.5-Omni processor output mapping by @DoubleVII in https://github.com/vllm-project/vllm/pull/23058
* [Bugfix][CI] Machete kernels: deterministic ordering for more cache hits by @andylolu2 in https://github.com/vllm-project/vllm/pull/23055
* [Misc] refactor function name by @andyxning in https://github.com/vllm-project/vllm/pull/23029
* [Misc] Fix backward compatibility from #23030 by @ywang96 in https://github.com/vllm-project/vllm/pull/23070
* [XPU] Fix compile size for xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/23069
* [XPU][CI]add xpu env vars in CI scripts by @jikunshang in https://github.com/vllm-project/vllm/pull/22946
* [Refactor] Define MultiModalKwargsItems separate from MultiModalKwargs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23053
* [Bugfix] fix IntermediateTensors equal method by @andyxning in https://github.com/vllm-project/vllm/pull/23027
* [Refactor] Get prompt updates earlier by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23097
* chore: remove unnecessary patch_padding_side for the chatglm model by @carlory in https://github.com/vllm-project/vllm/pull/23090
* [Bugfix] Support compile for Transformers multimodal by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/23095
* [CI Bugfix] Pin `openai<1.100` to unblock CI by @mgoin in https://github.com/vllm-project/vllm/pull/23118
* fix: OpenAI SDK compat (ResponseTextConfig) by @h-brenoskuk in https://github.com/vllm-project/vllm/pull/23126
* Use Blackwell FlashInfer MXFP4 MoE by default if available  by @mgoin in https://github.com/vllm-project/vllm/pull/23008
* Install tpu_info==0.4.0 to fix core dump for TPU by @xiangxu-google in https://github.com/vllm-project/vllm/pull/23135
* [Misc] Minor refactoring for prepare_inputs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23116
* [Spec Decode] Make `propose_draft_token_ids` non-blocking for lower TTFT by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23041
* [Misc] Add @tdoublep as a maintainer of hybrid model and Triton-attention related code by @tdoublep in https://github.com/vllm-project/vllm/pull/23122
* [CI][V0 Deprecation] Removed V0 Only Chunked Prefill and Prefix Caching Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/22871
* [V0 Deprecation] Remove V0 FlashInfer attention backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22776
* chore: disable enable_cpp_symbolic_shape_guards by @xiszishu in https://github.com/vllm-project/vllm/pull/23048
* [TPU] make ptxla not imported when using tpu_commons by @yaochengji in https://github.com/vllm-project/vllm/pull/23081
* [Hardware][IBM Z]Enable v1 for s390x and s390x dockerfile fixes by @nikheal2 in https://github.com/vllm-project/vllm/pull/22725
* Migrate InternVLImagePixelInputs (in nemotron_vl.py) to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/22023
* [Log] Warning Once for Cutlass MLA  by @yewentao256 in https://github.com/vllm-project/vllm/pull/23137
* [Model] Support Pipeline Parallelism for moonshotai/Kimi-VL-A3B-Thinking-2506 by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23114
* [misc] split engine_model into json file for nsys profile tool by @gracehonv in https://github.com/vllm-project/vllm/pull/23117
* [Benchmark] Add flag --served-model-name to benchmark_serving_multi_turn by @pliops-daniels in https://github.com/vllm-project/vllm/pull/22889
* Fix GLM-4.5V-FP8 numerical issue by @zixi-qi in https://github.com/vllm-project/vllm/pull/22949
* [Misc] Add request_id into benchmark_serve.py by @hustxiayang in https://github.com/vllm-project/vllm/pull/23065
* [Bugfix] Fix broken Minimax-01-VL model by @Isotr0py in https://github.com/vllm-project/vllm/pull/22116
* [bug fix] Fix llama4 spec decoding by @zixi-qi in https://github.com/vllm-project/vllm/pull/22691
* [Misc] Avoid accessing req_ids inside a loop by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23159
* [Doc] use power of 2 by @Tialo in https://github.com/vllm-project/vllm/pull/23172
* [Misc] Fix seq_lens for graph capture by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23175
* [NVIDIA] Support Flashinfer TRTLLM FP8-q/kv/out Attention Kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/21716
* [Model] Add transformers problem_type (e.g. multi_label_classification) support by @noooop in https://github.com/vllm-project/vllm/pull/23173
* [Model] support new model ovis2.5 by @myselvess in https://github.com/vllm-project/vllm/pull/23084
* [Bugfix] Fix benchmark_moe.py  by @jeejeelee in https://github.com/vllm-project/vllm/pull/23177
* [FEAT] [Performance] Enable DP for ViT in Qwen2.5VL by @tjtanaa in https://github.com/vllm-project/vllm/pull/22742
* [Model] Removes redundant all-reduce operation in Qwen3MoeSparseMoeBlock by @yiz-liu in https://github.com/vllm-project/vllm/pull/23169
* Add return_token_ids parameter to OpenAI API endpoints by @ultmaster in https://github.com/vllm-project/vllm/pull/22587
* Migrate LlavaOnevisionMultiInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21844
* [CI/Build] Update transformers to v4.55.2 by @Isotr0py in https://github.com/vllm-project/vllm/pull/23093
* [Misc] Fix the benchmark's README and improve the error messages for the benchmark's argument checks by @tanruixiang in https://github.com/vllm-project/vllm/pull/22654
* [Frontend] Add `/collective_rpc` API endpoint by @22quinn in https://github.com/vllm-project/vllm/pull/23075
* [Misc] Enable yapf for FlashInfer backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23193
* [Bugfix] Fix accuracy issue when using flashinfer cutlass moe, TP=1 and modelopt. by @bnellnm in https://github.com/vllm-project/vllm/pull/23125
* fix: use cache_salt for gpt-oss by @dr75 in https://github.com/vllm-project/vllm/pull/23186
* [Misc] Minor refactoring for FlashInfer backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23147
* [CI/Build] Add support for Python 3.13 by @mgoin in https://github.com/vllm-project/vllm/pull/13164
* [NVIDIA] Add SM100 Flashinfer Cutlass MoE fp8 backend by @amirkl94 in https://github.com/vllm-project/vllm/pull/22357
* [CI/Build] Replace lm-eval gsm8k tests with faster implementation by @mgoin in https://github.com/vllm-project/vllm/pull/23002
* [BugFix] fix CUTLASS MLA full cudagraph  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/23200
* [Benchmarks] Add video inputs to ShareGPTDataset.  by @huachenheli in https://github.com/vllm-project/vllm/pull/23199
* [Quantization] Bump Compressed Tensors Version by @kylesayrs in https://github.com/vllm-project/vllm/pull/23202
* [Core] Optimize scheduler request removal for single completions by @chi2liu in https://github.com/vllm-project/vllm/pull/21917
* [CI Perf] Only test bfloat16 for tests/compile/test_fusion_all_reduce.py by @mgoin in https://github.com/vllm-project/vllm/pull/23132
* [Core] Add torch profiler CPU traces for AsyncLLM. by @huachenheli in https://github.com/vllm-project/vllm/pull/21794
* [Doc] Update V1 status of various pooling models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23189
* [Attention] Optimize make_local_attention_virtual_batches for Flash Attention by @linzebing in https://github.com/vllm-project/vllm/pull/23185
* Fix a performance comparison issue in Benchmark Suite by @louie-tsai in https://github.com/vllm-project/vllm/pull/23047
* chore: support pytorch format in lora  by @KilJaeeun in https://github.com/vllm-project/vllm/pull/22790
* [CI/Build] Also check DP in benchmarks throughput script by @zhewenl in https://github.com/vllm-project/vllm/pull/23038
* [CI/Build] Sync multimodal tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23181
* [BugFix] Fix stuck stats/metrics after requests are aborted by @njhill in https://github.com/vllm-project/vllm/pull/22995
* fix cuda graph by @fsx950223 in https://github.com/vllm-project/vllm/pull/22721
* [Model] use autoWeightsLoader for gptoss by @calvin0327 in https://github.com/vllm-project/vllm/pull/22446
* Fix missing quotes by @wzshiming in https://github.com/vllm-project/vllm/pull/23242
* [Model] Support deepseek with eagle by @xyang16 in https://github.com/vllm-project/vllm/pull/21086
* [Bugfix] Ensure correctness of Cohere2Vision processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23245
* Update to flashinfer-python==0.2.12 and disable AOT compile for non-release image by @mgoin in https://github.com/vllm-project/vllm/pull/23129
* [Model][V1] Support Ernie MTP by @xyxinyang in https://github.com/vllm-project/vllm/pull/22169
* [Model] Improve olmo and olmo2 by @jeejeelee in https://github.com/vllm-project/vllm/pull/23228
* [Fix] fix offline env use local mode path by @lengrongfu in https://github.com/vllm-project/vllm/pull/22526
* [Bugfix] Ensure correctness of HCXVision processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23254
* [Kernel] CUTLASS MoE FP8: Integrate cuda moe permute/unpermute by @shixianc in https://github.com/vllm-project/vllm/pull/23045
* [CLI][Doc] Formalize `--mm-encoder-tp-mode` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23190
* [Misc] Add max_seq_len to CommonAttentionMetadata  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23216
* [FIXBUG ] Allow disabling rocm_aiter_fa backend for ROCm GPUs not compatible with AITER by @JartX in https://github.com/vllm-project/vllm/pull/22795
* Support conditional torch.compile per module by @sarckk in https://github.com/vllm-project/vllm/pull/22269
* Migrate Mistral3ImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21945
* Limit HTTP header count and size by @russellb in https://github.com/vllm-project/vllm/pull/23267
* Small fix for Command-A-Vision by @dongluw in https://github.com/vllm-project/vllm/pull/23268
* [Kernel/Quant] Remove the original marlin format and qqq by @mgoin in https://github.com/vllm-project/vllm/pull/23204
* [Fix] correct tool_id for kimi-k2 when use tool_choice=required by @MoyanZitto in https://github.com/vllm-project/vllm/pull/21259
* [Frontend] improve error logging of chat completion by @heheda12345 in https://github.com/vllm-project/vllm/pull/22957
* [Optimization] Speed up function `_convert_tokens_to_string_with_added_encoders` by 13.7x by @misrasaurabh1 in https://github.com/vllm-project/vllm/pull/20413
* Do not use eval() to convert unknown types by @russellb in https://github.com/vllm-project/vllm/pull/23266
* [Feature] use --eplb_config to set eplb param by @lengrongfu in https://github.com/vllm-project/vllm/pull/20562
* [misc] fix multiple arch wheels for the nightly index by @youkaichao in https://github.com/vllm-project/vllm/pull/23110
* Remove chunked_prefill_enabled flag in V1 MLA by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/23183
* Feature/mla tests by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/23195
* [Fix] remove is_marlin param in benchmark_moe by @shixianc in https://github.com/vllm-project/vllm/pull/23286
* [EP] Add logging for experts map by @22quinn in https://github.com/vllm-project/vllm/pull/22685
* Remove duplicate entry in vllm.attention.__all__ by @russellb in https://github.com/vllm-project/vllm/pull/23296
* [CI Bugfix] Fix CI by fully removing --enable-prompt-adapter by @mgoin in https://github.com/vllm-project/vllm/pull/23284
* [Optimization] Make new_block_ids None if empty by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23262
* [CPU] Refactor CPU W8A8 scaled_mm by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/23071
* [CI/Build] Split out mm processor tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23260
* [V1][Mamba1] - Full CUDA and Piecewise CUDA Graphs Support by @Josephasafg in https://github.com/vllm-project/vllm/pull/23035
* [Compile] Fix Compile Warning SM100 Cutlass MLA by @yewentao256 in https://github.com/vllm-project/vllm/pull/23287
* [Model][VLM] Support R-4B Model by @yannqi in https://github.com/vllm-project/vllm/pull/23246
* Delete images older than 24h. by @QiliangCui in https://github.com/vllm-project/vllm/pull/23291
* [CI] Block the cu126 wheel build while broken by @mgoin in https://github.com/vllm-project/vllm/pull/23285
* [Sampler] Support returning final logprobs by @22quinn in https://github.com/vllm-project/vllm/pull/22387
* [Bugfix] Fix extra whitespace in strings caused by newline by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23272
* [BugFix] Fix Python 3.9 Support by @jaredoconnell in https://github.com/vllm-project/vllm/pull/23306
* [Model] Add LFM2 architecture by @paulpak58 in https://github.com/vllm-project/vllm/pull/22845
* [Refactor] Simplify code for MM budget by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23310
* [Doc] Fix batch-level DP example by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23325
* [Performance] V1 Pooling Models E2E Performance Optimization by @noooop in https://github.com/vllm-project/vllm/pull/23162
* [V1] Remove unnecessary check for main thread by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/23298
* [Bugfix] set system_message in phi4mini chat template by @zhuangqh in https://github.com/vllm-project/vllm/pull/23309
* [Multimodal] Always enable hashing mm data by @ywang96 in https://github.com/vllm-project/vllm/pull/23308
* [ci/build] Fix abi tag for aarch64 by @youkaichao in https://github.com/vllm-project/vllm/pull/23329
* Migrate MolmoImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/22022
* Fix nvfp4 swizzling by @yiliu30 in https://github.com/vllm-project/vllm/pull/23140
* add tg-mxfp4-moe-test by @IwakuraRein in https://github.com/vllm-project/vllm/pull/22540
* [Bug] Fix R1 Accuracy 0 Bug by @yewentao256 in https://github.com/vllm-project/vllm/pull/23294
* [Bugfix] Fix port conflict by obtaining a list of open ports upfront by @minosfuture in https://github.com/vllm-project/vllm/pull/21894
* [Misc] Misc code cleanup/simplification by @njhill in https://github.com/vllm-project/vllm/pull/23304
* [BugFix][gpt-oss] Fix Chat Completion with Multiple Output Message by @heheda12345 in https://github.com/vllm-project/vllm/pull/23318
* [Misc] fix VLLM_TORCH_PROFILER_DIR to absolute path by @andyxning in https://github.com/vllm-project/vllm/pull/23191
* [Core] Always use tensor cores for Flashinfer Decode Wrapper by @pavanimajety in https://github.com/vllm-project/vllm/pull/23214
* Make sure that vectorize_with_alignment produced vectorized global loads by @elvircrn in https://github.com/vllm-project/vllm/pull/23182
* [Structured Outputs] Refactor bitmask construction into get_grammar_bitmask by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23361
* [CI] Clean up actions: remove helm, publish workflows and improve pr â€¦ by @simon-mo in https://github.com/vllm-project/vllm/pull/23377
* [CI] improve pr comments bot by @simon-mo in https://github.com/vllm-project/vllm/pull/23380
* [Perf] Small optimizations for silu_mul_fp8_quant_deep_gemm by @mgoin in https://github.com/vllm-project/vllm/pull/23265
* Always use cache mounts when installing vllm to avoid populating pip cache in the image. Also remove apt cache. by @tvalentyn in https://github.com/vllm-project/vllm/pull/23270
* [Feature][Responses API] Support logprobs(non-stream) by @kebe7jun in https://github.com/vllm-project/vllm/pull/23319
* [Core] Support custom executor qualname by @22quinn in https://github.com/vllm-project/vllm/pull/23314
* [Kernel] Add FP8 support with FlashMLA backend by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/22668
* [Deprecation] Remove `prompt_token_ids` arg fallback in `LLM.generate` and `LLM.embed` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18800
* Migrate MllamaImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/22020
* [CI/Build] Skip Idefics3 and SmolVLM generation test again by @Isotr0py in https://github.com/vllm-project/vllm/pull/23356
* [Feature] Enable DeepGEMM Linear on B200; 1.5% E2E throughput improvement by @yewentao256 in https://github.com/vllm-project/vllm/pull/23351
* [CI] Add end-to-end V1 min_tokens test coverage by @arjunbreddy22 in https://github.com/vllm-project/vllm/pull/22495
* [Misc] Add gemma3 chat template with pythonic-style function calling by @philipchung in https://github.com/vllm-project/vllm/pull/17149
* [New Model] Add Seed-Oss model by @FoolPlayer in https://github.com/vllm-project/vllm/pull/23241
* [Attention] Refactor AttentionMetadata Preparation for Encoder-only Models by @heheda12345 in https://github.com/vllm-project/vllm/pull/23154
* [P/D][Nixl] Make kv cache register compatible with hybrid memory allocator by @sfeng33 in https://github.com/vllm-project/vllm/pull/23079
* [gpt-oss] add input/output usage in responses api when harmony context is leveraged by @gcalmettes in https://github.com/vllm-project/vllm/pull/22667
* Migrate MiniCPMOAudioInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21847
* [Bugfix] Fix pooling models on non-CUDA devices by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/23392
* [V0 Deprecation] Remove V0 LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/23418
* [Misc] Move M-RoPE init logic to _init_mrope_positions by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23422
* [Attention] Allow V1 flash_attn to support cross-attention by @russellb in https://github.com/vllm-project/vllm/pull/23297
* [misc] Remove outdate comment about runai_model_streamer by @carlory in https://github.com/vllm-project/vllm/pull/23421
* [Doc] Update the doc for log probs + prefix caching by @heheda12345 in https://github.com/vllm-project/vllm/pull/23399
* [Misc] local import code clean by @andyxning in https://github.com/vllm-project/vllm/pull/23420
* [Bug fix] Dynamically setting the backend variable for genai_perf_tests in the run-nightly-benchmark script by @namanlalitnyu in https://github.com/vllm-project/vllm/pull/23375
* [Fix] Bump triton version in rocm-build requirements by @bringlein in https://github.com/vllm-project/vllm/pull/21630
* [Bugfix]: Installing dev environment due to pydantic incompatible version by @hickeyma in https://github.com/vllm-project/vllm/pull/23353
* [Speculators][Speculative Decoding] Fix Qwen 2 Eagle3 Support by @PapaGoose in https://github.com/vllm-project/vllm/pull/23337
* [BugFix] Fix the issue where image embeddings were incorrectly split.â€¦ by @bppps in https://github.com/vllm-project/vllm/pull/23366
* fix(tests): Ensure reliable CUDA cache clearing in MoE test by @AzizCode92 in https://github.com/vllm-project/vllm/pull/23416
* Add unit tests for batched guided and non-guided requests by @sarckk in https://github.com/vllm-project/vllm/pull/23389
* [Doc]: fix various typos in multiple files by @didier-durand in https://github.com/vllm-project/vllm/pull/23179
* [Model] Add Ovis2.5 PP support by @Isotr0py in https://github.com/vllm-project/vllm/pull/23405
* [Bugfix] Fix broken Florence-2 model by @Isotr0py in https://github.com/vllm-project/vllm/pull/23426
* [Quantization] Allow GGUF quantization to skip unquantized layer by @Isotr0py in https://github.com/vllm-project/vllm/pull/23188
* add an env var for path to pre-downloaded flashinfer cubin files by @842974287 in https://github.com/vllm-project/vllm/pull/22675
* [CI/Build] add EP dependencies to docker by @zhewenl in https://github.com/vllm-project/vllm/pull/21976
* [PERF] PyTorch Symmetric Memory All-Reduce by @ilmarkov in https://github.com/vllm-project/vllm/pull/20759
* [BugFix][AMD][Quantization] Fix torch.compile issue where wvSplitKQ not being called when it should when using quantized FP8 model by @rasmith in https://github.com/vllm-project/vllm/pull/22281
* [NVIDIA] Support Flashinfer TRTLLM FP8-q/kv NVFP4-out Attention Kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/22703
* [BugFix] Fix batch updates for pooling models by @njhill in https://github.com/vllm-project/vllm/pull/23398
* [BugFix] Fix `MinPLogitsProcessor.update_states()` by @njhill in https://github.com/vllm-project/vllm/pull/23401
* [Model] Support DP for ViT on MiniCPM-V-4 by @david6666666 in https://github.com/vllm-project/vllm/pull/23327
* [UX] Move Dockerfile DeepGEMM install to tools/install_deepgemm.sh by @mgoin in https://github.com/vllm-project/vllm/pull/23360
* Quantization: support FP4 quantized models on AMD CDNA2/CDNA3 GPUs by @fengli1702 in https://github.com/vllm-project/vllm/pull/22527
* Add glm4.5v tp2,4 fp8 config on H100_80GB by @chenxi-yang in https://github.com/vllm-project/vllm/pull/23443
* Revert "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23396
* fix(tests): Correct unreachable assertion in truncation test by @AzizCode92 in https://github.com/vllm-project/vllm/pull/23425
* Support DeepSeek-V3.1 tool call by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/23454
* [Misc] Modify CacheConfig import by @jeejeelee in https://github.com/vllm-project/vllm/pull/23459
* [gpt-oss] Streaming Output for Python Tool by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23409
* Migrate Pixtral inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23472
* [Bugfix] Add strong reference to CUDA pluggable allocator callbacks by @22quinn in https://github.com/vllm-project/vllm/pull/23477
* Migrate Paligemma inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23470
* [kernel] Support W4A8 on Hopper by @czhu-cohere in https://github.com/vllm-project/vllm/pull/23198
* [Misc] update dict parse to EPLBConfig from json dumps to dict unpacking by @lengrongfu in https://github.com/vllm-project/vllm/pull/23305
* (Misc): add missing test for zero truncation size. by @teekenl in https://github.com/vllm-project/vllm/pull/23457
* [New Model]Donut model by @princepride in https://github.com/vllm-project/vllm/pull/23229
* [Model] Enable BLOOM on V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23488
* [Misc] Remove unused slot_mapping buffer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23502
* fix incompatibililty with non cuda platform for nvfp4 by @luccafong in https://github.com/vllm-project/vllm/pull/23478
* [Doc: ]fix various typos in multiple files by @didier-durand in https://github.com/vllm-project/vllm/pull/23487
* [Perf] Add Triton config for DeepSeek V3 FP8 EP32 H200 by @minosfuture in https://github.com/vllm-project/vllm/pull/23504
* Frontend: Adding LM Format Enforcer support to V1 engine by @noamgat in https://github.com/vllm-project/vllm/pull/22564
* [Bugfix] Fix Qwen2.5-VL quantized model weights loading by @zifeitong in https://github.com/vllm-project/vllm/pull/23512
* [Misc] Unified linear print info by @jeejeelee in https://github.com/vllm-project/vllm/pull/23516
* Migrate tarsier inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23500
* Migrate skyworkr1v inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23499
* Migrate DonutImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23509
* [Bugfix] Fix Dense module loading for sentence-transformers embedding models (simplified V2) by @FFFfff1FFFfff in https://github.com/vllm-project/vllm/pull/23408
* [gpt-oss] use reasoning channel for reasoning text in serving_chat by @yuguo68 in https://github.com/vllm-project/vllm/pull/22920
* [Refactor] Dynamic `target` and `content` for prompt updates by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23411
* [Core][Multimodal] Track encode cache entries by mm_hash and enable embedding sharing between requests by @fake0fan in https://github.com/vllm-project/vllm/pull/22711
* [Fix] DeepSeek V3.1 tool parser error message by @skyloevil in https://github.com/vllm-project/vllm/pull/23492
* Feature/benchmark/random mm data/images by @h-brenoskuk in https://github.com/vllm-project/vllm/pull/23119
* [Bugfix] Allow dynamic number of patches for llava_onevision by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23525
* [misc] add shanghai meetup by @youkaichao in https://github.com/vllm-project/vllm/pull/23535
* [Attention] Unify mamba and attention backend selection by @ayushsatyam146 in https://github.com/vllm-project/vllm/pull/23171
* [Doc] Add caution for API server scale-out by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23550
* [Refactor] Pass `tokenizer` explicitly instead of binding to prompt update by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23542
* Updates to Flex + VLLm integration by @drisspg in https://github.com/vllm-project/vllm/pull/21416
* [Bugfix] Fix Qwen3 MoE GPTQ inference by @Isotr0py in https://github.com/vllm-project/vllm/pull/23490
* [Refactor] Refactor persistent buffers with CpuGpuBuffer  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23515
* [test][RL] Add sleep level 2 test and fix reload with sleep mode by @22quinn in https://github.com/vllm-project/vllm/pull/23521
* [Kernel] Add fused grouped_topk kernel for MoE by @xyang16 in https://github.com/vllm-project/vllm/pull/23274
* [Bugfix][V1][P/D]Fix the issue where repeated requests for the same input produce abnormal outputs for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/23403
* [XPU] Delay BF16 check to worker init for spawn compatibility by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/22979
* [TPU][Bugfix] Fixes prompt_token_ids error in tpu tests. by @patemotter in https://github.com/vllm-project/vllm/pull/23574
* [Docs] Update Documentation of Cohere Command-A Models by @Terrencezzj in https://github.com/vllm-project/vllm/pull/23584
* [Misc] Simplify FlashInfer attention metadata by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23585
* [Misc] Add release note draft to PR template by @simon-mo in https://github.com/vllm-project/vllm/pull/23598
* [CI Fix] Pin deepep and pplx tags in tools/ep_kernels/, gate multigpu tests by @mgoin in https://github.com/vllm-project/vllm/pull/23568
* Update Flashinfer to  0.2.14.post1 by @weireweire in https://github.com/vllm-project/vllm/pull/23537
* [Bug] Fix DeepGEMM Env Control by @yewentao256 in https://github.com/vllm-project/vllm/pull/23591
* [CI/Build] Use vLLM client's user agent to fetch images by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23561
* Remove graph_pool as member of VllmBackend and argument to CUDAGraphWrapper by @Copilot in https://github.com/vllm-project/vllm/pull/23385
* [Disagg][Perf] Use CUDA event sync instead of blocking `tolist` to avoid unintentional copy ops blocking across different CUDA streams, improving disagg TTIT/TTFT by @liuzijing2014 in https://github.com/vllm-project/vllm/pull/22760
* [CI/Build] Fix typo in #23561 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23616
* [fix] fix seed-oss-parser by @FoolPlayer in https://github.com/vllm-project/vllm/pull/23560
* [mypy] Fix incorrect type hint for EAGLE3 support by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23617
* [Benchmarks] add benchmark for embedding models by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23000
* [Docs] Fix titles for multi-file examples that are rendered in the docs by @hmellor in https://github.com/vllm-project/vllm/pull/23573
* Fix CLI parameter documentation inconsistency in pooling_models.md by @oneraghavan in https://github.com/vllm-project/vllm/pull/23630
* [Bugfix] Fix Qwen25VL packed_modules_mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/23604
* [Bugfix] Fix scheduling when repeated images in one request by @ywang96 in https://github.com/vllm-project/vllm/pull/23544
* [V1] Enable V1 for compute capability < 8.0 + FP32 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23614
* Fix nits from #20059 by @hmellor in https://github.com/vllm-project/vllm/pull/23548
* Fix writing benchmark results with tuple keys by @huydhn in https://github.com/vllm-project/vllm/pull/23633
* [Perf] Remove duplicated NVFP4 blockscales to save memory by @mgoin in https://github.com/vllm-project/vllm/pull/23379
* [Model] fix DeepSeek e_score_correction_bias dtype to fp32 by @jeejeelee in https://github.com/vllm-project/vllm/pull/23640
* [Bugfix] Add missing enable_log_outputs parameter to init_app_state function by @lordmathis in https://github.com/vllm-project/vllm/pull/23634
* feat: add usage to TranscriptionResponse (text and json response_format) by @gcalmettes in https://github.com/vllm-project/vllm/pull/23576
* Support FlashAttention Backend for Hybrid SSM Models by @heheda12345 in https://github.com/vllm-project/vllm/pull/23299
* [Docs] Fix broken links to `docs/api/summary.md` by @hmellor in https://github.com/vllm-project/vllm/pull/23637
* [Hardware][Mac] Fix the installation fail for Apple Silicon (CPU)  by @OYE93 in https://github.com/vllm-project/vllm/pull/23565
* [Kernel] Added flashinfer fp8 per-tensor gemms by @nvjullin in https://github.com/vllm-project/vllm/pull/22895
* [Doc]: fix various spelling issues in multiple files by @didier-durand in https://github.com/vllm-project/vllm/pull/23636
* [CPU] add cpu fused moe pytorch native implementation by @TianyuLi0 in https://github.com/vllm-project/vllm/pull/23146
* [ROCm] Starting to add AMD code reviewers for ROCm components by @hongxiayang in https://github.com/vllm-project/vllm/pull/23496
* [Docs] Reduce requirements for docs build by @hmellor in https://github.com/vllm-project/vllm/pull/23651
* [Bugfix] fix bf16 multimodal model hash by @yuekaizhang in https://github.com/vllm-project/vllm/pull/23623
* [model] support qwen2audio embedding input by @yuekaizhang in https://github.com/vllm-project/vllm/pull/23625
* [Misc] Add override for allreduce fusion thresholds by @nvjullin in https://github.com/vllm-project/vllm/pull/23639
* [CI] [Doc]: Add GH Action for auto labeling issues with `rocm` tag by @vllmellm in https://github.com/vllm-project/vllm/pull/20988
* [Bugfix] Fix cuda event usage with CPU model runner by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/23643
* [Docs] Fix warnings in `mkdocs build` by @Zerohertz in https://github.com/vllm-project/vllm/pull/23649
* [Docs] [V1] [Hybrid] Update docs to remove FlashInfer constraint for hybrid models by @tdoublep in https://github.com/vllm-project/vllm/pull/23665
* [v1] Add cross-attention KV cache support for encoder-decoder models by @russellb in https://github.com/vllm-project/vllm/pull/23664
* [Bugfix] Fix incorrect original shape in hashing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23672
* [Misc] Fix comments in `tests/kernels/quantization` by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23675
* [Model] Enable video support for InternVL3.5 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/23658
* [doc] Hybrid KV Cache Manager design doc by @heheda12345 in https://github.com/vllm-project/vllm/pull/22688
* Enhance the pre-notification policy by @sidhpurwala-huzaifa in https://github.com/vllm-project/vllm/pull/23532
* [Docs] Move quant supported hardware table to README by @hmellor in https://github.com/vllm-project/vllm/pull/23663
* [V1][P/D]P2pNcclConnector supports flashinfer by @Abatom in https://github.com/vllm-project/vllm/pull/23536
* [V1] [Hybrid] Enable Full CUDA graph by default for hybrid models in V1 by @tdoublep in https://github.com/vllm-project/vllm/pull/22594
* [Compile] Fix Cmake Warning by @yewentao256 in https://github.com/vllm-project/vllm/pull/23689
* [Bugfix] UnboundLocalError when GptOss reasoning specified by @coval3nte in https://github.com/vllm-project/vllm/pull/23054
* feat: add triton fused moe config for GLM-4.5-Air-FP8 on B200 by @zixuanzhang226 in https://github.com/vllm-project/vllm/pull/23695
* [Feature][Responses API] Support MCP tool in background mode by @wuhang2014 in https://github.com/vllm-project/vllm/pull/23494
* fix pynccl reduce_scatter by @youzhedian in https://github.com/vllm-project/vllm/pull/23648
* [quantization] use channel scales for w4a8 + misc fixes by @czhu-cohere in https://github.com/vllm-project/vllm/pull/23570
* [gpt-oss] Enable unit test for response API harmony integration by @heheda12345 in https://github.com/vllm-project/vllm/pull/23533
* [Bugfix] Lazy import gpt_oss_triton_kernels_moe for mxfp4 by @mgoin in https://github.com/vllm-project/vllm/pull/23678
* [Docs] Fix math rendering in docs by @hmellor in https://github.com/vllm-project/vllm/pull/23676
* [Bugfix][gpt-oss] passing the cache config in gpt-oss by @frank-wei in https://github.com/vllm-project/vllm/pull/23613
* [Bugfix]: Qwen3 Coder Tool Parser by @ranpox in https://github.com/vllm-project/vllm/pull/23099
* [Core] Asynchronous h2d in merge_multimodal_embeddings via pinned memory. by @huachenheli in https://github.com/vllm-project/vllm/pull/23686
* [Model] Add Ernie4.5 VL Model Support by @CSWYF3634076 in https://github.com/vllm-project/vllm/pull/22514
* [Frontend] Add --log-error-stack to print stack trace for error response by @heheda12345 in https://github.com/vllm-project/vllm/pull/22960
* [Frontend] Optimize beam search performance by limiting concurrency by @heheda12345 in https://github.com/vllm-project/vllm/pull/23599
* [Quantization] Expand compressed-tensors MoE matching logic to support NFP4 + FP8 MoEs by @dsikka in https://github.com/vllm-project/vllm/pull/22674
* [XPU] Add xpu torch.compile support by @jikunshang in https://github.com/vllm-project/vllm/pull/22609
* [CI/Build] Remove redundant LoRA model tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/23706
* [Bugfix] fix when config.yaml config value is list parse error by @lengrongfu in https://github.com/vllm-project/vllm/pull/23528
* [Core] Use key-only cache for `BaseMultiModalProcessor` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23018
* [XPU]fix cuda event used in XPU model runner by @jikunshang in https://github.com/vllm-project/vllm/pull/23708
* [CI/Build] Remove redundant register in model init tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23715
* [Docs] Fix an admonition important by @windsonsea in https://github.com/vllm-project/vllm/pull/23726
* Optimize input preparation for FlashInfer [2/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23174
* [Misc] Move CpuGpuBuffer to vllm/v1/utils.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23728
* [FlashInfer] Cache hyper params in metadata builder by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23732
* [CI/Build] Reduce LoRA layer test cases by @jeejeelee in https://github.com/vllm-project/vllm/pull/23721
* [XPU] Fix OOM issue for data parallel with Ray backend by @faaany in https://github.com/vllm-project/vllm/pull/22500
* [Docs] Fix a 1-2-3 list and style issues in tpu.md by @windsonsea in https://github.com/vllm-project/vllm/pull/23729
* [model] Support MiniCPM-V 4.5 by @tc-mb in https://github.com/vllm-project/vllm/pull/23586
* [Bugfix] Fix task field initialization when PYTHONOPTIMIZE is enabled by @cndoit18 in https://github.com/vllm-project/vllm/pull/23718
* [Misc] Remove unnecessary `_send_reconfig_message()` in `core_client.py` by @njhill in https://github.com/vllm-project/vllm/pull/23127
* [V1] [Hybrid] Disable prefix caching by default for hybrid or mamba-based models  by @tdoublep in https://github.com/vllm-project/vllm/pull/23716
* [Model] Explicit `default_pooling_type` interface by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23736
* Add vLLM Korea Meetup in the README.md and meetups.md by @rebel-hongseok in https://github.com/vllm-project/vllm/pull/23746
* Fix pre-commit on main by @hmellor in https://github.com/vllm-project/vllm/pull/23747
* [Model] Interface to enable batch-level DP support by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23733
* Only run `get_attr_docs` if generating help text by @hmellor in https://github.com/vllm-project/vllm/pull/23723
* [Feature] Add Hopper DeepGEMM E8M0 for DeepSeekV3.1 scale_fmt by @yewentao256 in https://github.com/vllm-project/vllm/pull/23666
* [Model] Enable native HF format InternVL support by @Isotr0py in https://github.com/vllm-project/vllm/pull/23742
* [Doc]: upgrade version of crate-ci tool for improved typo detection by @didier-durand in https://github.com/vllm-project/vllm/pull/23755
* [LogitsProcs] Deduplicate built-in LP implementation logic by @njhill in https://github.com/vllm-project/vllm/pull/23362
* [Docs] Remove in-tree Gaudi install instructions by @hmellor in https://github.com/vllm-project/vllm/pull/23628
* [BugFix] Fix topk_softmax assert by @ProExpertProg in https://github.com/vllm-project/vllm/pull/19764
* [Model] Merge `SupportsMultiModalWithRawInput` with `SupportsMultiModal` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23749
* [V1] [Hybrid] Enable compile and piecewise CUDA graph for MiniMax-Text models by @tdoublep in https://github.com/vllm-project/vllm/pull/22589
* [Docs] Fix warnings in `mkdocs build` (continued) by @Zerohertz in https://github.com/vllm-project/vllm/pull/23743
* ci: Add arm64 docker build to release pipeline by @seemethere in https://github.com/vllm-project/vllm/pull/23210
* Disable `torch.compile` for dynamic rope models in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/23738
* [Multimodal] Generate mm_hash based on request metadata when caching is turned off by @ywang96 in https://github.com/vllm-project/vllm/pull/23690
* [V1][Mamba] - Enable V1 by default for Mamba Models by @Josephasafg in https://github.com/vllm-project/vllm/pull/23650
* DP/EP Support for gpt-oss with deepep-ht comm kernel on SM100 by @zyongye in https://github.com/vllm-project/vllm/pull/23608
* [Bugfix] Fix Marlin NVFP4 for modelopt by @mgoin in https://github.com/vllm-project/vllm/pull/23659
* [Feature] Add `VLLM_DISABLE_PAD_FOR_CUDAGRAPH` to Avoid Hang Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/23595
* [Bugfix] Fix for V1 priority scheduling crashes at preemption by @Hanchenli in https://github.com/vllm-project/vllm/pull/23713
* Migrate Qwen inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23473
* [Feature] models: pass layer prefix to replace_linear_class for per-layer quantization routing. Addresses #23239 by @Shrey1306 in https://github.com/vllm-project/vllm/pull/23556
* [Perf] Tune configs for triton block fp8 gemm H100/H200 by @mgoin in https://github.com/vllm-project/vllm/pull/23748
* Gracefully handle edge cases in harmony utils by @Ithanil in https://github.com/vllm-project/vllm/pull/23155
* [CI] make all multi-gpu weight loading tests run nightly by @killershrimp in https://github.com/vllm-project/vllm/pull/23792
* Add deprecation warning for lora_extra_vocab_size by @ahengljh in https://github.com/vllm-project/vllm/pull/23635
* [Transform] [Quantization] Add transforms to compressed tensors by @kylesayrs in https://github.com/vllm-project/vllm/pull/22486
* [CI] enable idefics3 and fuyu-8b test in multimodal test by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23790
* [Bugfix] when set offline model running error by @lengrongfu in https://github.com/vllm-project/vllm/pull/23711
* [Kernel] cuda kernels for upcoming decode context parallel feature by @youzhedian in https://github.com/vllm-project/vllm/pull/23791
* [New Model]: Support GteNewModelForSequenceClassification by @noooop in https://github.com/vllm-project/vllm/pull/23524
* [Model] Add PP support and VLM backbone compatability for GPT-OSS by @Isotr0py in https://github.com/vllm-project/vllm/pull/23680
* [FIXBUG] Add return_success parameter to moe_wna16_weight_loader function by @JartX in https://github.com/vllm-project/vllm/pull/22797
* [Doc]: fix typos in .md files (including those of #23751) by @didier-durand in https://github.com/vllm-project/vllm/pull/23825
* [CI/Build][Bugfix] Fix Qwen VL tests on CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/23818
* [BugFix][Spec Decode] Use float64 for uniform_probs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23803
* [Model] [gpt-oss] fix gpt-oss pp support by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23815
* [Doc]: fix typos in Python scripts by @didier-durand in https://github.com/vllm-project/vllm/pull/23828
* [Bugfix] Fix benchmark_moe.py for blockwise fp8. by @crischeng in https://github.com/vllm-project/vllm/pull/23823
* [CI] Fix linting error on main by @tdoublep in https://github.com/vllm-project/vllm/pull/23835
* [Model][gpt-oss] Support DP+EP for GPT-OSS with FlashInfer trtllm-gen MoE by @nvpohanh in https://github.com/vllm-project/vllm/pull/23819
* [Bugfix] Add fake mode around passes by @angelayi in https://github.com/vllm-project/vllm/pull/23349
* [ci] breaks down V1 Test into 3 groups of approx 30 minutes runtime by @jeanschmidt in https://github.com/vllm-project/vllm/pull/23757
* Add scale_config.yml file for Meta autoscalers for GH Actions by @jeanschmidt in https://github.com/vllm-project/vllm/pull/23840
* Migrate Llama4ImagePatchInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/22021
* [ROCm][Aiter] Add triton fp8 bmm kernel for mla by @divakar-amd in https://github.com/vllm-project/vllm/pull/23264
* [bugfix] [spec-decoding] fix data race in sample_recovered_tokens_kernel (vLLM v1) by @He-Jingkai in https://github.com/vllm-project/vllm/pull/23829
* [NVIDIA] Support SiluMul + NVFP4 quant fusion by @elvischenv in https://github.com/vllm-project/vllm/pull/23671
* chore: build release image by default by @simon-mo in https://github.com/vllm-project/vllm/pull/23852
* [BugFix][FlashInfer] Fix potential race condition for paged_kv_indptr_cpu by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23737
* [V1] Enable prefill optimization for Gemma3n by @sarckk in https://github.com/vllm-project/vllm/pull/22628
* [Log] Use Debug Once for DeepGEMM E8M0 When not Enabled by @yewentao256 in https://github.com/vllm-project/vllm/pull/23858
* [V0 Deprecation] Remove V0 Samplers test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23862
* [XPU] support data parallel for MoE models on XPU by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/22887
* [Models] Improve iteration over layers by @lgeiger in https://github.com/vllm-project/vllm/pull/19497
* [ROCm][Fix] Fix rocm build caused by #23791 by @charlifu in https://github.com/vllm-project/vllm/pull/23847
* [tests] Improve speed and reliability of test_transcription_api_correctness by @russellb in https://github.com/vllm-project/vllm/pull/23854
* [Bugfix] Use `ReplicatedLinear` for SequenceClassification head by @Isotr0py in https://github.com/vllm-project/vllm/pull/23836
* [BugFix][AMD][Deepseek] fix a dtype mismatch error for deepseek running on AMD by @KingsleyZhang123 in https://github.com/vllm-project/vllm/pull/23864
* [Platform] import activation_quant_fusion for CUDA only by @wangxiyuan in https://github.com/vllm-project/vllm/pull/23882
* Fix(async): Add support for truncate_prompt_tokens in AsyncLLM by @oneraghavan in https://github.com/vllm-project/vllm/pull/23800
* [CI/Build] Clean up LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/23890
* [mrope][Qwen2-VL] Fix edge case where getting index of image/video token can potentially throw in default vl mrope implementation.  by @huachenheli in https://github.com/vllm-project/vllm/pull/23895
* [Misc] Fix warnings for mistral model by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23552
* Better errors for Transformers backend missing features by @hmellor in https://github.com/vllm-project/vllm/pull/23759
* [V0 Deprecation] Remove pooling model support in V0  by @maxdebayser in https://github.com/vllm-project/vllm/pull/23434
* [CPU] Enable data parallel for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/23903
* [Performance] V1 Classify Models E2E Performance Optimization by @noooop in https://github.com/vllm-project/vllm/pull/23541
* [Multimodal] Consolidate mm inputs into MultiModalFeatureSpec by @sfeng33 in https://github.com/vllm-project/vllm/pull/23779
* Update PyTorch to 2.8.0 by @huydhn in https://github.com/vllm-project/vllm/pull/20358
* Adds `json_count_leaves` utility function  by @aditchawdhary in https://github.com/vllm-project/vllm/pull/23899
* [MODEL] `Apertus` and `XIELU` by @EduardDurech in https://github.com/vllm-project/vllm/pull/23068
* [Models] Use in-place adds in Idefics2Vision by @lgeiger in https://github.com/vllm-project/vllm/pull/23932
* [BugFix] Async scheduling and PP compatibility with DP by @njhill in https://github.com/vllm-project/vllm/pull/23770
* [CI]  Add `aiter` to matching list of issue auto labeller for `rocm` tag by @vllmellm in https://github.com/vllm-project/vllm/pull/23942
* [BUGFIX ] fix undefined silu_and_mul_nvfp4_quant by @youzhedian in https://github.com/vllm-project/vllm/pull/23929
* [RL][BugFix] Fix missing tokenizer error for token-in-token-out by @22quinn in https://github.com/vllm-project/vllm/pull/23904
* Tuned H100/H200 triton fp8 block configs for fused_qkv_a_proj by @mgoin in https://github.com/vllm-project/vllm/pull/23939
* [Docs] [V1] [Hybrid] Add new documentation re: contributing mamba-based models  by @tdoublep in https://github.com/vllm-project/vllm/pull/23824
* Revert gemma3n fast prefill changes by @sarckk in https://github.com/vllm-project/vllm/pull/23897
* [Misc] Make `download_weights_from_hf` more reliable by @hmellor in https://github.com/vllm-project/vllm/pull/23863
* [CI] Fix unavailable image remote URL by @ywang96 in https://github.com/vllm-project/vllm/pull/23966
* [Bugfix] Fix --config arg expansion called from api_server.py by @dubejf in https://github.com/vllm-project/vllm/pull/23944
* Add routed_scaling_factor to MoE grouped topk by @xyang16 in https://github.com/vllm-project/vllm/pull/23123
* [CI] Move testing image from remote URL to S3 by @ywang96 in https://github.com/vllm-project/vllm/pull/23980
* [CI] Fix broken compile tests due to unsupported SiluMul+Nvfp4Quant fusion by @sarckk in https://github.com/vllm-project/vllm/pull/23973
* [Core] Cleanup TPU model runner for MM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23894
* [V1] [Hybrid] Move MiniMaxLinearAttention into layers/mamba by @tdoublep in https://github.com/vllm-project/vllm/pull/23831
* [Bugfix] Fix test_lora_resolvers.py by @jeejeelee in https://github.com/vllm-project/vllm/pull/23984
* [UT] fix unify_kv_cache_configs when kv cache config needs sort by @andyxning in https://github.com/vllm-project/vllm/pull/23843
* [Model] Enable encoder DP for MiniCPM-V by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23948
* Add LoRA support for DeepSeek models (V2, V3, R1-0528) by @sadeghja1070 in https://github.com/vllm-project/vllm/pull/23971
* [Misc] add reorder_batch AttentionMetadataBuilder by @andyxning in https://github.com/vllm-project/vllm/pull/23798
* [Refactor] refactor freezing_value/cuda_event initialize outside try finally by @andyxning in https://github.com/vllm-project/vllm/pull/23758
* [Misc] enhance type hint for rearrange return value by @andyxning in https://github.com/vllm-project/vllm/pull/23519
* [LoRA] Much faster startup when LoRA is enabled by @andylolu2 in https://github.com/vllm-project/vllm/pull/23777
* Fix wrong truncate_prompt_tokens type hint by @gmarinho2 in https://github.com/vllm-project/vllm/pull/22761
* [Core][Multimodal] Allow passing `multi_modal_uuids` as multimodal identifiers. by @ywang96 in https://github.com/vllm-project/vllm/pull/23394
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24001
* vllm fix check on max vocab size by @xw285cornell in https://github.com/vllm-project/vllm/pull/22471
* [Minor] Fix some random typos in comments by @njhill in https://github.com/vllm-project/vllm/pull/24009
* v1: Support KV events from connectors by @orozery in https://github.com/vllm-project/vllm/pull/19737
* [BUGFIX] GPTQ quantization compatibility for Qwen3 MOE models (AutoGPTQ and AutoRound-GPTQ) by @JartX in https://github.com/vllm-project/vllm/pull/23994
* [Misc] Avoid redundant copy for encoder-only models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24012
* Fix the bug related to loading GPTP INT3 weights. by @Jun-Howie in https://github.com/vllm-project/vllm/pull/23328
* [Misc] Move fast prefill logic to separate method by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24013
* [CI/Build] Improve Tensor Schema tests speed by avoid engine core initialization by @Isotr0py in https://github.com/vllm-project/vllm/pull/23357
* [Misc] refactor code by import as for torch._inductor.config by @andyxning in https://github.com/vllm-project/vllm/pull/23677
* Migrate Phi4 inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23471
* [Misc] IO Processor plugins for pooling models by @christian-pinto in https://github.com/vllm-project/vllm/pull/22820
* [Bugfix] Add support for `<tool_call>` format in streaming mode for XLAM Tool Parser by @DevonPeroutky in https://github.com/vllm-project/vllm/pull/22769
* [Misc] add hash_function doc string by @andyxning in https://github.com/vllm-project/vllm/pull/24014
* [Misc] Enable V1 FP16 inference on pre-Ampere GPUs by @Isotr0py in https://github.com/vllm-project/vllm/pull/24022
* [Frontend] Update the warning log when using VLLM_ALLOW_LONG_MAX_MODEL_LEN by @noooop in https://github.com/vllm-project/vllm/pull/20904
* [Kernel] Update DeepGEMM to latest commit by @jeejeelee in https://github.com/vllm-project/vllm/pull/23915
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24026
* [Frontend] Gemma3n audio `transcriptions`/`translations` endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/23735
* [Doc]: Fix CPU install docs: force torch-backend=cpu to avoid GPU torchvision errors by @yankay in https://github.com/vllm-project/vllm/pull/24033
* [Model]: support KeyeVL-1_5-8B by @Kwai-Keye in https://github.com/vllm-project/vllm/pull/23838
* Document multi-proc method selection for profiling by @hypdeb in https://github.com/vllm-project/vllm/pull/23802
* [Misc] Minor code simplification for spec decode by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24053
* [docs][misc] IOProcessor plugins fixes by @christian-pinto in https://github.com/vllm-project/vllm/pull/24046
* [Model] Support DP for ViT on Kimi-VL-A3B-Thinking-2506 by @david6666666 in https://github.com/vllm-project/vllm/pull/23817
* [Chore][V0 Deprecation] Move LogProb to a separate file by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24055
* [bugfix]fix MTP hidden states by @luccafong in https://github.com/vllm-project/vllm/pull/24056
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24042
* [V1][Mamba1] - FP32 SSM Kernel Support by @Josephasafg in https://github.com/vllm-project/vllm/pull/23506
* [Bugfix] Fix the issue that Blip2ForConditionalGeneration' object hasâ€¦ by @DamonJiang777 in https://github.com/vllm-project/vllm/pull/24028
* Remove runtime checks based on pooling params by @maxdebayser in https://github.com/vllm-project/vllm/pull/24051
* Migrate OvisImagePatchInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/22024
* [XPU][Feature] fp8 online quantization support for XPU by @yma11 in https://github.com/vllm-project/vllm/pull/23148
* Migrate Interns1 inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23510
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24077
* [Model] Support dp on ViT on GLM-4.5V by @david6666666 in https://github.com/vllm-project/vllm/pull/23168
* [CI]: reduce HTTP calls inside entrypoints openai tests by @AzizCode92 in https://github.com/vllm-project/vllm/pull/23646
* correct LWS deployment yaml by @cberge908 in https://github.com/vllm-project/vllm/pull/23104
* [Gemma3n] Fix audio batching by @NickLucche in https://github.com/vllm-project/vllm/pull/24052
* [BugFix] Fix EXAONE4 rotary embeddings by @lkm2835 in https://github.com/vllm-project/vllm/pull/23918
* [Model] Classification models support logit_bias / sigmoid_normalize by @noooop in https://github.com/vllm-project/vllm/pull/24031
* [CI Failure] Skip failing nvfp4 silu test by @mgoin in https://github.com/vllm-project/vllm/pull/23959
* [docs] add SYS_NICE cap & `security-opt` for docker/k8s by @panpan0000 in https://github.com/vllm-project/vllm/pull/24017
* [Benchmark] Add support for local hf dataset path in benchmark by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23999
* [Bugfix] Fix transform_config parsing in Compressed Tensors by @kylesayrs in https://github.com/vllm-project/vllm/pull/23945
* Run ruff format on a few files. by @huachenheli in https://github.com/vllm-project/vllm/pull/24075
* [Bugfix] Fix packed_factor missing attribute error by @kyuyeunk in https://github.com/vllm-project/vllm/pull/23902
* [Metrics] Deprecate TPOT in favor of ITL by @markmc in https://github.com/vllm-project/vllm/pull/24110
* Fix weights loading for Apertus by @nathanrchn in https://github.com/vllm-project/vllm/pull/24100
* [Log] Only Print Profiler Results on Rank 0 by @yewentao256 in https://github.com/vllm-project/vllm/pull/23370
* [CI] Enable all hf transformers baselines in test_hybrid by @tdoublep in https://github.com/vllm-project/vllm/pull/23936
* [AMD][Kernel][Bugfix] Cast offsets tensor bn to tl.int64 to avoid GPU segfault by @rasmith in https://github.com/vllm-project/vllm/pull/23692
* [Bug] R1 Accuracy: Fix `routed_scaling_factor` Double Mul Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/24119
* [CI/Build] Disable SiluMul NVFP4 quant fusion tests by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/24121
* [XPU] Fix the bug of LoRA logits on the XPU platform by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/24081
* Update release pipeline post PyTorch 2.8.0 update by @youkaichao in https://github.com/vllm-project/vllm/pull/24073
* Upgrade xgrammar to 0.1.23 by @russellb in https://github.com/vllm-project/vllm/pull/22988
* [V1] Wrapper which plumbs request-level logits processors into vLLM batch-level logits processing by @afeldman-nm in https://github.com/vllm-project/vllm/pull/23656
* fix some typos by @co63oc in https://github.com/vllm-project/vllm/pull/24071
* [Compile] Fix Compile Warning for `w4a8_mm_entry.cu` by @yewentao256 in https://github.com/vllm-project/vllm/pull/23660
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24093
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24115
* [Misc] Add check for dual_chunk_attention by @ZJY0516 in https://github.com/vllm-project/vllm/pull/24070
* [BugFix] Fix routed_scaling_factor double mul for dots1 and glm4 MoE models by @sarckk in https://github.com/vllm-project/vllm/pull/24132
* [distributed][rl] remove nccl cumem env var override by @youkaichao in https://github.com/vllm-project/vllm/pull/24141
* [Nixl] Heterogeneous TP support FlashInfer by @NickLucche in https://github.com/vllm-project/vllm/pull/20189
* [CI/Build] Serve images used by multimodal tests through local HTTP Server by @divyanshsinghvi in https://github.com/vllm-project/vllm/pull/23907
* [Misc] Clean up deadcode for legacy processing pipeline by @Isotr0py in https://github.com/vllm-project/vllm/pull/24153
* [CI] Accelerate mteb test by setting SentenceTransformers mteb score to a constant by @noooop in https://github.com/vllm-project/vllm/pull/24088
* Support add_generation_prompt in embeddings endpoint with chat request by @biba10 in https://github.com/vllm-project/vllm/pull/23931
* Fix MiniMax attention module prefix and remove useless code by @qscqesze in https://github.com/vllm-project/vllm/pull/23982
* FIX: Add libnuma-dev to Dockerfile for dev stage by @dongbo910220 in https://github.com/vllm-project/vllm/pull/20388
* [Bugfix] Fixing division by zero in triton_attn if query_heads/kv_heads > 16  by @bringlein in https://github.com/vllm-project/vllm/pull/23424
* [V1] v1 engine + full CUDA graph support for PLaMo2 by @nopperl in https://github.com/vllm-project/vllm/pull/23998
* [Kernels] Overlap shared experts with send/recv by @bnellnm in https://github.com/vllm-project/vllm/pull/23273
* Migrate whisper inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23505
* [Attention] Blackwell FP8 MLA support with CUTLASS_MLA backend by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/23289
* [Feature][P/D]: Optimize NIXL Connector xfer Launch by @david6666666 in https://github.com/vllm-project/vllm/pull/23887
* [Bugfix][DP] DP distribution does not require ray[default] by @kebe7jun in https://github.com/vllm-project/vllm/pull/23822
* [Feature][gpt-oss] Add support for num_cached_tokens and num_reasoning_tokens tracking by @NagyGeorge in https://github.com/vllm-project/vllm/pull/23460
* Remove deprecated `PyNcclConnector` by @panpan0000 in https://github.com/vllm-project/vllm/pull/24151
* [Feature][Responses API]Support MCP tools with streaming mode + background mode by @wuhang2014 in https://github.com/vllm-project/vllm/pull/23927
* [Kernel][Bugfix] Fix grouped topk cu by @mayuyuace in https://github.com/vllm-project/vllm/pull/24146
* [Refactor] Introduce basic Renderer for completion-style request by @sfeng33 in https://github.com/vllm-project/vllm/pull/24010
* Migrate ultravox inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23503
* [CPU] Refactor CPU unquantized linear by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/24150
* [Misc] Enhance output readability of helper script by @wdhongtw in https://github.com/vllm-project/vllm/pull/24214
* [Model] Add MiDashengLM model support by @bingchen-mi in https://github.com/vllm-project/vllm/pull/23652
* [Core][Model] Terratorch backend integration by @mgazz in https://github.com/vllm-project/vllm/pull/23513
* Improve flexibility of auto_tune.sh execution. by @anthonsu in https://github.com/vllm-project/vllm/pull/23766
* [Attention][Platform] Refactor MLA to support Custom Op by @whx-sjtu in https://github.com/vllm-project/vllm/pull/23332
* [Bugfix] Fix Incremental Detokenization with `tokenizers == 0.22.0` by @faaany in https://github.com/vllm-project/vllm/pull/24159
* [Attention] FlashAttn MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14258
* [Hardware][Apple-CPU] Disable OneDNN build for Apple Silicon by @ignaciosica in https://github.com/vllm-project/vllm/pull/24200
* [Feature][Response API] Add streaming support for non-harmony by @kebe7jun in https://github.com/vllm-project/vllm/pull/23741
* [Doc] Update vLLM Singapore Meetup info by @tjtanaa in https://github.com/vllm-project/vllm/pull/24234
* [Model] Add pp support for hunyuan by @ZJY0516 in https://github.com/vllm-project/vllm/pull/24212
* Use hidden_size_per_head as head_size fallback by @nopperl in https://github.com/vllm-project/vllm/pull/24221
* [XPU] support Triton Attention backend on Intel GPU by @jikunshang in https://github.com/vllm-project/vllm/pull/24149
* [LoRA]: Add lora support to qwen-2.5-omni by @pratapyash in https://github.com/vllm-project/vllm/pull/24231
* [Misc] Removed force_fp8_e4m3fnuz from FP8LinearOp by @nvjullin in https://github.com/vllm-project/vllm/pull/23725
* [Perf] Freeze core engine proc heap after init by @njhill in https://github.com/vllm-project/vllm/pull/24008
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24173
* [Misc] Slight improve deepgemm print by @jeejeelee in https://github.com/vllm-project/vllm/pull/24085
* Upgrade FlashInfer to v0.3.0 by @nvpohanh in https://github.com/vllm-project/vllm/pull/24086
* QWEN3 Coder Fused MoE kernels Optimization configs by @samanamp in https://github.com/vllm-project/vllm/pull/24266
* [Misc] Have AsyncLLM `custom_stat_loggers` extend default logger list by @eicherseiji in https://github.com/vllm-project/vllm/pull/20952
* [Bugfix][Misc] Fix silu_and_mul_nvfp4_quant issue and extract common utils for nvfp4 kernel source files by @elvischenv in https://github.com/vllm-project/vllm/pull/23727
* [CI/Build] Reduce the number of redundant cases to test for LoRA by @zhuohan123 in https://github.com/vllm-project/vllm/pull/24276
* [Frontend] Skip unnecessary detokenization when token_id is requested by @NickLucche in https://github.com/vllm-project/vllm/pull/24236
* [gpt-oss] tool parser supports for /chat/completions [1/n] by @aarnphm in https://github.com/vllm-project/vllm/pull/22386
* [XPU][P/D] Add XPU support in NixlConnector by @zhenwei-intel in https://github.com/vllm-project/vllm/pull/22436
* Adding int4 and int8 models for CPU benchmarking by @louie-tsai in https://github.com/vllm-project/vllm/pull/23709
* [docs] add shenzhen meetup by @youkaichao in https://github.com/vllm-project/vllm/pull/24326
* [gpt-oss][Bugfix]Fix streamableparser for missing handling of certain token_ids by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24306
* [Bugfix] Fix silu_mul+quant fusion test by @elvischenv in https://github.com/vllm-project/vllm/pull/24341
* [RFC] allow cancelation after shutdown in blocking collective_rpc by @842974287 in https://github.com/vllm-project/vllm/pull/23390
* [CI] Add timeouts to tests by @rafvasq in https://github.com/vllm-project/vllm/pull/24260
* [Perf][V1] Fully overlap model execution by @benchislett in https://github.com/vllm-project/vllm/pull/23569
* Add @22quinn as code reviewer for RL related components by @22quinn in https://github.com/vllm-project/vllm/pull/24346
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24294
* [KV Sharing] Raise error if using eagle with fast prefill by @sarckk in https://github.com/vllm-project/vllm/pull/24350
* [Feature] Support Decode Context Parallel (DCP) for MLA by @youzhedian in https://github.com/vllm-project/vllm/pull/23734
* [Bugfix] Catch and log invalid token ids in detokenizer by @njhill in https://github.com/vllm-project/vllm/pull/24351
* [Core] Allow disabling TP sharding for parallel Linear layer by @Isotr0py in https://github.com/vllm-project/vllm/pull/23024
* [New Model]: google/embeddinggemma-300m by @noooop in https://github.com/vllm-project/vllm/pull/24318
* refactor: Turn GPUModelRunner.inputs_embeds to a CpuGpuBuffer by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/24345
* [Multimodal] Improve max video embedding length estimation in V1 by @ywang96 in https://github.com/vllm-project/vllm/pull/24312
* [CI] Disable flaky structured output test from CI by @ywang96 in https://github.com/vllm-project/vllm/pull/24366
* Add @benchislett to codeowner for spec decode and structured outputs by @benchislett in https://github.com/vllm-project/vllm/pull/24362
* [Bugfix] Avoid uninitialized usage of azp_val when AZP is false. by @mohankku in https://github.com/vllm-project/vllm/pull/24335
* [Bugfix] Fix broken deepseek fp8 TP weights loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/24367
* [Bugfix] Fix test_mixtral_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/24371
* Lora bias(enable_lora_bias) deprecate warning by @ashwin-phadke in https://github.com/vllm-project/vllm/pull/24339
* [Fix] [gpt-oss] fix non-tool calling path for chat completion by @aarnphm in https://github.com/vllm-project/vllm/pull/24324
* [Frontend][Responses API] Support reporting tool output tokens and fix reasoning token count by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24285
* [Bugfix] Fix unstable silu_mul+nvfp4 quant fusion test by @elvischenv in https://github.com/vllm-project/vllm/pull/24370
* break execute_model in gpu_model_runner into sub-functions for custom scopes by @bangshengtang in https://github.com/vllm-project/vllm/pull/24265
* [V0 deprecation] Deprecate V0 Neuron backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21159
* [attention][DCP] use AttentionImpl.need_to_return_lse_for_decode by @youkaichao in https://github.com/vllm-project/vllm/pull/24372
* Migrate Qwen2 inputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/23475
* [CI][Fix] deterministic seed for flaky CI runs on structured outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/24380
* [Benchmark] add benchmark for custom activation op by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23908
* QWEN3 Thinking Fused MoE kernels Optimization configs by @samanamp in https://github.com/vllm-project/vllm/pull/24330
* [Misc] collect flashinfer version in collect_env.py by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24378
* [Bugfix] Fix Qwen3-coder moe tuned config by @jeejeelee in https://github.com/vllm-project/vllm/pull/24072
* [TPU] Remove TopKTopPSampler dependency for TPU sampler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24391
* Add renderer-based prompt processing for embedding and classification endpoints by @sfeng33 in https://github.com/vllm-project/vllm/pull/24356
* Skip MM Encoder for non-first PP ranks by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24387
* Add @luccafong to codeowner for spec decode by @luccafong in https://github.com/vllm-project/vllm/pull/24397
* [Kernel] Support decode context parallelism on Blackwell with CUTLASS MLA by @minosfuture in https://github.com/vllm-project/vllm/pull/24385
* [xpu] upgrade ipex/python3.12 for xpu by @yma11 in https://github.com/vllm-project/vllm/pull/23830
* [Sampler] Support returning all prompt logprobs by @charlotte12l in https://github.com/vllm-project/vllm/pull/23868
* [CI/Build] Disable flaky test_structured_output tests by @22quinn in https://github.com/vllm-project/vllm/pull/24404
* [CI/Build] Fix local image inputs in test_pixtral.py by @huachenheli in https://github.com/vllm-project/vllm/pull/24401
* [Doc] Fix UTF-8 encoding issues in documentation generation on Windows by @alhridoy in https://github.com/vllm-project/vllm/pull/24361
* [P/D] Add a shutdown method to the Connector API by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/22699
* [Model] Remove unnecessary CUDA sync of GLM-4.1V image and video preprocess by @what-in-the-nim in https://github.com/vllm-project/vllm/pull/24332
* [Model] Remove unnecessary CUDA sync of Qwen2VL image and video preprocess by @what-in-the-nim in https://github.com/vllm-project/vllm/pull/24334
* [gpt-oss][Responses API] Fix the function call id format by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24409
* [Docs] Fix a tip indentation and typo by @windsonsea in https://github.com/vllm-project/vllm/pull/24419
* [Doc]: fix typos in Python comments by @didier-durand in https://github.com/vllm-project/vllm/pull/24417
* [Doc] Fix issues in integrations/llamastack.md by @windsonsea in https://github.com/vllm-project/vllm/pull/24428
* [Bugfix] Fix get_quant_config when using modelscope by @Potabk in https://github.com/vllm-project/vllm/pull/24421
* [Bugfix] Fix mamba2 prefill chunking by @tomeras91 in https://github.com/vllm-project/vllm/pull/23279
* [Misc] Terratorch related fixes by @christian-pinto in https://github.com/vllm-project/vllm/pull/24337
* Move `KVEventsConfig` from `config/__init__.py` to `config/kv_events.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24433
* [Frontend] User-provided uuids for medias in chat. (RFC #22044) by @huachenheli in https://github.com/vllm-project/vllm/pull/23449
* [Docs] Move feature compatibility tables to README by @hmellor in https://github.com/vllm-project/vllm/pull/24431
* [Doc]: fix 2 hyperlinks leading to Ray site after they changed Ray's doc structure by @didier-durand in https://github.com/vllm-project/vllm/pull/24438
* [Docs]add eplb_config param use docs by @lengrongfu in https://github.com/vllm-project/vllm/pull/24213
* [Model] Enable BNB support for qwen2_5_omni_thinker by @jeejeelee in https://github.com/vllm-project/vllm/pull/24420
* [Spec Decode][Benchmark] Add Spec Bench Dataset for benchmarking by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/23563
* [Spec Decode][Benchmark] Add Blitzedit dataset by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/23605
* [Model] Remove quantized mixtral by @jeejeelee in https://github.com/vllm-project/vllm/pull/24437
* [CI] Enable encoder model compilation test by @ZJY0516 in https://github.com/vllm-project/vllm/pull/24442
* [Model loader]: support multi-thread model weight loading by @BraveY in https://github.com/vllm-project/vllm/pull/23928
* [Spec Decode] Fix offline spec_decode.py by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/24257
* [Attention] FlashAttention MLA cudagraph support by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/23958
* [Bugfix] Disable the statslogger if the api_server_count is greater than 1 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/22227
* [Hardware][IBM Z] Fix Outlines Core issue for s390x by @R3hankhan123 in https://github.com/vllm-project/vllm/pull/24034
* [CI] Add nightly multiarch manifests to dockerhub by @csahithi in https://github.com/vllm-project/vllm/pull/24102
* Update reviewers for modelopt related files by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/24468
* [Bugfix][Wide EP] Fix redundant work when using DeepEP, TP Attn, and EP MoE by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/24134
* [gpt-oss] Harmony changes with container tool support by @morgendave in https://github.com/vllm-project/vllm/pull/23386
* Bump actions/setup-python from 5.4.0 to 6.0.0 by @dependabot[bot] in https://github.com/vllm-project/vllm/pull/24414
* [doc] update `vllm serve` cli args documentation by @cjackal in https://github.com/vllm-project/vllm/pull/24329
* Bump actions/stale from 9.1.0 to 10.0.0 by @dependabot[bot] in https://github.com/vllm-project/vllm/pull/24412
* Bump actions/github-script from 7.0.1 to 8.0.0 by @dependabot[bot] in https://github.com/vllm-project/vllm/pull/24413
* Move `KVTransferConfig` from `config/__init__.py` to `config/kv_transfer.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24434
* [BugFix][Model] Fix Ernie4.5-VL hanging on long inputs by @CSWYF3634076 in https://github.com/vllm-project/vllm/pull/24074
* [Flashinfer] Support Flashinfer TRTLLM FP8-qkv BF16/FP16-out Attention Kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/23647
* [Core] Use sha256 bytes instead of BlockHash to reduce GC overhead by @linzebing in https://github.com/vllm-project/vllm/pull/23673
* Add data_parallel_size to VllmConfig string representation by @Prowindy in https://github.com/vllm-project/vllm/pull/24298
* [Bugfix] Fix Apertus HF repo name by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/24447
* [Misc] Improve Worker process title and logging prefix by @22quinn in https://github.com/vllm-project/vllm/pull/22205
* [Doc] mention fpdb for multiprocess breakpoints by @mickaelseznec in https://github.com/vllm-project/vllm/pull/24452
* [Misc] Support bench serve long context by @minosfuture in https://github.com/vllm-project/vllm/pull/24373
* [Doc]: fixing typos to improve docs by @didier-durand in https://github.com/vllm-project/vllm/pull/24480
* [Performance][MM] Building the inverse permutation in O(n) time in Qwen2_5_VisionTransformer by @david6666666 in https://github.com/vllm-project/vllm/pull/24443
* [Misc] Add claude settings to gitignore by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24492
* [Misc] Add Codex settings to gitignore by @ywang96 in https://github.com/vllm-project/vllm/pull/24493
* [gpt-oss] Validate gpt-oss python tool during initialization by @heheda12345 in https://github.com/vllm-project/vllm/pull/23856
* [RL] fast weight update with zmq + ipc handles by @weixiao-huang in https://github.com/vllm-project/vllm/pull/24295
* [CI/Build][Doc] Fully deprecate old bench scripts for serving / throughput / latency by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24411
* [Compilation][WideEP] Enable Piecewise CUDAGraph for DeepEPHT by @yewentao256 in https://github.com/vllm-project/vllm/pull/24123
* [Model] Systematic support for fp32 head, pooling models part by @noooop in https://github.com/vllm-project/vllm/pull/23810
* [Bugfix] Handle the edge case in detokenizer where processed tokens contain both `stop` str and `eos` token by @dtransposed in https://github.com/vllm-project/vllm/pull/23938
* [Core] Run garbage collector after CUDA graph capture to fix throughput regression by @micah-wil in https://github.com/vllm-project/vllm/pull/24128
* [Kernels] Add Flash Linear Attention Kernels by @youkaichao in https://github.com/vllm-project/vllm/pull/24518
* [ROCm][CI/Build] Sync ROCm dockerfiles with the ROCm fork by @gshtras in https://github.com/vllm-project/vllm/pull/24279
* [Bugfix] Fix  hidden_size for multimodal classification model by @jeejeelee in https://github.com/vllm-project/vllm/pull/24501
* Extend renderer with embedding support and integrate completion endpoint by @sfeng33 in https://github.com/vllm-project/vllm/pull/24405
* [Misc] bump outlines_core to fix the version conflicts with outlines >= 1.2.0 by @serihiro in https://github.com/vllm-project/vllm/pull/24368
* [Docs] Gemma3n `transcriptions` endpoint support by @NickLucche in https://github.com/vllm-project/vllm/pull/24512
* [TPU] Fix tpu structured decoding in mixed batches by @Chenyaaang in https://github.com/vllm-project/vllm/pull/24458
* [CI] execute all piecewise compilation tests together by @ZJY0516 in https://github.com/vllm-project/vllm/pull/24502
* [Feature] Disallow FlashMLA on Blackwell by @yewentao256 in https://github.com/vllm-project/vllm/pull/24521
* [Log] Use a relative path in debug-level logs to distinguish files with identical names by @ZJY0516 in https://github.com/vllm-project/vllm/pull/23846
* [Benchmark] Update bench doc with mtbench, blazedit, spec bench by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/24450
* [Benchmark] Add option to skip oversampling in benchmark by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/24457
* [ROCm][Feature] Enable Pipeline Parallelism with Ray Compiled Graph on ROCm by @charlifu in https://github.com/vllm-project/vllm/pull/24275
* [Bugfix] Improve EPLB config validation error message by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/24524
* [Bugfix] Fix for 24530. Fix naive all2all shared expert overlap. by @bnellnm in https://github.com/vllm-project/vllm/pull/24538
* [Perf] Convert np array to torch tensor to index into block table for attn chunking by @sarckk in https://github.com/vllm-project/vllm/pull/24474
* Add @heheda12345 to CODEOWNERS of KVCacheManager related code by @heheda12345 in https://github.com/vllm-project/vllm/pull/24546
* [CI] Retry flaky fp8 cutlass mla tests by @njhill in https://github.com/vllm-project/vllm/pull/24536
* [Hardware][Apple-CPU] Enable native bfloat16 on Apple Silicon (M2 and later) by @ignaciosica in https://github.com/vllm-project/vllm/pull/24129
* [BugFix] Fix async core engine client finalizer by @njhill in https://github.com/vllm-project/vllm/pull/24540
* [CI] Adjust threshold for flaky ngram spec decoding test by @njhill in https://github.com/vllm-project/vllm/pull/24528
* [KV Connector] More async support for `get_num_new_matched_tokens` by @ApostaC in https://github.com/vllm-project/vllm/pull/23620
* [P/D] MultiConnector supports shutdown by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24425
* [BugFix][Spec Decode] Fix out-of-range index triggered by eagle3; re-enable test for LlamaForCausalLMEagle3 by @wwl2755 in https://github.com/vllm-project/vllm/pull/24392
* [gpt-oss] Cache permute indices for faster MXFP4 MoE layer loading by @frank-wei in https://github.com/vllm-project/vllm/pull/24154
* [Core] Simplify and unify mm uuid handling & auto-generated mm hash overrides processing.  by @huachenheli in https://github.com/vllm-project/vllm/pull/24271
* [Bugfix] Update Run:AI Model Streamer Loading Integration by @pwschuurman in https://github.com/vllm-project/vllm/pull/23845
* [Docs] Enable relative links in examples to function when rendered in the docs by @hmellor in https://github.com/vllm-project/vllm/pull/24041
* [docs] promo pytorch conf and ray summit by @simon-mo in https://github.com/vllm-project/vllm/pull/24562
* [Bugfix] Guard `_may_reorder_batch` for encoder-only models on CPU (#24319) by @comsky in https://github.com/vllm-project/vllm/pull/24348
* Consolidate rendering parameters into RenderConfig dataclass by @sfeng33 in https://github.com/vllm-project/vllm/pull/24543
* [Model] Limit CPU threads for image transformations in InternVL to reduce cpu contention. by @li-jinpeng in https://github.com/vllm-project/vllm/pull/24519
* [Attention] add DCP support for FLASH_ATTN_MLA backend by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/24453
* [ROCm][Bugfix] Fix Aiter RMSNorm  by @vllmellm in https://github.com/vllm-project/vllm/pull/23412
* [Docs] Improve organisation of API Reference nav by @hmellor in https://github.com/vllm-project/vllm/pull/24569
* [Docs] Document the extra memory footprint overhead when using EPLB by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/24537
* Support for NemotronH Nano VLM by @danielafrimi in https://github.com/vllm-project/vllm/pull/23644
* Feature/vit attention unification# 23880 by @baonudesifeizhai in https://github.com/vllm-project/vllm/pull/23978
* [LoRA]: Add LoRA support to Mistral's Voxtral models by @pratapyash in https://github.com/vllm-project/vllm/pull/24517
* Move `LoadConfig` from `config/__init__.py` to `config/load.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24566
* [BugFix][Multi Modal] Fix TensorSchema shape mismatch in Molmo by @wwl2755 in https://github.com/vllm-project/vllm/pull/24559
* [BugFix][easy] Fix flaky test test_gpt_oss_multi_turn_chat by @lacora in https://github.com/vllm-project/vllm/pull/24549
* [BugFix] Ensure integrity of reused CPU tensors during async scheduling by @njhill in https://github.com/vllm-project/vllm/pull/24527
* [CI/Build] split true unit tests to Entrypoints Unit Tests by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/24418
* [rocm] enable torchao quantization for rocm by @draftbk in https://github.com/vllm-project/vllm/pull/24400
* [CI] Add PPL test for generation models by @noooop in https://github.com/vllm-project/vllm/pull/24485
* [CI/Build] bump timm dependency by @dtrifiro in https://github.com/vllm-project/vllm/pull/24189
* fix some typos by @co63oc in https://github.com/vllm-project/vllm/pull/24167
* Fix Auto_Round Quatization Loading on SM75 and Lower GPUs by @RoadToNowhereX in https://github.com/vllm-project/vllm/pull/24217
* [Docs] Fix warnings in `mkdocs build` (continued) by @Zerohertz in https://github.com/vllm-project/vllm/pull/24092
* [BugFix] `python collect_env.py` and `vllm collect-env` compatibility with uv venv by @yankay in https://github.com/vllm-project/vllm/pull/24066
* [Platform] Custom ops support for LMhead and LogitsProcessor by @zzhx1 in https://github.com/vllm-project/vllm/pull/23564
* [CI] Fix tensorizer test assertion by @pwschuurman in https://github.com/vllm-project/vllm/pull/24545
* [Core] Split LoRA layers by @jeejeelee in https://github.com/vllm-project/vllm/pull/24574
* [Doc] Add documentation for GLM-4.5 series models: tool-calling and reasoning parser by @WangErXiao in https://github.com/vllm-project/vllm/pull/24589
* [Logging] allow config logging stream by @842974287 in https://github.com/vllm-project/vllm/pull/24336
* [Bugfix] fix modelopt exclude_modules name mapping by @tomeras91 in https://github.com/vllm-project/vllm/pull/24178
* [Bugfix] Fix DeepEP config for DP4TP4 by @minosfuture in https://github.com/vllm-project/vllm/pull/23619
* [Core] Support configuration parsing plugin by @charlotte12l in https://github.com/vllm-project/vllm/pull/24277
* [Misc] update log level debug to warning when process port is used by by @lengrongfu in https://github.com/vllm-project/vllm/pull/24226
* [Bugfix] Enable FP8 KV cache for FlashInfer and Triton backend on non-sm100 GPUs by @gau-nernst in https://github.com/vllm-project/vllm/pull/24577
* [CI] Fail subprocess tests with root-cause error by @njhill in https://github.com/vllm-project/vllm/pull/23795
* [v1] Add Whisper model support (encoder-decoder) by @russellb in https://github.com/vllm-project/vllm/pull/21088
* [torch.compile][ROCm][V1] Enable attention output FP8 fusion for V1 attention backends by @gshtras in https://github.com/vllm-project/vllm/pull/19767
* [gpt-oss] raise error for flashinfer backend without trtllm by @heheda12345 in https://github.com/vllm-project/vllm/pull/24482
* [Perf] Warmup FlashInfer attention during startup by @mgoin in https://github.com/vllm-project/vllm/pull/23439
* [Kernel] Flashinfer MLA (trtllm-gen) decode kernel integration by @hjjq in https://github.com/vllm-project/vllm/pull/21078
* [Misc] Make timeout passable in init_distributed_environment by @jberkhahn in https://github.com/vllm-project/vllm/pull/24522
* [Models][Quantization] Add quantization configuration update in Voxtral model by @anmarques in https://github.com/vllm-project/vllm/pull/24122
* [distributed] update known issues by @youkaichao in https://github.com/vllm-project/vllm/pull/24624
* Add @chaunceyjiang to codeowner for reasoning Reasoning and Tool parser by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/24406
* [Bug] [Spec Decode] Fix model_initialization test and mismatch in aux_hidden_layers by @wwl2755 in https://github.com/vllm-project/vllm/pull/24613
* [Ultravox] Fix Gemma instantiation, support quantization via --hf-overrides by @petersalas in https://github.com/vllm-project/vllm/pull/24131
* [Bugfix] Add missing VIT backend dispatch on CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/24623
* [BugFix] Fix pipeline parallel by @njhill in https://github.com/vllm-project/vllm/pull/24621
* [Engine][Chore] use local variable and remove output var assignment by @GuyStone in https://github.com/vllm-project/vllm/pull/24554
* Kimi K2 Fused MoE kernels Optimization configs by @samanamp in https://github.com/vllm-project/vllm/pull/24597
* Enable --profile in 'vllm bench throughput' by @tomasruizt in https://github.com/vllm-project/vllm/pull/24575
* [Core] feat: Add --safetensors-load-strategy flag for faster safetensors loading from Lustre by @shengshiqi-google in https://github.com/vllm-project/vllm/pull/24469
* [Doc]: fixing doc typos by @didier-durand in https://github.com/vllm-project/vllm/pull/24635
* [Model] New model support for Motif-1-Tiny by @ca1207 in https://github.com/vllm-project/vllm/pull/23414
* Remove redundant all gather + split by @chenxi-yang in https://github.com/vllm-project/vllm/pull/23441
* [torchao] Support quantization configs using module swap by @jerryzh168 in https://github.com/vllm-project/vllm/pull/21982
* Add the support for the qwen3 next model (a hybrid attention model). by @sighingnow in https://github.com/vllm-project/vllm/pull/24526
* [Bugfix] Fix incorrect import of CacheConfig by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/24631
* [Docs] Revise frameworks/anything-llm.md by @windsonsea in https://github.com/vllm-project/vllm/pull/24489
* [Docs] Update V1 doc to reflect whisper support by @russellb in https://github.com/vllm-project/vllm/pull/24606
* [Docs] Use 1-2-3 list for deploy steps in deployment/frameworks/ by @windsonsea in https://github.com/vllm-project/vllm/pull/24633
* [CI]Add transformers_utils to Async Engine, Inputs, Utils, Worker Test by @charlotte12l in https://github.com/vllm-project/vllm/pull/24615
* [Bugfix] Fix _synced_weight_loader by @kyuyeunk in https://github.com/vllm-project/vllm/pull/24565
* [CI] Split pooling from entrypoints Test by @noooop in https://github.com/vllm-project/vllm/pull/24632
* [Misc] Add @NickLucche to codeowners by @NickLucche in https://github.com/vllm-project/vllm/pull/24647
* [CI Failure] fix models/language/pooling/test_auto_prefix_cache_support.py by @noooop in https://github.com/vllm-project/vllm/pull/24636
* Fix typing for `safetensors_load_strategy` by @hmellor in https://github.com/vllm-project/vllm/pull/24641
* Move `LoRAConfig` from `config/__init__.py` to `config/lora.py` by @hmellor in https://github.com/vllm-project/vllm/pull/24644
* [XPU] add missing dependency tblib for XPU CI by @faaany in https://github.com/vllm-project/vllm/pull/24639
* [Docs] Fixes a typo in the qwen3next model name. by @sighingnow in https://github.com/vllm-project/vllm/pull/24654
* [build] add torch to tool.uv no-build-isolation-package by @youkaichao in https://github.com/vllm-project/vllm/pull/24303
* [Bench] Add qwen-next in benchmark_moe.py by @jeejeelee in https://github.com/vllm-project/vllm/pull/24661
* [CI] Split mteb test from Language Models Test by @noooop in https://github.com/vllm-project/vllm/pull/24634
* Allow users to specify kv cache memory size by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/21489
* [HybridKVCache][Platform] Add support_hybrid_kv_cache for platform by @MengqingCao in https://github.com/vllm-project/vllm/pull/24646
* [Bugifx] Fix qwen-next packed_modules_mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/24656
* [Docs] Add transcription support to model by @NickLucche in https://github.com/vllm-project/vllm/pull/24664
* [Doc] Fix Markdown Pre-commit Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/24670
* [Docs] Fix typos in EP deployment doc by @hmellor in https://github.com/vllm-project/vllm/pull/24669
* [VLM] Optimize GLM4.5-V-style video processing to only decode necessary frames by @Isotr0py in https://github.com/vllm-project/vllm/pull/24161
* [Kernels] Enable Torch Symmetric Memory All-Reduce By Default by @ilmarkov in https://github.com/vllm-project/vllm/pull/24111
* [Bugfix] Fix platform-specific routing in CustomOp implementations by @kzawora-intel in https://github.com/vllm-project/vllm/pull/24444
* Fix model name included in responses by @hmellor in https://github.com/vllm-project/vllm/pull/24663
* fix some typos by @co63oc in https://github.com/vllm-project/vllm/pull/24616
* [Docs] Fix formatting of transcription doc by @hmellor in https://github.com/vllm-project/vllm/pull/24676
* [VLM] Migrate remain DP-supported ViT models to use `disable_tp` by @Isotr0py in https://github.com/vllm-project/vllm/pull/24363
* [Ultravox] Use wrapped_model_config to instantiate inner model by @petersalas in https://github.com/vllm-project/vllm/pull/24679
* [Doc] Remove Useless Comments by @yewentao256 in https://github.com/vllm-project/vllm/pull/24687
* [Qwen3-Next] Add MoE Config for H200 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24688
* [BugFix] Fix tokenize asyncio task leak by @njhill in https://github.com/vllm-project/vllm/pull/24677
* update spec decode metrics to use throughput by @qandrew in https://github.com/vllm-project/vllm/pull/24127
* [Kernel][B200] `mxfp4` fused cutlass moe by @djmmoss in https://github.com/vllm-project/vllm/pull/23696
* [flashinfer] [kernel] support for fp8 kv cache for trtllm prefill attention by @mxz297 in https://github.com/vllm-project/vllm/pull/24197
* [Bugfix] Set `VLLM_ALLREDUCE_USE_SYMM_MEM` default to False by @yewentao256 in https://github.com/vllm-project/vllm/pull/24696
* [Qwen3-Next] MoE configs for H200 TP=1,2,4 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/24695
* [CI/Build] Add bc-linter to vLLM CI by @zhewenl in https://github.com/vllm-project/vllm/pull/21234
* [Qwen3-Next] Add B200 MoE configs for Qwen3-next by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/24698
* [Bugfix][Attention] Fix FlashInfer MLA block size logic by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/24692
* [Perf] Use upstream CUTLASS for SM90 Block FP8 kernel by @mgoin in https://github.com/vllm-project/vllm/pull/23280
* [Qwen3-Next] MOE configs for H100 TP4 by @heheda12345 in https://github.com/vllm-project/vllm/pull/24699
* [Doc] Clarify cudagraph capture size logic and default behavior in scheduler by @Zazzle516 in https://github.com/vllm-project/vllm/pull/18698
* [Bug] Fix Layer `weight_block_size` Assertion Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/24674
* [Startup] Make DeepGEMM warmup scale with max-num-batched-tokens by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/24693
* [V1] feat:add engine v1 tracing by @RichardoMrMu in https://github.com/vllm-project/vllm/pull/20372
* [Bugfix] fixes the causal_conv1d_update kernel update non-speculative decoding cases by @sighingnow in https://github.com/vllm-project/vllm/pull/24680

## New Contributors
* @DoubleVII made their first contribution in https://github.com/vllm-project/vllm/pull/23058
* @carlory made their first contribution in https://github.com/vllm-project/vllm/pull/23090
* @nikheal2 made their first contribution in https://github.com/vllm-project/vllm/pull/22725
* @Tialo made their first contribution in https://github.com/vllm-project/vllm/pull/23172
* @myselvess made their first contribution in https://github.com/vllm-project/vllm/pull/23084
* @yiz-liu made their first contribution in https://github.com/vllm-project/vllm/pull/23169
* @ultmaster made their first contribution in https://github.com/vllm-project/vllm/pull/22587
* @KilJaeeun made their first contribution in https://github.com/vllm-project/vllm/pull/22790
* @wzshiming made their first contribution in https://github.com/vllm-project/vllm/pull/23242
* @misrasaurabh1 made their first contribution in https://github.com/vllm-project/vllm/pull/20413
* @yannqi made their first contribution in https://github.com/vllm-project/vllm/pull/23246
* @jaredoconnell made their first contribution in https://github.com/vllm-project/vllm/pull/23306
* @paulpak58 made their first contribution in https://github.com/vllm-project/vllm/pull/22845
* @zhuangqh made their first contribution in https://github.com/vllm-project/vllm/pull/23309
* @tvalentyn made their first contribution in https://github.com/vllm-project/vllm/pull/23270
* @arjunbreddy22 made their first contribution in https://github.com/vllm-project/vllm/pull/22495
* @philipchung made their first contribution in https://github.com/vllm-project/vllm/pull/17149
* @FoolPlayer made their first contribution in https://github.com/vllm-project/vllm/pull/23241
* @namanlalitnyu made their first contribution in https://github.com/vllm-project/vllm/pull/23375
* @hickeyma made their first contribution in https://github.com/vllm-project/vllm/pull/23353
* @PapaGoose made their first contribution in https://github.com/vllm-project/vllm/pull/23337
* @bppps made their first contribution in https://github.com/vllm-project/vllm/pull/23366
* @AzizCode92 made their first contribution in https://github.com/vllm-project/vllm/pull/23416
* @fengli1702 made their first contribution in https://github.com/vllm-project/vllm/pull/22527
* @FFFfff1FFFfff made their first contribution in https://github.com/vllm-project/vllm/pull/23408
* @ayushsatyam146 made their first contribution in https://github.com/vllm-project/vllm/pull/23171
* @patemotter made their first contribution in https://github.com/vllm-project/vllm/pull/23574
* @Terrencezzj made their first contribution in https://github.com/vllm-project/vllm/pull/23584
* @Copilot made their first contribution in https://github.com/vllm-project/vllm/pull/23385
* @oneraghavan made their first contribution in https://github.com/vllm-project/vllm/pull/23630
* @lordmathis made their first contribution in https://github.com/vllm-project/vllm/pull/23634
* @OYE93 made their first contribution in https://github.com/vllm-project/vllm/pull/23565
* @TianyuLi0 made their first contribution in https://github.com/vllm-project/vllm/pull/23146
* @yuekaizhang made their first contribution in https://github.com/vllm-project/vllm/pull/23623
* @coval3nte made their first contribution in https://github.com/vllm-project/vllm/pull/23054
* @youzhedian made their first contribution in https://github.com/vllm-project/vllm/pull/23648
* @frank-wei made their first contribution in https://github.com/vllm-project/vllm/pull/23613
* @faaany made their first contribution in https://github.com/vllm-project/vllm/pull/22500
* @cndoit18 made their first contribution in https://github.com/vllm-project/vllm/pull/23718
* @rebel-hongseok made their first contribution in https://github.com/vllm-project/vllm/pull/23746
* @Hanchenli made their first contribution in https://github.com/vllm-project/vllm/pull/23713
* @Shrey1306 made their first contribution in https://github.com/vllm-project/vllm/pull/23556
* @Ithanil made their first contribution in https://github.com/vllm-project/vllm/pull/23155
* @killershrimp made their first contribution in https://github.com/vllm-project/vllm/pull/23792
* @crischeng made their first contribution in https://github.com/vllm-project/vllm/pull/23823
* @angelayi made their first contribution in https://github.com/vllm-project/vllm/pull/23349
* @jeanschmidt made their first contribution in https://github.com/vllm-project/vllm/pull/23757
* @He-Jingkai made their first contribution in https://github.com/vllm-project/vllm/pull/23829
* @aditchawdhary made their first contribution in https://github.com/vllm-project/vllm/pull/23899
* @EduardDurech made their first contribution in https://github.com/vllm-project/vllm/pull/23068
* @dubejf made their first contribution in https://github.com/vllm-project/vllm/pull/23944
* @sadeghja1070 made their first contribution in https://github.com/vllm-project/vllm/pull/23971
* @DevonPeroutky made their first contribution in https://github.com/vllm-project/vllm/pull/22769
* @hypdeb made their first contribution in https://github.com/vllm-project/vllm/pull/23802
* @DamonJiang777 made their first contribution in https://github.com/vllm-project/vllm/pull/24028
* @cberge908 made their first contribution in https://github.com/vllm-project/vllm/pull/23104
* @lkm2835 made their first contribution in https://github.com/vllm-project/vllm/pull/23918
* @nathanrchn made their first contribution in https://github.com/vllm-project/vllm/pull/24100
* @co63oc made their first contribution in https://github.com/vllm-project/vllm/pull/24071
* @divyanshsinghvi made their first contribution in https://github.com/vllm-project/vllm/pull/23907
* @biba10 made their first contribution in https://github.com/vllm-project/vllm/pull/23931
* @dongbo910220 made their first contribution in https://github.com/vllm-project/vllm/pull/20388
* @NagyGeorge made their first contribution in https://github.com/vllm-project/vllm/pull/23460
* @wdhongtw made their first contribution in https://github.com/vllm-project/vllm/pull/24214
* @bingchen-mi made their first contribution in https://github.com/vllm-project/vllm/pull/23652
* @anthonsu made their first contribution in https://github.com/vllm-project/vllm/pull/23766
* @whx-sjtu made their first contribution in https://github.com/vllm-project/vllm/pull/23332
* @pratapyash made their first contribution in https://github.com/vllm-project/vllm/pull/24231
* @samanamp made their first contribution in https://github.com/vllm-project/vllm/pull/24266
* @mohankku made their first contribution in https://github.com/vllm-project/vllm/pull/24335
* @ashwin-phadke made their first contribution in https://github.com/vllm-project/vllm/pull/24339
* @bangshengtang made their first contribution in https://github.com/vllm-project/vllm/pull/24265
* @charlotte12l made their first contribution in https://github.com/vllm-project/vllm/pull/23868
* @alhridoy made their first contribution in https://github.com/vllm-project/vllm/pull/24361
* @what-in-the-nim made their first contribution in https://github.com/vllm-project/vllm/pull/24332
* @BraveY made their first contribution in https://github.com/vllm-project/vllm/pull/23928
* @R3hankhan123 made their first contribution in https://github.com/vllm-project/vllm/pull/24034
* @csahithi made their first contribution in https://github.com/vllm-project/vllm/pull/24102
* @Prowindy made their first contribution in https://github.com/vllm-project/vllm/pull/24298
* @micah-wil made their first contribution in https://github.com/vllm-project/vllm/pull/24128
* @pwschuurman made their first contribution in https://github.com/vllm-project/vllm/pull/23845
* @comsky made their first contribution in https://github.com/vllm-project/vllm/pull/24348
* @li-jinpeng made their first contribution in https://github.com/vllm-project/vllm/pull/24519
* @baonudesifeizhai made their first contribution in https://github.com/vllm-project/vllm/pull/23978
* @lacora made their first contribution in https://github.com/vllm-project/vllm/pull/24549
* @RoadToNowhereX made their first contribution in https://github.com/vllm-project/vllm/pull/24217
* @zzhx1 made their first contribution in https://github.com/vllm-project/vllm/pull/23564
* @hjjq made their first contribution in https://github.com/vllm-project/vllm/pull/21078
* @tomasruizt made their first contribution in https://github.com/vllm-project/vllm/pull/24575
* @shengshiqi-google made their first contribution in https://github.com/vllm-project/vllm/pull/24469
* @ca1207 made their first contribution in https://github.com/vllm-project/vllm/pull/23414
* @qandrew made their first contribution in https://github.com/vllm-project/vllm/pull/24127
* @Zazzle516 made their first contribution in https://github.com/vllm-project/vllm/pull/18698
* @RichardoMrMu made their first contribution in https://github.com/vllm-project/vllm/pull/20372

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.1.1...v0.10.2rc3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.2)

---

## v0.10.1.1: v0.10.1.1
**Published:** 2025-08-20

This is a critical bugfix and security release:
* Fix CUTLASS MLA Full CUDAGraph (#23200)
* Limit HTTP header count and size (#23267): https://github.com/vllm-project/vllm/security/advisories/GHSA-rxc4-3w6r-4v47

* Do not use eval() to convert unknown types (#23266): https://github.com/vllm-project/vllm/security/advisories/GHSA-79j6-g2m3-jgfw



**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.1...v0.10.1.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.1.1)

---

## v0.10.1: v0.10.1
**Published:** 2025-08-18

## Highlights
v0.10.1 release includes 727 commits, 245 committers (105 new contributors). 

**NOTE: This release deprecates V0 FA3 support and as a result FP8 kv-cache in V0 may have issues** 

### Model Support
* **New model families**: GPT-OSS with comprehensive tool calling and streaming support (#22327, #22330, #22332, #22335, #22339, #22340, #22342), Command-A-Vision (#22660), mBART (#22883), and SmolLM3 using Transformers backend (#22665).
* **Vision-language models**: Official Eagle multimodal support with Llama4 backend (#20788), Step3 vision-language models (#21998), Gemma3n multimodal (#20495), MiniCPM-V 4.0 (#22166), HyperCLOVAX-SEED-Vision-Instruct-3B (#20931), Emu3 with Transformers backend (#21319), Intern-S1 (#21628), and Prithvi in online serving mode (#21518).
* **Enhanced existing models**: NemotronH support (#22349), Ernie 4.5 Base 0.3B model name change (#21735), GLM-4.5 series improvements (#22215), Granite models with fused MoE configurations (#21332) and quantized checkpoint loading (#22925), Ultravox support for Llama 4 and Gemma 3 backends (#17818), Mamba1 and Jamba model support in V1 (without CUDA graphs) (#21249)
* **Advanced model capabilities**: Qwen3 EPLB (#20815) and dual-chunk attention support (#21924), Qwen native Eagle3 target support (#22333).
* **Architecture expansions**: Encoder-only models without KV-cache enabling BERT-style architectures (#21270), expanded tensor parallelism support in Transformers backend (#22651), tensor parallelism for Deepseek_vl2 vision transformer (#21494), and tensor/pipeline parallelism with Mamba2 kernel for PLaMo2 (#19674).
* **V1 engine compatibility**: Extended support for additional pooling models (#21747) and Step3VisionEncoder distributed processing option (#22697).

### Engine Core
* **CUDA graph performance**: Full CUDA graph support with separate attention routines, adding FA2 and FlashInfer compatibility (#20059), plus 6% end-to-end throughput improvement from Cutlass MLA (#22763).
* **Attention system advances**: Multiple attention metadata builders per KV cache specification (#21588), tree attention backend for v1 engine (experimental) (#20401), FlexAttention encoder-only support (#22273), upgraded FlashAttention 3 with attention sink support (#22313), and multiple attention groups for KV sharing patterns (#22672).
* **Speculative decoding optimizations**: N-gram speculative decoding with single KMP token proposal algorithm (#22437), explicit EAGLE3 interface for enhanced compatibility (#22642).
* **Default behavior improvements**: Pooling models now default to chunked prefill and prefix caching (#20930), disabled chunked local attention by default for Llama4 for better performance (#21761).
* **Extensibility and configuration**: Model loader plugin system (#21067), custom operations support for FusedMoe (#22509), rate limiting with bucket algorithm for proxy server (#22643), torch.compile support for bailing MoE (#21664).
* **Performance optimizations**: Improved startup time by disabling C++ compilation of symbolic shapes (#20836), enhanced headless models for pooling in Transformers backend (#21767).

### Hardware & Performance
* **NVIDIA Blackwell (SM100) optimizations**: CutlassMLA as default backend (#21626), FlashInfer MoE per-tensor scale FP8 backend (#21458), SM90 CUTLASS FP8 GEMM with kernel tuning and swap AB support (#20396).
* **NVIDIA RTX 5090/RTX PRO 6000 (SM120) support**: Block FP8 quantization (#22131) and CUTLASS NVFP4 4-bit weights/activations support (#21309).
* **AMD ROCm platform enhancements**: Flash Attention backend for Qwen-VL models (#22069), AITER HIP block quantization kernels (#21242), reduced device-to-host transfers (#22683), and optimized kernel performance for small batch sizes 1-4 (#21350).
* **Attention and compute optimizations**: FlashAttention 3 attention sinks performance boost (#22478), Triton-based multi-dimensional RoPE replacing PyTorch implementation (#22375), async tensor parallelism for scaled matrix multiplication (#20155), optimized FlashInfer metadata building (#21137).
* **Memory and throughput improvements**: Mamba2 reduced device-to-device copy overhead (#21075), fused Triton kernels for RMSNorm (#20839, #22184), improved multimodal hasher performance for repeated image prompts (#22825), multithreaded async multimodal loading (#22710).
* **Parallelization and MoE optimizations**: Guided decoding throughput improvements (#21862), balanced expert sharding for MoE models (#21497), expanded fused kernel support for topk softmax (#22211), fused MoE for nomic-embed-text-v2-moe (#18321).
* **Hardware compatibility and kernels**: ARM CPU build fixes for systems without BF16 support (#21848), Machete memory-bound performance improvements (#21556), FlashInfer TRT-LLM prefill attention kernel support (#22095), optimized `reshape_and_cache_flash` CUDA kernel (#22036), CPU transfer support in NixlConnector (#18293).
* **Specialized CUDA kernels**: GPT-OSS activation functions (#22538), RLHF weight loading acceleration (#21164).

### Quantization
* **Advanced quantization techniques**: MXFP4 and bias support for Marlin kernel (#22428), NVFP4 GEMM FlashInfer backends (#22346), compressed-tensors mixed-precision model loading (#22468), FlashInfer MoE support for NVFP4 (#21639).
* **Hardware-optimized quantization**: Dynamic 4-bit quantization with Kleidiai kernels for CPU inference (#17112), TensorRT-LLM FP4 quantization optimized for MoE low-latency inference (#21331).
* **Expanded model quantization support**: BitsAndBytes quantization for InternS1 (#21953) and additional MoE models (#21370, #21548), Gemma3n quantization compatibility (#21974), calibration-free RTN quantization for MoE models (#20766), ModelOpt Qwen3 NVFP4 support (#20101).
* **Performance and compatibility improvements**: CUDA kernel optimization for Int8 per-token group quantization (#21476), non-contiguous tensor support in FP8 quantization (#21961), automatic detection of ModelOpt quantization formats (#22073).
* **Breaking change**: Removed AQLM quantization support (#22943) - users should migrate to alternative quantization methods.

### API & Frontend
* **OpenAI API compatibility**: Unix domain socket support for local communication (#18097), improved error response format matching upstream specification (#22099), aligned tool_choice="required" behavior with OpenAI when tools list is empty (#21052).
* **New API capabilities**: Dedicated LLM.reward interface for reward models (#21720), chunked processing for long inputs in embedding models (#22280), AsyncLLM proper response handling for aborted requests (#22283).
* **Configuration and environment**: Multiple API keys support for enhanced authentication (#18548), custom vLLM tuned configuration paths (#22791), environment variable control for logging statistics (#22905), multimodal cache size (#22441), and DeepGEMM E8M0 scaling behavior (#21968).
* **CLI and tooling improvements**: V1 API support for run-batch command (#21541), custom process naming for better monitoring (#21445), improved help display showing available choices (#21760), optional memory profiling skip for multimodal models (#22950), enhanced logging of non-default arguments (#21680).
* **Tool and parser support**: HermesToolParser for models without special tokens (#16890), multi-turn conversation benchmarking tool (#20267).
* **Distributed serving enhancements**: Enhanced hybrid distributed serving with multiple API servers in load balancing mode (#21510), request_id support for external load balancers (#21009).
* **User experience enhancements**: Improved error messaging for multimodal items (#22114), per-request pooling control via PoolingParams (#20538).

### Dependencies
* **FlashInfer updates**: Updated to v0.2.8 for improved performance (#21385), moved to optional dependency install with `pip install vllm[flashinfer]` for flexible installation (#21959).
* **Mamba SSM restructuring**: Updated to version 2.2.5 (#21421), removed from core requirements to reduce installation complexity (#22541).
* **Docker and deployment**: Docker-aware precompiled wheel support for easier containerized deployment (#21127, #22106).
* **Python package updates**: OpenAI Python dependency updated to latest version for API compatibility (#22316).
* **Dependency optimizations**: Removed xformers requirement for Mistral-format Pixtral and Mistral3 models (#21154), deprecation warnings added for old DeepGEMM version (#22194).

### V0 Deprecation
**Important**: As part of the ongoing V0 engine cleanup, several breaking changes have been introduced:
* **CLI flag updates**: Replaced `--task` with `--runner` and `--convert` options (#21470), deprecated `--disable-log-requests` in favor of `--enable-log-requests` for clearer semantics (#21739), renamed `--expand-tools-even-if-tool-choice-none` to `--exclude-tools-when-tool-choice-none` for consistency (#20544).
* **API cleanup**: Removed previously deprecated arguments and methods as part of ongoing V0 engine codebase cleanup (#21907).


## What's Changed
* Deduplicate Transformers backend code using inheritance by @hmellor in https://github.com/vllm-project/vllm/pull/21461
* [Bugfix][ROCm] Fix for warp_size uses on host by @gshtras in https://github.com/vllm-project/vllm/pull/21205
* [TPU][Bugfix] fix moe layer by @yaochengji in https://github.com/vllm-project/vllm/pull/21340
* [v1][Core] Clean up usages of `SpecializedManager` by @zhouwfang in https://github.com/vllm-project/vllm/pull/21407
* [Misc] Fix duplicate FusedMoEConfig debug messages by @njhill in https://github.com/vllm-project/vllm/pull/21455
* [Core] Support model loader plugins by @22quinn in https://github.com/vllm-project/vllm/pull/21067
* remove GLM-4 quantization wrong Code by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21435
* Replace `--expand-tools-even-if-tool-choice-none` with `--exclude-tools-when-tool-choice-none` for v0.10.0 by @okdshin in https://github.com/vllm-project/vllm/pull/20544
* [Misc] Improve comment for DPEngineCoreActor._set_cuda_visible_devices() by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21501
* [Feat] Allow custom naming of vLLM processes by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21445
* bump `flashinfer` to `v0.2.8` by @cjackal in https://github.com/vllm-project/vllm/pull/21385
* [Attention] Optimize FlashInfer MetadataBuilder Build call by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21137
* [Model] Officially support Emu3 with Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21319
* [Bugfix] Fix CUDA arch flags for MoE permute by @minosfuture in https://github.com/vllm-project/vllm/pull/21426
* [Fix] Update mamba_ssm to 2.2.5 by @elvischenv in https://github.com/vllm-project/vllm/pull/21421
* [Docs] Update Tensorizer usage documentation by @sangstar in https://github.com/vllm-project/vllm/pull/21190
* [Docs] Rewrite Distributed Inference and Serving guide by @crypdick in https://github.com/vllm-project/vllm/pull/20593
* [Bug] Fix Compressed Tensor NVFP4 `cutlass_fp4_group_mm` illegal memory access by @yewentao256 in https://github.com/vllm-project/vllm/pull/21465
* Update flashinfer CUTLASS MoE Kernel by @wenscarl in https://github.com/vllm-project/vllm/pull/21408
* [XPU] Conditionally import CUDA-specific passes to avoid import errors on xpu platform by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21036
* [P/D] Move FakeNixlWrapper to test dir by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21328
* [P/D] Support CPU Transfer in NixlConnector by @juncgu in https://github.com/vllm-project/vllm/pull/18293
* [Docs][minor] Fix broken gh-file link in distributed serving docs by @crypdick in https://github.com/vllm-project/vllm/pull/21543
* [Docs] Add Expert Parallelism Initial Documentation by @simon-mo in https://github.com/vllm-project/vllm/pull/21373
* update flashinfer to v0.2.9rc1 by @weireweire in https://github.com/vllm-project/vllm/pull/21485
* [TPU][TEST] HF_HUB_DISABLE_XET=1 the test 3. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21539
* [MoE] More balanced expert sharding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21497
* [Frontend] `run-batch` supports V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21541
* [Docs] Fix `site_url` for RunLLM by @hmellor in https://github.com/vllm-project/vllm/pull/21564
* [Bug] Fix DeepGemm Init Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/21554
* Fix GLM-4 PP Missing Layer When using with PP. by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21531
* [Kernel] adding fused_moe configs for upcoming granite4 by @bringlein in https://github.com/vllm-project/vllm/pull/21332
* [Bugfix] DeepGemm utils : Fix hardcoded type-cast by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21517
* [DP] Support api-server-count > 0 in hybrid DP LB mode by @njhill in https://github.com/vllm-project/vllm/pull/21510
* [TPU][Test] Temporarily suspend this MoE model in test_basic.py. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21560
* [Docs] Add `requirements/common.txt` to run unit tests by @zhouwfang in https://github.com/vllm-project/vllm/pull/21572
* Integrate TensorSchema with shape validation for Phi3VImagePixelInputs by @bbeckca in https://github.com/vllm-project/vllm/pull/21232
* [CI] Update CODEOWNERS for CPU and Intel GPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21582
* [Bugfix] fix modelscope snapshot_download serialization by @andyxning in https://github.com/vllm-project/vllm/pull/21536
* [Model] Support tensor parallel for timm ViT in Deepseek_vl2 by @wzqd in https://github.com/vllm-project/vllm/pull/21494
* [Model] Fix a check for None but the return value was empty list in Gemma3 MM vision_embeddings by @hfan in https://github.com/vllm-project/vllm/pull/21479
* [Misc][Tools] make max-model-len a parameter in auto_tune script by @yaochengji in https://github.com/vllm-project/vllm/pull/21321
* [CI/Build] fix cpu_extension for apple silicon by @ignaciosica in https://github.com/vllm-project/vllm/pull/21195
* [Misc] Removed undefined cmake variables MOE_PERMUTE_ARCHS by @chenyang78 in https://github.com/vllm-project/vllm/pull/21262
* [TPU][Bugfix] fix OOM issue in CI test by @yaochengji in https://github.com/vllm-project/vllm/pull/21550
* [Tests] Harden DP tests by @njhill in https://github.com/vllm-project/vllm/pull/21508
* Add H20-3e fused MoE kernel tuning configs for Qwen3-Coder-480B-A35B-Instruct by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/21598
* [Bugfix] GGUF: fix AttributeError: 'PosixPath' object has no attribute 'startswith' by @kebe7jun in https://github.com/vllm-project/vllm/pull/21579
* [Quantization] Enable BNB support for more MoE models by @jeejeelee in https://github.com/vllm-project/vllm/pull/21370
* [V1] Get supported tasks from model runner instead of model config by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21585
* [Bugfix][Logprobs] Fix logprobs op to support more backend by @MengqingCao in https://github.com/vllm-project/vllm/pull/21591
* [Model] Fix Ernie4.5MoE e_score_correction_bias parameter by @xyxinyang in https://github.com/vllm-project/vllm/pull/21586
* [MODEL] New model support for naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B by @bigshanedogg in https://github.com/vllm-project/vllm/pull/20931
* [Frontend] Add request_id to the Request object so they can be controlled better via external load balancers by @kouroshHakha in https://github.com/vllm-project/vllm/pull/21009
* [Model] Replace Mamba2 RMSNorm Gated with Fused Triton Kernel by @cyang49 in https://github.com/vllm-project/vllm/pull/20839
* [ROCm][AITER] Enable fp8 kv cache on rocm aiter backend. by @fsx950223 in https://github.com/vllm-project/vllm/pull/20295
* [Kernel] Improve machete memory bound perf by @czhu-cohere in https://github.com/vllm-project/vllm/pull/21556
* Add support for Prithvi in Online serving mode by @mgazz in https://github.com/vllm-project/vllm/pull/21518
* [CI] Unifying Dockerfiles for ARM and X86 Builds by @kebe7jun in https://github.com/vllm-project/vllm/pull/21343
* [Docs] add auto-round quantization readme  by @wenhuach21 in https://github.com/vllm-project/vllm/pull/21600
* [TPU][Test] Rollback PR-21550. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21619
* Add Unsloth to RLHF.md by @danielhanchen in https://github.com/vllm-project/vllm/pull/21636
* [Perf] Cuda Kernel for Int8 Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/21476
* Add interleaved RoPE test for Llama4 (Maverick) by @sarckk in https://github.com/vllm-project/vllm/pull/21478
* [Bugfix] Fix sync_and_slice_intermediate_tensors by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21537
* [Bugfix] Always set RAY_ADDRESS for Ray actor before spawn by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21540
* [TPU] Update ptxla nightly version to 20250724 by @yaochengji in https://github.com/vllm-project/vllm/pull/21555
* [Feature] Add support for MoE models in the calibration-free RTN-based quantization by @sakogan in https://github.com/vllm-project/vllm/pull/20766
* [Model] Ultravox: Support Llama 4 and Gemma 3 backends by @farzadab in https://github.com/vllm-project/vllm/pull/17818
* [Docs] add offline serving multi-modal video input expamle Qwen2.5-VL by @david6666666 in https://github.com/vllm-project/vllm/pull/21530
* Correctly kill vLLM processes after finishing serving benchmarks by @huydhn in https://github.com/vllm-project/vllm/pull/21641
* [Bugfix] Fix isinstance check for tensor types in _load_prompt_embeds to use dtype comparison by @Mitix-EPI in https://github.com/vllm-project/vllm/pull/21612
* [TPU][Test] Divide TPU v1 Test into 2 parts. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21431
* Support Intern-S1 by @lvhan028 in https://github.com/vllm-project/vllm/pull/21628
* [Misc] remove unused try-except in pooling config check by @reidliu41 in https://github.com/vllm-project/vllm/pull/21618
* [Take 2] Correctly kill vLLM processes after benchmarks by @huydhn in https://github.com/vllm-project/vllm/pull/21646
* Migrate AriaImagePixelInputs to TensorSchema for shape validation by @bbeckca in https://github.com/vllm-project/vllm/pull/21620
* Migrate AyaVisionImagePixelInputs to TensorSchema for shape validation by @bbeckca in https://github.com/vllm-project/vllm/pull/21622
* [Bugfix] Investigate Qwen2-VL failing test by @Isotr0py in https://github.com/vllm-project/vllm/pull/21527
* Support encoder-only models without KV-Cache by @maxdebayser in https://github.com/vllm-project/vllm/pull/21270
* [Bug] Fix `has_flashinfer_moe` Import Error when it is not installed by @yewentao256 in https://github.com/vllm-project/vllm/pull/21634
* [Misc] Improve memory profiling debug message by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21429
* [BugFix] Fix shared storage connector load kv only load attention layer by @david6666666 in https://github.com/vllm-project/vllm/pull/21428
* [Refactor] Remove `moe_align_block_size_triton` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21335
* [Bugfix][Apple Silicon] fix missing symbols when build from source on Mac with Apple Silicon by @zhouyeju in https://github.com/vllm-project/vllm/pull/21380
* [CI/Build][Doc] Move existing benchmark scripts in CI/document/example to vllm bench CLI by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21355
* [NVIDIA] Explicitly disable shuffled weights for flashinfer blockscale moe fp8 kernels by @kaixih in https://github.com/vllm-project/vllm/pull/21411
* Remove xformers requirement for Mistral-format Pixtral and Mistral3 by @wenchen76 in https://github.com/vllm-project/vllm/pull/21154
* support `torch.compile` for bailing moe by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/21664
* Migrate Blip2ImagePixelInputs and Blip2ImageEmbeddingInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21656
* Migrate DeepseekVL2ImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21658
* Migrate FuyuImagePatchInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21662
* Migrate ChameleonImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21657
* [VLM] Support HF format Phi-4-MM model by @Isotr0py in https://github.com/vllm-project/vllm/pull/17121
* Handle non-serializable objects in vllm bench by @huydhn in https://github.com/vllm-project/vllm/pull/21665
* [CI/Build][Doc] Clean up more docs that point to old bench scripts by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21667
* Refactor: Remove numpy dependency from LoggingStatLogger by @skyloevil in https://github.com/vllm-project/vllm/pull/20529
* [Misc] add default value for file pattern arg by @andyxning in https://github.com/vllm-project/vllm/pull/21659
* Migrate Florence2ImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21663
* [VLM] Add video support for Intern-S1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/21671
* [Refactor] Refactor MOE NVFP4 Code Base: ModelOpt + Compressed Tensor by @yewentao256 in https://github.com/vllm-project/vllm/pull/21631
* Fix CUDA permute/unpermute for use with DeepGemm Moe by @CalebDu in https://github.com/vllm-project/vllm/pull/17934
* [Misc] Refactor vllm config str by @andyxning in https://github.com/vllm-project/vllm/pull/21666
* [Attention] Make CutlassMLA the default backend for SM100 (blackwell) by @alexm-redhat in https://github.com/vllm-project/vllm/pull/21626
* [Deprecation][2/N] Replace `--task` with `--runner` and `--convert` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21470
* Fix typo for limit-mm-per-prompt in docs by @joa-stdn in https://github.com/vllm-project/vllm/pull/21697
* Fix GLM tool parser by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21668
* [Misc]  Add fused_moe configs for Qwen3-Coder-480B-A35B-Instruct-FP8 by @jeejeelee in https://github.com/vllm-project/vllm/pull/21700
* [V1] Exception Handling when Loading KV Cache from Remote Store by @liuyumoye in https://github.com/vllm-project/vllm/pull/21534
* [Model] Support TP/PP/mamba2 kernel for PLaMo2 by @Alnusjaponica in https://github.com/vllm-project/vllm/pull/19674
* [FEAT] [ROCm] [AITER]: Add AITER HIP block quant kernel by @tjtanaa in https://github.com/vllm-project/vllm/pull/21242
* Migrate Gemma3ImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21676
* Migrate Glm4vImageInputs, Glm4vVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21678
* Migrate GLMVImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21679
* Migrate GraniteSpeechAudioInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21682
* Migrate Idefics3ImagePixelInputs and Idefics3ImageEmbeddingInputs to â€¦ by @bbeckca in https://github.com/vllm-project/vllm/pull/21683
* [Bugfix] [issue-21565] Fix the incompatibility issue with stream and named function calling when Thinking is disabled by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21573
* [bugfix] fix profile impact benchmark results by @lengrongfu in https://github.com/vllm-project/vllm/pull/21507
* [Bugfix] Fix shape checking for Fuyu by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21709
* [Bugfix] fix max-file-size type from str to int by @andyxning in https://github.com/vllm-project/vllm/pull/21675
* [BugFix] Fix ChunkedLocalAttention when the hybrid kv-cache is disabled by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21707
* [v1][mamba] Added mamba_type into MambaSpec by @Josephasafg in https://github.com/vllm-project/vllm/pull/21715
* Migrate KeyeImageInputs and KeyeVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21686
* [Model] Prioritize Transformers fallback over suffix matching by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21719
* [feature] add log non default args in LLM by @lengrongfu in https://github.com/vllm-project/vllm/pull/21680
* [Bugfix] Fix Ernie4_5_MoeForCausalLM shared experts by @jeejeelee in https://github.com/vllm-project/vllm/pull/21717
* [Bugfix] Fix environment variable setting in CPU Dockerfile by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21730
* [Bugfix] Fix glm4.1v video_grid_thw tensor shape scheme by @Isotr0py in https://github.com/vllm-project/vllm/pull/21744
* [PD] let p2p nccl toy proxy handle /chat/completions by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21734
* [`Ernie 4.5`] Name Change for Base 0.3B Model by @vasqu in https://github.com/vllm-project/vllm/pull/21735
* [Bugfix] Improve JSON extraction in LlamaToolParser by @key4ng in https://github.com/vllm-project/vllm/pull/19024
* [Docs] Add revision date to rendered docs by @hmellor in https://github.com/vllm-project/vllm/pull/21752
* [Bugfix]check health for engine core process exiting unexpectedly by @wuhang2014 in https://github.com/vllm-project/vllm/pull/21728
* [Bugfix][CI/Build] Update peft version in test requirement by @Isotr0py in https://github.com/vllm-project/vllm/pull/21729
* [Logs] Change flashinfer sampler logs to once by @mgoin in https://github.com/vllm-project/vllm/pull/21759
* [Misc] Reduce logs for model resolution by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21765
* [Bugfix] Mistral crashes on tool with no description by @HugoMichard in https://github.com/vllm-project/vllm/pull/21167
* [CI/Build] Fix plugin tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21758
* [XPU] IPEX-optimized Punica Wrapper on XPU by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21703
* [Bugfix] Fix granite speech shape validation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21762
* [P/D] Log warnings related to prefill KV expiry by @njhill in https://github.com/vllm-project/vllm/pull/21753
* Use `metavar` to list the choices for a CLI arg when custom values are also accepted by @hmellor in https://github.com/vllm-project/vllm/pull/21760
* update flashinfer to v0.2.9rc2 by @weireweire in https://github.com/vllm-project/vllm/pull/21701
* [AMD][BugFix] Fix omission  of wvSplitK kernel for small batch sizes (1-4) due to torch.compile by @rasmith in https://github.com/vllm-project/vllm/pull/21350
* [Bug] Enforce contiguous input for `dynamic_scaled_fp8_quant` and `static_scaled_fp8_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21773
* [AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure by @houseroad in https://github.com/vllm-project/vllm/pull/21647
* Revert "[V1] Exception Handling when Loading KV Cache from Remote Store" by @KuntaiDu in https://github.com/vllm-project/vllm/pull/21778
* [Bugfix] DeepGEMM is not enabled on B200 due to `_lazy_init()` by @smarterclayton in https://github.com/vllm-project/vllm/pull/21472
* [Feat]: Add support for Dynamic Quant 4 bit CPU kleidiai kernels by @nikhil-arm in https://github.com/vllm-project/vllm/pull/17112
* [Perf] Disable chunked local attention by default with llama4 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21761
* [Kernel] SM90 CUTLASS FP8 GEMM: add support for swap AB + kernel tuning by @LyrisZhong in https://github.com/vllm-project/vllm/pull/20396
* [Docs] Minimize spacing for supported_hardware.md table by @mgoin in https://github.com/vllm-project/vllm/pull/21779
* [Refactor] Merge Compressed Tensor FP8 `CompressedTensorsW8A8Fp8MoEMethod` and `CompressedTensorsW8A8Fp8MoECutlassMethod` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21775
* [CI] Parallelize Kernels MoE Test by @mgoin in https://github.com/vllm-project/vllm/pull/21764
* skip fusedmoe layer for start_load_kv by @calvin0327 in https://github.com/vllm-project/vllm/pull/21378
* [AMD][CI/Build][Bugfix] Guarding CUDA specific functions by ifndef ROCM by @gshtras in https://github.com/vllm-project/vllm/pull/21766
* Migrate InternVLImageInputs and InternVLVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21684
* [Misc] Rework process titles by @njhill in https://github.com/vllm-project/vllm/pull/21780
* [Doc] Link to RFC for pooling optimizations by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21806
* [Model]: Fused MoE for nomic-embed-text-v2-moe by @Isotr0py in https://github.com/vllm-project/vllm/pull/18321
* [V0 deprecation] Guided decoding by @rzabarazesh in https://github.com/vllm-project/vllm/pull/21347
* [Model] Refactor JambaForCausalLM by @jeejeelee in https://github.com/vllm-project/vllm/pull/21394
* [Docs] Fix the outdated URL for installing from vLLM binaries by @yankay in https://github.com/vllm-project/vllm/pull/21523
* [KVCache] Make KVCacheSpec hashable by @heheda12345 in https://github.com/vllm-project/vllm/pull/21791
* [Doc] Update compatibility matrix for pooling and multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21831
* [Bugfix] VLLM_V1 supports passing other compilation levels by @zou3519 in https://github.com/vllm-project/vllm/pull/19340
* [Docs] Merge design docs for a V1 only future by @hmellor in https://github.com/vllm-project/vllm/pull/21832
* [TPU] Add an optimization doc on TPU by @bvrockwell in https://github.com/vllm-project/vllm/pull/21155
* [Bugfix]fix mixed bits and visual language model quantization in AutoRound by @wenhuach21 in https://github.com/vllm-project/vllm/pull/21802
* [Bugfix] Fix workspace buffer None issue for Flashinfer TRTLLM Backend by @elvischenv in https://github.com/vllm-project/vllm/pull/21525
* [Docs] use `uv` in GPU installation docs by @davidxia in https://github.com/vllm-project/vllm/pull/20277
* [Doc] Add FusedMoE Modular Kernel Documentation by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21623
* [Doc] update Contributing page's testing section by @davidxia in https://github.com/vllm-project/vllm/pull/18272
* Add `flashinfer_python` to CUDA wheel requirements by @mgoin in https://github.com/vllm-project/vllm/pull/21389
* docker: docker-aware precompiled wheel support by @dougbtv in https://github.com/vllm-project/vllm/pull/21127
* Revert "[AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure (#21647)" by @gshtras in https://github.com/vllm-project/vllm/pull/21850
* [BugFix] Fix interleaved sliding window not set for Gemma3n by @sarckk in https://github.com/vllm-project/vllm/pull/21863
* [ci] add b200 test placeholder by @simon-mo in https://github.com/vllm-project/vllm/pull/21866
* [ci] mark blackwell test optional for now by @simon-mo in https://github.com/vllm-project/vllm/pull/21878
* [Bugfix] Correct max tokens for non-contiguous embeds by @milesial in https://github.com/vllm-project/vllm/pull/21798
* [v1][attention] Support Hybrid Allocator + FlashInfer by @heheda12345 in https://github.com/vllm-project/vllm/pull/21412
* [Docs] Switch to better markdown linting pre-commit hook by @hmellor in https://github.com/vllm-project/vllm/pull/21851
* [DOC] Fix path of v1 related figures by @heheda12345 in https://github.com/vllm-project/vllm/pull/21868
* [Docs] Update docker.md with HF_TOKEN, new model, and podman fix by @mgoin in https://github.com/vllm-project/vllm/pull/21856
* Expose PyTorch profiler configuration to environment variables by @Csrayz in https://github.com/vllm-project/vllm/pull/21803
* [Bugfix] Fix shape mismatch assertion error when loading Gemma3n model with BitsAndBytes quantization by @sydarb in https://github.com/vllm-project/vllm/pull/21808
* [Bugfix] Fix comment typo of get_num_common_prefix_blocks() by @MingzhenHan in https://github.com/vllm-project/vllm/pull/21827
* [Bugfix] Actually disable processing cache when API server is scaled out by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21839
* [Perf] Using `__nv_fp8_e4m3` instead of `c10::e4m3` for `per_token_group_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21867
* [Frontend] Add LLM.reward specific to reward models by @noooop in https://github.com/vllm-project/vllm/pull/21720
* [XPU] use `ZE_AFFINITY_MASK` for device select on xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/21815
* Add @sighingnow as maintainer of qwen's related files. by @sighingnow in https://github.com/vllm-project/vllm/pull/21895
* [CI/Build] Fix pre-commit failure in docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21897
* [Docs] Expand introduction to Ray in Multi-node deployment section by @crypdick in https://github.com/vllm-project/vllm/pull/21584
* Update vLLM Benchmark Suite for Xeon based on 0.9.2 release  by @louie-tsai in https://github.com/vllm-project/vllm/pull/21486
* [Misc] Remove redundant config definitions by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21891
* [Doc] Update Intern-S1 info  by @jeejeelee in https://github.com/vllm-project/vllm/pull/21908
* [CI] rollback lint-and-deploy pipeline using amd machine by @kebe7jun in https://github.com/vllm-project/vllm/pull/21912
* [Tests] Fixing bug inside MultiModalProfiler. by @shenoyvvarun in https://github.com/vllm-project/vllm/pull/21842
* [Model] Remove DSV2 unused code by @jeejeelee in https://github.com/vllm-project/vllm/pull/21903
* [benchmark] add max-concurrency in result table by @panpan0000 in https://github.com/vllm-project/vllm/pull/21095
* [Doc] Update partial support by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21916
* [Docs] Fix the example code of streaming chat completions in reasoning by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21825
* Add @patrickvonplaten as maintainer of mistral's related files. by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/21928
* [Hardware][CPU] Build fix for ARM without BF16 by @ericcurtin in https://github.com/vllm-project/vllm/pull/21848
* [Feature][EPLB] Add eplb support for Qwen3 by @aladerran in https://github.com/vllm-project/vllm/pull/20815
* [Doc] Remove vLLM prefix and add citation for PagedAttention by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21910
* [Bugfix] we should use metavar is not choices by @lengrongfu in https://github.com/vllm-project/vllm/pull/21902
* [Feature] Support multiple api keys in server by @Yanpas in https://github.com/vllm-project/vllm/pull/18548
* [misc] skip p2p check by default by @youkaichao in https://github.com/vllm-project/vllm/pull/21904
* [Test] Add Benchmark and Unit Test for `per_token_group_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21860
* [CI/Build] Only run markdownlint in CI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21892
* Reduce time wasted in GitHub Actions using `concurrency` by @hmellor in https://github.com/vllm-project/vllm/pull/21919
* [Misc] Improve code readability of KVCacheManager by @tanruixiang in https://github.com/vllm-project/vllm/pull/21673
* [NVIDIA] Fix Llama4 Scout FP4 functionality issues by @nvpohanh in https://github.com/vllm-project/vllm/pull/21499
* [Docs] Reduce the size of the built docs by @hmellor in https://github.com/vllm-project/vllm/pull/21920
* [Bugfix] Fix OOM tests in initialization test by @Isotr0py in https://github.com/vllm-project/vllm/pull/21921
* [Bugfix] Fix multi-api server not working for text models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21933
* Override attention metadata for fast prefill in some KV sharing setups by @sarckk in https://github.com/vllm-project/vllm/pull/21590
* [Bugfix] Fix TypeError in scheduler when comparing mixed request_id types by @chi2liu in https://github.com/vllm-project/vllm/pull/21816
* [CI/Build] Fix registry tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21934
* [Bugfix] SharedStorage Connector for V1 PD multimodal by @fake0fan in https://github.com/vllm-project/vllm/pull/21611
* feat(distributed): add `get_required_kvcache_layout` class method to kv connector api by @wxsms in https://github.com/vllm-project/vllm/pull/20433
* [TPU] Support Pathways in vLLM by @wenxindongwork in https://github.com/vllm-project/vllm/pull/21417
* [Misc] Support more collective_rpc return types by @njhill in https://github.com/vllm-project/vllm/pull/21845
* For VLLM_USE_PRECOMPILED, only compiled .so files should be extracted by @dougbtv in https://github.com/vllm-project/vllm/pull/21964
* [Misc] Use dracut on CentOS and skip clone if repo exists for EP kernel installation by @minosfuture in https://github.com/vllm-project/vllm/pull/21635
* [Feature] Add async tensor parallelism for scaled mm by @cascade812 in https://github.com/vllm-project/vllm/pull/20155
* [Bugfix] Fix None value handling in trace span creation for cancelled requests by @br4mm in https://github.com/vllm-project/vllm/pull/20272
* [Core] Move EngineCoreRequest to Request conversion out of EngineCore by @linzebing in https://github.com/vllm-project/vllm/pull/21627
* [Example] Add `async_llm_streaming.py` example for AsyncLLM streaming in python by @mgoin in https://github.com/vllm-project/vllm/pull/21763
* [Bugfix] Relax lang pin for voxtral by @sanchit-gandhi in https://github.com/vllm-project/vllm/pull/21833
* [UX] Rename CUTLASS_MLA_VLLM_V1 to CUTLASS_MLA by @mgoin in https://github.com/vllm-project/vllm/pull/21966
* [Misc] Expand SUPPORTED_HIDDEN_SIZES  for DeepEP low-latency kernels by @jeejeelee in https://github.com/vllm-project/vllm/pull/21818
* [CI Bugfix] Fix CI OOM for `test_shared_storage_connector_hashes` by @mgoin in https://github.com/vllm-project/vllm/pull/21973
* [Bugfix]: fix metadata file copy in test_sharded_state_loader by @andyxning in https://github.com/vllm-project/vllm/pull/21830
* [Deprecation] Remove deprecated args and methods by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21907
* [CI/Build] get rid of unused VLLM_FA_CMAKE_GPU_ARCHES by @dtrifiro in https://github.com/vllm-project/vllm/pull/21599
* [Model][CI] Let more pooling models support v1 by @noooop in https://github.com/vllm-project/vllm/pull/21747
* [BugFix] Fix case where `collective_rpc` returns `None` by @njhill in https://github.com/vllm-project/vllm/pull/22006
* [NVIDIA] Add SM100 Flashinfer MoE per tensor scale fp8 backend by @amirkl94 in https://github.com/vllm-project/vllm/pull/21458
* [Model] Add step3 vl by @Oliver-ss in https://github.com/vllm-project/vllm/pull/21998
* [ez] Remove a trailing space from compilation/decorators.py by @zhxchen17 in https://github.com/vllm-project/vllm/pull/22028
* fix(setup): improve precompiled wheel setup for Docker builds by @dougbtv in https://github.com/vllm-project/vllm/pull/22025
* Removing amdproduction Tests by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/22027
* Update torch_xla pin to 20250730 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21956
* [Meta] Official Eagle mm support, first enablement on llama4 by @morgendave in https://github.com/vllm-project/vllm/pull/20788
* [Misc] Add unit tests for chunked local attention by @sarckk in https://github.com/vllm-project/vllm/pull/21692
* [Bugfix] Fix MTP weight loading  by @benchislett in https://github.com/vllm-project/vllm/pull/21941
* Add FlashInfer allreduce RMSNorm Quant fusion by @ilmarkov in https://github.com/vllm-project/vllm/pull/21069
* [Feature] Add Flashinfer MoE Support for Compressed Tensor NVFP4 by @yewentao256 in https://github.com/vllm-project/vllm/pull/21639
* Add DeepGEMM to Dockerfile in vllm-base image by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/21533
* Move flashinfer-python to optional extra `vllm[flashinfer]` by @mgoin in https://github.com/vllm-project/vllm/pull/21959
* [Refactor] Remove Duplicate `per_block_cast_to_fp8`, Remove Dependencies of DeepGEMM by @yewentao256 in https://github.com/vllm-project/vllm/pull/21787
* [Bugfix] Fix: Fix multi loras with tp >=2 and LRU cache by @charent in https://github.com/vllm-project/vllm/pull/20873
* [Misc] Automatically resolve HF processor init kwargs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22005
* [BugFix] fix: aot passes kvcache dtype information by @mickaelseznec in https://github.com/vllm-project/vllm/pull/19750
* [Model] [Quantization] Support quantization for Gemma3n by @kylesayrs in https://github.com/vllm-project/vllm/pull/21974
* [Doc] Add Voxtral to Supported Models page by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22059
* Update sampling_metadata.py by @Aviadr-neureality in https://github.com/vllm-project/vllm/pull/21937
* [Doc] Fix a syntax error of example code in structured_outputs.md by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/22045
* [Bugfix] Disable multi-modal preprocessor cache for DP by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21896
* [Core] Avoid repeated len(block_token_ids) check in hash_request_tokens by @linzebing in https://github.com/vllm-project/vllm/pull/21781
* [Frontend] Align tool_choice="required" behavior with OpenAI when tools is empty by @n0gu-furiosa in https://github.com/vllm-project/vllm/pull/21052
* Revert precompile wheel changes by @simon-mo in https://github.com/vllm-project/vllm/pull/22055
* [Doc] Add example for Step3-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/22061
* [Bugfix] Add log prefix in non-dp mode engine core by @wuhang2014 in https://github.com/vllm-project/vllm/pull/21889
* [Misc] Remove upper bound in openai package version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22060
* [Doc] Added warning of speculating with draft model by @david6666666 in https://github.com/vllm-project/vllm/pull/22047
* [Quantization] Enable BNB support for InternS1 by @jeejeelee in https://github.com/vllm-project/vllm/pull/21953
* Revert "Update sampling_metadata.py (#21937)" by @hmellor in https://github.com/vllm-project/vllm/pull/22088
* [Speculative Decoding] Add `speculators` config support by @dsikka in https://github.com/vllm-project/vllm/pull/21345
* [BUG] [ROCm] Fix import bug on ROCm by @tjtanaa in https://github.com/vllm-project/vllm/pull/22083
* Fix `get_kwargs` for case where type hint is `list[Union[str, type]]` by @hmellor in https://github.com/vllm-project/vllm/pull/22016
* [Bugfix] Check NVIDIA artifactory is accessible before using flashinfer cubin kernels by @mgoin in https://github.com/vllm-project/vllm/pull/21893
* feat(multimodal): Add customizable background color for RGBA to RGB conversion by @ahengljh in https://github.com/vllm-project/vllm/pull/22052
* [Bugfix][PD] set max_completion_tokens=1 if req has this value by @Abirdcfly in https://github.com/vllm-project/vllm/pull/21841
* [Refactor] Fix Compile Warning #1444-D by @yewentao256 in https://github.com/vllm-project/vllm/pull/21462
* [BugFix] Update AttnFusionPass cache key by @zou3519 in https://github.com/vllm-project/vllm/pull/21947
* [BugFix] Don't change title of top-level process by @njhill in https://github.com/vllm-project/vllm/pull/22032
* [Docs] use `uv` in CPU installation docs by @davidxia in https://github.com/vllm-project/vllm/pull/22089
* Deprecate `--disable-log-requests` and replace with `--enable-log-requests` by @hmellor in https://github.com/vllm-project/vllm/pull/21739
* Improve documentation of `ModelConfig.try_get_generation_config` to prevent future confusion by @hmellor in https://github.com/vllm-project/vllm/pull/21526
* [Bugfix] Fix glm4.1v video inference issue by @Isotr0py in https://github.com/vllm-project/vllm/pull/22067
* [Bugfix] fix when skip tokenizer init by @lengrongfu in https://github.com/vllm-project/vllm/pull/21922
* security policy: take 1 by @sidhpurwala-huzaifa in https://github.com/vllm-project/vllm/pull/21119
* [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21837
* Enable headless models for pooling in the Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21767
* [Misc] Minor enhancement of benchmark_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/22068
* Fix pre-commit failure for SECURTIY.md by @mgoin in https://github.com/vllm-project/vllm/pull/22102
* [compile][startup] Disable C++ compilation of symbolic shapes by @anijain2305 in https://github.com/vllm-project/vllm/pull/20836
* Introduce RayPPCommunicator for ray-based PP by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21660
* Add lora test for tp>1 case for TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21970
* [BugFix] Harden distributed DP startup by @njhill in https://github.com/vllm-project/vllm/pull/21538
* [CI] Initial tests for SM100 Blackwell runner by @mgoin in https://github.com/vllm-project/vllm/pull/21877
* [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel by @yewentao256 in https://github.com/vllm-project/vllm/pull/22036
* feat: Add Support GPTQ Quantization MOE on ROCM vllm serve by @JartX in https://github.com/vllm-project/vllm/pull/21733
* [V1][CUDA] Full cudagraph support for FlashInfer by @fhl2000 in https://github.com/vllm-project/vllm/pull/21367
* [Model] Qwen2.5 VL SiLU-and-Mul by @vllmellm in https://github.com/vllm-project/vllm/pull/22066
* [Misc] `VLLM_TARGET_DEVICE.lower()` by @NickLucche in https://github.com/vllm-project/vllm/pull/22101
* [Misc] DeepGemmExperts : Avoid JIT generation in the hot-path by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21955
* [Speculators][Speculative Decoding] Add Qwen Eagle3 Support by @dsikka in https://github.com/vllm-project/vllm/pull/21835
* [BugFix] Improve internal DP load balancing by @njhill in https://github.com/vllm-project/vllm/pull/21617
* [Test] Add Unit Test for Batched DeepGEMM by @yewentao256 in https://github.com/vllm-project/vllm/pull/21559
* [Attention][DBO] Add support for "splitting" the CommonAttentionMetadata by @SageMoore in https://github.com/vllm-project/vllm/pull/21153
* [FEAT][ROCm] Enable running Flash Attention as ViT attn backend for Qwen-VL models on ROCm platform. by @vllmellm in https://github.com/vllm-project/vllm/pull/22069
* [Misc] Getting and passing ray runtime_env to workers by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22040
* Fix test_kv_sharing_fast_prefill flakiness by @sarckk in https://github.com/vllm-project/vllm/pull/22038
* [Bugfix] Mamba2 remove bugged initial state condition in chunk scan by @cyang49 in https://github.com/vllm-project/vllm/pull/22034
* docs: remove deprecated disable-log-requests flag by @ywang96 in https://github.com/vllm-project/vllm/pull/22113
* [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20000
* for glm-4.1V update by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22000
* [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead by @cyang49 in https://github.com/vllm-project/vllm/pull/21075
* [Frontend] Improve error message for too many mm items by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22114
* [V1] [Hybrid] Validate compatibility of attention backend batch reordering at init time by @tdoublep in https://github.com/vllm-project/vllm/pull/21557
* [xpu]support moe models on XPU platform by @yma11 in https://github.com/vllm-project/vllm/pull/21643
* Revert "[compile][startup] Disable C++ compilation of symbolic shapes" by @xiszishu in https://github.com/vllm-project/vllm/pull/22122
* [Misc] Bump ray to 2.48.0 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22123
* [Fix] Fix llama4 modelopt weight loading error by @jiahanc in https://github.com/vllm-project/vllm/pull/22107
* [Misc] Add tensor schema test coverage for multimodal models by @Isotr0py in https://github.com/vllm-project/vllm/pull/21754
* [Benchmark] Support ready check timeout in `vllm bench serve` by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21696
* Support CUTLASS NVFP4 (w4a4) for Blackwell Geforce GPUs (SM120) by @LopezCastroRoberto in https://github.com/vllm-project/vllm/pull/21309
* [Misc] update doc comment for send by @andyxning in https://github.com/vllm-project/vllm/pull/22026
* [executor] feat: add supports_pp attr to executors by @eric-haibin-lin in https://github.com/vllm-project/vllm/pull/21786
* [V1] [P/D] Refactor KV Connector Path by @sdavidbd in https://github.com/vllm-project/vllm/pull/21980
* [Responses API] Disable response store by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22137
* [CI/Build][Bugfix] Fix Qwen2.5 tests in CPU CI via fallback silu_and_mul to torch native implementation by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/22145
* Add chat doc in quick start by @TankNee in https://github.com/vllm-project/vllm/pull/21213
* fuse fp32 for GLM-4.5 e_score_correction_bias by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22143
* [Bugfix] Fix failing multimodal standard test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22153
* Use `aiohttp` connection pool for benchmarking by @eicherseiji in https://github.com/vllm-project/vllm/pull/21981
* [fix] fix correct assertion syntax error in attention utils. by @skyloevil in https://github.com/vllm-project/vllm/pull/22154
* [RLHF] Fix torch.dtype not serializable in example by @22quinn in https://github.com/vllm-project/vllm/pull/22158
* [PD] add test for chat completions endpoint by @Abirdcfly in https://github.com/vllm-project/vllm/pull/21925
* remove duplicate code within cleanup_dist_env_and_memory by @andyxning in https://github.com/vllm-project/vllm/pull/22147
* Add tree attention backend for v1 (part 1) by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/20401
* [refactor] improve ConstantList exception specificity by @skyloevil in https://github.com/vllm-project/vllm/pull/22156
* Remove index_put from MM embeddings merging by @chenxi-yang in https://github.com/vllm-project/vllm/pull/22105
* [CI Bugfix] Fix wNa16 kernel not found for test_shared_storage_connector_hashes by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/22163
* [Misc] Modify the organization of GLM series  by @jeejeelee in https://github.com/vllm-project/vllm/pull/22171
* [feat] move WEIGHT_SCALE_SUPPORTED into raise block to accelerate RLHF weight loading by @weixiao-huang in https://github.com/vllm-project/vllm/pull/21164
* [Bugfix] Fix failing GGUF models test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22174
* [Sampler] Support returning all logprobs or logits by @22quinn in https://github.com/vllm-project/vllm/pull/21792
* [Doc] Update pooling model docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22186
* Fix Arcee model weight loading: Add custom load_weights by @alyosha-swamy in https://github.com/vllm-project/vllm/pull/21725
* [Responses API] Ignore `store=True` and process the request by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22185
* [Bug] Update auto_tune.sh to separate benchmarking and profiling. by @ericehanley in https://github.com/vllm-project/vllm/pull/21629
* [Bugfix][V1][P/D]Fix the uneven polling issue in the toy proxy for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/21819
* [NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading by @nvpohanh in https://github.com/vllm-project/vllm/pull/22073
* [Bugfix] V1 Fix the cursor leakage issue during request scheduling. by @CLFutureX in https://github.com/vllm-project/vllm/pull/21173
* Revert "[Bugfix] V1 Fix the cursor leakage issue during request scheduling." by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22223
* [V1] reduce block size for tree attention correctness test to fix 'ouâ€¦ by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/22207
* [V0 deprecation][P/D] Deprecate v0 `KVConnectorBase` code (1/2) by @lk-chen in https://github.com/vllm-project/vllm/pull/21785
* [FEAT] Refactor ROPE into module by @tjtanaa in https://github.com/vllm-project/vllm/pull/22192
* [ROCm][Bugfix] Compilation passes fix by @gshtras in https://github.com/vllm-project/vllm/pull/22202
* self.gate dtype update for GLM-4.5 by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22203
* [Log] DeepGEMM Update Log for Unaligned Problem Size by @yewentao256 in https://github.com/vllm-project/vllm/pull/22208
* fix: kimi_k2 return empty tool call list by @tlipoca9 in https://github.com/vllm-project/vllm/pull/22149
* [Misc] Remove pass_config from CompilationConfig dump_json excluded by @elvischenv in https://github.com/vllm-project/vllm/pull/21911
* [Doc] add backend to doc string of initialize_model_parallel by @andyxning in https://github.com/vllm-project/vllm/pull/22142
* [Misc] log more detailed message for ensure_model_parallel_initialized by @andyxning in https://github.com/vllm-project/vllm/pull/22144
* Optimize configuration access with LRU cache in custom ops by @skyloevil in https://github.com/vllm-project/vllm/pull/22204
* [Bugfix] Misaligned params in TreeAttentionImpl by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22226
* [UX] Fail if an invalid attention backend is specified by @mgoin in https://github.com/vllm-project/vllm/pull/22217
* [Core] Factor out common logic for MM budget calculation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22228
* [Model] Pooling model activation supports per request control by PoolingParams by @noooop in https://github.com/vllm-project/vllm/pull/20538
* [Docs][TPU] Highlight TPU Software version selection by @NickLucche in https://github.com/vllm-project/vllm/pull/22242
* Migrate KimiVLImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21769
* [Feature] Non-contiguous Support for FP8 Quantization by @yewentao256 in https://github.com/vllm-project/vllm/pull/21961
* [NVIDIA] Support Flashinfer TRT-LLM Prefill Attention Kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/22095
* [Misc] correct static type check for GroupCoordinator by @andyxning in https://github.com/vllm-project/vllm/pull/21946
* [V0 Deprecation][TPU] Remove V1 flag check from tests by @NickLucche in https://github.com/vllm-project/vllm/pull/22248
* Use UV_LINK_MODE=copy in Dockerfile to avoid hardlink fail by @mgoin in https://github.com/vllm-project/vllm/pull/22128
* [CI/Build] Update flashinfer to 0.2.9 by @mgoin in https://github.com/vllm-project/vllm/pull/22233
* [Refactor] Remove Unused Environment Variable `VLLM_NO_DEPRECATION_WARNING` by @yewentao256 in https://github.com/vllm-project/vllm/pull/22199
* [V1] port xformers backend to v1 by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/21342
* [bugfix] fix blackwell deepep installation by @youkaichao in https://github.com/vllm-project/vllm/pull/22255
* [CI][TPU] Fix docker clean up by @lsy323 in https://github.com/vllm-project/vllm/pull/22271
* [Bugfix] Remove faulty test for oot attention backend by @mgoin in https://github.com/vllm-project/vllm/pull/22286
* [Bugfix] Fix 3D input passed into cutlass_scaled_mm by @mgoin in https://github.com/vllm-project/vllm/pull/22278
* [Bugfix] Fix MoE BNB version by @jeejeelee in https://github.com/vllm-project/vllm/pull/22260
* [Perf] Parallelize fill_bitmask to accelerate high-throughput guided decoding by @benchislett in https://github.com/vllm-project/vllm/pull/21862
* [Bugfix] Skip dead and non-GPU nodes for Ray DP engine allocation by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22275
* [Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/22264
* Upgrade FA3 for attention sink by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22313
* Increase openai-python version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22316
* Add attention sink in attention backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22320
* Update transformers to `v4.55` by @hmellor in https://github.com/vllm-project/vllm/pull/21931
* Add GPT-OSS model code and config [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22327
* [ROCm] Add attention sink to use_rocm_custom_paged_attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22329
* [GptOss] Add GptOss reasoning parser to support structure output by @heheda12345 in https://github.com/vllm-project/vllm/pull/22322
* [gpt-oss] flashinfer attention sink init by @zyongye in https://github.com/vllm-project/vllm/pull/22330
* [gpt-oss] Add openai-harmony as default dependency by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22332
* [Misc] Clean up duplicated hf overrides by @Isotr0py in https://github.com/vllm-project/vllm/pull/22311
* [gpt-oss] Add Tool/ConversationContext classes and harmony_utils by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22340
* [gpt-oss] add model to supported models doc by @ywang96 in https://github.com/vllm-project/vllm/pull/22336
* [gpt-oss] Support chat completion api by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22342
* [Minor] Fix type  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22347
* [BugFix] Fix FA2 RuntimeError when sinks is provided by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22365
* add the codes to check AMD Instinct GPU number by @zhangnju in https://github.com/vllm-project/vllm/pull/22367
* [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22368
* [Bugfix] Make condition in triton kernel constexpr by @gshtras in https://github.com/vllm-project/vllm/pull/22370
* [gpt-oss] Add loop for built-in tool call by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22374
* [gpt-oss] attention sink init fix gemini by @zyongye in https://github.com/vllm-project/vllm/pull/22335
* [gpt-oss] flashinfer mxfp4 by @zyongye in https://github.com/vllm-project/vllm/pull/22339
* [v1] - Mamba1 Attention Metadata by @Josephasafg in https://github.com/vllm-project/vllm/pull/21249
* [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/22399
* [gpt-oss] add demo tool server by @heheda12345 in https://github.com/vllm-project/vllm/pull/22393
* [gpt-oss] fix model config with hf_config by @zyongye in https://github.com/vllm-project/vllm/pull/22401
* Fix trtllm-gen attention env and add attention sink by @IwakuraRein in https://github.com/vllm-project/vllm/pull/22378
* Update `flashinfer-python==0.2.10` by @mgoin in https://github.com/vllm-project/vllm/pull/22389
* [model] Support MiniCPM-V 4.0 by @tc-mb in https://github.com/vllm-project/vllm/pull/22166
* Support encoder_only attention for FlexAttention by @maxdebayser in https://github.com/vllm-project/vllm/pull/22273
* [Attention] Support multiple attention metadata builders per kv_cache_spec  + proper local attention no hybrid kv cache fix by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21588
* [XPU]Fix `flash_attn_varlen_func` interface on xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/22350
* [Qwen3] Enable dual-chunk-attention support for Qwen3 models. by @sighingnow in https://github.com/vllm-project/vllm/pull/21924
* [Bugfix] Fix wrong method name in Intern-S1 image processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22417
* Use float32 for test_completion.py by @mgoin in https://github.com/vllm-project/vllm/pull/22385
* [Bugfix]: Fix the streaming output for function calls in the minimax by @qscqesze in https://github.com/vllm-project/vllm/pull/22015
* [Bugfix] Add proper comparison for package versions by @syedmba in https://github.com/vllm-project/vllm/pull/22314
* Update `hf_xet` pin to resolve hangs by @hmellor in https://github.com/vllm-project/vllm/pull/22356
* Optimize logger init performance by using module-level constants by @skyloevil in https://github.com/vllm-project/vllm/pull/22373
* preload heavy modules when mp method is forkserver by @lionelvillard in https://github.com/vllm-project/vllm/pull/22214
* [gpt-oss] Convert user input to harmony format by @heheda12345 in https://github.com/vllm-project/vllm/pull/22402
* [Bugfix] EPLB load statistics problem by @david6666666 in https://github.com/vllm-project/vllm/pull/22167
* [CI] Skip the pooling models that do not support transformers v4.55 by @noooop in https://github.com/vllm-project/vllm/pull/22411
* [Bench] Split serve.py:main into async/async versions by @lk-chen in https://github.com/vllm-project/vllm/pull/22405
* [Model] Switch to Fused RMS norm in Qwen2.5_VL model. by @vllmellm in https://github.com/vllm-project/vllm/pull/22184
* [Frontend] Update OpenAI error response to upstream format by @msanft in https://github.com/vllm-project/vllm/pull/22099
* [Misc] Support routing logic simulation by @minosfuture in https://github.com/vllm-project/vllm/pull/21990
* feat: Add --enable-log-outputs flag for logging model generations by @mizadri in https://github.com/vllm-project/vllm/pull/20707
* [Docs] Add missing dependency for docs build by @hmellor in https://github.com/vllm-project/vllm/pull/22435
* Add H20-3e fused MoE kernel tuning configs for GLM-4.5 by @JaceyShao in https://github.com/vllm-project/vllm/pull/22433
* [Misc] Enhance code formatting in mxfp4.py  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22423
* [Doc] Fix link to prefix caching design by @sarckk in https://github.com/vllm-project/vllm/pull/22384
* [Docs] Factor out troubleshooting to its own guide; add section for Ray Observability by @crypdick in https://github.com/vllm-project/vllm/pull/21578
* [Doc] update docs for nightly benchmarks by @andrewkchan in https://github.com/vllm-project/vllm/pull/12022
* [Docs] Update features/disagg_prefill, add v1 examples and development by @david6666666 in https://github.com/vllm-project/vllm/pull/22165
* [Core] Store only the keys for multi-modal data in P0 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22198
* [Bugfix] Add missing `packed_modules_mapping` to `DeepseekV2ForCausalLM` by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/22352
* [Tool] Fix auto tool call by @heheda12345 in https://github.com/vllm-project/vllm/pull/22434
* [gpt-oss] Generate ResponseOutputItem from Harmony Message by @heheda12345 in https://github.com/vllm-project/vllm/pull/22410
* Fix pre-commit error in main by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22462
* [Core] Simplify mm processing cache by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22457
* [Frontend] Use engine argument to control MM cache size by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22441
* Remove `from_dict` from `SpeculativeConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/22451
* [Misc] normalize multiprocessing Queue usage by @andyxning in https://github.com/vllm-project/vllm/pull/22371
* [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine by @tjtanaa in https://github.com/vllm-project/vllm/pull/21496
* [PERF] Use pybase64 to more quickly decode prompt embeddings by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/22469
* Add ModelOpt Qwen3 nvfp4 support by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/20101
* Support Tensorrt-LLM MoE fp4 for low-latency by @wenscarl in https://github.com/vllm-project/vllm/pull/21331
* Fix Flashinfer CUTLASS MOE Allgather by @wenscarl in https://github.com/vllm-project/vllm/pull/21963
* [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) by @0xjunhao in https://github.com/vllm-project/vllm/pull/22131
* [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/22065
* not tie_word_embeddings for glm-4.5 and glm-4.5v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22460
* Optimize MiniCPMO mask creation with vectorized implementation by @skyloevil in https://github.com/vllm-project/vllm/pull/22464
* Fix pre-commit by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22487
* [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 by @nvpohanh in https://github.com/vllm-project/vllm/pull/22426
* [Doc] Sleep mode documentation by @iAmir97 in https://github.com/vllm-project/vllm/pull/22310
* [bench] Fix benchmark/serve.py to ignore unavailable results by @lk-chen in https://github.com/vllm-project/vllm/pull/22382
* [CI/Build] Fix multimodal tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22491
* [Misc] Begin deprecation of `get_tensor_model_*_group` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22494
* [Misc] fix openai version by @lengrongfu in https://github.com/vllm-project/vllm/pull/22485
* [BugFix] Don't cancel asyncio tasks directly from destructors by @njhill in https://github.com/vllm-project/vllm/pull/22476
* [Docs] Improve API docs (+small tweaks) by @hmellor in https://github.com/vllm-project/vllm/pull/22459
* Remove exception for Python 3.8 typing from linter by @hmellor in https://github.com/vllm-project/vllm/pull/22506
* [gpt-oss] triton kernel mxfp4 by @zyongye in https://github.com/vllm-project/vllm/pull/22421
* [Benchmark] Add benchmark tool for multi turn conversations by @pliops-daniels in https://github.com/vllm-project/vllm/pull/20267
* [gpt-oss] guard import when triton kernel is not installed by @zyongye in https://github.com/vllm-project/vllm/pull/22529
* [Docs] Rename â€œDistributed inference and servingâ€ to â€œParallelism & Scalingâ€ by @crypdick in https://github.com/vllm-project/vllm/pull/22466
* [gpt-oss] Support tool call and implement MCP tool server by @heheda12345 in https://github.com/vllm-project/vllm/pull/22427
* [BugFix] Fix IMA FlashMLA full cuda-graph and DP + Update FlashMLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21691
* [Misc] DeepGEMM : Avoid JIT generation in the hot-path by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/22215
* [Bugfix] Update FA commit hash by @tdoublep in https://github.com/vllm-project/vllm/pull/22546
* Skip Qwen 1 in CI because remote code is no longer compatible with Transformers by @hmellor in https://github.com/vllm-project/vllm/pull/22536
* [Docs] fix broken links in metrics.md by @GuyStone in https://github.com/vllm-project/vllm/pull/22315
* [Frontend] Add unix domain socket support by @yyweiss in https://github.com/vllm-project/vllm/pull/18097
* Extract `CompilationConfig` from `config.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22524
* Drop flaky test_healthcheck_response_time by @russellb in https://github.com/vllm-project/vllm/pull/22539
* [XPU] upgrade torch 2.8 on for XPU by @jikunshang in https://github.com/vllm-project/vllm/pull/22300
* [BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D by @Pradyun92 in https://github.com/vllm-project/vllm/pull/22317
* [Bugfix] Fix ModernBert cuda graph capturing in v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/21901
* Implicit language-model-only mode via limit-mm-per-prompt by @ywang96 in https://github.com/vllm-project/vllm/pull/22299
* [Doc] Add usage of implicit text-only mode  by @ywang96 in https://github.com/vllm-project/vllm/pull/22561
* Remove mamba_ssm from vLLM requirements; install inside test container using `--no-build-isolation` by @tdoublep in https://github.com/vllm-project/vllm/pull/22541
* [Log] Add Warning for Deprecation of DeepGEMM old version by @yewentao256 in https://github.com/vllm-project/vllm/pull/22194
* [V1] [Hybrid] Support Minimax-Text-01 in V1  by @tdoublep in https://github.com/vllm-project/vllm/pull/22151
* v1: Pass KVConnectorOutput to scheduler-side by @orozery in https://github.com/vllm-project/vllm/pull/22157
* [Misc] Use config definitions from Transformers library by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21913
* Fix loading of quantized BigCode models by @eldarkurtic in https://github.com/vllm-project/vllm/pull/22463
* [TPU] Add support for online w8a8 quantization by @kyuyeunk in https://github.com/vllm-project/vllm/pull/22425
* [ROCm][Misc] Rename the context_len to seq_len in ROCm custom paged attention kernel by @charlifu in https://github.com/vllm-project/vllm/pull/22097
* [Bugfix] Fix failing GPT-OSS initialization test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22557
* [Bugfix] Fix CI moe kernel failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/22556
* Update docs for Minimax-Text support by @tdoublep in https://github.com/vllm-project/vllm/pull/22562
* GLM-4.5V with new class name at transformers by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22520
* [CI] [Hybrid] Speed up hybrid models test by removing large models  by @tdoublep in https://github.com/vllm-project/vllm/pull/22563
* [Docs] Reduce noise in docs and `--help` from the JSON tip by @hmellor in https://github.com/vllm-project/vllm/pull/22567
* Move `ParallelConfig` from `config/__init__.py` to `config/parallel.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22565
* [Model] Gemma3n MM by @NickLucche in https://github.com/vllm-project/vllm/pull/20495
* [Bugfix] Fix basic models tests hanging due to mm processor creation by @Isotr0py in https://github.com/vllm-project/vllm/pull/22571
* [FEAT] [Performance] Add triton mrope to replace the torch code path by @tjtanaa in https://github.com/vllm-project/vllm/pull/22375
* [V1] [Hybrid] Enable Full CUDA Graph (decode-only) for Mamba layers by @tdoublep in https://github.com/vllm-project/vllm/pull/21401
* [oss] Init gpt-oss bf16 support by @jeejeelee in https://github.com/vllm-project/vllm/pull/22508
* [Config] add "qwen" as a native eagle3 target supported model by @lec77 in https://github.com/vllm-project/vllm/pull/22333
* Improve fast_topk function with type hints and documentation by @skyloevil in https://github.com/vllm-project/vllm/pull/22530
* [TPU] kv cache update kernel doesn't need to be padded slices to multiple of num_slices_per_block by @yaochengji in https://github.com/vllm-project/vllm/pull/22394
* Refactor sliding window configuration to Transformers best practice by @hmellor in https://github.com/vllm-project/vllm/pull/21927
* [Minor] Fix pre-commit error on main by @Isotr0py in https://github.com/vllm-project/vllm/pull/22579
* [Misc] code clean duplicate set_current_vllm_config in _set_vllm_config by @andyxning in https://github.com/vllm-project/vllm/pull/22566
* [Doc] Fix API doc link in side navigation by @22quinn in https://github.com/vllm-project/vllm/pull/22585
* [Misc] Further refine type annotations in parallel state by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22499
* [Docs] Fix warnings in docs build by @hmellor in https://github.com/vllm-project/vllm/pull/22588
* [Misc] Replace flaky image urls in pixtral test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22574
* Move `CacheConfig` from `config/__init__.py` to `config/cache.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22586
* [doc] add beijing meetup links by @youkaichao in https://github.com/vllm-project/vllm/pull/22596
* [doc] add alibaba cloud as sponsor by @youkaichao in https://github.com/vllm-project/vllm/pull/22597
* [Bugfix][Kernel] Support partial rotary embedding for MRoPE triton kernel by @Isotr0py in https://github.com/vllm-project/vllm/pull/22593
* Fix(benchmarks): allow multiple mm contents in OpenAI Chat Completion Benchmarks by @h-brenoskuk in https://github.com/vllm-project/vllm/pull/22534
* Migrate LlavaNextImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21774
* Remove redundant row_indices unsqueeze operation in MiniCPMO by @skyloevil in https://github.com/vllm-project/vllm/pull/22528
* Fix TensorSchema validation test for symbolic dims by @bbeckca in https://github.com/vllm-project/vllm/pull/22366
* enable Docker-aware precompiled wheel setup by @dougbtv in https://github.com/vllm-project/vllm/pull/22106
* Migrate LlavaNextVideoPixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21843
* Migrate LlavaImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21770
* [CI/Build] Fix tensorizer test for load_format change by @22quinn in https://github.com/vllm-project/vllm/pull/22583
* [BugFix] Fix KVConnectorOutput TPU breakage by @njhill in https://github.com/vllm-project/vllm/pull/22598
* [Misc][gpt-oss] Add rules to label gpt-oss related PRs by @draftbk in https://github.com/vllm-project/vllm/pull/22600
* [Misc][gpt-oss] guard import when triton kernel when not up to date  by @zhewenl in https://github.com/vllm-project/vllm/pull/22584
* [BugFix] Fix logits repetition penalty cuda check by @PicoCreator in https://github.com/vllm-project/vllm/pull/22592
* [ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module. by @vllmellm in https://github.com/vllm-project/vllm/pull/22521
* Support token_type_ids in V1 with less code changes by @maxdebayser in https://github.com/vllm-project/vllm/pull/21985
* [Misc] benchmark_moe supports expert parallel by @jeejeelee in https://github.com/vllm-project/vllm/pull/22251
* [BUGFIX] KeyError 'layers.14.mlp.gate.g_idx' for Qwen3-MoE with GPTQ on ROCm by @JartX in https://github.com/vllm-project/vllm/pull/22017
* [Docs] Add comprehensive CLI reference for all large `vllm` subcommands by @hmellor in https://github.com/vllm-project/vllm/pull/22601
* [Misc] Move tensor schema tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22612
* [Misc] Move jsontree to utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22622
* [Model] NemotronH Support  by @danielafrimi in https://github.com/vllm-project/vllm/pull/22349
* Document aarch64 CPU support works by @ericcurtin in https://github.com/vllm-project/vllm/pull/22646
* [Misc] Further clean up some redundant config definitions by @Isotr0py in https://github.com/vllm-project/vllm/pull/22649
* [Feature] Add `VLLM_USE_DEEP_GEMM_E8M0` Env to Control E8M0 Scale by @yewentao256 in https://github.com/vllm-project/vllm/pull/21968
* fix: NIXL connector transfers partial block to pass full multi-modal context by @GuanLuo in https://github.com/vllm-project/vllm/pull/21074
* [Model] Pooling models default to using chunked prefill & prefix caching if supported. by @noooop in https://github.com/vllm-project/vllm/pull/20930
* [CI/Build] Skip Mllama HF runner tests with Transformers v4.55.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/22659
* [BugFix] [Spec Decode] Remove LlamaForCausalLMEagle3 to fix CI by @22quinn in https://github.com/vllm-project/vllm/pull/22611
* [CI] Skip Tree Attn Test in `test_max_len.py` to unblock CI by @tjtanaa in https://github.com/vllm-project/vllm/pull/22664
* Support more parallel styles in Transformers backend TP by @hmellor in https://github.com/vllm-project/vllm/pull/22651
* [gpt-oss] Support streaming in response API by @heheda12345 in https://github.com/vllm-project/vllm/pull/22431
* [gpt-oss] Add test for response API + harmony (but skipped) by @heheda12345 in https://github.com/vllm-project/vllm/pull/22554
* Enable 4bit bnb prequant MOE by @py-andy-c in https://github.com/vllm-project/vllm/pull/21548
* Re-enable Xet on TPU tests now that `hf_xet` has been updated by @hmellor in https://github.com/vllm-project/vllm/pull/22666
* Upgrade FlashInfer to v0.2.11 by @nvpohanh in https://github.com/vllm-project/vllm/pull/22613
* [CI Failure] Use float32 for tests/entrypoints/openai/test_audio.py by @mgoin in https://github.com/vllm-project/vllm/pull/22686
* [CI] Increase timeout for test_completion_with_image_embeds by @mgoin in https://github.com/vllm-project/vllm/pull/22670
* Migrate MiniCPMVImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21939
* [gpt-oss] Fix mxfp4 support by @heheda12345 in https://github.com/vllm-project/vllm/pull/22700
* [gpt-oss] Small bug fixes for frontend by @heheda12345 in https://github.com/vllm-project/vllm/pull/22512
* Fix passing `SpeculativeConfig` from the CLI by @hmellor in https://github.com/vllm-project/vllm/pull/22652
* [Doc] Added unmentioned required option "method" in the usage of EAGLE-3 based models by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21737
* [doc] Update x86 CPU-inference installation doc to reflect optionality of AVX512f  by @sooraj-satheesh in https://github.com/vllm-project/vllm/pull/22707
* [Bugfix] Fix ModernBert load & Enable sliding window attention for bidirectional attention. by @noooop in https://github.com/vllm-project/vllm/pull/22637
* Move `SchedulerConfig` from `config/__init__.py` to `config/scheduler.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22626
* [DOC] update v1_guide with INTEL HW by @xuechendi in https://github.com/vllm-project/vllm/pull/22679
* [New Model] Support Command-A-Vision by @dongluw in https://github.com/vllm-project/vllm/pull/22660
* [V0] Correct CUDA Graph capture for encoder-decoder models by @Sugar-zsg in https://github.com/vllm-project/vllm/pull/22630
* [Bugfix] Fix erroneous randomly generated cases in bad word testing by @phantomlei3 in https://github.com/vllm-project/vllm/pull/22170
* Fix: AWQ Marlin get_quant_method does not recognize "modules_to_not_convert" by @Jun-Howie in https://github.com/vllm-project/vllm/pull/21888
* [Bugfix] Mamba2 SSD varlen bug fix initstates decay, improve test, assert chunk pwr 2 by @RishiAstra in https://github.com/vllm-project/vllm/pull/21783
* [LMCache][Example] Align the PYTHONHASHSEED for prefillers and decoders for KV chunks hashing by @zejunchen-zejun in https://github.com/vllm-project/vllm/pull/21161
* [Misc] remove GH discussions link by @jeejeelee in https://github.com/vllm-project/vllm/pull/22722
* [gpt-oss] Enable gpt-oss on ampere by @zyongye in https://github.com/vllm-project/vllm/pull/22714
* [Docs] Improve docs navigation by @hmellor in https://github.com/vllm-project/vllm/pull/22720
* [BugFix][Nixl][PD] Fix heterogenous TP by @NickLucche in https://github.com/vllm-project/vllm/pull/22663
* Officially support SmolLM3 using the Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/22665
* [CI Failure] fix tests/entrypoints/openai/test_skip_tokenizer.py by @noooop in https://github.com/vllm-project/vllm/pull/22708
* Fix Llama4 FlashInfer FP4 MoE issues by @nvpohanh in https://github.com/vllm-project/vllm/pull/22511
* [Bugfix][CI] Fix `test_remote_decode_lifecycle.py::test_short_prompt_lifecycle` by @NickLucche in https://github.com/vllm-project/vllm/pull/22727
* [Benchmark] Fix terminal colors in benchmark_serving_multi_turn (python 3.12) by @pliops-daniels in https://github.com/vllm-project/vllm/pull/22730
* Add: `SupportsEagle3` interface for explicit EAGLE3 support by @rahul-tuli in https://github.com/vllm-project/vllm/pull/22642
* Add more test scenario for tensor schema by @teekenl in https://github.com/vllm-project/vllm/pull/22733
* [Chore] Update CODEOWNERS to include @yewentao256 for CUDA kernels, attention backends, quantization, and related tests by @yewentao256 in https://github.com/vllm-project/vllm/pull/22741
* [Kernel][AMD] Avoid D2H copy and cumsum kernel by @mxz297 in https://github.com/vllm-project/vllm/pull/22683
* [CI][Nixl] Check kv cache layout during handshake by @NickLucche in https://github.com/vllm-project/vllm/pull/22745
* Fix torch version check for SM100 mxfp4  by @zifeitong in https://github.com/vllm-project/vllm/pull/22535
* [Misc] parametrize 'dtype' in test_flash_mla by @RUTHLESS-BOT in https://github.com/vllm-project/vllm/pull/22641
* [Bugfix] Bump DeepGEMM Version to Fix SMXX Layout Issues by @frankwang28 in https://github.com/vllm-project/vllm/pull/22606
* [Docs] Hide the navigation and toc sidebars on home page by @hmellor in https://github.com/vllm-project/vllm/pull/22749
* Fix Transformers backend tensor parallel for multimodal models by @hmellor in https://github.com/vllm-project/vllm/pull/22673
* [Model] Decouple glm4v by @jeejeelee in https://github.com/vllm-project/vllm/pull/22751
* Add hardware plugins to installation doc by @mgoin in https://github.com/vllm-project/vllm/pull/22732
* [V0 Deprecation] Remove multi-step scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22138
* [Misc] Remove tests/multi_step/__init__.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22778
* [V0 Deprecation] Remove args for multi-step scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22779
* Fix cuda illegal mem access with Llama4 TP8 + rms_norm custom op by @nvpohanh in https://github.com/vllm-project/vllm/pull/22701
* [Bugfix] Fix default enable for CUTLASS MLA on SM100 by @mgoin in https://github.com/vllm-project/vllm/pull/22738
* Force TRTLLM attention for gpt-oss on SM100 by @mgoin in https://github.com/vllm-project/vllm/pull/22678
* Remove unneeded ROCm platform import when using CUDA by @mgoin in https://github.com/vllm-project/vllm/pull/22765
* [Bug] Fix Unexpected Keyword Argument 'w1_bias' by @yewentao256 in https://github.com/vllm-project/vllm/pull/22757
* [Perf] Support topk softmax fused kernel for broader num_experts by @shixianc in https://github.com/vllm-project/vllm/pull/22211
* [gpt-oss] upgrade gpt-oss to v0.0.3 and add version check by @heheda12345 in https://github.com/vllm-project/vllm/pull/22768
* [Model] Add option to run Step3VisionEncoder in DP by @zzh142857 in https://github.com/vllm-project/vllm/pull/22697
* [Model] Add missing prefix to glm4_1v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22716
* [Bugfix] Fix Nemotron VL image processing by @ducviet00 in https://github.com/vllm-project/vllm/pull/22739
* [Doc] Add max_lora_rank configuration guide by @chi2liu in https://github.com/vllm-project/vllm/pull/22782
* [V1] Add tree drafting tests for eagle spec decoding by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/22705
* [Platform] Custom ops support for FusedMoe by @wangxiyuan in https://github.com/vllm-project/vllm/pull/22509
* [Frontend] Add chunked processing to handle long inputs in embedding models by @x22x22 in https://github.com/vllm-project/vllm/pull/22280
* [FEATURE] support custom vllm tuned config path by @vermouth1992 in https://github.com/vllm-project/vllm/pull/22791
* [Nixl][CI] Fix tests by @NickLucche in https://github.com/vllm-project/vllm/pull/22806
* [Bugfix][mamba] Fix type annotation of Mamba2Metadata by @heheda12345 in https://github.com/vllm-project/vllm/pull/22787
* Remove unnecessary CUDA sync of qwen image and video preprocess by @cyyever in https://github.com/vllm-project/vllm/pull/22792
* Fix GGUF loader for Qwen3 MoE. by @Gh0u1L5 in https://github.com/vllm-project/vllm/pull/22785
* [Frontend] Multithreaded async multimodal load_bytes by @milesial in https://github.com/vllm-project/vllm/pull/22710
* [Core] Use individual MM items in P0/P1 cache and model runner by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22570
* [Misc] clear and separate error messages for input too long and input + max-tokens too long by @ywang96 in https://github.com/vllm-project/vllm/pull/22803
* [Bugfix] Fix MiniCPMV Image input inference failed by @jio-H in https://github.com/vllm-project/vllm/pull/22813
* [CI/Build] Update VLM common tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22841
* [CI] Fix `tests/v1/e2e/test_kv_sharing_fast_prefill.py` import on test by @NickLucche in https://github.com/vllm-project/vllm/pull/22815
* [CI/Build] Fix param mismatch in `test_eagle_correctness` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22847
* [CI/Build] Skip gpt_big model test because of broken HF model by @Isotr0py in https://github.com/vllm-project/vllm/pull/22848
* [ROCm][Bugfix] Fix compilation error in topk softmax fused kernel by @kliuae in https://github.com/vllm-project/vllm/pull/22819
* Move checklist in PR template by @ProExpertProg in https://github.com/vllm-project/vllm/pull/22852
* [Core] [N-gram SD Optimization][1/n] Propose tokens with a single KMP by @Jialin in https://github.com/vllm-project/vllm/pull/22437
* [CI/Build] Increase pooling tolerance to pass CI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22844
* [CI][Entrypoints]: add filter to generation to filter out invalid tool calls by @wseaton in https://github.com/vllm-project/vllm/pull/22826
* [CI] Fix `tests/distributed/test_ca_buffer_sharing.py` by @ilmarkov in https://github.com/vllm-project/vllm/pull/22849
* [CI] remove flaky v0 test by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/22864
* vLLM Benchmark suite improvement by @louie-tsai in https://github.com/vllm-project/vllm/pull/22119
* [Bugfix] Fix `PixtralHFImagePixelInputs` dynamic shape check by @Isotr0py in https://github.com/vllm-project/vllm/pull/22827
* [BugFix] Threadsafe close async zmq sockets by @njhill in https://github.com/vllm-project/vllm/pull/22877
* Remove Phi 4 Flash configuration workaround by @hmellor in https://github.com/vllm-project/vllm/pull/22723
* [Bugfix] Add reset prefix cache for online serving by @iAmir97 in https://github.com/vllm-project/vllm/pull/22726
* [Doc] fix dead link by @dtrifiro in https://github.com/vllm-project/vllm/pull/22898
* [CI] Re-enable transcriptions `test_long_audio_request` by @NickLucche in https://github.com/vllm-project/vllm/pull/22890
* [Perf] Dont create unnecessary pooling params by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22876
* [Model] Modify the gate implementation of glm4_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/22832
* [Bugfix] Replace custom Encoding class with BatchEncoding in MistralTokenizer by @ZJY0516 in https://github.com/vllm-project/vllm/pull/22786
* [Bugfix] Fix parsing of `--disable-mm-preprocessor-cache` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22909
* [CI] [Hybrid]  Bump min transformers version for Bamba and Jamba by @tdoublep in https://github.com/vllm-project/vllm/pull/22908
* [Kernel] [Quantization] Add MXFP4 and bias support for marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/22428
* docs: update fastsafetensors usage instructions by @NirLevy98 in https://github.com/vllm-project/vllm/pull/22891
* [CI] Temporarily disable flaky test  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22930
* [Kernel] Add nvfp4 gemm flashinfer backends by @nvjullin in https://github.com/vllm-project/vllm/pull/22346
* [Quantization]: Support compressed-tensors mixed-precision model loading by @dsikka in https://github.com/vllm-project/vllm/pull/22468
* [Core] Return final response for aborted requests from `AsyncLLM.generate` by @njhill in https://github.com/vllm-project/vllm/pull/22283
* [BugFix] Fix initial DP request load imbalance by @njhill in https://github.com/vllm-project/vllm/pull/22910
* [Bugfix] use flash attn on sm90 by @zyongye in https://github.com/vllm-project/vllm/pull/22933
* [Kernel]  Add cuda kernel for gpt_oss activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/22538
* Revert "[Kernel]  Add cuda kernel for gpt_oss activation" by @simon-mo in https://github.com/vllm-project/vllm/pull/22948
* [BugFix][KVConn] Fix use of `get_required_kvcache_layout` by @njhill in https://github.com/vllm-project/vllm/pull/22734
* [BugFix] Fix port lookup in internal DP LB tests by @njhill in https://github.com/vllm-project/vllm/pull/22252
* [CI Perf] Prune tests in `tests/kernels/quantization/` by @mgoin in https://github.com/vllm-project/vllm/pull/22942
* [CI Perf] Prune tests in `tests/kernels/moe/` by @mgoin in https://github.com/vllm-project/vllm/pull/22939
* [CI Perf] Prune tests in `tests/kernels/attention/` by @mgoin in https://github.com/vllm-project/vllm/pull/22936
* refactor: Change scaling factors calculation for flashinfer FusedMoE by @amirkl94 in https://github.com/vllm-project/vllm/pull/22812
* [Feature] Full Cuda Graph Support for Cutlass MLA and 6% E2E Throughput Improvement by @yewentao256 in https://github.com/vllm-project/vllm/pull/22763
* [Mamba] - refactor: Renamed mamba_attn to mamba2_attn by @Josephasafg in https://github.com/vllm-project/vllm/pull/22818
* Revert "[ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module." by @tjtanaa in https://github.com/vllm-project/vllm/pull/22956
* [P/D]Provide bucket algorithm rate limiter  for proxy_server by @frankie-ys in https://github.com/vllm-project/vllm/pull/22643
* [CI] Pooling models mteb test uses enforce_eager by @noooop in https://github.com/vllm-project/vllm/pull/22878
* [V1] - Split Prefill and Decode for Mamba1 models by @amirai21 in https://github.com/vllm-project/vllm/pull/22653
* [Bugfix] Unquote file uri before reading image by @sayandipdutta in https://github.com/vllm-project/vllm/pull/22912
* [Bugfix] fix cuda 12.6 and 11.8 build by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/22952
* [MM] Allow skipping memory profiling for multimodal models. by @ywang96 in https://github.com/vllm-project/vllm/pull/22950
* Improve multimodal hasher performance for re-used Image prompts by @p88h in https://github.com/vllm-project/vllm/pull/22825
* [V1] [Hybrid] Support using float32 for state in Hybrid Models (Mamba2, Mamba1, Minimax) by @tdoublep in https://github.com/vllm-project/vllm/pull/22928
* [Misc] Ignore ep_kernels_workspace by @jeejeelee in https://github.com/vllm-project/vllm/pull/22807
* [CI] Remove duplicated docs build from buildkite by @hmellor in https://github.com/vllm-project/vllm/pull/22924
* [Frontend] Expose do_log_stats interval to env by @Csrayz in https://github.com/vllm-project/vllm/pull/22905
* [Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer by @fhl2000 in https://github.com/vllm-project/vllm/pull/20059
* [V0 Deprecation] Remove advance_step by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22969
* [BugFix] Skip the Q component for QKVParallelLinear in the case of QKVCrossParallelLinear since its width is 0 by @sstamenk in https://github.com/vllm-project/vllm/pull/22369
* [FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches by @JartX in https://github.com/vllm-project/vllm/pull/22896
* [Benchmarks] Include image data when ShareGPT4V dataset is used. by @huachenheli in https://github.com/vllm-project/vllm/pull/22955
* [Structured Output] Make the output of structured output example more complete by @shen-shanshan in https://github.com/vllm-project/vllm/pull/22481
* [Kernels] Clean up FusedMoeMethodBase and modular kernel setup.  Remove extra arguments from modular kernel methods. by @bnellnm in https://github.com/vllm-project/vllm/pull/22035
* [Model] Granite-4 support loading quantized checkpoint by @cyang49 in https://github.com/vllm-project/vllm/pull/22925
* [Log] Debug Once for Randomizing dummy data for DP Rank by @yewentao256 in https://github.com/vllm-project/vllm/pull/22860
* [Core] direct indexing on self.block_table_np in compute_slot_mapping by @linzebing in https://github.com/vllm-project/vllm/pull/22940
* [Bugfix] Added more env vars to hash by @nvjullin in https://github.com/vllm-project/vllm/pull/22449
* Use regex in convert-results-json-to-markdown.py by @mgoin in https://github.com/vllm-project/vllm/pull/22989
* [CI] Speed up Whisper tests by reusing server by @mgoin in https://github.com/vllm-project/vllm/pull/22859
* [Fix] enable swap_ab for pplx problem size computation by @shixianc in https://github.com/vllm-project/vllm/pull/22991
* Add PrefixRepetitionRandomDataset to `vllm bench serve` datasets by @eicherseiji in https://github.com/vllm-project/vllm/pull/20638
* minor: zero workspace buffer init for flashinfer trtllm-gen attn by @yyihuang in https://github.com/vllm-project/vllm/pull/22603
* [Attention] FA3 Attention Sinks Perf Boost by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22478
* [BugFix] Fix regression caused by mamba state dtype PR by @tdoublep in https://github.com/vllm-project/vllm/pull/22998
* ci: Add CUDA + arm64 release builds by @seemethere in https://github.com/vllm-project/vllm/pull/21201
* [Structured Outputs] [Bug] Fix misalignment in apply_grammar_bitmask causing unintended masking and NaN logits by @rishitdholakia13 in https://github.com/vllm-project/vllm/pull/22963
* [BugFix] Handle case where async utility call is cancelled by @njhill in https://github.com/vllm-project/vllm/pull/22996
* [v1] Move block_hashes from KVCacheManager to Request.block_hashes (#19728) by @orozery in https://github.com/vllm-project/vllm/pull/19728
* Support multiple attention groups for KV sharing by @sarckk in https://github.com/vllm-project/vllm/pull/22672
* [BugFix] Make `run_once` thread-safe by @oraluben in https://github.com/vllm-project/vllm/pull/22978
* [Misc] Support passing multiple request ids at once to `AsyncLLM.abort()` by @njhill in https://github.com/vllm-project/vllm/pull/22944
* [Kernel] Simplify `get_kv_cache_layout` and cache `use_trtllm_attention` env-dependent bit by @NickLucche in https://github.com/vllm-project/vllm/pull/22735
* [Bugfix] Fix DeepSeek MTP by @benchislett in https://github.com/vllm-project/vllm/pull/22934
* [Frontend] Avoid list copies in `serving_chat.py` by @njhill in https://github.com/vllm-project/vllm/pull/22947
* [V1] support min_tokens for detokener by @calvin0327 in https://github.com/vllm-project/vllm/pull/22014
* [misc] nsys profile output kernel classifier and visualizer by @gracehonv in https://github.com/vllm-project/vllm/pull/22971
* [XPU]avoid circular import during XPU init by @jikunshang in https://github.com/vllm-project/vllm/pull/23017
* [Build] Env var to disable sccache by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22968
* [BugFix] Add support for loading prompt embeds tensors serialized on unavailable devices and sparse tensors by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/22962
* [Misc] Add --save-dir option to benchmark_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/23020
* [Multimodal] Update Tensor schema test to cover arbitrary shape mm inputs by @Isotr0py in https://github.com/vllm-project/vllm/pull/22867
* [Core] Make cudagraph check cuda platform only by @yaochengji in https://github.com/vllm-project/vllm/pull/23005
* [CI][Bugfix] Skip Ovis2 generation test because of broken remote code by @Isotr0py in https://github.com/vllm-project/vllm/pull/22954
* Add docs for PrefixRepetitionDataset + enable usage with `vllm bench throughput` by @eicherseiji in https://github.com/vllm-project/vllm/pull/23012
* [Refactor] Allow optional MultiModalKwargsItem in IPC by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23022
* [New Model]mBART model by @princepride in https://github.com/vllm-project/vllm/pull/22883
* Fix handling of `max_num_batched_tokens` for pooling tasks by @maxdebayser in https://github.com/vllm-project/vllm/pull/23004
* [Frontend] Added support for HermesToolParser for models without special tokens by @minpeter in https://github.com/vllm-project/vllm/pull/16890
* [Bugfix gpt-oss] Fix float32 convert for flashinfer sink support by @mgoin in https://github.com/vllm-project/vllm/pull/23016
* [Flaky CI] Increase timeout tolerance for test_mp_crash_detection+test_default_mm_lora_chat_completions by @mgoin in https://github.com/vllm-project/vllm/pull/23028
* [Kernel/Quant] Remove AQLM by @mgoin in https://github.com/vllm-project/vllm/pull/22943
* [V1] Logits processors extensibility by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19912
* [Bugfix] fix qwen3 moe fp8 accuracy issue by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/23031
* [UX] Separate marlin moe config logic from triton moe by @mgoin in https://github.com/vllm-project/vllm/pull/23006
* [Refactor] Defer tensor data construction in MultiModalKwargs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23030
* [Misc] method name typo fix by @andyxning in https://github.com/vllm-project/vllm/pull/23042
* [Kernel] Add cuda kernel for gpt_oss activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/22951
* [Bugfix] should use stack instead of concat by @947132885 in https://github.com/vllm-project/vllm/pull/22972
* [Misc] fix typo in the multimodal doc by @KevinZeng08 in https://github.com/vllm-project/vllm/pull/23051
* [BugFix] Fix for IMA in FA3 varlen combine by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22967
* [Misc] Remove dead return by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23061
* [Misc] Convert use_structured_output property into constant by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23060
* [XPU] fix xpu to set cudagraph batch sizes by @calvin0327 in https://github.com/vllm-project/vllm/pull/23044
* fix: gptq marlin weight loading failure by @simon-mo in https://github.com/vllm-project/vllm/pull/23066

## New Contributors
* @zhouwfang made their first contribution in https://github.com/vllm-project/vllm/pull/21407
* @juncgu made their first contribution in https://github.com/vllm-project/vllm/pull/18293
* @weireweire made their first contribution in https://github.com/vllm-project/vllm/pull/21485
* @bbeckca made their first contribution in https://github.com/vllm-project/vllm/pull/21232
* @wzqd made their first contribution in https://github.com/vllm-project/vllm/pull/21494
* @hfan made their first contribution in https://github.com/vllm-project/vllm/pull/21479
* @ignaciosica made their first contribution in https://github.com/vllm-project/vllm/pull/21195
* @xyxinyang made their first contribution in https://github.com/vllm-project/vllm/pull/21586
* @bigshanedogg made their first contribution in https://github.com/vllm-project/vllm/pull/20931
* @fsx950223 made their first contribution in https://github.com/vllm-project/vllm/pull/20295
* @mgazz made their first contribution in https://github.com/vllm-project/vllm/pull/21518
* @Mitix-EPI made their first contribution in https://github.com/vllm-project/vllm/pull/21612
* @lvhan028 made their first contribution in https://github.com/vllm-project/vllm/pull/21628
* @zhouyeju made their first contribution in https://github.com/vllm-project/vllm/pull/21380
* @wenchen76 made their first contribution in https://github.com/vllm-project/vllm/pull/21154
* @skyloevil made their first contribution in https://github.com/vllm-project/vllm/pull/20529
* @joa-stdn made their first contribution in https://github.com/vllm-project/vllm/pull/21697
* @liuyumoye made their first contribution in https://github.com/vllm-project/vllm/pull/21534
* @hsliuustc0106 made their first contribution in https://github.com/vllm-project/vllm/pull/21573
* @Josephasafg made their first contribution in https://github.com/vllm-project/vllm/pull/21715
* @vasqu made their first contribution in https://github.com/vllm-project/vllm/pull/21735
* @key4ng made their first contribution in https://github.com/vllm-project/vllm/pull/19024
* @wuhang2014 made their first contribution in https://github.com/vllm-project/vllm/pull/21728
* @HugoMichard made their first contribution in https://github.com/vllm-project/vllm/pull/21167
* @smarterclayton made their first contribution in https://github.com/vllm-project/vllm/pull/21472
* @nikhil-arm made their first contribution in https://github.com/vllm-project/vllm/pull/17112
* @LyrisZhong made their first contribution in https://github.com/vllm-project/vllm/pull/20396
* @rzabarazesh made their first contribution in https://github.com/vllm-project/vllm/pull/21347
* @milesial made their first contribution in https://github.com/vllm-project/vllm/pull/21798
* @Csrayz made their first contribution in https://github.com/vllm-project/vllm/pull/21803
* @MingzhenHan made their first contribution in https://github.com/vllm-project/vllm/pull/21827
* @aladerran made their first contribution in https://github.com/vllm-project/vllm/pull/20815
* @Yanpas made their first contribution in https://github.com/vllm-project/vllm/pull/18548
* @tanruixiang made their first contribution in https://github.com/vllm-project/vllm/pull/21673
* @nvpohanh made their first contribution in https://github.com/vllm-project/vllm/pull/21499
* @chi2liu made their first contribution in https://github.com/vllm-project/vllm/pull/21816
* @fake0fan made their first contribution in https://github.com/vllm-project/vllm/pull/21611
* @wxsms made their first contribution in https://github.com/vllm-project/vllm/pull/20433
* @wenxindongwork made their first contribution in https://github.com/vllm-project/vllm/pull/21417
* @br4mm made their first contribution in https://github.com/vllm-project/vllm/pull/20272
* @linzebing made their first contribution in https://github.com/vllm-project/vllm/pull/21627
* @sanchit-gandhi made their first contribution in https://github.com/vllm-project/vllm/pull/21833
* @amirkl94 made their first contribution in https://github.com/vllm-project/vllm/pull/21458
* @zhxchen17 made their first contribution in https://github.com/vllm-project/vllm/pull/22028
* @charent made their first contribution in https://github.com/vllm-project/vllm/pull/20873
* @Aviadr-neureality made their first contribution in https://github.com/vllm-project/vllm/pull/21937
* @n0gu-furiosa made their first contribution in https://github.com/vllm-project/vllm/pull/21052
* @ahengljh made their first contribution in https://github.com/vllm-project/vllm/pull/22052
* @sidhpurwala-huzaifa made their first contribution in https://github.com/vllm-project/vllm/pull/21119
* @anijain2305 made their first contribution in https://github.com/vllm-project/vllm/pull/20836
* @JartX made their first contribution in https://github.com/vllm-project/vllm/pull/21733
* @xiszishu made their first contribution in https://github.com/vllm-project/vllm/pull/22122
* @LopezCastroRoberto made their first contribution in https://github.com/vllm-project/vllm/pull/21309
* @TankNee made their first contribution in https://github.com/vllm-project/vllm/pull/21213
* @TheEpicDolphin made their first contribution in https://github.com/vllm-project/vllm/pull/20401
* @chenxi-yang made their first contribution in https://github.com/vllm-project/vllm/pull/22105
* @weixiao-huang made their first contribution in https://github.com/vllm-project/vllm/pull/21164
* @CLFutureX made their first contribution in https://github.com/vllm-project/vllm/pull/21173
* @tlipoca9 made their first contribution in https://github.com/vllm-project/vllm/pull/22149
* @zyongye made their first contribution in https://github.com/vllm-project/vllm/pull/22330
* @zhangnju made their first contribution in https://github.com/vllm-project/vllm/pull/22367
* @tc-mb made their first contribution in https://github.com/vllm-project/vllm/pull/22166
* @syedmba made their first contribution in https://github.com/vllm-project/vllm/pull/22314
* @msanft made their first contribution in https://github.com/vllm-project/vllm/pull/22099
* @mizadri made their first contribution in https://github.com/vllm-project/vllm/pull/20707
* @JaceyShao made their first contribution in https://github.com/vllm-project/vllm/pull/22433
* @andrewkchan made their first contribution in https://github.com/vllm-project/vllm/pull/12022
* @iAmir97 made their first contribution in https://github.com/vllm-project/vllm/pull/22310
* @pliops-daniels made their first contribution in https://github.com/vllm-project/vllm/pull/20267
* @yyweiss made their first contribution in https://github.com/vllm-project/vllm/pull/18097
* @Pradyun92 made their first contribution in https://github.com/vllm-project/vllm/pull/22317
* @kyuyeunk made their first contribution in https://github.com/vllm-project/vllm/pull/22425
* @lec77 made their first contribution in https://github.com/vllm-project/vllm/pull/22333
* @h-brenoskuk made their first contribution in https://github.com/vllm-project/vllm/pull/22534
* @zhewenl made their first contribution in https://github.com/vllm-project/vllm/pull/22584
* @PicoCreator made their first contribution in https://github.com/vllm-project/vllm/pull/22592
* @danielafrimi made their first contribution in https://github.com/vllm-project/vllm/pull/22349
* @GuanLuo made their first contribution in https://github.com/vllm-project/vllm/pull/21074
* @sooraj-satheesh made their first contribution in https://github.com/vllm-project/vllm/pull/22707
* @dongluw made their first contribution in https://github.com/vllm-project/vllm/pull/22660
* @Sugar-zsg made their first contribution in https://github.com/vllm-project/vllm/pull/22630
* @phantomlei3 made their first contribution in https://github.com/vllm-project/vllm/pull/22170
* @RishiAstra made their first contribution in https://github.com/vllm-project/vllm/pull/21783
* @zejunchen-zejun made their first contribution in https://github.com/vllm-project/vllm/pull/21161
* @teekenl made their first contribution in https://github.com/vllm-project/vllm/pull/22733
* @mxz297 made their first contribution in https://github.com/vllm-project/vllm/pull/22683
* @RUTHLESS-BOT made their first contribution in https://github.com/vllm-project/vllm/pull/22641
* @frankwang28 made their first contribution in https://github.com/vllm-project/vllm/pull/22606
* @zzh142857 made their first contribution in https://github.com/vllm-project/vllm/pull/22697
* @ducviet00 made their first contribution in https://github.com/vllm-project/vllm/pull/22739
* @x22x22 made their first contribution in https://github.com/vllm-project/vllm/pull/22280
* @Gh0u1L5 made their first contribution in https://github.com/vllm-project/vllm/pull/22785
* @jio-H made their first contribution in https://github.com/vllm-project/vllm/pull/22813
* @ZJY0516 made their first contribution in https://github.com/vllm-project/vllm/pull/22786
* @NirLevy98 made their first contribution in https://github.com/vllm-project/vllm/pull/22891
* @nvjullin made their first contribution in https://github.com/vllm-project/vllm/pull/22346
* @frankie-ys made their first contribution in https://github.com/vllm-project/vllm/pull/22643
* @amirai21 made their first contribution in https://github.com/vllm-project/vllm/pull/22653
* @sayandipdutta made their first contribution in https://github.com/vllm-project/vllm/pull/22912
* @yyihuang made their first contribution in https://github.com/vllm-project/vllm/pull/22603
* @rishitdholakia13 made their first contribution in https://github.com/vllm-project/vllm/pull/22963
* @oraluben made their first contribution in https://github.com/vllm-project/vllm/pull/22978
* @minpeter made their first contribution in https://github.com/vllm-project/vllm/pull/16890
* @947132885 made their first contribution in https://github.com/vllm-project/vllm/pull/22972
* @KevinZeng08 made their first contribution in https://github.com/vllm-project/vllm/pull/23051

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.0...v0.10.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.1)

---

## v0.10.1rc1: v0.10.1rc1
**Published:** 2025-08-17
**Pre-release**

## What's Changed
* Deduplicate Transformers backend code using inheritance by @hmellor in https://github.com/vllm-project/vllm/pull/21461
* [Bugfix][ROCm] Fix for warp_size uses on host by @gshtras in https://github.com/vllm-project/vllm/pull/21205
* [TPU][Bugfix] fix moe layer by @yaochengji in https://github.com/vllm-project/vllm/pull/21340
* [v1][Core] Clean up usages of `SpecializedManager` by @zhouwfang in https://github.com/vllm-project/vllm/pull/21407
* [Misc] Fix duplicate FusedMoEConfig debug messages by @njhill in https://github.com/vllm-project/vllm/pull/21455
* [Core] Support model loader plugins by @22quinn in https://github.com/vllm-project/vllm/pull/21067
* remove GLM-4 quantization wrong Code by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21435
* Replace `--expand-tools-even-if-tool-choice-none` with `--exclude-tools-when-tool-choice-none` for v0.10.0 by @okdshin in https://github.com/vllm-project/vllm/pull/20544
* [Misc] Improve comment for DPEngineCoreActor._set_cuda_visible_devices() by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21501
* [Feat] Allow custom naming of vLLM processes by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21445
* bump `flashinfer` to `v0.2.8` by @cjackal in https://github.com/vllm-project/vllm/pull/21385
* [Attention] Optimize FlashInfer MetadataBuilder Build call by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21137
* [Model] Officially support Emu3 with Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21319
* [Bugfix] Fix CUDA arch flags for MoE permute by @minosfuture in https://github.com/vllm-project/vllm/pull/21426
* [Fix] Update mamba_ssm to 2.2.5 by @elvischenv in https://github.com/vllm-project/vllm/pull/21421
* [Docs] Update Tensorizer usage documentation by @sangstar in https://github.com/vllm-project/vllm/pull/21190
* [Docs] Rewrite Distributed Inference and Serving guide by @crypdick in https://github.com/vllm-project/vllm/pull/20593
* [Bug] Fix Compressed Tensor NVFP4 `cutlass_fp4_group_mm` illegal memory access by @yewentao256 in https://github.com/vllm-project/vllm/pull/21465
* Update flashinfer CUTLASS MoE Kernel by @wenscarl in https://github.com/vllm-project/vllm/pull/21408
* [XPU] Conditionally import CUDA-specific passes to avoid import errors on xpu platform by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21036
* [P/D] Move FakeNixlWrapper to test dir by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21328
* [P/D] Support CPU Transfer in NixlConnector by @juncgu in https://github.com/vllm-project/vllm/pull/18293
* [Docs][minor] Fix broken gh-file link in distributed serving docs by @crypdick in https://github.com/vllm-project/vllm/pull/21543
* [Docs] Add Expert Parallelism Initial Documentation by @simon-mo in https://github.com/vllm-project/vllm/pull/21373
* update flashinfer to v0.2.9rc1 by @weireweire in https://github.com/vllm-project/vllm/pull/21485
* [TPU][TEST] HF_HUB_DISABLE_XET=1 the test 3. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21539
* [MoE] More balanced expert sharding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21497
* [Frontend] `run-batch` supports V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21541
* [Docs] Fix `site_url` for RunLLM by @hmellor in https://github.com/vllm-project/vllm/pull/21564
* [Bug] Fix DeepGemm Init Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/21554
* Fix GLM-4 PP Missing Layer When using with PP. by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21531
* [Kernel] adding fused_moe configs for upcoming granite4 by @bringlein in https://github.com/vllm-project/vllm/pull/21332
* [Bugfix] DeepGemm utils : Fix hardcoded type-cast by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21517
* [DP] Support api-server-count > 0 in hybrid DP LB mode by @njhill in https://github.com/vllm-project/vllm/pull/21510
* [TPU][Test] Temporarily suspend this MoE model in test_basic.py. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21560
* [Docs] Add `requirements/common.txt` to run unit tests by @zhouwfang in https://github.com/vllm-project/vllm/pull/21572
* Integrate TensorSchema with shape validation for Phi3VImagePixelInputs by @bbeckca in https://github.com/vllm-project/vllm/pull/21232
* [CI] Update CODEOWNERS for CPU and Intel GPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21582
* [Bugfix] fix modelscope snapshot_download serialization by @andyxning in https://github.com/vllm-project/vllm/pull/21536
* [Model] Support tensor parallel for timm ViT in Deepseek_vl2 by @wzqd in https://github.com/vllm-project/vllm/pull/21494
* [Model] Fix a check for None but the return value was empty list in Gemma3 MM vision_embeddings by @hfan in https://github.com/vllm-project/vllm/pull/21479
* [Misc][Tools] make max-model-len a parameter in auto_tune script by @yaochengji in https://github.com/vllm-project/vllm/pull/21321
* [CI/Build] fix cpu_extension for apple silicon by @ignaciosica in https://github.com/vllm-project/vllm/pull/21195
* [Misc] Removed undefined cmake variables MOE_PERMUTE_ARCHS by @chenyang78 in https://github.com/vllm-project/vllm/pull/21262
* [TPU][Bugfix] fix OOM issue in CI test by @yaochengji in https://github.com/vllm-project/vllm/pull/21550
* [Tests] Harden DP tests by @njhill in https://github.com/vllm-project/vllm/pull/21508
* Add H20-3e fused MoE kernel tuning configs for Qwen3-Coder-480B-A35B-Instruct by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/21598
* [Bugfix] GGUF: fix AttributeError: 'PosixPath' object has no attribute 'startswith' by @kebe7jun in https://github.com/vllm-project/vllm/pull/21579
* [Quantization] Enable BNB support for more MoE models by @jeejeelee in https://github.com/vllm-project/vllm/pull/21370
* [V1] Get supported tasks from model runner instead of model config by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21585
* [Bugfix][Logprobs] Fix logprobs op to support more backend by @MengqingCao in https://github.com/vllm-project/vllm/pull/21591
* [Model] Fix Ernie4.5MoE e_score_correction_bias parameter by @xyxinyang in https://github.com/vllm-project/vllm/pull/21586
* [MODEL] New model support for naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B by @bigshanedogg in https://github.com/vllm-project/vllm/pull/20931
* [Frontend] Add request_id to the Request object so they can be controlled better via external load balancers by @kouroshHakha in https://github.com/vllm-project/vllm/pull/21009
* [Model] Replace Mamba2 RMSNorm Gated with Fused Triton Kernel by @cyang49 in https://github.com/vllm-project/vllm/pull/20839
* [ROCm][AITER] Enable fp8 kv cache on rocm aiter backend. by @fsx950223 in https://github.com/vllm-project/vllm/pull/20295
* [Kernel] Improve machete memory bound perf by @czhu-cohere in https://github.com/vllm-project/vllm/pull/21556
* Add support for Prithvi in Online serving mode by @mgazz in https://github.com/vllm-project/vllm/pull/21518
* [CI] Unifying Dockerfiles for ARM and X86 Builds by @kebe7jun in https://github.com/vllm-project/vllm/pull/21343
* [Docs] add auto-round quantization readme  by @wenhuach21 in https://github.com/vllm-project/vllm/pull/21600
* [TPU][Test] Rollback PR-21550. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21619
* Add Unsloth to RLHF.md by @danielhanchen in https://github.com/vllm-project/vllm/pull/21636
* [Perf] Cuda Kernel for Int8 Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/21476
* Add interleaved RoPE test for Llama4 (Maverick) by @sarckk in https://github.com/vllm-project/vllm/pull/21478
* [Bugfix] Fix sync_and_slice_intermediate_tensors by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21537
* [Bugfix] Always set RAY_ADDRESS for Ray actor before spawn by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21540
* [TPU] Update ptxla nightly version to 20250724 by @yaochengji in https://github.com/vllm-project/vllm/pull/21555
* [Feature] Add support for MoE models in the calibration-free RTN-based quantization by @sakogan in https://github.com/vllm-project/vllm/pull/20766
* [Model] Ultravox: Support Llama 4 and Gemma 3 backends by @farzadab in https://github.com/vllm-project/vllm/pull/17818
* [Docs] add offline serving multi-modal video input expamle Qwen2.5-VL by @david6666666 in https://github.com/vllm-project/vllm/pull/21530
* Correctly kill vLLM processes after finishing serving benchmarks by @huydhn in https://github.com/vllm-project/vllm/pull/21641
* [Bugfix] Fix isinstance check for tensor types in _load_prompt_embeds to use dtype comparison by @Mitix-EPI in https://github.com/vllm-project/vllm/pull/21612
* [TPU][Test] Divide TPU v1 Test into 2 parts. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21431
* Support Intern-S1 by @lvhan028 in https://github.com/vllm-project/vllm/pull/21628
* [Misc] remove unused try-except in pooling config check by @reidliu41 in https://github.com/vllm-project/vllm/pull/21618
* [Take 2] Correctly kill vLLM processes after benchmarks by @huydhn in https://github.com/vllm-project/vllm/pull/21646
* Migrate AriaImagePixelInputs to TensorSchema for shape validation by @bbeckca in https://github.com/vllm-project/vllm/pull/21620
* Migrate AyaVisionImagePixelInputs to TensorSchema for shape validation by @bbeckca in https://github.com/vllm-project/vllm/pull/21622
* [Bugfix] Investigate Qwen2-VL failing test by @Isotr0py in https://github.com/vllm-project/vllm/pull/21527
* Support encoder-only models without KV-Cache by @maxdebayser in https://github.com/vllm-project/vllm/pull/21270
* [Bug] Fix `has_flashinfer_moe` Import Error when it is not installed by @yewentao256 in https://github.com/vllm-project/vllm/pull/21634
* [Misc] Improve memory profiling debug message by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21429
* [BugFix] Fix shared storage connector load kv only load attention layer by @david6666666 in https://github.com/vllm-project/vllm/pull/21428
* [Refactor] Remove `moe_align_block_size_triton` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21335
* [Bugfix][Apple Silicon] fix missing symbols when build from source on Mac with Apple Silicon by @zhouyeju in https://github.com/vllm-project/vllm/pull/21380
* [CI/Build][Doc] Move existing benchmark scripts in CI/document/example to vllm bench CLI by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21355
* [NVIDIA] Explicitly disable shuffled weights for flashinfer blockscale moe fp8 kernels by @kaixih in https://github.com/vllm-project/vllm/pull/21411
* Remove xformers requirement for Mistral-format Pixtral and Mistral3 by @wenchen76 in https://github.com/vllm-project/vllm/pull/21154
* support `torch.compile` for bailing moe by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/21664
* Migrate Blip2ImagePixelInputs and Blip2ImageEmbeddingInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21656
* Migrate DeepseekVL2ImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21658
* Migrate FuyuImagePatchInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21662
* Migrate ChameleonImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21657
* [VLM] Support HF format Phi-4-MM model by @Isotr0py in https://github.com/vllm-project/vllm/pull/17121
* Handle non-serializable objects in vllm bench by @huydhn in https://github.com/vllm-project/vllm/pull/21665
* [CI/Build][Doc] Clean up more docs that point to old bench scripts by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21667
* Refactor: Remove numpy dependency from LoggingStatLogger by @skyloevil in https://github.com/vllm-project/vllm/pull/20529
* [Misc] add default value for file pattern arg by @andyxning in https://github.com/vllm-project/vllm/pull/21659
* Migrate Florence2ImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21663
* [VLM] Add video support for Intern-S1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/21671
* [Refactor] Refactor MOE NVFP4 Code Base: ModelOpt + Compressed Tensor by @yewentao256 in https://github.com/vllm-project/vllm/pull/21631
* Fix CUDA permute/unpermute for use with DeepGemm Moe by @CalebDu in https://github.com/vllm-project/vllm/pull/17934
* [Misc] Refactor vllm config str by @andyxning in https://github.com/vllm-project/vllm/pull/21666
* [Attention] Make CutlassMLA the default backend for SM100 (blackwell) by @alexm-redhat in https://github.com/vllm-project/vllm/pull/21626
* [Deprecation][2/N] Replace `--task` with `--runner` and `--convert` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21470
* Fix typo for limit-mm-per-prompt in docs by @joa-stdn in https://github.com/vllm-project/vllm/pull/21697
* Fix GLM tool parser by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/21668
* [Misc]  Add fused_moe configs for Qwen3-Coder-480B-A35B-Instruct-FP8 by @jeejeelee in https://github.com/vllm-project/vllm/pull/21700
* [V1] Exception Handling when Loading KV Cache from Remote Store by @liuyumoye in https://github.com/vllm-project/vllm/pull/21534
* [Model] Support TP/PP/mamba2 kernel for PLaMo2 by @Alnusjaponica in https://github.com/vllm-project/vllm/pull/19674
* [FEAT] [ROCm] [AITER]: Add AITER HIP block quant kernel by @tjtanaa in https://github.com/vllm-project/vllm/pull/21242
* Migrate Gemma3ImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21676
* Migrate Glm4vImageInputs, Glm4vVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21678
* Migrate GLMVImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21679
* Migrate GraniteSpeechAudioInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21682
* Migrate Idefics3ImagePixelInputs and Idefics3ImageEmbeddingInputs to â€¦ by @bbeckca in https://github.com/vllm-project/vllm/pull/21683
* [Bugfix] [issue-21565] Fix the incompatibility issue with stream and named function calling when Thinking is disabled by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21573
* [bugfix] fix profile impact benchmark results by @lengrongfu in https://github.com/vllm-project/vllm/pull/21507
* [Bugfix] Fix shape checking for Fuyu by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21709
* [Bugfix] fix max-file-size type from str to int by @andyxning in https://github.com/vllm-project/vllm/pull/21675
* [BugFix] Fix ChunkedLocalAttention when the hybrid kv-cache is disabled by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21707
* [v1][mamba] Added mamba_type into MambaSpec by @Josephasafg in https://github.com/vllm-project/vllm/pull/21715
* Migrate KeyeImageInputs and KeyeVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21686
* [Model] Prioritize Transformers fallback over suffix matching by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21719
* [feature] add log non default args in LLM by @lengrongfu in https://github.com/vllm-project/vllm/pull/21680
* [Bugfix] Fix Ernie4_5_MoeForCausalLM shared experts by @jeejeelee in https://github.com/vllm-project/vllm/pull/21717
* [Bugfix] Fix environment variable setting in CPU Dockerfile by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21730
* [Bugfix] Fix glm4.1v video_grid_thw tensor shape scheme by @Isotr0py in https://github.com/vllm-project/vllm/pull/21744
* [PD] let p2p nccl toy proxy handle /chat/completions by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21734
* [`Ernie 4.5`] Name Change for Base 0.3B Model by @vasqu in https://github.com/vllm-project/vllm/pull/21735
* [Bugfix] Improve JSON extraction in LlamaToolParser by @key4ng in https://github.com/vllm-project/vllm/pull/19024
* [Docs] Add revision date to rendered docs by @hmellor in https://github.com/vllm-project/vllm/pull/21752
* [Bugfix]check health for engine core process exiting unexpectedly by @wuhang2014 in https://github.com/vllm-project/vllm/pull/21728
* [Bugfix][CI/Build] Update peft version in test requirement by @Isotr0py in https://github.com/vllm-project/vllm/pull/21729
* [Logs] Change flashinfer sampler logs to once by @mgoin in https://github.com/vllm-project/vllm/pull/21759
* [Misc] Reduce logs for model resolution by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21765
* [Bugfix] Mistral crashes on tool with no description by @HugoMichard in https://github.com/vllm-project/vllm/pull/21167
* [CI/Build] Fix plugin tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21758
* [XPU] IPEX-optimized Punica Wrapper on XPU by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21703
* [Bugfix] Fix granite speech shape validation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21762
* [P/D] Log warnings related to prefill KV expiry by @njhill in https://github.com/vllm-project/vllm/pull/21753
* Use `metavar` to list the choices for a CLI arg when custom values are also accepted by @hmellor in https://github.com/vllm-project/vllm/pull/21760
* update flashinfer to v0.2.9rc2 by @weireweire in https://github.com/vllm-project/vllm/pull/21701
* [AMD][BugFix] Fix omission  of wvSplitK kernel for small batch sizes (1-4) due to torch.compile by @rasmith in https://github.com/vllm-project/vllm/pull/21350
* [Bug] Enforce contiguous input for `dynamic_scaled_fp8_quant` and `static_scaled_fp8_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21773
* [AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure by @houseroad in https://github.com/vllm-project/vllm/pull/21647
* Revert "[V1] Exception Handling when Loading KV Cache from Remote Store" by @KuntaiDu in https://github.com/vllm-project/vllm/pull/21778
* [Bugfix] DeepGEMM is not enabled on B200 due to `_lazy_init()` by @smarterclayton in https://github.com/vllm-project/vllm/pull/21472
* [Feat]: Add support for Dynamic Quant 4 bit CPU kleidiai kernels by @nikhil-arm in https://github.com/vllm-project/vllm/pull/17112
* [Perf] Disable chunked local attention by default with llama4 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21761
* [Kernel] SM90 CUTLASS FP8 GEMM: add support for swap AB + kernel tuning by @LyrisZhong in https://github.com/vllm-project/vllm/pull/20396
* [Docs] Minimize spacing for supported_hardware.md table by @mgoin in https://github.com/vllm-project/vllm/pull/21779
* [Refactor] Merge Compressed Tensor FP8 `CompressedTensorsW8A8Fp8MoEMethod` and `CompressedTensorsW8A8Fp8MoECutlassMethod` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21775
* [CI] Parallelize Kernels MoE Test by @mgoin in https://github.com/vllm-project/vllm/pull/21764
* skip fusedmoe layer for start_load_kv by @calvin0327 in https://github.com/vllm-project/vllm/pull/21378
* [AMD][CI/Build][Bugfix] Guarding CUDA specific functions by ifndef ROCM by @gshtras in https://github.com/vllm-project/vllm/pull/21766
* Migrate InternVLImageInputs and InternVLVideoInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21684
* [Misc] Rework process titles by @njhill in https://github.com/vllm-project/vllm/pull/21780
* [Doc] Link to RFC for pooling optimizations by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21806
* [Model]: Fused MoE for nomic-embed-text-v2-moe by @Isotr0py in https://github.com/vllm-project/vllm/pull/18321
* [V0 deprecation] Guided decoding by @rzabarazesh in https://github.com/vllm-project/vllm/pull/21347
* [Model] Refactor JambaForCausalLM by @jeejeelee in https://github.com/vllm-project/vllm/pull/21394
* [Docs] Fix the outdated URL for installing from vLLM binaries by @yankay in https://github.com/vllm-project/vllm/pull/21523
* [KVCache] Make KVCacheSpec hashable by @heheda12345 in https://github.com/vllm-project/vllm/pull/21791
* [Doc] Update compatibility matrix for pooling and multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21831
* [Bugfix] VLLM_V1 supports passing other compilation levels by @zou3519 in https://github.com/vllm-project/vllm/pull/19340
* [Docs] Merge design docs for a V1 only future by @hmellor in https://github.com/vllm-project/vllm/pull/21832
* [TPU] Add an optimization doc on TPU by @bvrockwell in https://github.com/vllm-project/vllm/pull/21155
* [Bugfix]fix mixed bits and visual language model quantization in AutoRound by @wenhuach21 in https://github.com/vllm-project/vllm/pull/21802
* [Bugfix] Fix workspace buffer None issue for Flashinfer TRTLLM Backend by @elvischenv in https://github.com/vllm-project/vllm/pull/21525
* [Docs] use `uv` in GPU installation docs by @davidxia in https://github.com/vllm-project/vllm/pull/20277
* [Doc] Add FusedMoE Modular Kernel Documentation by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21623
* [Doc] update Contributing page's testing section by @davidxia in https://github.com/vllm-project/vllm/pull/18272
* Add `flashinfer_python` to CUDA wheel requirements by @mgoin in https://github.com/vllm-project/vllm/pull/21389
* docker: docker-aware precompiled wheel support by @dougbtv in https://github.com/vllm-project/vllm/pull/21127
* Revert "[AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure (#21647)" by @gshtras in https://github.com/vllm-project/vllm/pull/21850
* [BugFix] Fix interleaved sliding window not set for Gemma3n by @sarckk in https://github.com/vllm-project/vllm/pull/21863
* [ci] add b200 test placeholder by @simon-mo in https://github.com/vllm-project/vllm/pull/21866
* [ci] mark blackwell test optional for now by @simon-mo in https://github.com/vllm-project/vllm/pull/21878
* [Bugfix] Correct max tokens for non-contiguous embeds by @milesial in https://github.com/vllm-project/vllm/pull/21798
* [v1][attention] Support Hybrid Allocator + FlashInfer by @heheda12345 in https://github.com/vllm-project/vllm/pull/21412
* [Docs] Switch to better markdown linting pre-commit hook by @hmellor in https://github.com/vllm-project/vllm/pull/21851
* [DOC] Fix path of v1 related figures by @heheda12345 in https://github.com/vllm-project/vllm/pull/21868
* [Docs] Update docker.md with HF_TOKEN, new model, and podman fix by @mgoin in https://github.com/vllm-project/vllm/pull/21856
* Expose PyTorch profiler configuration to environment variables by @Csrayz in https://github.com/vllm-project/vllm/pull/21803
* [Bugfix] Fix shape mismatch assertion error when loading Gemma3n model with BitsAndBytes quantization by @sydarb in https://github.com/vllm-project/vllm/pull/21808
* [Bugfix] Fix comment typo of get_num_common_prefix_blocks() by @MingzhenHan in https://github.com/vllm-project/vllm/pull/21827
* [Bugfix] Actually disable processing cache when API server is scaled out by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21839
* [Perf] Using `__nv_fp8_e4m3` instead of `c10::e4m3` for `per_token_group_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21867
* [Frontend] Add LLM.reward specific to reward models by @noooop in https://github.com/vllm-project/vllm/pull/21720
* [XPU] use `ZE_AFFINITY_MASK` for device select on xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/21815
* Add @sighingnow as maintainer of qwen's related files. by @sighingnow in https://github.com/vllm-project/vllm/pull/21895
* [CI/Build] Fix pre-commit failure in docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21897
* [Docs] Expand introduction to Ray in Multi-node deployment section by @crypdick in https://github.com/vllm-project/vllm/pull/21584
* Update vLLM Benchmark Suite for Xeon based on 0.9.2 release  by @louie-tsai in https://github.com/vllm-project/vllm/pull/21486
* [Misc] Remove redundant config definitions by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21891
* [Doc] Update Intern-S1 info  by @jeejeelee in https://github.com/vllm-project/vllm/pull/21908
* [CI] rollback lint-and-deploy pipeline using amd machine by @kebe7jun in https://github.com/vllm-project/vllm/pull/21912
* [Tests] Fixing bug inside MultiModalProfiler. by @shenoyvvarun in https://github.com/vllm-project/vllm/pull/21842
* [Model] Remove DSV2 unused code by @jeejeelee in https://github.com/vllm-project/vllm/pull/21903
* [benchmark] add max-concurrency in result table by @panpan0000 in https://github.com/vllm-project/vllm/pull/21095
* [Doc] Update partial support by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21916
* [Docs] Fix the example code of streaming chat completions in reasoning by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21825
* Add @patrickvonplaten as maintainer of mistral's related files. by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/21928
* [Hardware][CPU] Build fix for ARM without BF16 by @ericcurtin in https://github.com/vllm-project/vllm/pull/21848
* [Feature][EPLB] Add eplb support for Qwen3 by @aladerran in https://github.com/vllm-project/vllm/pull/20815
* [Doc] Remove vLLM prefix and add citation for PagedAttention by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21910
* [Bugfix] we should use metavar is not choices by @lengrongfu in https://github.com/vllm-project/vllm/pull/21902
* [Feature] Support multiple api keys in server by @Yanpas in https://github.com/vllm-project/vllm/pull/18548
* [misc] skip p2p check by default by @youkaichao in https://github.com/vllm-project/vllm/pull/21904
* [Test] Add Benchmark and Unit Test for `per_token_group_quant` by @yewentao256 in https://github.com/vllm-project/vllm/pull/21860
* [CI/Build] Only run markdownlint in CI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21892
* Reduce time wasted in GitHub Actions using `concurrency` by @hmellor in https://github.com/vllm-project/vllm/pull/21919
* [Misc] Improve code readability of KVCacheManager by @tanruixiang in https://github.com/vllm-project/vllm/pull/21673
* [NVIDIA] Fix Llama4 Scout FP4 functionality issues by @nvpohanh in https://github.com/vllm-project/vllm/pull/21499
* [Docs] Reduce the size of the built docs by @hmellor in https://github.com/vllm-project/vllm/pull/21920
* [Bugfix] Fix OOM tests in initialization test by @Isotr0py in https://github.com/vllm-project/vllm/pull/21921
* [Bugfix] Fix multi-api server not working for text models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21933
* Override attention metadata for fast prefill in some KV sharing setups by @sarckk in https://github.com/vllm-project/vllm/pull/21590
* [Bugfix] Fix TypeError in scheduler when comparing mixed request_id types by @chi2liu in https://github.com/vllm-project/vllm/pull/21816
* [CI/Build] Fix registry tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21934
* [Bugfix] SharedStorage Connector for V1 PD multimodal by @fake0fan in https://github.com/vllm-project/vllm/pull/21611
* feat(distributed): add `get_required_kvcache_layout` class method to kv connector api by @wxsms in https://github.com/vllm-project/vllm/pull/20433
* [TPU] Support Pathways in vLLM by @wenxindongwork in https://github.com/vllm-project/vllm/pull/21417
* [Misc] Support more collective_rpc return types by @njhill in https://github.com/vllm-project/vllm/pull/21845
* For VLLM_USE_PRECOMPILED, only compiled .so files should be extracted by @dougbtv in https://github.com/vllm-project/vllm/pull/21964
* [Misc] Use dracut on CentOS and skip clone if repo exists for EP kernel installation by @minosfuture in https://github.com/vllm-project/vllm/pull/21635
* [Feature] Add async tensor parallelism for scaled mm by @cascade812 in https://github.com/vllm-project/vllm/pull/20155
* [Bugfix] Fix None value handling in trace span creation for cancelled requests by @br4mm in https://github.com/vllm-project/vllm/pull/20272
* [Core] Move EngineCoreRequest to Request conversion out of EngineCore by @linzebing in https://github.com/vllm-project/vllm/pull/21627
* [Example] Add `async_llm_streaming.py` example for AsyncLLM streaming in python by @mgoin in https://github.com/vllm-project/vllm/pull/21763
* [Bugfix] Relax lang pin for voxtral by @sanchit-gandhi in https://github.com/vllm-project/vllm/pull/21833
* [UX] Rename CUTLASS_MLA_VLLM_V1 to CUTLASS_MLA by @mgoin in https://github.com/vllm-project/vllm/pull/21966
* [Misc] Expand SUPPORTED_HIDDEN_SIZES  for DeepEP low-latency kernels by @jeejeelee in https://github.com/vllm-project/vllm/pull/21818
* [CI Bugfix] Fix CI OOM for `test_shared_storage_connector_hashes` by @mgoin in https://github.com/vllm-project/vllm/pull/21973
* [Bugfix]: fix metadata file copy in test_sharded_state_loader by @andyxning in https://github.com/vllm-project/vllm/pull/21830
* [Deprecation] Remove deprecated args and methods by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21907
* [CI/Build] get rid of unused VLLM_FA_CMAKE_GPU_ARCHES by @dtrifiro in https://github.com/vllm-project/vllm/pull/21599
* [Model][CI] Let more pooling models support v1 by @noooop in https://github.com/vllm-project/vllm/pull/21747
* [BugFix] Fix case where `collective_rpc` returns `None` by @njhill in https://github.com/vllm-project/vllm/pull/22006
* [NVIDIA] Add SM100 Flashinfer MoE per tensor scale fp8 backend by @amirkl94 in https://github.com/vllm-project/vllm/pull/21458
* [Model] Add step3 vl by @Oliver-ss in https://github.com/vllm-project/vllm/pull/21998
* [ez] Remove a trailing space from compilation/decorators.py by @zhxchen17 in https://github.com/vllm-project/vllm/pull/22028
* fix(setup): improve precompiled wheel setup for Docker builds by @dougbtv in https://github.com/vllm-project/vllm/pull/22025
* Removing amdproduction Tests by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/22027
* Update torch_xla pin to 20250730 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21956
* [Meta] Official Eagle mm support, first enablement on llama4 by @morgendave in https://github.com/vllm-project/vllm/pull/20788
* [Misc] Add unit tests for chunked local attention by @sarckk in https://github.com/vllm-project/vllm/pull/21692
* [Bugfix] Fix MTP weight loading  by @benchislett in https://github.com/vllm-project/vllm/pull/21941
* Add FlashInfer allreduce RMSNorm Quant fusion by @ilmarkov in https://github.com/vllm-project/vllm/pull/21069
* [Feature] Add Flashinfer MoE Support for Compressed Tensor NVFP4 by @yewentao256 in https://github.com/vllm-project/vllm/pull/21639
* Add DeepGEMM to Dockerfile in vllm-base image by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/21533
* Move flashinfer-python to optional extra `vllm[flashinfer]` by @mgoin in https://github.com/vllm-project/vllm/pull/21959
* [Refactor] Remove Duplicate `per_block_cast_to_fp8`, Remove Dependencies of DeepGEMM by @yewentao256 in https://github.com/vllm-project/vllm/pull/21787
* [Bugfix] Fix: Fix multi loras with tp >=2 and LRU cache by @charent in https://github.com/vllm-project/vllm/pull/20873
* [Misc] Automatically resolve HF processor init kwargs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22005
* [BugFix] fix: aot passes kvcache dtype information by @mickaelseznec in https://github.com/vllm-project/vllm/pull/19750
* [Model] [Quantization] Support quantization for Gemma3n by @kylesayrs in https://github.com/vllm-project/vllm/pull/21974
* [Doc] Add Voxtral to Supported Models page by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22059
* Update sampling_metadata.py by @Aviadr-neureality in https://github.com/vllm-project/vllm/pull/21937
* [Doc] Fix a syntax error of example code in structured_outputs.md by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/22045
* [Bugfix] Disable multi-modal preprocessor cache for DP by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21896
* [Core] Avoid repeated len(block_token_ids) check in hash_request_tokens by @linzebing in https://github.com/vllm-project/vllm/pull/21781
* [Frontend] Align tool_choice="required" behavior with OpenAI when tools is empty by @n0gu-furiosa in https://github.com/vllm-project/vllm/pull/21052
* Revert precompile wheel changes by @simon-mo in https://github.com/vllm-project/vllm/pull/22055
* [Doc] Add example for Step3-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/22061
* [Bugfix] Add log prefix in non-dp mode engine core by @wuhang2014 in https://github.com/vllm-project/vllm/pull/21889
* [Misc] Remove upper bound in openai package version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22060
* [Doc] Added warning of speculating with draft model by @david6666666 in https://github.com/vllm-project/vllm/pull/22047
* [Quantization] Enable BNB support for InternS1 by @jeejeelee in https://github.com/vllm-project/vllm/pull/21953
* Revert "Update sampling_metadata.py (#21937)" by @hmellor in https://github.com/vllm-project/vllm/pull/22088
* [Speculative Decoding] Add `speculators` config support by @dsikka in https://github.com/vllm-project/vllm/pull/21345
* [BUG] [ROCm] Fix import bug on ROCm by @tjtanaa in https://github.com/vllm-project/vllm/pull/22083
* Fix `get_kwargs` for case where type hint is `list[Union[str, type]]` by @hmellor in https://github.com/vllm-project/vllm/pull/22016
* [Bugfix] Check NVIDIA artifactory is accessible before using flashinfer cubin kernels by @mgoin in https://github.com/vllm-project/vllm/pull/21893
* feat(multimodal): Add customizable background color for RGBA to RGB conversion by @ahengljh in https://github.com/vllm-project/vllm/pull/22052
* [Bugfix][PD] set max_completion_tokens=1 if req has this value by @Abirdcfly in https://github.com/vllm-project/vllm/pull/21841
* [Refactor] Fix Compile Warning #1444-D by @yewentao256 in https://github.com/vllm-project/vllm/pull/21462
* [BugFix] Update AttnFusionPass cache key by @zou3519 in https://github.com/vllm-project/vllm/pull/21947
* [BugFix] Don't change title of top-level process by @njhill in https://github.com/vllm-project/vllm/pull/22032
* [Docs] use `uv` in CPU installation docs by @davidxia in https://github.com/vllm-project/vllm/pull/22089
* Deprecate `--disable-log-requests` and replace with `--enable-log-requests` by @hmellor in https://github.com/vllm-project/vllm/pull/21739
* Improve documentation of `ModelConfig.try_get_generation_config` to prevent future confusion by @hmellor in https://github.com/vllm-project/vllm/pull/21526
* [Bugfix] Fix glm4.1v video inference issue by @Isotr0py in https://github.com/vllm-project/vllm/pull/22067
* [Bugfix] fix when skip tokenizer init by @lengrongfu in https://github.com/vllm-project/vllm/pull/21922
* security policy: take 1 by @sidhpurwala-huzaifa in https://github.com/vllm-project/vllm/pull/21119
* [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21837
* Enable headless models for pooling in the Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21767
* [Misc] Minor enhancement of benchmark_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/22068
* Fix pre-commit failure for SECURTIY.md by @mgoin in https://github.com/vllm-project/vllm/pull/22102
* [compile][startup] Disable C++ compilation of symbolic shapes by @anijain2305 in https://github.com/vllm-project/vllm/pull/20836
* Introduce RayPPCommunicator for ray-based PP by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21660
* Add lora test for tp>1 case for TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21970
* [BugFix] Harden distributed DP startup by @njhill in https://github.com/vllm-project/vllm/pull/21538
* [CI] Initial tests for SM100 Blackwell runner by @mgoin in https://github.com/vllm-project/vllm/pull/21877
* [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel by @yewentao256 in https://github.com/vllm-project/vllm/pull/22036
* feat: Add Support GPTQ Quantization MOE on ROCM vllm serve by @JartX in https://github.com/vllm-project/vllm/pull/21733
* [V1][CUDA] Full cudagraph support for FlashInfer by @fhl2000 in https://github.com/vllm-project/vllm/pull/21367
* [Model] Qwen2.5 VL SiLU-and-Mul by @vllmellm in https://github.com/vllm-project/vllm/pull/22066
* [Misc] `VLLM_TARGET_DEVICE.lower()` by @NickLucche in https://github.com/vllm-project/vllm/pull/22101
* [Misc] DeepGemmExperts : Avoid JIT generation in the hot-path by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21955
* [Speculators][Speculative Decoding] Add Qwen Eagle3 Support by @dsikka in https://github.com/vllm-project/vllm/pull/21835
* [BugFix] Improve internal DP load balancing by @njhill in https://github.com/vllm-project/vllm/pull/21617
* [Test] Add Unit Test for Batched DeepGEMM by @yewentao256 in https://github.com/vllm-project/vllm/pull/21559
* [Attention][DBO] Add support for "splitting" the CommonAttentionMetadata by @SageMoore in https://github.com/vllm-project/vllm/pull/21153
* [FEAT][ROCm] Enable running Flash Attention as ViT attn backend for Qwen-VL models on ROCm platform. by @vllmellm in https://github.com/vllm-project/vllm/pull/22069
* [Misc] Getting and passing ray runtime_env to workers by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22040
* Fix test_kv_sharing_fast_prefill flakiness by @sarckk in https://github.com/vllm-project/vllm/pull/22038
* [Bugfix] Mamba2 remove bugged initial state condition in chunk scan by @cyang49 in https://github.com/vllm-project/vllm/pull/22034
* docs: remove deprecated disable-log-requests flag by @ywang96 in https://github.com/vllm-project/vllm/pull/22113
* [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20000
* for glm-4.1V update by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22000
* [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead by @cyang49 in https://github.com/vllm-project/vllm/pull/21075
* [Frontend] Improve error message for too many mm items by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22114
* [V1] [Hybrid] Validate compatibility of attention backend batch reordering at init time by @tdoublep in https://github.com/vllm-project/vllm/pull/21557
* [xpu]support moe models on XPU platform by @yma11 in https://github.com/vllm-project/vllm/pull/21643
* Revert "[compile][startup] Disable C++ compilation of symbolic shapes" by @xiszishu in https://github.com/vllm-project/vllm/pull/22122
* [Misc] Bump ray to 2.48.0 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22123
* [Fix] Fix llama4 modelopt weight loading error by @jiahanc in https://github.com/vllm-project/vllm/pull/22107
* [Misc] Add tensor schema test coverage for multimodal models by @Isotr0py in https://github.com/vllm-project/vllm/pull/21754
* [Benchmark] Support ready check timeout in `vllm bench serve` by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/21696
* Support CUTLASS NVFP4 (w4a4) for Blackwell Geforce GPUs (SM120) by @LopezCastroRoberto in https://github.com/vllm-project/vllm/pull/21309
* [Misc] update doc comment for send by @andyxning in https://github.com/vllm-project/vllm/pull/22026
* [executor] feat: add supports_pp attr to executors by @eric-haibin-lin in https://github.com/vllm-project/vllm/pull/21786
* [V1] [P/D] Refactor KV Connector Path by @sdavidbd in https://github.com/vllm-project/vllm/pull/21980
* [Responses API] Disable response store by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22137
* [CI/Build][Bugfix] Fix Qwen2.5 tests in CPU CI via fallback silu_and_mul to torch native implementation by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/22145
* Add chat doc in quick start by @TankNee in https://github.com/vllm-project/vllm/pull/21213
* fuse fp32 for GLM-4.5 e_score_correction_bias by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22143
* [Bugfix] Fix failing multimodal standard test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22153
* Use `aiohttp` connection pool for benchmarking by @eicherseiji in https://github.com/vllm-project/vllm/pull/21981
* [fix] fix correct assertion syntax error in attention utils. by @skyloevil in https://github.com/vllm-project/vllm/pull/22154
* [RLHF] Fix torch.dtype not serializable in example by @22quinn in https://github.com/vllm-project/vllm/pull/22158
* [PD] add test for chat completions endpoint by @Abirdcfly in https://github.com/vllm-project/vllm/pull/21925
* remove duplicate code within cleanup_dist_env_and_memory by @andyxning in https://github.com/vllm-project/vllm/pull/22147
* Add tree attention backend for v1 (part 1) by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/20401
* [refactor] improve ConstantList exception specificity by @skyloevil in https://github.com/vllm-project/vllm/pull/22156
* Remove index_put from MM embeddings merging by @chenxi-yang in https://github.com/vllm-project/vllm/pull/22105
* [CI Bugfix] Fix wNa16 kernel not found for test_shared_storage_connector_hashes by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/22163
* [Misc] Modify the organization of GLM series  by @jeejeelee in https://github.com/vllm-project/vllm/pull/22171
* [feat] move WEIGHT_SCALE_SUPPORTED into raise block to accelerate RLHF weight loading by @weixiao-huang in https://github.com/vllm-project/vllm/pull/21164
* [Bugfix] Fix failing GGUF models test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22174
* [Sampler] Support returning all logprobs or logits by @22quinn in https://github.com/vllm-project/vllm/pull/21792
* [Doc] Update pooling model docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22186
* Fix Arcee model weight loading: Add custom load_weights by @alyosha-swamy in https://github.com/vllm-project/vllm/pull/21725
* [Responses API] Ignore `store=True` and process the request by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22185
* [Bug] Update auto_tune.sh to separate benchmarking and profiling. by @ericehanley in https://github.com/vllm-project/vllm/pull/21629
* [Bugfix][V1][P/D]Fix the uneven polling issue in the toy proxy for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/21819
* [NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading by @nvpohanh in https://github.com/vllm-project/vllm/pull/22073
* [Bugfix] V1 Fix the cursor leakage issue during request scheduling. by @CLFutureX in https://github.com/vllm-project/vllm/pull/21173
* Revert "[Bugfix] V1 Fix the cursor leakage issue during request scheduling." by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22223
* [V1] reduce block size for tree attention correctness test to fix 'ouâ€¦ by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/22207
* [V0 deprecation][P/D] Deprecate v0 `KVConnectorBase` code (1/2) by @lk-chen in https://github.com/vllm-project/vllm/pull/21785
* [FEAT] Refactor ROPE into module by @tjtanaa in https://github.com/vllm-project/vllm/pull/22192
* [ROCm][Bugfix] Compilation passes fix by @gshtras in https://github.com/vllm-project/vllm/pull/22202
* self.gate dtype update for GLM-4.5 by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22203
* [Log] DeepGEMM Update Log for Unaligned Problem Size by @yewentao256 in https://github.com/vllm-project/vllm/pull/22208
* fix: kimi_k2 return empty tool call list by @tlipoca9 in https://github.com/vllm-project/vllm/pull/22149
* [Misc] Remove pass_config from CompilationConfig dump_json excluded by @elvischenv in https://github.com/vllm-project/vllm/pull/21911
* [Doc] add backend to doc string of initialize_model_parallel by @andyxning in https://github.com/vllm-project/vllm/pull/22142
* [Misc] log more detailed message for ensure_model_parallel_initialized by @andyxning in https://github.com/vllm-project/vllm/pull/22144
* Optimize configuration access with LRU cache in custom ops by @skyloevil in https://github.com/vllm-project/vllm/pull/22204
* [Bugfix] Misaligned params in TreeAttentionImpl by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22226
* [UX] Fail if an invalid attention backend is specified by @mgoin in https://github.com/vllm-project/vllm/pull/22217
* [Core] Factor out common logic for MM budget calculation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22228
* [Model] Pooling model activation supports per request control by PoolingParams by @noooop in https://github.com/vllm-project/vllm/pull/20538
* [Docs][TPU] Highlight TPU Software version selection by @NickLucche in https://github.com/vllm-project/vllm/pull/22242
* Migrate KimiVLImagePixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21769
* [Feature] Non-contiguous Support for FP8 Quantization by @yewentao256 in https://github.com/vllm-project/vllm/pull/21961
* [NVIDIA] Support Flashinfer TRT-LLM Prefill Attention Kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/22095
* [Misc] correct static type check for GroupCoordinator by @andyxning in https://github.com/vllm-project/vllm/pull/21946
* [V0 Deprecation][TPU] Remove V1 flag check from tests by @NickLucche in https://github.com/vllm-project/vllm/pull/22248
* Use UV_LINK_MODE=copy in Dockerfile to avoid hardlink fail by @mgoin in https://github.com/vllm-project/vllm/pull/22128
* [CI/Build] Update flashinfer to 0.2.9 by @mgoin in https://github.com/vllm-project/vllm/pull/22233
* [Refactor] Remove Unused Environment Variable `VLLM_NO_DEPRECATION_WARNING` by @yewentao256 in https://github.com/vllm-project/vllm/pull/22199
* [V1] port xformers backend to v1 by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/21342
* [bugfix] fix blackwell deepep installation by @youkaichao in https://github.com/vllm-project/vllm/pull/22255
* [CI][TPU] Fix docker clean up by @lsy323 in https://github.com/vllm-project/vllm/pull/22271
* [Bugfix] Remove faulty test for oot attention backend by @mgoin in https://github.com/vllm-project/vllm/pull/22286
* [Bugfix] Fix 3D input passed into cutlass_scaled_mm by @mgoin in https://github.com/vllm-project/vllm/pull/22278
* [Bugfix] Fix MoE BNB version by @jeejeelee in https://github.com/vllm-project/vllm/pull/22260
* [Perf] Parallelize fill_bitmask to accelerate high-throughput guided decoding by @benchislett in https://github.com/vllm-project/vllm/pull/21862
* [Bugfix] Skip dead and non-GPU nodes for Ray DP engine allocation by @ruisearch42 in https://github.com/vllm-project/vllm/pull/22275
* [Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/22264
* Upgrade FA3 for attention sink by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22313
* Increase openai-python version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22316
* Add attention sink in attention backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22320
* Update transformers to `v4.55` by @hmellor in https://github.com/vllm-project/vllm/pull/21931
* Add GPT-OSS model code and config [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22327
* [ROCm] Add attention sink to use_rocm_custom_paged_attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22329
* [GptOss] Add GptOss reasoning parser to support structure output by @heheda12345 in https://github.com/vllm-project/vllm/pull/22322
* [gpt-oss] flashinfer attention sink init by @zyongye in https://github.com/vllm-project/vllm/pull/22330
* [gpt-oss] Add openai-harmony as default dependency by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22332
* [Misc] Clean up duplicated hf overrides by @Isotr0py in https://github.com/vllm-project/vllm/pull/22311
* [gpt-oss] Add Tool/ConversationContext classes and harmony_utils by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22340
* [gpt-oss] add model to supported models doc by @ywang96 in https://github.com/vllm-project/vllm/pull/22336
* [gpt-oss] Support chat completion api by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22342
* [Minor] Fix type  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22347
* [BugFix] Fix FA2 RuntimeError when sinks is provided by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22365
* add the codes to check AMD Instinct GPU number by @zhangnju in https://github.com/vllm-project/vllm/pull/22367
* [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22368
* [Bugfix] Make condition in triton kernel constexpr by @gshtras in https://github.com/vllm-project/vllm/pull/22370
* [gpt-oss] Add loop for built-in tool call by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22374
* [gpt-oss] attention sink init fix gemini by @zyongye in https://github.com/vllm-project/vllm/pull/22335
* [gpt-oss] flashinfer mxfp4 by @zyongye in https://github.com/vllm-project/vllm/pull/22339
* [v1] - Mamba1 Attention Metadata by @Josephasafg in https://github.com/vllm-project/vllm/pull/21249
* [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/22399
* [gpt-oss] add demo tool server by @heheda12345 in https://github.com/vllm-project/vllm/pull/22393
* [gpt-oss] fix model config with hf_config by @zyongye in https://github.com/vllm-project/vllm/pull/22401
* Fix trtllm-gen attention env and add attention sink by @IwakuraRein in https://github.com/vllm-project/vllm/pull/22378
* Update `flashinfer-python==0.2.10` by @mgoin in https://github.com/vllm-project/vllm/pull/22389
* [model] Support MiniCPM-V 4.0 by @tc-mb in https://github.com/vllm-project/vllm/pull/22166
* Support encoder_only attention for FlexAttention by @maxdebayser in https://github.com/vllm-project/vllm/pull/22273
* [Attention] Support multiple attention metadata builders per kv_cache_spec  + proper local attention no hybrid kv cache fix by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21588
* [XPU]Fix `flash_attn_varlen_func` interface on xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/22350
* [Qwen3] Enable dual-chunk-attention support for Qwen3 models. by @sighingnow in https://github.com/vllm-project/vllm/pull/21924
* [Bugfix] Fix wrong method name in Intern-S1 image processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22417
* Use float32 for test_completion.py by @mgoin in https://github.com/vllm-project/vllm/pull/22385
* [Bugfix]: Fix the streaming output for function calls in the minimax by @qscqesze in https://github.com/vllm-project/vllm/pull/22015
* [Bugfix] Add proper comparison for package versions by @syedmba in https://github.com/vllm-project/vllm/pull/22314
* Update `hf_xet` pin to resolve hangs by @hmellor in https://github.com/vllm-project/vllm/pull/22356
* Optimize logger init performance by using module-level constants by @skyloevil in https://github.com/vllm-project/vllm/pull/22373
* preload heavy modules when mp method is forkserver by @lionelvillard in https://github.com/vllm-project/vllm/pull/22214
* [gpt-oss] Convert user input to harmony format by @heheda12345 in https://github.com/vllm-project/vllm/pull/22402
* [Bugfix] EPLB load statistics problem by @david6666666 in https://github.com/vllm-project/vllm/pull/22167
* [CI] Skip the pooling models that do not support transformers v4.55 by @noooop in https://github.com/vllm-project/vllm/pull/22411
* [Bench] Split serve.py:main into async/async versions by @lk-chen in https://github.com/vllm-project/vllm/pull/22405
* [Model] Switch to Fused RMS norm in Qwen2.5_VL model. by @vllmellm in https://github.com/vllm-project/vllm/pull/22184
* [Frontend] Update OpenAI error response to upstream format by @msanft in https://github.com/vllm-project/vllm/pull/22099
* [Misc] Support routing logic simulation by @minosfuture in https://github.com/vllm-project/vllm/pull/21990
* feat: Add --enable-log-outputs flag for logging model generations by @mizadri in https://github.com/vllm-project/vllm/pull/20707
* [Docs] Add missing dependency for docs build by @hmellor in https://github.com/vllm-project/vllm/pull/22435
* Add H20-3e fused MoE kernel tuning configs for GLM-4.5 by @JaceyShao in https://github.com/vllm-project/vllm/pull/22433
* [Misc] Enhance code formatting in mxfp4.py  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22423
* [Doc] Fix link to prefix caching design by @sarckk in https://github.com/vllm-project/vllm/pull/22384
* [Docs] Factor out troubleshooting to its own guide; add section for Ray Observability by @crypdick in https://github.com/vllm-project/vllm/pull/21578
* [Doc] update docs for nightly benchmarks by @andrewkchan in https://github.com/vllm-project/vllm/pull/12022
* [Docs] Update features/disagg_prefill, add v1 examples and development by @david6666666 in https://github.com/vllm-project/vllm/pull/22165
* [Core] Store only the keys for multi-modal data in P0 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22198
* [Bugfix] Add missing `packed_modules_mapping` to `DeepseekV2ForCausalLM` by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/22352
* [Tool] Fix auto tool call by @heheda12345 in https://github.com/vllm-project/vllm/pull/22434
* [gpt-oss] Generate ResponseOutputItem from Harmony Message by @heheda12345 in https://github.com/vllm-project/vllm/pull/22410
* Fix pre-commit error in main by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22462
* [Core] Simplify mm processing cache by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22457
* [Frontend] Use engine argument to control MM cache size by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22441
* Remove `from_dict` from `SpeculativeConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/22451
* [Misc] normalize multiprocessing Queue usage by @andyxning in https://github.com/vllm-project/vllm/pull/22371
* [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine by @tjtanaa in https://github.com/vllm-project/vllm/pull/21496
* [PERF] Use pybase64 to more quickly decode prompt embeddings by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/22469
* Add ModelOpt Qwen3 nvfp4 support by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/20101
* Support Tensorrt-LLM MoE fp4 for low-latency by @wenscarl in https://github.com/vllm-project/vllm/pull/21331
* Fix Flashinfer CUTLASS MOE Allgather by @wenscarl in https://github.com/vllm-project/vllm/pull/21963
* [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) by @0xjunhao in https://github.com/vllm-project/vllm/pull/22131
* [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/22065
* not tie_word_embeddings for glm-4.5 and glm-4.5v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22460
* Optimize MiniCPMO mask creation with vectorized implementation by @skyloevil in https://github.com/vllm-project/vllm/pull/22464
* Fix pre-commit by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22487
* [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 by @nvpohanh in https://github.com/vllm-project/vllm/pull/22426
* [Doc] Sleep mode documentation by @iAmir97 in https://github.com/vllm-project/vllm/pull/22310
* [bench] Fix benchmark/serve.py to ignore unavailable results by @lk-chen in https://github.com/vllm-project/vllm/pull/22382
* [CI/Build] Fix multimodal tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22491
* [Misc] Begin deprecation of `get_tensor_model_*_group` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22494
* [Misc] fix openai version by @lengrongfu in https://github.com/vllm-project/vllm/pull/22485
* [BugFix] Don't cancel asyncio tasks directly from destructors by @njhill in https://github.com/vllm-project/vllm/pull/22476
* [Docs] Improve API docs (+small tweaks) by @hmellor in https://github.com/vllm-project/vllm/pull/22459
* Remove exception for Python 3.8 typing from linter by @hmellor in https://github.com/vllm-project/vllm/pull/22506
* [gpt-oss] triton kernel mxfp4 by @zyongye in https://github.com/vllm-project/vllm/pull/22421
* [Benchmark] Add benchmark tool for multi turn conversations by @pliops-daniels in https://github.com/vllm-project/vllm/pull/20267
* [gpt-oss] guard import when triton kernel is not installed by @zyongye in https://github.com/vllm-project/vllm/pull/22529
* [Docs] Rename â€œDistributed inference and servingâ€ to â€œParallelism & Scalingâ€ by @crypdick in https://github.com/vllm-project/vllm/pull/22466
* [gpt-oss] Support tool call and implement MCP tool server by @heheda12345 in https://github.com/vllm-project/vllm/pull/22427
* [BugFix] Fix IMA FlashMLA full cuda-graph and DP + Update FlashMLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21691
* [Misc] DeepGEMM : Avoid JIT generation in the hot-path by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/22215
* [Bugfix] Update FA commit hash by @tdoublep in https://github.com/vllm-project/vllm/pull/22546
* Skip Qwen 1 in CI because remote code is no longer compatible with Transformers by @hmellor in https://github.com/vllm-project/vllm/pull/22536
* [Docs] fix broken links in metrics.md by @GuyStone in https://github.com/vllm-project/vllm/pull/22315
* [Frontend] Add unix domain socket support by @yyweiss in https://github.com/vllm-project/vllm/pull/18097
* Extract `CompilationConfig` from `config.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22524
* Drop flaky test_healthcheck_response_time by @russellb in https://github.com/vllm-project/vllm/pull/22539
* [XPU] upgrade torch 2.8 on for XPU by @jikunshang in https://github.com/vllm-project/vllm/pull/22300
* [BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D by @Pradyun92 in https://github.com/vllm-project/vllm/pull/22317
* [Bugfix] Fix ModernBert cuda graph capturing in v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/21901
* Implicit language-model-only mode via limit-mm-per-prompt by @ywang96 in https://github.com/vllm-project/vllm/pull/22299
* [Doc] Add usage of implicit text-only mode  by @ywang96 in https://github.com/vllm-project/vllm/pull/22561
* Remove mamba_ssm from vLLM requirements; install inside test container using `--no-build-isolation` by @tdoublep in https://github.com/vllm-project/vllm/pull/22541
* [Log] Add Warning for Deprecation of DeepGEMM old version by @yewentao256 in https://github.com/vllm-project/vllm/pull/22194
* [V1] [Hybrid] Support Minimax-Text-01 in V1  by @tdoublep in https://github.com/vllm-project/vllm/pull/22151
* v1: Pass KVConnectorOutput to scheduler-side by @orozery in https://github.com/vllm-project/vllm/pull/22157
* [Misc] Use config definitions from Transformers library by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21913
* Fix loading of quantized BigCode models by @eldarkurtic in https://github.com/vllm-project/vllm/pull/22463
* [TPU] Add support for online w8a8 quantization by @kyuyeunk in https://github.com/vllm-project/vllm/pull/22425
* [ROCm][Misc] Rename the context_len to seq_len in ROCm custom paged attention kernel by @charlifu in https://github.com/vllm-project/vllm/pull/22097
* [Bugfix] Fix failing GPT-OSS initialization test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22557
* [Bugfix] Fix CI moe kernel failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/22556
* Update docs for Minimax-Text support by @tdoublep in https://github.com/vllm-project/vllm/pull/22562
* GLM-4.5V with new class name at transformers by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22520
* [CI] [Hybrid] Speed up hybrid models test by removing large models  by @tdoublep in https://github.com/vllm-project/vllm/pull/22563
* [Docs] Reduce noise in docs and `--help` from the JSON tip by @hmellor in https://github.com/vllm-project/vllm/pull/22567
* Move `ParallelConfig` from `config/__init__.py` to `config/parallel.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22565
* [Model] Gemma3n MM by @NickLucche in https://github.com/vllm-project/vllm/pull/20495
* [Bugfix] Fix basic models tests hanging due to mm processor creation by @Isotr0py in https://github.com/vllm-project/vllm/pull/22571
* [FEAT] [Performance] Add triton mrope to replace the torch code path by @tjtanaa in https://github.com/vllm-project/vllm/pull/22375
* [V1] [Hybrid] Enable Full CUDA Graph (decode-only) for Mamba layers by @tdoublep in https://github.com/vllm-project/vllm/pull/21401
* [oss] Init gpt-oss bf16 support by @jeejeelee in https://github.com/vllm-project/vllm/pull/22508
* [Config] add "qwen" as a native eagle3 target supported model by @lec77 in https://github.com/vllm-project/vllm/pull/22333
* Improve fast_topk function with type hints and documentation by @skyloevil in https://github.com/vllm-project/vllm/pull/22530
* [TPU] kv cache update kernel doesn't need to be padded slices to multiple of num_slices_per_block by @yaochengji in https://github.com/vllm-project/vllm/pull/22394
* Refactor sliding window configuration to Transformers best practice by @hmellor in https://github.com/vllm-project/vllm/pull/21927
* [Minor] Fix pre-commit error on main by @Isotr0py in https://github.com/vllm-project/vllm/pull/22579
* [Misc] code clean duplicate set_current_vllm_config in _set_vllm_config by @andyxning in https://github.com/vllm-project/vllm/pull/22566
* [Doc] Fix API doc link in side navigation by @22quinn in https://github.com/vllm-project/vllm/pull/22585
* [Misc] Further refine type annotations in parallel state by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22499
* [Docs] Fix warnings in docs build by @hmellor in https://github.com/vllm-project/vllm/pull/22588
* [Misc] Replace flaky image urls in pixtral test by @Isotr0py in https://github.com/vllm-project/vllm/pull/22574
* Move `CacheConfig` from `config/__init__.py` to `config/cache.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22586
* [doc] add beijing meetup links by @youkaichao in https://github.com/vllm-project/vllm/pull/22596
* [doc] add alibaba cloud as sponsor by @youkaichao in https://github.com/vllm-project/vllm/pull/22597
* [Bugfix][Kernel] Support partial rotary embedding for MRoPE triton kernel by @Isotr0py in https://github.com/vllm-project/vllm/pull/22593
* Fix(benchmarks): allow multiple mm contents in OpenAI Chat Completion Benchmarks by @h-brenoskuk in https://github.com/vllm-project/vllm/pull/22534
* Migrate LlavaNextImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21774
* Remove redundant row_indices unsqueeze operation in MiniCPMO by @skyloevil in https://github.com/vllm-project/vllm/pull/22528
* Fix TensorSchema validation test for symbolic dims by @bbeckca in https://github.com/vllm-project/vllm/pull/22366
* enable Docker-aware precompiled wheel setup by @dougbtv in https://github.com/vllm-project/vllm/pull/22106
* Migrate LlavaNextVideoPixelInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21843
* Migrate LlavaImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21770
* [CI/Build] Fix tensorizer test for load_format change by @22quinn in https://github.com/vllm-project/vllm/pull/22583
* [BugFix] Fix KVConnectorOutput TPU breakage by @njhill in https://github.com/vllm-project/vllm/pull/22598
* [Misc][gpt-oss] Add rules to label gpt-oss related PRs by @draftbk in https://github.com/vllm-project/vllm/pull/22600
* [Misc][gpt-oss] guard import when triton kernel when not up to date  by @zhewenl in https://github.com/vllm-project/vllm/pull/22584
* [BugFix] Fix logits repetition penalty cuda check by @PicoCreator in https://github.com/vllm-project/vllm/pull/22592
* [ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module. by @vllmellm in https://github.com/vllm-project/vllm/pull/22521
* Support token_type_ids in V1 with less code changes by @maxdebayser in https://github.com/vllm-project/vllm/pull/21985
* [Misc] benchmark_moe supports expert parallel by @jeejeelee in https://github.com/vllm-project/vllm/pull/22251
* [BUGFIX] KeyError 'layers.14.mlp.gate.g_idx' for Qwen3-MoE with GPTQ on ROCm by @JartX in https://github.com/vllm-project/vllm/pull/22017
* [Docs] Add comprehensive CLI reference for all large `vllm` subcommands by @hmellor in https://github.com/vllm-project/vllm/pull/22601
* [Misc] Move tensor schema tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22612
* [Misc] Move jsontree to utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22622
* [Model] NemotronH Support  by @danielafrimi in https://github.com/vllm-project/vllm/pull/22349
* Document aarch64 CPU support works by @ericcurtin in https://github.com/vllm-project/vllm/pull/22646
* [Misc] Further clean up some redundant config definitions by @Isotr0py in https://github.com/vllm-project/vllm/pull/22649
* [Feature] Add `VLLM_USE_DEEP_GEMM_E8M0` Env to Control E8M0 Scale by @yewentao256 in https://github.com/vllm-project/vllm/pull/21968
* fix: NIXL connector transfers partial block to pass full multi-modal context by @GuanLuo in https://github.com/vllm-project/vllm/pull/21074
* [Model] Pooling models default to using chunked prefill & prefix caching if supported. by @noooop in https://github.com/vllm-project/vllm/pull/20930
* [CI/Build] Skip Mllama HF runner tests with Transformers v4.55.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/22659
* [BugFix] [Spec Decode] Remove LlamaForCausalLMEagle3 to fix CI by @22quinn in https://github.com/vllm-project/vllm/pull/22611
* [CI] Skip Tree Attn Test in `test_max_len.py` to unblock CI by @tjtanaa in https://github.com/vllm-project/vllm/pull/22664
* Support more parallel styles in Transformers backend TP by @hmellor in https://github.com/vllm-project/vllm/pull/22651
* [gpt-oss] Support streaming in response API by @heheda12345 in https://github.com/vllm-project/vllm/pull/22431
* [gpt-oss] Add test for response API + harmony (but skipped) by @heheda12345 in https://github.com/vllm-project/vllm/pull/22554
* Enable 4bit bnb prequant MOE by @py-andy-c in https://github.com/vllm-project/vllm/pull/21548
* Re-enable Xet on TPU tests now that `hf_xet` has been updated by @hmellor in https://github.com/vllm-project/vllm/pull/22666
* Upgrade FlashInfer to v0.2.11 by @nvpohanh in https://github.com/vllm-project/vllm/pull/22613
* [CI Failure] Use float32 for tests/entrypoints/openai/test_audio.py by @mgoin in https://github.com/vllm-project/vllm/pull/22686
* [CI] Increase timeout for test_completion_with_image_embeds by @mgoin in https://github.com/vllm-project/vllm/pull/22670
* Migrate MiniCPMVImageInputs to TensorSchema by @bbeckca in https://github.com/vllm-project/vllm/pull/21939
* [gpt-oss] Fix mxfp4 support by @heheda12345 in https://github.com/vllm-project/vllm/pull/22700
* [gpt-oss] Small bug fixes for frontend by @heheda12345 in https://github.com/vllm-project/vllm/pull/22512
* Fix passing `SpeculativeConfig` from the CLI by @hmellor in https://github.com/vllm-project/vllm/pull/22652
* [Doc] Added unmentioned required option "method" in the usage of EAGLE-3 based models by @hsliuustc0106 in https://github.com/vllm-project/vllm/pull/21737
* [doc] Update x86 CPU-inference installation doc to reflect optionality of AVX512f  by @sooraj-satheesh in https://github.com/vllm-project/vllm/pull/22707
* [Bugfix] Fix ModernBert load & Enable sliding window attention for bidirectional attention. by @noooop in https://github.com/vllm-project/vllm/pull/22637
* Move `SchedulerConfig` from `config/__init__.py` to `config/scheduler.py` by @hmellor in https://github.com/vllm-project/vllm/pull/22626
* [DOC] update v1_guide with INTEL HW by @xuechendi in https://github.com/vllm-project/vllm/pull/22679
* [New Model] Support Command-A-Vision by @dongluw in https://github.com/vllm-project/vllm/pull/22660
* [V0] Correct CUDA Graph capture for encoder-decoder models by @Sugar-zsg in https://github.com/vllm-project/vllm/pull/22630
* [Bugfix] Fix erroneous randomly generated cases in bad word testing by @phantomlei3 in https://github.com/vllm-project/vllm/pull/22170
* Fix: AWQ Marlin get_quant_method does not recognize "modules_to_not_convert" by @Jun-Howie in https://github.com/vllm-project/vllm/pull/21888
* [Bugfix] Mamba2 SSD varlen bug fix initstates decay, improve test, assert chunk pwr 2 by @RishiAstra in https://github.com/vllm-project/vllm/pull/21783
* [LMCache][Example] Align the PYTHONHASHSEED for prefillers and decoders for KV chunks hashing by @zejunchen-zejun in https://github.com/vllm-project/vllm/pull/21161
* [Misc] remove GH discussions link by @jeejeelee in https://github.com/vllm-project/vllm/pull/22722
* [gpt-oss] Enable gpt-oss on ampere by @zyongye in https://github.com/vllm-project/vllm/pull/22714
* [Docs] Improve docs navigation by @hmellor in https://github.com/vllm-project/vllm/pull/22720
* [BugFix][Nixl][PD] Fix heterogenous TP by @NickLucche in https://github.com/vllm-project/vllm/pull/22663
* Officially support SmolLM3 using the Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/22665
* [CI Failure] fix tests/entrypoints/openai/test_skip_tokenizer.py by @noooop in https://github.com/vllm-project/vllm/pull/22708
* Fix Llama4 FlashInfer FP4 MoE issues by @nvpohanh in https://github.com/vllm-project/vllm/pull/22511
* [Bugfix][CI] Fix `test_remote_decode_lifecycle.py::test_short_prompt_lifecycle` by @NickLucche in https://github.com/vllm-project/vllm/pull/22727
* [Benchmark] Fix terminal colors in benchmark_serving_multi_turn (python 3.12) by @pliops-daniels in https://github.com/vllm-project/vllm/pull/22730
* Add: `SupportsEagle3` interface for explicit EAGLE3 support by @rahul-tuli in https://github.com/vllm-project/vllm/pull/22642
* Add more test scenario for tensor schema by @teekenl in https://github.com/vllm-project/vllm/pull/22733
* [Chore] Update CODEOWNERS to include @yewentao256 for CUDA kernels, attention backends, quantization, and related tests by @yewentao256 in https://github.com/vllm-project/vllm/pull/22741
* [Kernel][AMD] Avoid D2H copy and cumsum kernel by @mxz297 in https://github.com/vllm-project/vllm/pull/22683
* [CI][Nixl] Check kv cache layout during handshake by @NickLucche in https://github.com/vllm-project/vllm/pull/22745
* Fix torch version check for SM100 mxfp4  by @zifeitong in https://github.com/vllm-project/vllm/pull/22535
* [Misc] parametrize 'dtype' in test_flash_mla by @RUTHLESS-BOT in https://github.com/vllm-project/vllm/pull/22641
* [Bugfix] Bump DeepGEMM Version to Fix SMXX Layout Issues by @frankwang28 in https://github.com/vllm-project/vllm/pull/22606
* [Docs] Hide the navigation and toc sidebars on home page by @hmellor in https://github.com/vllm-project/vllm/pull/22749
* Fix Transformers backend tensor parallel for multimodal models by @hmellor in https://github.com/vllm-project/vllm/pull/22673
* [Model] Decouple glm4v by @jeejeelee in https://github.com/vllm-project/vllm/pull/22751
* Add hardware plugins to installation doc by @mgoin in https://github.com/vllm-project/vllm/pull/22732
* [V0 Deprecation] Remove multi-step scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22138
* [Misc] Remove tests/multi_step/__init__.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22778
* [V0 Deprecation] Remove args for multi-step scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22779
* Fix cuda illegal mem access with Llama4 TP8 + rms_norm custom op by @nvpohanh in https://github.com/vllm-project/vllm/pull/22701
* [Bugfix] Fix default enable for CUTLASS MLA on SM100 by @mgoin in https://github.com/vllm-project/vllm/pull/22738
* Force TRTLLM attention for gpt-oss on SM100 by @mgoin in https://github.com/vllm-project/vllm/pull/22678
* Remove unneeded ROCm platform import when using CUDA by @mgoin in https://github.com/vllm-project/vllm/pull/22765
* [Bug] Fix Unexpected Keyword Argument 'w1_bias' by @yewentao256 in https://github.com/vllm-project/vllm/pull/22757
* [Perf] Support topk softmax fused kernel for broader num_experts by @shixianc in https://github.com/vllm-project/vllm/pull/22211
* [gpt-oss] upgrade gpt-oss to v0.0.3 and add version check by @heheda12345 in https://github.com/vllm-project/vllm/pull/22768
* [Model] Add option to run Step3VisionEncoder in DP by @zzh142857 in https://github.com/vllm-project/vllm/pull/22697
* [Model] Add missing prefix to glm4_1v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/22716
* [Bugfix] Fix Nemotron VL image processing by @ducviet00 in https://github.com/vllm-project/vllm/pull/22739
* [Doc] Add max_lora_rank configuration guide by @chi2liu in https://github.com/vllm-project/vllm/pull/22782
* [V1] Add tree drafting tests for eagle spec decoding by @TheEpicDolphin in https://github.com/vllm-project/vllm/pull/22705
* [Platform] Custom ops support for FusedMoe by @wangxiyuan in https://github.com/vllm-project/vllm/pull/22509
* [Frontend] Add chunked processing to handle long inputs in embedding models by @x22x22 in https://github.com/vllm-project/vllm/pull/22280
* [FEATURE] support custom vllm tuned config path by @vermouth1992 in https://github.com/vllm-project/vllm/pull/22791
* [Nixl][CI] Fix tests by @NickLucche in https://github.com/vllm-project/vllm/pull/22806
* [Bugfix][mamba] Fix type annotation of Mamba2Metadata by @heheda12345 in https://github.com/vllm-project/vllm/pull/22787
* Remove unnecessary CUDA sync of qwen image and video preprocess by @cyyever in https://github.com/vllm-project/vllm/pull/22792
* Fix GGUF loader for Qwen3 MoE. by @Gh0u1L5 in https://github.com/vllm-project/vllm/pull/22785
* [Frontend] Multithreaded async multimodal load_bytes by @milesial in https://github.com/vllm-project/vllm/pull/22710
* [Core] Use individual MM items in P0/P1 cache and model runner by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22570
* [Misc] clear and separate error messages for input too long and input + max-tokens too long by @ywang96 in https://github.com/vllm-project/vllm/pull/22803
* [Bugfix] Fix MiniCPMV Image input inference failed by @jio-H in https://github.com/vllm-project/vllm/pull/22813
* [CI/Build] Update VLM common tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22841
* [CI] Fix `tests/v1/e2e/test_kv_sharing_fast_prefill.py` import on test by @NickLucche in https://github.com/vllm-project/vllm/pull/22815
* [CI/Build] Fix param mismatch in `test_eagle_correctness` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22847
* [CI/Build] Skip gpt_big model test because of broken HF model by @Isotr0py in https://github.com/vllm-project/vllm/pull/22848
* [ROCm][Bugfix] Fix compilation error in topk softmax fused kernel by @kliuae in https://github.com/vllm-project/vllm/pull/22819
* Move checklist in PR template by @ProExpertProg in https://github.com/vllm-project/vllm/pull/22852
* [Core] [N-gram SD Optimization][1/n] Propose tokens with a single KMP by @Jialin in https://github.com/vllm-project/vllm/pull/22437
* [CI/Build] Increase pooling tolerance to pass CI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22844
* [CI][Entrypoints]: add filter to generation to filter out invalid tool calls by @wseaton in https://github.com/vllm-project/vllm/pull/22826
* [CI] Fix `tests/distributed/test_ca_buffer_sharing.py` by @ilmarkov in https://github.com/vllm-project/vllm/pull/22849
* [CI] remove flaky v0 test by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/22864
* vLLM Benchmark suite improvement by @louie-tsai in https://github.com/vllm-project/vllm/pull/22119
* [Bugfix] Fix `PixtralHFImagePixelInputs` dynamic shape check by @Isotr0py in https://github.com/vllm-project/vllm/pull/22827
* [BugFix] Threadsafe close async zmq sockets by @njhill in https://github.com/vllm-project/vllm/pull/22877
* Remove Phi 4 Flash configuration workaround by @hmellor in https://github.com/vllm-project/vllm/pull/22723
* [Bugfix] Add reset prefix cache for online serving by @iAmir97 in https://github.com/vllm-project/vllm/pull/22726
* [Doc] fix dead link by @dtrifiro in https://github.com/vllm-project/vllm/pull/22898
* [CI] Re-enable transcriptions `test_long_audio_request` by @NickLucche in https://github.com/vllm-project/vllm/pull/22890
* [Perf] Dont create unnecessary pooling params by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22876
* [Model] Modify the gate implementation of glm4_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/22832
* [Bugfix] Replace custom Encoding class with BatchEncoding in MistralTokenizer by @ZJY0516 in https://github.com/vllm-project/vllm/pull/22786
* [Bugfix] Fix parsing of `--disable-mm-preprocessor-cache` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/22909
* [CI] [Hybrid]  Bump min transformers version for Bamba and Jamba by @tdoublep in https://github.com/vllm-project/vllm/pull/22908
* [Kernel] [Quantization] Add MXFP4 and bias support for marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/22428
* docs: update fastsafetensors usage instructions by @NirLevy98 in https://github.com/vllm-project/vllm/pull/22891
* [CI] Temporarily disable flaky test  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22930
* [Kernel] Add nvfp4 gemm flashinfer backends by @nvjullin in https://github.com/vllm-project/vllm/pull/22346
* [Quantization]: Support compressed-tensors mixed-precision model loading by @dsikka in https://github.com/vllm-project/vllm/pull/22468
* [Core] Return final response for aborted requests from `AsyncLLM.generate` by @njhill in https://github.com/vllm-project/vllm/pull/22283
* [BugFix] Fix initial DP request load imbalance by @njhill in https://github.com/vllm-project/vllm/pull/22910
* [Bugfix] use flash attn on sm90 by @zyongye in https://github.com/vllm-project/vllm/pull/22933
* [Kernel]  Add cuda kernel for gpt_oss activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/22538
* Revert "[Kernel]  Add cuda kernel for gpt_oss activation" by @simon-mo in https://github.com/vllm-project/vllm/pull/22948
* [BugFix][KVConn] Fix use of `get_required_kvcache_layout` by @njhill in https://github.com/vllm-project/vllm/pull/22734
* [BugFix] Fix port lookup in internal DP LB tests by @njhill in https://github.com/vllm-project/vllm/pull/22252
* [CI Perf] Prune tests in `tests/kernels/quantization/` by @mgoin in https://github.com/vllm-project/vllm/pull/22942
* [CI Perf] Prune tests in `tests/kernels/moe/` by @mgoin in https://github.com/vllm-project/vllm/pull/22939
* [CI Perf] Prune tests in `tests/kernels/attention/` by @mgoin in https://github.com/vllm-project/vllm/pull/22936
* refactor: Change scaling factors calculation for flashinfer FusedMoE by @amirkl94 in https://github.com/vllm-project/vllm/pull/22812
* [Feature] Full Cuda Graph Support for Cutlass MLA and 6% E2E Throughput Improvement by @yewentao256 in https://github.com/vllm-project/vllm/pull/22763
* [Mamba] - refactor: Renamed mamba_attn to mamba2_attn by @Josephasafg in https://github.com/vllm-project/vllm/pull/22818
* Revert "[ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module." by @tjtanaa in https://github.com/vllm-project/vllm/pull/22956
* [P/D]Provide bucket algorithm rate limiter  for proxy_server by @frankie-ys in https://github.com/vllm-project/vllm/pull/22643
* [CI] Pooling models mteb test uses enforce_eager by @noooop in https://github.com/vllm-project/vllm/pull/22878
* [V1] - Split Prefill and Decode for Mamba1 models by @amirai21 in https://github.com/vllm-project/vllm/pull/22653
* [Bugfix] Unquote file uri before reading image by @sayandipdutta in https://github.com/vllm-project/vllm/pull/22912
* [Bugfix] fix cuda 12.6 and 11.8 build by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/22952
* [MM] Allow skipping memory profiling for multimodal models. by @ywang96 in https://github.com/vllm-project/vllm/pull/22950
* Improve multimodal hasher performance for re-used Image prompts by @p88h in https://github.com/vllm-project/vllm/pull/22825
* [V1] [Hybrid] Support using float32 for state in Hybrid Models (Mamba2, Mamba1, Minimax) by @tdoublep in https://github.com/vllm-project/vllm/pull/22928
* [Misc] Ignore ep_kernels_workspace by @jeejeelee in https://github.com/vllm-project/vllm/pull/22807
* [CI] Remove duplicated docs build from buildkite by @hmellor in https://github.com/vllm-project/vllm/pull/22924
* [Frontend] Expose do_log_stats interval to env by @Csrayz in https://github.com/vllm-project/vllm/pull/22905
* [Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer by @fhl2000 in https://github.com/vllm-project/vllm/pull/20059
* [V0 Deprecation] Remove advance_step by @WoosukKwon in https://github.com/vllm-project/vllm/pull/22969
* [BugFix] Skip the Q component for QKVParallelLinear in the case of QKVCrossParallelLinear since its width is 0 by @sstamenk in https://github.com/vllm-project/vllm/pull/22369
* [FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches by @JartX in https://github.com/vllm-project/vllm/pull/22896
* [Benchmarks] Include image data when ShareGPT4V dataset is used. by @huachenheli in https://github.com/vllm-project/vllm/pull/22955
* [Structured Output] Make the output of structured output example more complete by @shen-shanshan in https://github.com/vllm-project/vllm/pull/22481
* [Kernels] Clean up FusedMoeMethodBase and modular kernel setup.  Remove extra arguments from modular kernel methods. by @bnellnm in https://github.com/vllm-project/vllm/pull/22035
* [Model] Granite-4 support loading quantized checkpoint by @cyang49 in https://github.com/vllm-project/vllm/pull/22925
* [Log] Debug Once for Randomizing dummy data for DP Rank by @yewentao256 in https://github.com/vllm-project/vllm/pull/22860
* [Core] direct indexing on self.block_table_np in compute_slot_mapping by @linzebing in https://github.com/vllm-project/vllm/pull/22940
* [Bugfix] Added more env vars to hash by @nvjullin in https://github.com/vllm-project/vllm/pull/22449
* Use regex in convert-results-json-to-markdown.py by @mgoin in https://github.com/vllm-project/vllm/pull/22989
* [CI] Speed up Whisper tests by reusing server by @mgoin in https://github.com/vllm-project/vllm/pull/22859
* [Fix] enable swap_ab for pplx problem size computation by @shixianc in https://github.com/vllm-project/vllm/pull/22991
* Add PrefixRepetitionRandomDataset to `vllm bench serve` datasets by @eicherseiji in https://github.com/vllm-project/vllm/pull/20638
* minor: zero workspace buffer init for flashinfer trtllm-gen attn by @yyihuang in https://github.com/vllm-project/vllm/pull/22603
* [Attention] FA3 Attention Sinks Perf Boost by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22478
* [BugFix] Fix regression caused by mamba state dtype PR by @tdoublep in https://github.com/vllm-project/vllm/pull/22998
* ci: Add CUDA + arm64 release builds by @seemethere in https://github.com/vllm-project/vllm/pull/21201
* [Structured Outputs] [Bug] Fix misalignment in apply_grammar_bitmask causing unintended masking and NaN logits by @rishitdholakia13 in https://github.com/vllm-project/vllm/pull/22963
* [BugFix] Handle case where async utility call is cancelled by @njhill in https://github.com/vllm-project/vllm/pull/22996
* [v1] Move block_hashes from KVCacheManager to Request.block_hashes (#19728) by @orozery in https://github.com/vllm-project/vllm/pull/19728
* Support multiple attention groups for KV sharing by @sarckk in https://github.com/vllm-project/vllm/pull/22672
* [BugFix] Make `run_once` thread-safe by @oraluben in https://github.com/vllm-project/vllm/pull/22978
* [Misc] Support passing multiple request ids at once to `AsyncLLM.abort()` by @njhill in https://github.com/vllm-project/vllm/pull/22944
* [Kernel] Simplify `get_kv_cache_layout` and cache `use_trtllm_attention` env-dependent bit by @NickLucche in https://github.com/vllm-project/vllm/pull/22735
* [Bugfix] Fix DeepSeek MTP by @benchislett in https://github.com/vllm-project/vllm/pull/22934
* [Frontend] Avoid list copies in `serving_chat.py` by @njhill in https://github.com/vllm-project/vllm/pull/22947
* [V1] support min_tokens for detokener by @calvin0327 in https://github.com/vllm-project/vllm/pull/22014
* [misc] nsys profile output kernel classifier and visualizer by @gracehonv in https://github.com/vllm-project/vllm/pull/22971
* [XPU]avoid circular import during XPU init by @jikunshang in https://github.com/vllm-project/vllm/pull/23017
* [Build] Env var to disable sccache by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22968
* [BugFix] Add support for loading prompt embeds tensors serialized on unavailable devices and sparse tensors by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/22962
* [Misc] Add --save-dir option to benchmark_moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/23020
* [Multimodal] Update Tensor schema test to cover arbitrary shape mm inputs by @Isotr0py in https://github.com/vllm-project/vllm/pull/22867
* [Core] Make cudagraph check cuda platform only by @yaochengji in https://github.com/vllm-project/vllm/pull/23005
* [CI][Bugfix] Skip Ovis2 generation test because of broken remote code by @Isotr0py in https://github.com/vllm-project/vllm/pull/22954
* Add docs for PrefixRepetitionDataset + enable usage with `vllm bench throughput` by @eicherseiji in https://github.com/vllm-project/vllm/pull/23012
* [Refactor] Allow optional MultiModalKwargsItem in IPC by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23022
* [New Model]mBART model by @princepride in https://github.com/vllm-project/vllm/pull/22883
* Fix handling of `max_num_batched_tokens` for pooling tasks by @maxdebayser in https://github.com/vllm-project/vllm/pull/23004
* [Frontend] Added support for HermesToolParser for models without special tokens by @minpeter in https://github.com/vllm-project/vllm/pull/16890
* [Bugfix gpt-oss] Fix float32 convert for flashinfer sink support by @mgoin in https://github.com/vllm-project/vllm/pull/23016
* [Flaky CI] Increase timeout tolerance for test_mp_crash_detection+test_default_mm_lora_chat_completions by @mgoin in https://github.com/vllm-project/vllm/pull/23028
* [Kernel/Quant] Remove AQLM by @mgoin in https://github.com/vllm-project/vllm/pull/22943
* [V1] Logits processors extensibility by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19912
* [Bugfix] fix qwen3 moe fp8 accuracy issue by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/23031
* [UX] Separate marlin moe config logic from triton moe by @mgoin in https://github.com/vllm-project/vllm/pull/23006
* [Refactor] Defer tensor data construction in MultiModalKwargs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/23030
* [Misc] method name typo fix by @andyxning in https://github.com/vllm-project/vllm/pull/23042
* [Kernel] Add cuda kernel for gpt_oss activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/22951
* [Bugfix] should use stack instead of concat by @947132885 in https://github.com/vllm-project/vllm/pull/22972
* [Misc] fix typo in the multimodal doc by @KevinZeng08 in https://github.com/vllm-project/vllm/pull/23051
* [BugFix] Fix for IMA in FA3 varlen combine by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/22967
* [Misc] Remove dead return by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23061
* [Misc] Convert use_structured_output property into constant by @WoosukKwon in https://github.com/vllm-project/vllm/pull/23060
* [XPU] fix xpu to set cudagraph batch sizes by @calvin0327 in https://github.com/vllm-project/vllm/pull/23044
* fix: gptq marlin weight loading failure by @simon-mo in https://github.com/vllm-project/vllm/pull/23066

## New Contributors
* @zhouwfang made their first contribution in https://github.com/vllm-project/vllm/pull/21407
* @juncgu made their first contribution in https://github.com/vllm-project/vllm/pull/18293
* @weireweire made their first contribution in https://github.com/vllm-project/vllm/pull/21485
* @bbeckca made their first contribution in https://github.com/vllm-project/vllm/pull/21232
* @wzqd made their first contribution in https://github.com/vllm-project/vllm/pull/21494
* @hfan made their first contribution in https://github.com/vllm-project/vllm/pull/21479
* @ignaciosica made their first contribution in https://github.com/vllm-project/vllm/pull/21195
* @xyxinyang made their first contribution in https://github.com/vllm-project/vllm/pull/21586
* @bigshanedogg made their first contribution in https://github.com/vllm-project/vllm/pull/20931
* @fsx950223 made their first contribution in https://github.com/vllm-project/vllm/pull/20295
* @mgazz made their first contribution in https://github.com/vllm-project/vllm/pull/21518
* @Mitix-EPI made their first contribution in https://github.com/vllm-project/vllm/pull/21612
* @lvhan028 made their first contribution in https://github.com/vllm-project/vllm/pull/21628
* @zhouyeju made their first contribution in https://github.com/vllm-project/vllm/pull/21380
* @wenchen76 made their first contribution in https://github.com/vllm-project/vllm/pull/21154
* @skyloevil made their first contribution in https://github.com/vllm-project/vllm/pull/20529
* @joa-stdn made their first contribution in https://github.com/vllm-project/vllm/pull/21697
* @liuyumoye made their first contribution in https://github.com/vllm-project/vllm/pull/21534
* @hsliuustc0106 made their first contribution in https://github.com/vllm-project/vllm/pull/21573
* @Josephasafg made their first contribution in https://github.com/vllm-project/vllm/pull/21715
* @vasqu made their first contribution in https://github.com/vllm-project/vllm/pull/21735
* @key4ng made their first contribution in https://github.com/vllm-project/vllm/pull/19024
* @wuhang2014 made their first contribution in https://github.com/vllm-project/vllm/pull/21728
* @HugoMichard made their first contribution in https://github.com/vllm-project/vllm/pull/21167
* @smarterclayton made their first contribution in https://github.com/vllm-project/vllm/pull/21472
* @nikhil-arm made their first contribution in https://github.com/vllm-project/vllm/pull/17112
* @LyrisZhong made their first contribution in https://github.com/vllm-project/vllm/pull/20396
* @rzabarazesh made their first contribution in https://github.com/vllm-project/vllm/pull/21347
* @milesial made their first contribution in https://github.com/vllm-project/vllm/pull/21798
* @Csrayz made their first contribution in https://github.com/vllm-project/vllm/pull/21803
* @MingzhenHan made their first contribution in https://github.com/vllm-project/vllm/pull/21827
* @aladerran made their first contribution in https://github.com/vllm-project/vllm/pull/20815
* @Yanpas made their first contribution in https://github.com/vllm-project/vllm/pull/18548
* @tanruixiang made their first contribution in https://github.com/vllm-project/vllm/pull/21673
* @nvpohanh made their first contribution in https://github.com/vllm-project/vllm/pull/21499
* @chi2liu made their first contribution in https://github.com/vllm-project/vllm/pull/21816
* @fake0fan made their first contribution in https://github.com/vllm-project/vllm/pull/21611
* @wxsms made their first contribution in https://github.com/vllm-project/vllm/pull/20433
* @wenxindongwork made their first contribution in https://github.com/vllm-project/vllm/pull/21417
* @br4mm made their first contribution in https://github.com/vllm-project/vllm/pull/20272
* @linzebing made their first contribution in https://github.com/vllm-project/vllm/pull/21627
* @sanchit-gandhi made their first contribution in https://github.com/vllm-project/vllm/pull/21833
* @amirkl94 made their first contribution in https://github.com/vllm-project/vllm/pull/21458
* @zhxchen17 made their first contribution in https://github.com/vllm-project/vllm/pull/22028
* @charent made their first contribution in https://github.com/vllm-project/vllm/pull/20873
* @Aviadr-neureality made their first contribution in https://github.com/vllm-project/vllm/pull/21937
* @n0gu-furiosa made their first contribution in https://github.com/vllm-project/vllm/pull/21052
* @ahengljh made their first contribution in https://github.com/vllm-project/vllm/pull/22052
* @sidhpurwala-huzaifa made their first contribution in https://github.com/vllm-project/vllm/pull/21119
* @anijain2305 made their first contribution in https://github.com/vllm-project/vllm/pull/20836
* @JartX made their first contribution in https://github.com/vllm-project/vllm/pull/21733
* @xiszishu made their first contribution in https://github.com/vllm-project/vllm/pull/22122
* @LopezCastroRoberto made their first contribution in https://github.com/vllm-project/vllm/pull/21309
* @TankNee made their first contribution in https://github.com/vllm-project/vllm/pull/21213
* @TheEpicDolphin made their first contribution in https://github.com/vllm-project/vllm/pull/20401
* @chenxi-yang made their first contribution in https://github.com/vllm-project/vllm/pull/22105
* @weixiao-huang made their first contribution in https://github.com/vllm-project/vllm/pull/21164
* @CLFutureX made their first contribution in https://github.com/vllm-project/vllm/pull/21173
* @tlipoca9 made their first contribution in https://github.com/vllm-project/vllm/pull/22149
* @zyongye made their first contribution in https://github.com/vllm-project/vllm/pull/22330
* @zhangnju made their first contribution in https://github.com/vllm-project/vllm/pull/22367
* @tc-mb made their first contribution in https://github.com/vllm-project/vllm/pull/22166
* @syedmba made their first contribution in https://github.com/vllm-project/vllm/pull/22314
* @msanft made their first contribution in https://github.com/vllm-project/vllm/pull/22099
* @mizadri made their first contribution in https://github.com/vllm-project/vllm/pull/20707
* @JaceyShao made their first contribution in https://github.com/vllm-project/vllm/pull/22433
* @andrewkchan made their first contribution in https://github.com/vllm-project/vllm/pull/12022
* @iAmir97 made their first contribution in https://github.com/vllm-project/vllm/pull/22310
* @pliops-daniels made their first contribution in https://github.com/vllm-project/vllm/pull/20267
* @yyweiss made their first contribution in https://github.com/vllm-project/vllm/pull/18097
* @Pradyun92 made their first contribution in https://github.com/vllm-project/vllm/pull/22317
* @kyuyeunk made their first contribution in https://github.com/vllm-project/vllm/pull/22425
* @lec77 made their first contribution in https://github.com/vllm-project/vllm/pull/22333
* @h-brenoskuk made their first contribution in https://github.com/vllm-project/vllm/pull/22534
* @zhewenl made their first contribution in https://github.com/vllm-project/vllm/pull/22584
* @PicoCreator made their first contribution in https://github.com/vllm-project/vllm/pull/22592
* @danielafrimi made their first contribution in https://github.com/vllm-project/vllm/pull/22349
* @GuanLuo made their first contribution in https://github.com/vllm-project/vllm/pull/21074
* @sooraj-satheesh made their first contribution in https://github.com/vllm-project/vllm/pull/22707
* @dongluw made their first contribution in https://github.com/vllm-project/vllm/pull/22660
* @Sugar-zsg made their first contribution in https://github.com/vllm-project/vllm/pull/22630
* @phantomlei3 made their first contribution in https://github.com/vllm-project/vllm/pull/22170
* @RishiAstra made their first contribution in https://github.com/vllm-project/vllm/pull/21783
* @zejunchen-zejun made their first contribution in https://github.com/vllm-project/vllm/pull/21161
* @teekenl made their first contribution in https://github.com/vllm-project/vllm/pull/22733
* @mxz297 made their first contribution in https://github.com/vllm-project/vllm/pull/22683
* @RUTHLESS-BOT made their first contribution in https://github.com/vllm-project/vllm/pull/22641
* @frankwang28 made their first contribution in https://github.com/vllm-project/vllm/pull/22606
* @zzh142857 made their first contribution in https://github.com/vllm-project/vllm/pull/22697
* @ducviet00 made their first contribution in https://github.com/vllm-project/vllm/pull/22739
* @x22x22 made their first contribution in https://github.com/vllm-project/vllm/pull/22280
* @Gh0u1L5 made their first contribution in https://github.com/vllm-project/vllm/pull/22785
* @jio-H made their first contribution in https://github.com/vllm-project/vllm/pull/22813
* @ZJY0516 made their first contribution in https://github.com/vllm-project/vllm/pull/22786
* @NirLevy98 made their first contribution in https://github.com/vllm-project/vllm/pull/22891
* @nvjullin made their first contribution in https://github.com/vllm-project/vllm/pull/22346
* @frankie-ys made their first contribution in https://github.com/vllm-project/vllm/pull/22643
* @amirai21 made their first contribution in https://github.com/vllm-project/vllm/pull/22653
* @sayandipdutta made their first contribution in https://github.com/vllm-project/vllm/pull/22912
* @yyihuang made their first contribution in https://github.com/vllm-project/vllm/pull/22603
* @rishitdholakia13 made their first contribution in https://github.com/vllm-project/vllm/pull/22963
* @oraluben made their first contribution in https://github.com/vllm-project/vllm/pull/22978
* @minpeter made their first contribution in https://github.com/vllm-project/vllm/pull/16890
* @947132885 made their first contribution in https://github.com/vllm-project/vllm/pull/22972
* @KevinZeng08 made their first contribution in https://github.com/vllm-project/vllm/pull/23051

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.0...v0.10.1rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.1rc1)

---

## v0.10.0: v0.10.0
**Published:** 2025-07-24

## Highlights
v0.10.0 release includes 308 commits, 168 contributors (62 new!).

**NOTE: This release begins the cleanup of V0 engine codebase.** We have removed V0 CPU/XPU/TPU/HPU backends (#20412), long context LoRA (#21169), Prompt Adapters (#20588), Phi3-Small & BlockSparse Attention (#21217), and Spec Decode workers (#21152) so far and plan to continued to delete code that is no longer used. 

### Model Support
* New families: Llama 4 with EAGLE support (#20591), EXAONE 4.0 (#21060), Microsoft Phi-4-mini-flash-reasoning (#20702), Hunyuan V1 Dense + A13B with reasoning/tool parsing (#21368, #20625, #20820), Ling MoE models (#20680), JinaVL Reranker (#20260), Nemotron-Nano-VL-8B-V1 (#20349), Arcee (#21296), Voxtral (#20970).
* Enhanced compatibility: BERT/RoBERTa with AutoWeightsLoader (#20534), HF format support for MiniMax (#20211), Gemini configuration (#20971), GLM-4 updates (#20736).
* Architecture expansions: Attention-free model support (#20811), Hybrid SSM/Attention models on V1 (#20016), LlamaForSequenceClassification (#20807), expanded Mamba2 layer support (#20660).
* VLM improvements: VLM support with transformers backend (#20543), PrithviMAE on V1 engine (#20577).

### Engine Core
* Experimental async scheduling `--async-scheduling` flag to overlap engine core scheduling with GPU runner (#19970).
* V1 engine improvements: backend-agnostic local attention (#21093), MLA FlashInfer ragged prefill (#20034), hybrid KV cache with local chunked attention (#19351).
* Multi-task support: models can now support multiple tasks (#20771), multiple poolers (#21227), and dynamic pooling parameter configuration (#21128).
* RLHF Support: new RPC methods for runtime weight reloading (#20096) and config updates (#20095), logprobs mode for selecting which stage of logprobs to return (#21398).
* Enhanced caching: multi-modal caching for transformers backend (#21358), reproducible prefix cache hashing using SHA-256 + CBOR (#20511).
* Startup time reduction via CUDA graph capture speedup via frozen GC (#21146).
* Elastic expert parallel for dynamic GPU scaling while preserving state (#20775).

### Hardwares & Performance
* NVIDIA Blackwell/SM100 optimizations: CUTLASS block scaled group GEMM for smaller batches (#20640), FP8 groupGEMM support (#20447), DeepGEMM integration (#20087), FlashInfer MoE blockscale FP8 backend (#20645), CUDNN prefill API for MLA (#20411), Triton Fused MoE kernel config for FP8 E=16 on B200 (#20516).
* Performance improvements: 48% request duration reduction via microbatch tokenization for concurrent requests (#19334), fused MLA QKV + strided layernorm (#21116), Triton causal-conv1d for Mamba models (#18218).
* Hardware expansion: ARM CPU int8 quantization (#14129), PPC64LE/ARM V1 support (#20554), Intel XPU ray distributed execution (#20659), shared-memory pipeline parallel for CPU (#21289), FlashInfer ARM CUDA support (#21013).

### Quantization
* New quantization support: MXFP4 for MoE models (#17888), BNB support for Mixtral and additional MoE models (#20893, #21100), in-flight quantization for MoE (#20061).
* Hardware-specific: FP8 KV cache quantization on TPU (#19292), FP8 support for BatchedTritonExperts (#18864), optimized INT8 vectorization kernels (#20331).
* Performance optimizations: Triton backend for DeepGEMM per-token group quantization (#20841), CUDA kernel for per-token group quantization (#21083), CustomOp abstraction for FP8 (#19830).

### API & Frontend
* OpenAI compatibility: Responses API implementation (#20504, #20975), image object support in llm.chat (#19635), tool calling with required choice and $defs (#20629).
* New endpoints: `get_tokenizer_info` for tokenizer/chat-template information (#20575), cache_salt support for completions/responses (#20981).
* Model loading: Tensorizer S3 integration with arbitrary arguments (#19619), HF repo paths & URLs for GGUF models (#20793), tokenization_kwargs for embedding truncation (#21033).
* CLI improvements: `--help=page` option for enhanced help documentation (#20961), default model changed to Qwen3-0.6B (#20335).

### Dependencies
* Updated PyTorch to 2.7.1 for CUDA (#21011)
* FlashInfer updated to v0.2.8rc1 (#20718)

## What's Changed
* [Docs] Note that alternative structured output backends are supported by @russellb in https://github.com/vllm-project/vllm/pull/19426
* [ROCm][V1] Adding ROCm to the list of plaforms using V1 by default by @gshtras in https://github.com/vllm-project/vllm/pull/19440
* [Model] use AutoWeightsLoader for commandr by @py-andy-c in https://github.com/vllm-project/vllm/pull/19399
* Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B-FP8 by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19401
* [BugFix] Allow use_cudagraph to work with dynamic VLLM_USE_V1 by @zou3519 in https://github.com/vllm-project/vllm/pull/19390
* [New Model]: Support Qwen3 Embedding & Reranker  by @noooop in https://github.com/vllm-project/vllm/pull/19260
* [BugFix] Fix docker build cpu-dev image error by @2niuhe in https://github.com/vllm-project/vllm/pull/19394
* Fix test_max_model_len in tests/entrypoints/llm/test_generate.py by @houseroad in https://github.com/vllm-project/vllm/pull/19451
* [CI] Disable failing GGUF model test by @mgoin in https://github.com/vllm-project/vllm/pull/19454
* [Misc] Remove unused `MultiModalHasher.hash_prompt_mm_data` by @lgeiger in https://github.com/vllm-project/vllm/pull/19422
* Add fused MOE config for Qwen3 30B A3B on B200 by @0xjunhao in https://github.com/vllm-project/vllm/pull/19455
* Fix Typo in Documentation and Function Name by @leopardracer in https://github.com/vllm-project/vllm/pull/19442
* [ROCm] Add rules to automatically label ROCm related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19405
* [Kernel] Support deep_gemm for linear methods by @artetaout in https://github.com/vllm-project/vllm/pull/19085
* [Doc] Update V1 User Guide for Hardware and Models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19474
* [Doc] Fix quantization link titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19478
* [Doc] Support "important" and "announcement" admonitions by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19479
* [Misc] Reduce warning message introduced in env_override by @houseroad in https://github.com/vllm-project/vllm/pull/19476
* Support non-string values in JSON keys from CLI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19471
* Add cache to cuda get_device_capability by @mgoin in https://github.com/vllm-project/vllm/pull/19436
* Fix some typo by @Ximingwang-09 in https://github.com/vllm-project/vllm/pull/19475
* Support no privileged mode on CPU for docker and kubernetes deployments by @louie-tsai in https://github.com/vllm-project/vllm/pull/19241
* [Bugfix] Update the example code, make it work with the latest lmcache by @runzhen in https://github.com/vllm-project/vllm/pull/19453
* [CI] Update FlashInfer to 0.2.6.post1 by @mgoin in https://github.com/vllm-project/vllm/pull/19297
* [doc] fix "Other AI accelerators" getting started page by @davidxia in https://github.com/vllm-project/vllm/pull/19457
* [Misc] Fix  misleading ROCm warning by @jeejeelee in https://github.com/vllm-project/vllm/pull/19486
* [Docs] Remove WIP features in V1 guide by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19498
* [Kernels] Add activation chunking logic to FusedMoEModularKernel by @bnellnm in https://github.com/vllm-project/vllm/pull/19168
* [AMD] [Quantization] Add override flag for attention dtype instead of using kv_cache_dtype trigger by @rasmith in https://github.com/vllm-project/vllm/pull/17331
* [UX] Add Feedback During CUDAGraph Capture by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19501
* [CI/Build] Fix torch nightly CI dependencies by @zou3519 in https://github.com/vllm-project/vllm/pull/19505
* [CI] change spell checker from codespell to typos by @andyxning in https://github.com/vllm-project/vllm/pull/18711
* [BugFix] Force registration of w8a8_block_fp8_matmul_deepgemm via lazy import by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19514
* Add Triton Fused MoE kernel config for E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/19518
* [Frontend] Improve error message in tool_choice validation by @22quinn in https://github.com/vllm-project/vllm/pull/19239
* [BugFix] Work-around incremental detokenization edge case error by @njhill in https://github.com/vllm-project/vllm/pull/19449
* [BugFix] Handle missing sep_token for Qwen3-Reranker in Score API by @strutive07 in https://github.com/vllm-project/vllm/pull/19522
* [AMD][Kernel][BugFix] fix test_rocm_compressed_tensors_w8a8 for rocm by @rasmith in https://github.com/vllm-project/vllm/pull/19509
* Fix typo by @2niuhe in https://github.com/vllm-project/vllm/pull/19525
* [Security] Prevent new imports of (cloud)pickle by @russellb in https://github.com/vllm-project/vllm/pull/18018
* [Bugfix][V1] Allow manual FlashAttention for Blackwell by @mgoin in https://github.com/vllm-project/vllm/pull/19492
* [Bugfix] Respect num-gpu-blocks-override in v1 by @jmswen in https://github.com/vllm-project/vllm/pull/19503
* [Quantization] Improve AWQ logic by @jeejeelee in https://github.com/vllm-project/vllm/pull/19431
* [Doc] Add V1 column to supported models list by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19523
* [NixlConnector] Drop `num_blocks` check  by @NickLucche in https://github.com/vllm-project/vllm/pull/19532
* [Perf] Vectorize static / dynamic INT8 quant kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/19233
* Fix TorchAOConfig skip layers by @mobicham in https://github.com/vllm-project/vllm/pull/19265
* [torch.compile][ROCm] Fuse quantization onto attention using a torch.compile pass by @ProExpertProg in https://github.com/vllm-project/vllm/pull/16756
* [doc] Make top navigation sticky by @reidliu41 in https://github.com/vllm-project/vllm/pull/19540
* [Spec Decode][Benchmark] Generalize spec decode offline benchmark to more methods and datasets by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18847
* [Misc] Turn MOE_DP_CHUNK_SIZE into an env var by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19506
* [Bugfix] Enforce contiguous input for dynamic_per_token FP8/INT8 quant by @mgoin in https://github.com/vllm-project/vllm/pull/19452
* [Doc] Unify structured outputs examples by @aarnphm in https://github.com/vllm-project/vllm/pull/18196
* [V1] Resolve failed concurrent structred output requests by @russellb in https://github.com/vllm-project/vllm/pull/19565
* Revert "[Build/CI] Add tracing deps to vllm container image (#15224)" by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19378
* [BugFix] : Fix Batched DeepGemm Experts by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19515
* [Bugfix] Fix EAGLE vocab embedding for multimodal target model by @zixi-qi in https://github.com/vllm-project/vllm/pull/19570
* [Doc] uses absolute links for structured outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/19582
* [doc] fix incorrect link by @reidliu41 in https://github.com/vllm-project/vllm/pull/19586
* [Misc] Correct broken docs link by @Zerohertz in https://github.com/vllm-project/vllm/pull/19553
* [CPU] Refine default config for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19539
* [Fix] bump mistral common to support magistral by @princepride in https://github.com/vllm-project/vllm/pull/19533
* [Fix] The zip function in Python 3.9 does not have the strict argument by @princepride in https://github.com/vllm-project/vllm/pull/19549
* use base version for version comparison by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/19587
* [torch.compile] reorganize the cache directory to support compiling multiple models by @youkaichao in https://github.com/vllm-project/vllm/pull/19064
* [BugFix] Honor `enable_caching` in connector-delayed kvcache load case by @njhill in https://github.com/vllm-project/vllm/pull/19435
* [Model] Fix minimax model cache & lm_head precision by @qscqesze in https://github.com/vllm-project/vllm/pull/19592
* [Refactor] Remove unused variables in `moe_permute_unpermute_kernel.inl` by @yewentao256 in https://github.com/vllm-project/vllm/pull/19573
* [doc][mkdocs] fix the  duplicate Supported features sections in GPU docs by @reidliu41 in https://github.com/vllm-project/vllm/pull/19606
* [CUDA] Enable full cudagraph for FlashMLA by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18581
* [Doc] Add troubleshooting section to k8s deployment by @annapendleton in https://github.com/vllm-project/vllm/pull/19377
* [torch.compile] Use custom ops when use_inductor=False by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19618
* Adding "AMD: Multi-step Tests" to amdproduction. by @Concurrensee in https://github.com/vllm-project/vllm/pull/19508
* [BugFix] Fix DP Coordinator incorrect debug log message by @njhill in https://github.com/vllm-project/vllm/pull/19624
* [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics. by @sahelib25 in https://github.com/vllm-project/vllm/pull/18354
* [Bugfix][1/n] Fix the speculative decoding test by setting the target dtype by @houseroad in https://github.com/vllm-project/vllm/pull/19633
* [Misc] Modularize CLI Argument Parsing in Benchmark Scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19593
* [Bugfix] Fix auto dtype casting for BatchFeature by @Isotr0py in https://github.com/vllm-project/vllm/pull/19316
* [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization by @jiahanc in https://github.com/vllm-project/vllm/pull/19500
* Only build CUTLASS MoE kernels on Hopper by @huydhn in https://github.com/vllm-project/vllm/pull/19648
* [Bugfix] Don't attempt to use triton if no driver is active by @kzawora-intel in https://github.com/vllm-project/vllm/pull/19561
* [Fix] Convert kv_transfer_config from dict to KVTransferConfig by @maobaolong in https://github.com/vllm-project/vllm/pull/19262
* [Perf] Further tunings for SM100 FP8 CUTLASS kernel by @ilmarkov in https://github.com/vllm-project/vllm/pull/19566
* [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness by @houseroad in https://github.com/vllm-project/vllm/pull/19644
* [Kernel] Raise verbose error and consolidate `num_heads/num_kv_heads` divisibility check by @22quinn in https://github.com/vllm-project/vllm/pull/19339
* [Benchmark] Refactor benchmark script for fp8 & int8 by @yewentao256 in https://github.com/vllm-project/vllm/pull/19627
* Enable prefix caching with full cuda graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19617
* [CI/Build] Fix torch nightly CI dependencies part 2 by @zou3519 in https://github.com/vllm-project/vllm/pull/19589
* [Misc] Remove duplicate multiproc method setting for CPU platform by @Isotr0py in https://github.com/vllm-project/vllm/pull/19649
* [MISC] Remove unused variableds in C++ by @houseroad in https://github.com/vllm-project/vllm/pull/19609
* [Bugfix][Core] Prefix caching causes incorrect outputs due to outdated ComputedBlocksTracker by @quanliu1991 in https://github.com/vllm-project/vllm/pull/18957
* [Misc][Frontend] passthrough `bad_words` by @f14-bertolotti in https://github.com/vllm-project/vllm/pull/19564
* [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19660
* [TPU] support attention head dim smaller than 128 by @yaochengji in https://github.com/vllm-project/vllm/pull/19620
* [MISC] typo fix by @andyxning in https://github.com/vllm-project/vllm/pull/19672
* [CI] Add mteb testing for rerank models by @noooop in https://github.com/vllm-project/vllm/pull/19344
* [Docs] Move multiproc doc to v1 dir by @russellb in https://github.com/vllm-project/vllm/pull/19651
* [Kernel] GGUF MMVQ kernel for multiple input vectors by @SzymonOzog in https://github.com/vllm-project/vllm/pull/18754
* [BugFix] Don't catch BaseException when dumping execute_model errors by @njhill in https://github.com/vllm-project/vllm/pull/19626
* [DOC] Add reasoning capability to vLLM streamlit code by @Navanit-git in https://github.com/vllm-project/vllm/pull/19557
* [Feature]:Allow for Granite MoE Hybrid models with _only_ shared experts. by @shawntan in https://github.com/vllm-project/vllm/pull/19652
* [Bugfix] Fix TP inference for Flex attention backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/19657
* [MISC] bump huggingface_hub pkg to 0.33.0 by @andyxning in https://github.com/vllm-project/vllm/pull/19547
* [Bugfix] fix missing 'finish_reason': null in streaming chat by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19662
* [Kernels] Use empty for modular MoE workspaces by @bnellnm in https://github.com/vllm-project/vllm/pull/19667
* [Model] Add support for MiniMaxM1ForCausalLM (shares architecture with MiniMaxText01ForCausalLM) by @qscqesze in https://github.com/vllm-project/vllm/pull/19677
* [V1] Change return type on get_multimodal_embeddings() by @russellb in https://github.com/vllm-project/vllm/pull/19446
* [Quantization] Remove FP4 emulation; Fall-back to marlin for device < 100 by @dsikka in https://github.com/vllm-project/vllm/pull/19563
* [Fix] Fall back to Gloo when NCCL backend is unavailable by @conroy-cheers in https://github.com/vllm-project/vllm/pull/19641
* [doc] add project flag to gcloud TPU command by @davidxia in https://github.com/vllm-project/vllm/pull/19664
* [Wheel Size] Only build FA2 8.0+PTX by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19336
* [Frontend] add chunking audio for > 30s audio by @nguyenhoangthuan99 in https://github.com/vllm-project/vllm/pull/19597
* [DOC] fix doc typos by @diliu0349 in https://github.com/vllm-project/vllm/pull/19600
* Fixes IMA for TP w/ flex-attention by @drisspg in https://github.com/vllm-project/vllm/pull/19712
* [Core] add remove_seq_from_computed_blocks_tracker to BlockSpaceManager by @quanliu1991 in https://github.com/vllm-project/vllm/pull/19686
* [Doc] Add missing llava family multi-image examples by @Isotr0py in https://github.com/vllm-project/vllm/pull/19698
* Add a doc on how to update PyTorch version by @huydhn in https://github.com/vllm-project/vllm/pull/19705
* [Kernel] Add Split-KV Support to Unified Triton Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/19152
* [doc][mkdocs] Add edit  button to documentation by @reidliu41 in https://github.com/vllm-project/vllm/pull/19637
* [doc] split "Other AI Accelerators" tabs by @davidxia in https://github.com/vllm-project/vllm/pull/19708
* [V1][Kernel] Flashinfer HND KV cache layout by @NickLucche in https://github.com/vllm-project/vllm/pull/19280
* [Mis] remove duplicate engine status checks by @googs1025 in https://github.com/vllm-project/vllm/pull/19647
* [Bugfix] Update multimodel models mapping to fit new checkpoint after Transformers v4.52 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19151
* [Perf] Optimize `moe_align_block_size` CUDA kernel by @yewentao256 in https://github.com/vllm-project/vllm/pull/19572
* Remove sm120 arch from sm100 cutlass kernel arch list by @mgoin in https://github.com/vllm-project/vllm/pull/19716
* [Misc] Update lmcache connector with the latest connector apis by @YaoJiayi in https://github.com/vllm-project/vllm/pull/19441
* [Bugfix] Fix faulty triton importing logic when using Ray for DP by @mgoin in https://github.com/vllm-project/vllm/pull/19734
* [Feature][ROCm] Add full graph capture support for TritonAttentionBackend by @charlifu in https://github.com/vllm-project/vllm/pull/19158
* [TPU] Update torch version to include paged attention kernel change by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19706
* [MISC] correct copy_blocks src_to_dists param type by @andyxning in https://github.com/vllm-project/vllm/pull/19696
* [MISC] correct DeviceConfig device field static type analysis by @andyxning in https://github.com/vllm-project/vllm/pull/19699
* [Misc] Add __str__ for RequestStatus by @lk-chen in https://github.com/vllm-project/vllm/pull/19780
* [V1] Add API docs for EncoderCacheManager by @russellb in https://github.com/vllm-project/vllm/pull/19294
* [V1][P/D] An native implementation of xPyD based on P2P NCCL by @Abatom in https://github.com/vllm-project/vllm/pull/18242
* [V1] Decouple GPU and TPU `InputBatch` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19778
* [Minor] Zero-initialize attn output buffer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19784
* [doc] fix the incorrect label by @reidliu41 in https://github.com/vllm-project/vllm/pull/19787
* [Platform] Allow platform use V1 Engine by default by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19792
* [Qwen] Add tagging rule for Qwen related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19799
* [Hardware][AMD] integrate aiter chunked prefill into vllm by @Zzz9990 in https://github.com/vllm-project/vllm/pull/18596
* [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19725
* [Docs] Add Huzaifa Sidhpurwala to vuln mgmt team doc by @russellb in https://github.com/vllm-project/vllm/pull/19808
* [v1] Support mamba2 by @heheda12345 in https://github.com/vllm-project/vllm/pull/19327
* docs: fix Slack bulletpoint in README by @nathan-weinberg in https://github.com/vllm-project/vllm/pull/19811
* Disable "Forbid direct 'import triton'" check for `vllm/triton_utils/importing.py` in an extensible way by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19783
* [Core] Do not copy array during hashing by @lgeiger in https://github.com/vllm-project/vllm/pull/19484
* [TPU] Update torch-xla version to include paged attention tuned block change by @QiliangCui in https://github.com/vllm-project/vllm/pull/19813
* [Core] More fixes to MultiModalEmbeddings type handling by @russellb in https://github.com/vllm-project/vllm/pull/19715
* [Multimodal] Use fast processor for Qwen2/2.5-VL by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19789
* [BugFix] Fix use_cudagraph=False by @zou3519 in https://github.com/vllm-project/vllm/pull/19612
* [Frontend] Expose custom args in OpenAI APIs by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16862
* Fix FA2 fallback for Blackwell V1 by @mgoin in https://github.com/vllm-project/vllm/pull/19781
* [Misc][ROCm] Enforce no unused variable in ROCm C++ files by @houseroad in https://github.com/vllm-project/vllm/pull/19796
* [Quantization] Modify the logic of BNB double quantization by @jeejeelee in https://github.com/vllm-project/vllm/pull/19742
* Support embedding models in V1 by @maxdebayser in https://github.com/vllm-project/vllm/pull/16188
* [Bugfix] Fix the linter by @houseroad in https://github.com/vllm-project/vllm/pull/19826
* [Bugfix] Add check_health to v1 async client. by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19821
* Mark invariant normalizer in Gemma as non-persistent by @yhtang in https://github.com/vllm-project/vllm/pull/19788
* [ROCm] [AITER] [Bugfix] Patch for AITER commit `648764942e552a8bb5fe16026703716a81f05374` by @tjtanaa in https://github.com/vllm-project/vllm/pull/18990
* [Misc] [ROCm] Prevent surplus tensor reshape by @zsolt-borbely-htec in https://github.com/vllm-project/vllm/pull/19803
* raise exception for pin_lora by @andyxning in https://github.com/vllm-project/vllm/pull/19809
* [Minor] Allow redirecting model path for HfRunner in test by @Isotr0py in https://github.com/vllm-project/vllm/pull/19795
* Add xLAM tool parser support by @zuxin666 in https://github.com/vllm-project/vllm/pull/17148
* [Frontend] Add optional token-level progress bar to `LLM.beam_search` by @NekoMimiUnagi in https://github.com/vllm-project/vllm/pull/19301
* Fixing Chunked Prefill Test. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/19762
* [Doc] Update V1 user guide for embedding models by @22quinn in https://github.com/vllm-project/vllm/pull/19842
* [CI][CPU] Improve dummy Triton interfaces and fix the CPU CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19838
* [Core][Bugfix] Fix Online MM Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/19688
* [Frontend] early return chat format resolution when specified by @xzbdmw in https://github.com/vllm-project/vllm/pull/19735
* [Benchmark][Bugfix] Fix Dataset Length Calculation by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19868
* [CI/Build][Bugfix] Fix deadlock on v1 engine test CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/19872
* [CI][Neuron] Fail and exit on first error by @elaineyz in https://github.com/vllm-project/vllm/pull/19622
* [Benchmark] Fix `Value of type "SampleRequest" is not indexable` by @b8zhong in https://github.com/vllm-project/vllm/pull/18032
* [Chore]: qwen3-moe-type-hints-mistake by @Xerxes-cn in https://github.com/vllm-project/vllm/pull/19860
* [Bugfix] Enable PP with AITER+V1 by @qli88 in https://github.com/vllm-project/vllm/pull/19822
* [Bugfix][Ray] Set the cuda context eagerly in the ray worker  by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19583
* [Misc] update cuda version by @reidliu41 in https://github.com/vllm-project/vllm/pull/19526
* [Misc] refactor example - openai_transcription_client by @reidliu41 in https://github.com/vllm-project/vllm/pull/19851
* [Kernel] correct cpu worker function parameter type by @andyxning in https://github.com/vllm-project/vllm/pull/19745
* [Fix] import regex instead of re by @tdoublep in https://github.com/vllm-project/vllm/pull/19875
* [Model] GPT2ForSequenceClassification model by @nie3e in https://github.com/vllm-project/vllm/pull/19663
* [custom_op][vllm-plugin] update custom_op class to use op_registry by @xuechendi in https://github.com/vllm-project/vllm/pull/19164
* Export NaNs in logits to scheduler_stats if output is corrupted by @vladmihailescu in https://github.com/vllm-project/vllm/pull/18777
* [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19901
* [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError by @andyxning in https://github.com/vllm-project/vllm/pull/19749
* [Misc] Clean up useless code by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19889
* Fix: Check the type of params to be a Sequence not list. by @rabinadk1 in https://github.com/vllm-project/vllm/pull/19910
* [Bugfix] Fix bnb 8bit model weights loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/19917
* [New model support]Support Tarsier2 by @princepride in https://github.com/vllm-project/vllm/pull/19887
* [doc] add contact us in community by @reidliu41 in https://github.com/vllm-project/vllm/pull/19922
* [Multimodal] Optimize Qwen2/2.5-VL startup time by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19756
* [Docs] Add GPT2ForSequenceClassification to supported models in docs by @nie3e in https://github.com/vllm-project/vllm/pull/19932
* [Misc] add vllm_config in __init__ by @andyxning in https://github.com/vllm-project/vllm/pull/19866
* [MISC] add cpu_kvcache_space_bytes to CacheConfig by @andyxning in https://github.com/vllm-project/vllm/pull/19812
* [Benchmark] fix request loss if "ping" is returned by @sywangyi in https://github.com/vllm-project/vllm/pull/19535
* [CI/Build] Auto tag perf benchmarks related PRs by @22quinn in https://github.com/vllm-project/vllm/pull/19943
* [doc] use snippets for contact us by @reidliu41 in https://github.com/vllm-project/vllm/pull/19944
* [Misc] Update model-specific PR tagging by @ywang96 in https://github.com/vllm-project/vllm/pull/19949
* [Misc] Simplify vllm bench cli subcommand implementation by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19948
* [Chore] dedup logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19955
* [BugFix] Add an env to disable moe chunking to work around compile incompatibility by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19642
* [Perf][CLI] Improve overall startup time by @aarnphm in https://github.com/vllm-project/vllm/pull/19941
* [Core] feat: Implement Priority Scheduling in V1 Engine by @amitm02 in https://github.com/vllm-project/vllm/pull/19057
* [Misc] Configurable timeout for execute_model RPC calls via env var by @jinqinn in https://github.com/vllm-project/vllm/pull/19544
* Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor by @Flink-ddd in https://github.com/vllm-project/vllm/pull/19643
* [doc] Fold long code blocks to improve readability by @reidliu41 in https://github.com/vllm-project/vllm/pull/19926
* [P/D][NixlConnector] Support `tp_size > num_kv_heads` deployments by @NickLucche in https://github.com/vllm-project/vllm/pull/19691
* [BugFix][P/D] Fix for cases where _recving_transfers can be cleaned up when *all* transfer done by @lk-chen in https://github.com/vllm-project/vllm/pull/19874
* [Doc] Update V1 status for decoder-only embedding models by @Isotr0py in https://github.com/vllm-project/vllm/pull/19952
* [doc] use MkDocs collapsible blocks - supplement by @reidliu41 in https://github.com/vllm-project/vllm/pull/19973
* [Bugfix] Fix CI bitsandbytes failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/19969
* [doc] improve readability for long commands by @reidliu41 in https://github.com/vllm-project/vllm/pull/19920
* [Docs] Fix syntax highlighting of shell commands by @lgeiger in https://github.com/vllm-project/vllm/pull/19870
* [EP+DP] Optimize the little operations in the DeepGEMM + DeepEP low latency case by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19885
* [Bugfix][v1] Fix step pooler implementation and step pooling usage in v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19956
* [Misc] Add type alias `ReqId` and `EngineId` for better readability by @lk-chen in https://github.com/vllm-project/vllm/pull/19880
* [Feature] Support sequence parallelism for static fp8 quantization by @cascade812 in https://github.com/vllm-project/vllm/pull/19181
* [CI/Build] Push latest tag for cpu and neuron docker image by @22quinn in https://github.com/vllm-project/vllm/pull/19897
* Feat Dynamic Quantization for MoE Layers in GPTQ Marlin Backend by @Jun-Howie in https://github.com/vllm-project/vllm/pull/19395
* [Bugfix][Benchmark] Fix Marlin benchmark by @22quinn in https://github.com/vllm-project/vllm/pull/19929
* [TPU] Fix tpu model runner test by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19995
* Update test case parameter to have the throughput above 8.0 by @QiliangCui in https://github.com/vllm-project/vllm/pull/19994
* [Misc][Tools][Benchmark] Add profile to autotune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19711
* [doc] Fix broken link in the installation for CPU by @yankay in https://github.com/vllm-project/vllm/pull/19980
* add some examples for other benchmark scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19893
* [PERF] Speedup of MRoPE prepare inputs by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/19939
* [Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20014
* refactor example - qwen3_reranker by @reidliu41 in https://github.com/vllm-project/vllm/pull/19847
* [Fix][V1] Remove --scheduling-policy oracle by @amitm02 in https://github.com/vllm-project/vllm/pull/20010
* [Perf] Improve/Fix-regression for FA3 in High QPS regimes by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19463
* [Misc][Benchmarking] Add variable request-rate ("ramp-up") to the benchmarking client. by @dtransposed in https://github.com/vllm-project/vllm/pull/19423
* [BugFix] Fix multi-node offline data parallel by @njhill in https://github.com/vllm-project/vllm/pull/19937
* [P/D] Asynchronously do _nixl_handshake by @lk-chen in https://github.com/vllm-project/vllm/pull/19836
* [Feature] Integrate new deepgemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/19820
* [Easy] Remove submodule added in #19463 by @b8zhong in https://github.com/vllm-project/vllm/pull/20039
* use .dev for version comparison with pytorch nightly release by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20031
* cmake: Update vllm_flash_attn for vllm_kernels by @seemethere in https://github.com/vllm-project/vllm/pull/20032
* [Llama4] Update `attn_temperature_tuning` by @b8zhong in https://github.com/vllm-project/vllm/pull/19997
* Revert "[Feature] Integrate new deepgemm (#19820)" by @yewentao256 in https://github.com/vllm-project/vllm/pull/20049
* Revert "Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor" by @Isotr0py in https://github.com/vllm-project/vllm/pull/20030
* Move to a faster base64 implementation by @h-avsha in https://github.com/vllm-project/vllm/pull/19984
* [Frontend] speed up import time of vllm.config by @davidxia in https://github.com/vllm-project/vllm/pull/18036
* [Refactor] Remove duplicate `ceil_div` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20023
* [Feat][CLI] enforce-include-usage by @max-wittig in https://github.com/vllm-project/vllm/pull/19695
* [Kernels][Bugfix] Use torch op for all kernels in FusedMoE forward.  Add additional testing for cudagraphs. by @bnellnm in https://github.com/vllm-project/vllm/pull/19717
* [Chore] debloat some initial logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19438
* [BugFix] Fix full-cuda-graph illegal memory access in FA3 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20057
* [doc] add reference link for Intel XPU by @reidliu41 in https://github.com/vllm-project/vllm/pull/20064
* [Doc] Guide for Incremental Compilation Workflow by @mgoin in https://github.com/vllm-project/vllm/pull/19109
* [V1][Speculative Decoding] Fix DeepSeek MTP by @cjackal in https://github.com/vllm-project/vllm/pull/20022
* [Frontend] Add `/v1/audio/translations` OpenAI API endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/19615
* [Quantization] Add compressed-tensors emulations support for NVFP4 by @dsikka in https://github.com/vllm-project/vllm/pull/19879
* [Fix] Support cls pooling in ModernBertPooler by @lsz05 in https://github.com/vllm-project/vllm/pull/20067
* static_scaled_fp8_quant should not run when scale.numel is not 1 by @eldarkurtic in https://github.com/vllm-project/vllm/pull/20076
* [PD] let toy proxy handle /chat/completions by @lk-chen in https://github.com/vllm-project/vllm/pull/19730
* [Misc] Add parallel state `node_count` function by @njhill in https://github.com/vllm-project/vllm/pull/20045
* Fix the path to the testing script. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20082
* [Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine by @izhuhaoran in https://github.com/vllm-project/vllm/pull/20062
* [TPU][Bugfix] fix kv cache padding by @yaochengji in https://github.com/vllm-project/vllm/pull/20048
* [P/D] Avoid stranding blocks in P when aborted in D's waiting queue by @njhill in https://github.com/vllm-project/vllm/pull/19223
* [TPU] Add TPU specific var VLLM_TPU_MOST_MODEL_LEN by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19919
* [CI] Add SM120 to the Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/19794
* [Bugfix] Fix Mistral tool-parser regex for nested JSON by @mgoin in https://github.com/vllm-project/vllm/pull/20093
* [PD] Skip `tp_size` exchange with rank0 by @NickLucche in https://github.com/vllm-project/vllm/pull/19413
* [Benchmark][Bug] Fix multiple bugs in bench and add args to spec_decode offline by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/20083
* [Bugfix] Allow `CUDA_VISIBLE_DEVICES=''` in `Platform.device_id_to_physical_device_id` by @eicherseiji in https://github.com/vllm-project/vllm/pull/18979
* [Doc] Update docs for New Model Implementation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20115
* [Refactor] Remove unused library by @yewentao256 in https://github.com/vllm-project/vllm/pull/20099
* [CPU] Fix torch version in x86 CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19258
* [Misc] Use collapsible blocks for benchmark examples. by @reidliu41 in https://github.com/vllm-project/vllm/pull/20017
* [Docs] Improve frameworks/helm.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20113
* [Bugfix][V1][ROCm] Fix AITER Flash Attention Backend (Fix API Break and Local Attention Logic: affecting Llama4) by @tjtanaa in https://github.com/vllm-project/vllm/pull/19904
* Revert "[Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine" by @mgoin in https://github.com/vllm-project/vllm/pull/20128
* [Bug Fix] Fix address/port already in use error for pplx test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20094
* [Doc] Automatically signed-off by PyCharm by @noooop in https://github.com/vllm-project/vllm/pull/20120
* [Doc] Auto sign-off for VSCode by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20132
* [Doc] Rename page titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20130
* Spam folks if config.py changes by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20131
* [Hardware][Intel GPU] Add v1 Intel GPU support with Flash attention backend. by @jikunshang in https://github.com/vllm-project/vllm/pull/19560
* [TPU] add kv cache update kernel by @yaochengji in https://github.com/vllm-project/vllm/pull/19928
* [Refactor] Rename commnication utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20091
* [Doc] correct LoRA capitalization by @kyolebu in https://github.com/vllm-project/vllm/pull/20135
* [Feature] Expert Parallelism Load Balancer (EPLB) by @abmfy in https://github.com/vllm-project/vllm/pull/18343
* [CI Failure] Fix OOM with test_oot_registration_embedding by @mgoin in https://github.com/vllm-project/vllm/pull/20144
* [Quantization] Bump to use latest `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/20033
* [Perf] SM100 FP8 GEMM Optimizations after cutlass_profiler by @ilmarkov in https://github.com/vllm-project/vllm/pull/20071
* [Bugfix] Build moe_data for both sm100 and sm90 by @mgoin in https://github.com/vllm-project/vllm/pull/20086
* [Feature][Rocm] add quick all reduce for rocm by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/19744
* [CI] Sync test dependency with test.in for torch nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/19632
* [Fix] Fix gemma CI test failing on main by @tdoublep in https://github.com/vllm-project/vllm/pull/20124
* [Model][1/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20012
* [Perf][Frontend]: eliminate api_key and x_request_id headers middleware overhead by @Yazan-Sharaya in https://github.com/vllm-project/vllm/pull/19946
* Quick Fix by adding conditional import for flash_attn_varlen_func in flash_attn by @xuechendi in https://github.com/vllm-project/vllm/pull/20143
* Gemma3n (Text-only) by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/20134
* [Bugfix] Fix flaky failure when getting DP ports by @mgoin in https://github.com/vllm-project/vllm/pull/20151
* [Perf][Frontend] Cached resolution for resolving chat templates by @ilyal-cerebras in https://github.com/vllm-project/vllm/pull/20065
* [Fix][ROCm] Remove unused variables to fix build error on GFX11/12 by @hyoon1 in https://github.com/vllm-project/vllm/pull/19891
* [Fix][torch.compile] Enable custom ops by default when Inductor off by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20102
* [Bugfix] Mark 'hidden_states' as mutable in moe_forward registration. by @bnellnm in https://github.com/vllm-project/vllm/pull/20152
* [Bugfix] Fix some narrowing conversion warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20141
* [CI/Build] Allow hermetic builds by @fabiendupont in https://github.com/vllm-project/vllm/pull/18064
* [CI Fix] Pin tests/models/registry.py MiniMaxText01ForCausalLM to revision due to model changes by @mgoin in https://github.com/vllm-project/vllm/pull/20199
* [Misc] Add type assertion of request_id for LLMEngine.add_request by @SHA-4096 in https://github.com/vllm-project/vllm/pull/19700
* Fix num_token_padding support for static per-tensor scaled_fp8_quant by @mgoin in https://github.com/vllm-project/vllm/pull/20188
* fix ci issue distributed 4 gpu test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20204
* [Bugfix] Properly reject requests with empty list guided_choice by @mgoin in https://github.com/vllm-project/vllm/pull/20195
* [BugFix] Fix the incorrect func name in the comments. (config.py) by @1195343015 in https://github.com/vllm-project/vllm/pull/20185
* [CI/Build] Add new CI job to validate Hybrid Models for every PR  by @tdoublep in https://github.com/vllm-project/vllm/pull/20147
* [Frontend] Generalize `v1/audio/transcriptions` endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/20179
* [Bugfix] Correct behavior of GraniteMoeHybrid for TensorParallel execution by @s3woz in https://github.com/vllm-project/vllm/pull/20137
* [Refactor] Create a function util and cache the results for `has_deepgemm`, `has_deepep`, `has_pplx` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20187
* [CI Fix] Try fixing eagle e2e test OOM by reducing block allocation by @mgoin in https://github.com/vllm-project/vllm/pull/20213
* [Quantization] Add compressed-tensors NVFP4 MoE Support by @dsikka in https://github.com/vllm-project/vllm/pull/19990
* Fix cuda_archs_loose_intersection when handling sm_*a by @huydhn in https://github.com/vllm-project/vllm/pull/20207
* [Model] support dots1 by @redmoe-moutain in https://github.com/vllm-project/vllm/pull/18254
* [BUGFIX][DEEPSEEK][MODEL_LOAD] fix w13, w2 weight not initialized assert by @xuechendi in https://github.com/vllm-project/vllm/pull/20202
* [Misc] Fix import by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20233
* [doc] Add Slack and Forum to the top navigation by @reidliu41 in https://github.com/vllm-project/vllm/pull/20208
* [Bugfix] Skip loading extra parameters for modelopt Qwen3 MoE model by @noiji in https://github.com/vllm-project/vllm/pull/19598
* [Bugfix] Fix processor initialization in transformers 4.53.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/20244
* [Quantization] Improve BitsAndBytesModelLoader by @jeejeelee in https://github.com/vllm-project/vllm/pull/20242
* [Docs] Fix 1-2-3 list in v1/prefix_caching.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20243
* [Bugfix] fix quark ptpc by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/20251
* [Spec Decode] Refactor spec decoding into a separate function by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20238
* [Spec Decode] Clean up spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20240
* [Optimization] Use Shared `CachedRequestData` Instance Across All Requests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20232
* [Unit Test] Add unit test for deep gemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/20090
* [Core] [Bugfix] [Multimodal] Fix multimodal profiling and generation for SFT/PTQed models by @kylesayrs in https://github.com/vllm-project/vllm/pull/20058
* [Refactor] Remove useless pdb comment by @yewentao256 in https://github.com/vllm-project/vllm/pull/20266
* [Bugfix][V1][P/D]Fix the issue of occasional garbled output  for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/20263
* [CLI] Improve CLI arg parsing for `-O`/`--compilation-config` by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20156
* [Bugfix] Fix include prompt in stream response when echo=true by @fyuan1316 in https://github.com/vllm-project/vllm/pull/15233
* [Misc] Fix spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20296
* [Example] add one-click runnable example for P2P NCCL XpYd by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20246
* [CI][Intel Gaudi][vllm-Plugin]Add CI for hpu-plugin-v1-test by @xuechendi in https://github.com/vllm-project/vllm/pull/20196
* [Doc] add config and troubleshooting guide for NCCL & GPUDirect RDMA by @chewong in https://github.com/vllm-project/vllm/pull/15897
* [Feature] A calibration-free RTN-based quantization for accurate and accelerated INT4/INT8 inference by @sakogan in https://github.com/vllm-project/vllm/pull/18768
* [V1] Only print cudagraph tqdm on rank 0 with `is_global_first_rank` by @mgoin in https://github.com/vllm-project/vllm/pull/19516
* Fix `numel()` downcast in vllm/csrc/moe/moe_align_sum_kernels.cu +2 by @r-barnes in https://github.com/vllm-project/vllm/pull/17082
* [Misc] add xgrammar for arm64 by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/18359
* Enable ZP Support for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20268
* [CPU] Update custom ops for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20255
* [Bugfix] Fix deepep tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20288
* [Misc] remove redundant char by @kebe7jun in https://github.com/vllm-project/vllm/pull/20287
* [BugFix][V1][ROCm] Triton MLA uses V0 backend on V1 engine by @tywuAMD in https://github.com/vllm-project/vllm/pull/19067
* [doc] fix the incorrect logo in dark mode by @reidliu41 in https://github.com/vllm-project/vllm/pull/20289
* [Perf] Validate @config in pre-commit instead of dynamically by @lionelvillard in https://github.com/vllm-project/vllm/pull/20200
* [Quant] [Bugfix] Fix quantization config matching with `hf_to_vllm_mapper` by @kylesayrs in https://github.com/vllm-project/vllm/pull/20046
* [Misc] Minor refactor of NIXL background handshake by @NickLucche in https://github.com/vllm-project/vllm/pull/20068
* Add GLM-4.1V model by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/19331
* [Model]Add Tencent HunYuanMoEV1 Model Support by @aiyiwang2025 in https://github.com/vllm-project/vllm/pull/20114
* [Misc] Minor refactoring for scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20299
* [Docs] Update transcriptions API to use openai client with `stream=True`  by @NickLucche in https://github.com/vllm-project/vllm/pull/20271
* [CUDA graphs] Enable full cuda graphs with FA3 AoT scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20301
* [Frontend] Expand tools even if tool_choice="none" by @okdshin in https://github.com/vllm-project/vllm/pull/17177
* [V1] [ROCm] Enable EP with AITER Fused MoE by @tjtanaa in https://github.com/vllm-project/vllm/pull/20270
* [Optimization] Cache sampled token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20291
* remove unused variables in marlin_template.h by @zhoutianzi666 in https://github.com/vllm-project/vllm/pull/20236
* [Refactor] Refactor import utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20269
* Enable group size 64 for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20290
* [Kernel][Bugfix] Fixup some warnings in nvfp4_blockwise_moe when CUDA < 12.8 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20324
* [UT][intel GPU] use current_platform instead of device hardcode in v1 tests by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20169
* [Refactor] Remove duplicate `find_free_port` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20333
* [Refactor] Remove Unused Env `VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20334
* [Misc][Doc] Add missing comment for LLM by @draftbk in https://github.com/vllm-project/vllm/pull/20285
* [FIX][Intel GPU]fix ipex flash_attn_varlen_func api missing parameter by @jikunshang in https://github.com/vllm-project/vllm/pull/20348
* [Bugfix] Fix dynamic rotary embedding by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20343
* fix[Docs]: link anchor is incorrect #20309 by @yyzxw in https://github.com/vllm-project/vllm/pull/20315
* [Doc][TPU] Add models and features supporting matrix. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20230
* [TPU] kv cache update kernel supports dynamic grid by @yaochengji in https://github.com/vllm-project/vllm/pull/20235
* [Frontend] Support configurable mm placeholder strings & flexible video sampling policies via CLI flags. by @huachenheli in https://github.com/vllm-project/vllm/pull/20105
* [Model][VLM] Support Keye-VL-8B-Preview by @Kwai-Keye in https://github.com/vllm-project/vllm/pull/20126
* [Bugfix] Keye-VL compatibility with `tok_kwargs` (#20058) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20353
* [Docs] Fix indentations for 2-level items in deprecation_policy.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20352
* [Docs] Make TPU ref prettier in google_tpu.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20356
* [Model] Add Ernie4.5 and Ernie4.5MoE Model Support by @CSWYF3634076 in https://github.com/vllm-project/vllm/pull/20220
* [Build/CI] Automatically tag DeepSeek related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/20370
* [NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120) by @kaln27 in https://github.com/vllm-project/vllm/pull/17280
* [Bugfix] Fix the max_seq_len limit of 16384 for DeepSeek models by @huaqiangwang in https://github.com/vllm-project/vllm/pull/20322
* [Model] Adds support for SlimMoE models Phi-tiny-MoE-instruct by @zichongli5 in https://github.com/vllm-project/vllm/pull/20286
* Documentation update tool_calling: mapping back to function from response by @cronoik-inceptionai in https://github.com/vllm-project/vllm/pull/20373
* [Kernels] MoE refactor by @bnellnm in https://github.com/vllm-project/vllm/pull/19636
* [V1] LogitsProcessor programming model by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16728
* [Minor] Clean up incorrect comment in test by @njhill in https://github.com/vllm-project/vllm/pull/20382
* [Misc] add handler HF_TOKEN is emptry string by @lengrongfu in https://github.com/vllm-project/vllm/pull/20369
* [ROCm][FEAT] Enable Full Graph Mode in AITER MLA V1 Attn Backend (Decode Phase only) by @vllmellm in https://github.com/vllm-project/vllm/pull/20254
* [DP] Support external DP Load Balancer mode by @njhill in https://github.com/vllm-project/vllm/pull/19790
* [Docs] Update EAGLE example by @NickLucche in https://github.com/vllm-project/vllm/pull/20375
* [Bugfix] Fixes for FlashInfer's TORCH_CUDA_ARCH_LIST by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20136
* [BugFix] Fix DP headless mode arg validation by @njhill in https://github.com/vllm-project/vllm/pull/20398
* Enable CPU nightly performance benchmark and its Markdown report by @louie-tsai in https://github.com/vllm-project/vllm/pull/18444
* [Bugfix] Fix import of CutlassExpertsFp8 in compressed_tensors_moe.py by @bnellnm in https://github.com/vllm-project/vllm/pull/20381
* [Misc] Small: Fix video loader return type annotations. by @huachenheli in https://github.com/vllm-project/vllm/pull/20389
* [Bugfix][CI/CD][CPU] Fix CPU CI tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20383
* [TPU] Add a case to cover RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8 by @QiliangCui in https://github.com/vllm-project/vllm/pull/20385
* [Feature] Support MiniMax-M1 function calls features by @qscqesze in https://github.com/vllm-project/vllm/pull/20297
* [Tests] Update online DP tests to verify that requests are balanced by @njhill in https://github.com/vllm-project/vllm/pull/20157
* [Misc] Add rules to label Speculative Decoding Related PRs by @draftbk in https://github.com/vllm-project/vllm/pull/20406
* [doc] fix link by @reidliu41 in https://github.com/vllm-project/vllm/pull/20417
* [Docs] Replace two list with tables in intel_gaudi.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20414
* [Core] Move multimodal placeholder from chat utils to model definition by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20355
* [Kernel] refactor cpu worker v0 cache dtype by @andyxning in https://github.com/vllm-project/vllm/pull/20080
* [CI/Build][CPU] Enable cross compilation in CPU release pipeline by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20423
* [Quantization] Bump to use latest bitsandbytes by @jeejeelee in https://github.com/vllm-project/vllm/pull/20424
* [Model][2/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/19978
* [Misc] Automatically tag PRs to add new models by @Isotr0py in https://github.com/vllm-project/vllm/pull/20222
* [Frontend] improve vllm bench <bench_type> --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/20430
* [Bugfix] Fix flaky `test_streaming_response` test by @NickLucche in https://github.com/vllm-project/vllm/pull/20363
* [Frontend] fix duplicate output for bench subcmd by @reidliu41 in https://github.com/vllm-project/vllm/pull/20446
* [CI] Trimming some failing test groups from AMDPRODUCTION. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/20390
* [Misc] Clean up InternVL family config registration by @Isotr0py in https://github.com/vllm-project/vllm/pull/19992
* [Misc] adjust for ipv6 for mookcacke url parse by @andyxning in https://github.com/vllm-project/vllm/pull/20107
* [Misc] Remove _maybe_ignore_quant_config from GLM4.1v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/20432
* [Kernel] Enable fp8 support for pplx and BatchedTritonExperts. by @bnellnm in https://github.com/vllm-project/vllm/pull/18864
* [Misc] Fix `Unable to detect current VLLM config. Defaulting to NHD kv cache layout` warning by @NickLucche in https://github.com/vllm-project/vllm/pull/20400
* [Bugfix] Register reducer even if transformers_modules not available by @eicherseiji in https://github.com/vllm-project/vllm/pull/19510
* Change warn_for_unimplemented_methods to debug by @mgoin in https://github.com/vllm-project/vllm/pull/20455
* [Platform] Add custom default max tokens by @gmarinho2 in https://github.com/vllm-project/vllm/pull/18557
* Add ignore consolidated file in mistral example code by @princepride in https://github.com/vllm-project/vllm/pull/20420
* [Misc] small update by @reidliu41 in https://github.com/vllm-project/vllm/pull/20462
* [Structured Outputs][V1] Skipping with models doesn't contain tokenizers by @aarnphm in https://github.com/vllm-project/vllm/pull/20365
* [Perf] Optimize Vectorization Utils for Int 8 Quantization Kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/20331
* [Misc] Add SPDX-FileCopyrightText by @jeejeelee in https://github.com/vllm-project/vllm/pull/20428
* Support Llama 4 for fused_marlin_moe by @mgoin in https://github.com/vllm-project/vllm/pull/20457
* [Bug][Frontend] Fix structure of transcription's decoder_prompt by @sangbumlikeagod in https://github.com/vllm-project/vllm/pull/18809
* [Model][3/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20168
* [Doc] Fix classification table in list of supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20489
* [CI] add kvcache-connector dependency definition and add into CI build by @panpan0000 in https://github.com/vllm-project/vllm/pull/18193
* [Misc] Small: Remove global media connector. Each test should have its own test connector object. by @huachenheli in https://github.com/vllm-project/vllm/pull/20395
* Enable V1 for Hybrid SSM/Attention Models by @tdoublep in https://github.com/vllm-project/vllm/pull/20016
* [feat]: CUTLASS block scaled group gemm for SM100 by @djmmoss in https://github.com/vllm-project/vllm/pull/19757
* [CI Bugfix] Fix pre-commit failures on main by @mgoin in https://github.com/vllm-project/vllm/pull/20502
* [Doc] fix mutltimodal_inputs.md gh examples link by @GuyStone in https://github.com/vllm-project/vllm/pull/20497
* [Misc] Add security warning for development mode endpoints by @reidliu41 in https://github.com/vllm-project/vllm/pull/20508
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/20506
* [Misc] Remove the unused LoRA test code by @jeejeelee in https://github.com/vllm-project/vllm/pull/20494
* Fix unknown attribute of topk_indices_dtype in CompressedTensorsW8A8Fp8MoECutlassMethod by @luccafong in https://github.com/vllm-project/vllm/pull/20507
* [v1] Re-add fp32 support to v1 engine through FlexAttention by @Isotr0py in https://github.com/vllm-project/vllm/pull/19754
* [Misc] Add logger.exception for TPU information collection failures by @reidliu41 in https://github.com/vllm-project/vllm/pull/20510
* [Misc] remove unused import by @reidliu41 in https://github.com/vllm-project/vllm/pull/20517
* test_attention compat with coming xformers change by @bottler in https://github.com/vllm-project/vllm/pull/20487
* [BUG] Fix #20484. Support empty sequence in cuda penalty kernel by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20491
* [Bugfix] Fix missing per_act_token parameter in compressed_tensors_moe by @luccafong in https://github.com/vllm-project/vllm/pull/20509
* [BugFix] Fix: ImportError when building on hopper systems by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20513
* [TPU][Bugfix] fix the MoE OOM issue by @yaochengji in https://github.com/vllm-project/vllm/pull/20339
* [Frontend] Support image object in llm.chat by @sfeng33 in https://github.com/vllm-project/vllm/pull/19635
* [Benchmark] Add support for multiple batch size benchmark through CLI in `benchmark_moe.py` + Add Triton Fused MoE kernel config for FP8 E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/20516
* [Misc] call the pre-defined func by @reidliu41 in https://github.com/vllm-project/vllm/pull/20518
* [V0 deprecation] Remove V0 CPU/XPU/TPU backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20412
* [V1] Support any head size for FlexAttention backend by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20467
* [BugFix][Spec Decode] Fix spec token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20530
* [Bugfix] Add `use_cross_encoder` flag to use correct activation in `ClassifierPooler` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20527
* Implement OpenAI Responses API [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20504
* [Misc] add a tip for pre-commit by @reidliu41 in https://github.com/vllm-project/vllm/pull/20536
* [Refactor]Abstract Platform Interface for Distributed Backend and Add xccl Support for Intel XPU by @dbyoung18 in https://github.com/vllm-project/vllm/pull/19410
* [CI/Build] Enable phi2 lora test by @jeejeelee in https://github.com/vllm-project/vllm/pull/20540
* [XPU][CI] add v1/core test in xpu hardware ci by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20537
* Add docstrings to url_schemes.py to improve readability by @windsonsea in https://github.com/vllm-project/vllm/pull/20545
* [XPU] log clean up for XPU platform by @yma11 in https://github.com/vllm-project/vllm/pull/20553
* [Docs] Clean up tables in supported_models.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20552
* [Misc] remove unused jinaai_serving_reranking by @Abirdcfly in https://github.com/vllm-project/vllm/pull/18878
* [Misc] Set the minimum openai version by @jeejeelee in https://github.com/vllm-project/vllm/pull/20539
* [Doc] Remove extra whitespace from CI failures doc by @hmellor in https://github.com/vllm-project/vllm/pull/20565
* [Doc] Use `gh-pr` and `gh-issue` everywhere we can in the docs by @hmellor in https://github.com/vllm-project/vllm/pull/20564
* [Doc] Fix internal links so they don't always point to latest by @hmellor in https://github.com/vllm-project/vllm/pull/20563
* [Doc] Add outline for content tabs by @hmellor in https://github.com/vllm-project/vllm/pull/20571
* [Doc] Fix some MkDocs snippets used in the installation docs by @hmellor in https://github.com/vllm-project/vllm/pull/20572
* [Model][Last/4] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/19675
* [Bugfix] Prevent IndexError for cached requests when pipeline parallelism is disabled by @panpan0000 in https://github.com/vllm-project/vllm/pull/20486
* [Feature] microbatch tokenization by @ztang2370 in https://github.com/vllm-project/vllm/pull/19334
* [DP] Copy environment variables to Ray DPEngineCoreActors by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20344
* [Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/20308
* [Misc] Add fully interleaved support for multimodal 'string' content format by @Dekakhrone in https://github.com/vllm-project/vllm/pull/14047
* [Misc] feat output content in stream response by @lengrongfu in https://github.com/vllm-project/vllm/pull/19608
* Fix links in multi-modal model contributing page by @hmellor in https://github.com/vllm-project/vllm/pull/18615
* [Config] Refactor mistral configs  by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/20570
* [Misc] Improve logging for dynamic shape cache compilation by @kyolebu in https://github.com/vllm-project/vllm/pull/20573
* [Bugfix] Fix Maverick correctness by filling zero to cache space in cutlass_moe by @minosfuture in https://github.com/vllm-project/vllm/pull/20167
* [Optimize] Don't send token ids when kv connector is not used by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20586
* Make distinct `code` and `console` admonitions so readers are less likely to miss them by @hmellor in https://github.com/vllm-project/vllm/pull/20585
* [Bugfix]: Fix messy code when using logprobs by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19209
* [Doc] Syntax highlight request responses as JSON instead of bash by @hmellor in https://github.com/vllm-project/vllm/pull/20582
* [Docs] Rewrite offline inference guide by @crypdick in https://github.com/vllm-project/vllm/pull/20594
* [Docs] Improve docstring for ray data llm example by @crypdick in https://github.com/vllm-project/vllm/pull/20597
* [Docs] Add Ray Serve LLM section to openai compatible server guide by @crypdick in https://github.com/vllm-project/vllm/pull/20595
* [Docs] Add Anyscale to frameworks by @crypdick in https://github.com/vllm-project/vllm/pull/20590
* [Misc] improve error msg by @reidliu41 in https://github.com/vllm-project/vllm/pull/20604
* [CI/Build][CPU] Fix CPU CI and remove all CPU V0 files by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20560
* [TPU] Temporary fix vmem oom for long model len by reducing page size by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20278
* [Frontend] [Core] Integrate Tensorizer in to S3 loading machinery, allow passing arbitrary arguments during save/load by @sangstar in https://github.com/vllm-project/vllm/pull/19619
* [PD][Nixl] Remote consumer READ timeout for clearing request blocks  by @NickLucche in https://github.com/vllm-project/vllm/pull/20139
* [Docs] Improve documentation for Deepseek R1 on Ray Serve LLM by @crypdick in https://github.com/vllm-project/vllm/pull/20601
* Remove unnecessary explicit title anchors and use relative links instead by @hmellor in https://github.com/vllm-project/vllm/pull/20620
* Stop using title frontmatter and fix doc that can only be reached by search by @hmellor in https://github.com/vllm-project/vllm/pull/20623
* [xpu]feat: support multi-lora on xpu by @yma11 in https://github.com/vllm-project/vllm/pull/20616
* Update torch/xla pin to 20250703 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/20589
* [Model] Implement missing `get_language_model` for Keye-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20631
* Revert invalid spellchecker fix on deepseek_vl2 by @viravera in https://github.com/vllm-project/vllm/pull/20618
* [CI] Increase the threshold of the MTEB RERANK tests by @noooop in https://github.com/vllm-project/vllm/pull/20615
* [Bugfix] Fix topk_ids indices_type for CUTLASS w8a8 FP8 MoE by @minosfuture in https://github.com/vllm-project/vllm/pull/20166
* [Core] Rename `get_max_tokens_per_item` for backward compatibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20630
* [Bugfix] Fix GLM-4.1-V video prompt update by @Isotr0py in https://github.com/vllm-project/vllm/pull/20635
* [TPU][Bugfix] disable phi-3 test by @QiliangCui in https://github.com/vllm-project/vllm/pull/20632
* Replace `multiply_add` with `homogeneous_multiply_add` to Address Clang Template Parameter Issue by @wenxin0319 in https://github.com/vllm-project/vllm/pull/20142
* [misc]refactor `Platform.set_device` method by @jikunshang in https://github.com/vllm-project/vllm/pull/20262
* [tech debt] Revisit lora request model checker by @kouroshHakha in https://github.com/vllm-project/vllm/pull/20636
* [BugFix][Intel GPU] Use refactored API for dist_backend in V1 worker by @ratnampa in https://github.com/vllm-project/vllm/pull/20596
* [Docs] Improve documentation for multi-node service helper script by @crypdick in https://github.com/vllm-project/vllm/pull/20600
* [Hardware][PPC64LE] Enable V1 for ppc64le and ARM by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/20554
* [Bugfix] set default set cuda_graph_sizes to min(self.max_num_seqs * 2, 512) by @izhuhaoran in https://github.com/vllm-project/vllm/pull/20628
* [feat] enable SM100 CUTLASS block scaled group gemm for smaller batch sizes by @djmmoss in https://github.com/vllm-project/vllm/pull/20640
* Fix bullets in incremental_build.md by @mgoin in https://github.com/vllm-project/vllm/pull/20642
* [Misc] Fix the size of batched_dummy_mm_inputs in profile_run by @B-201 in https://github.com/vllm-project/vllm/pull/20434
* [XPU] Use spawn with XPU multiprocessing by @dvrogozh in https://github.com/vllm-project/vllm/pull/20649
* [Intel GPU] support ray as distributed executor backend for XPU. by @jikunshang in https://github.com/vllm-project/vllm/pull/20659
* [Docs] fix minimax tool_calling docs error by @qscqesze in https://github.com/vllm-project/vllm/pull/20667
* [Bugfix] Fix the issue where `reasoning_content` is `None` when Thinkng is enabled and `tool_choice` is set to `'required'`. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20662
* [V1] [Doc] Update V1 docs for Mamba models by @tdoublep in https://github.com/vllm-project/vllm/pull/20499
* [Doc] Update notes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20668
* [Benchmark] Parameterization of streaming loading of multimodal datasets by @Potabk in https://github.com/vllm-project/vllm/pull/20528
* [Docs] Improve docs for RLHF co-location example by @crypdick in https://github.com/vllm-project/vllm/pull/20599
* [doc] update doc format by @reidliu41 in https://github.com/vllm-project/vllm/pull/20673
* [Bugfix] Fix handling of Tensorizer arguments for LoadConfig by @sangstar in https://github.com/vllm-project/vllm/pull/20643
* [TPU][Bugfix] fix test_pallas by @yaochengji in https://github.com/vllm-project/vllm/pull/20666
* [XPU][CI] enhance xpu test support by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20652
* [Bench] Add NVFP4 GEMM benchmark script by @mgoin in https://github.com/vllm-project/vllm/pull/20578
* [Doc] Update CPU doc by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20676
* Remove heading from installation `inc.md` file by @hmellor in https://github.com/vllm-project/vllm/pull/20697
* [CI/Build] Enlarge tolerance for a CPU multi-modal test by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20684
* Support Llama 4 for cutlass_moe_fp4 by @mgoin in https://github.com/vllm-project/vllm/pull/20453
* [Kernel] Triton implementation of causal-conv1d for Mamba-based models by @thoangtrvn in https://github.com/vllm-project/vllm/pull/18218
* [Kernel] Add Conch backend for mixed-precision linear layer by @jmanning-stackav in https://github.com/vllm-project/vllm/pull/19818
* [Feature][Quantization] MXFP4 support for MOE models by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/17888
* [BugFix]: Properly set engine_id when using multi connector by @Missmiaom in https://github.com/vllm-project/vllm/pull/19487
* [Misc] Simplify the prefix caching logic on draft tokens by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20701
* [CI/Build] Fix FlashInfer double build in Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/20651
* [Misc] DP : Add ExpertTokensMetadata by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20332
* Use NVCC `--compress-mode` to reduce binary size by 30% by @mgoin in https://github.com/vllm-project/vllm/pull/20694
* Correct PPMissingLayer handling in Deepseek-V2-Lite PP deployment by @eicherseiji in https://github.com/vllm-project/vllm/pull/20665
* [Frontend] Support Tool Calling with both `tool_choice='required'` and `$defs`. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20629
* [BugFix][CPU] Fix CPU worker dependency on cumem_allocator by @njhill in https://github.com/vllm-project/vllm/pull/20696
* [BugFix] Fix `VllmConfig()` construction on all platforms by @njhill in https://github.com/vllm-project/vllm/pull/20695
* [TPU][Core]Make load weight exceed hbm error more instructive for customers by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20644
* [KVConnector] Aggregate finished requests on the scheduler by @orozery in https://github.com/vllm-project/vllm/pull/19555
* [Misc] loose new-model tagger conditions by @Isotr0py in https://github.com/vllm-project/vllm/pull/20747
* [CI/Build] Fix Basic Models Test by @jeejeelee in https://github.com/vllm-project/vllm/pull/20728
* [Bugfix][Build][Non-CUDA] Only referencing CMAKE_CUDA_COMPILER_VERSION on CUDA where it is defined by @gshtras in https://github.com/vllm-project/vllm/pull/20738
* [doc] fix ordered list by @reidliu41 in https://github.com/vllm-project/vllm/pull/20749
* [CI Bugfix] Skip failing Tensorizer+LoRA test by @mgoin in https://github.com/vllm-project/vllm/pull/20724
* Normalize lm-eval command between baseline and correctness test by @mgoin in https://github.com/vllm-project/vllm/pull/18560
* [Misc] Clean up mark to fork process in BNB tests by @Isotr0py in https://github.com/vllm-project/vllm/pull/20692
* [Doc] Add engine args back in to the docs by @hmellor in https://github.com/vllm-project/vllm/pull/20674
* Update Dockerfile FlashInfer to v0.2.8rc1 by @mgoin in https://github.com/vllm-project/vllm/pull/20718
* [Hardware][CPU] Vllm int8 quantization enablement for ARM CPU by @nishith-fujitsu in https://github.com/vllm-project/vllm/pull/14129
* [ROCm][Regression] Remove tensor creation that harms performance on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/20741
* [Model] Add reason parser for Hunyuan A13B Model. by @kzjeef in https://github.com/vllm-project/vllm/pull/20625
* [Model][VLM] Support JinaVL Reranker by @shineran96 in https://github.com/vllm-project/vllm/pull/20260
* Fix DeepSeek-R1-0528 chat template by @sfbemerk in https://github.com/vllm-project/vllm/pull/20717
* [Test] Remove docker build from test. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20542
* [Bugfix] [CI] Fix Tensorizer LoRA test by @sangstar in https://github.com/vllm-project/vllm/pull/20760
* [V0][V1][Core] Add outlines integration for V1, and update V0 integration. by @unaidedelf8777 in https://github.com/vllm-project/vllm/pull/15975
* [CI] Fix pre commit issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/20782
* [Bugfix] Remove assertion of expert_map being None by @minosfuture in https://github.com/vllm-project/vllm/pull/20714
* [Core] Add Support for Default Modality Specific LoRAs [generate / chat completions] by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/19126
* [Bugfix] Fused MoE Modular Kernel chunking loop by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20392
* [KVConnector] Always call connector `clear_metadata()` at end of step by @njhill in https://github.com/vllm-project/vllm/pull/20756
* [Misc] MoE ModularKernel : Introduce TopKWeightAndReduce  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20648
* [Bugfix][Benchmark] Make sure the output length > 0 when testing prefill workload. by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20786
* [Docs] Lazy import gguf by @simon-mo in https://github.com/vllm-project/vllm/pull/20785
* [CI Bugfix] Specify same TORCH_CUDA_ARCH_LIST for flashinfer aot and install by @mgoin in https://github.com/vllm-project/vllm/pull/20772
* Add kimi-k2 tool parser by @MoyanZitto in https://github.com/vllm-project/vllm/pull/20789
* [fix]: disable cutlass block scaled group gemm for EP by @djmmoss in https://github.com/vllm-project/vllm/pull/20781
* [Model] Support HF format of minimax by @mgoin in https://github.com/vllm-project/vllm/pull/20211
* [Attention] MLA - Flashinfer Ragged Prefill by @alexm-redhat in https://github.com/vllm-project/vllm/pull/20034
* [Feature] Integrate SM100 DeepGEMM support by @yewentao256 in https://github.com/vllm-project/vllm/pull/20087
* [XPU] XCCL support enabled in torch 2.8.0.dev nightly builds by @ratnampa in https://github.com/vllm-project/vllm/pull/20705
* [Perf][fp8] Use CustomOp abstraction for fp8 quant for better perf by @ProExpertProg in https://github.com/vllm-project/vllm/pull/19830
* [V1] Enable Mamba2 layers other than MambaMixer2 in the v1 engine by @nopperl in https://github.com/vllm-project/vllm/pull/20660
* [doc] fold long code block by @reidliu41 in https://github.com/vllm-project/vllm/pull/20795
* [Bugfix] Upgrade depyf to 0.19 and streamline custom pass logging by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20777
* [Quantization][1/N] MoE support BNB-Inflight Quantization by @jeejeelee in https://github.com/vllm-project/vllm/pull/20061
* [Core] Add Flashinfer TRTLLM Backend for Flashinfer decode path (SM100).  by @pavanimajety in https://github.com/vllm-project/vllm/pull/19825
* [Bugfix] Refactor `/invocations` to be task-agnostic by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20764
* Temporarily suspend google/gemma-3-1b-it. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20722
* [Bugfix] Add missing field to TritonLanguagePlaceholder by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20812
* [doc] fix ordered list issue by @reidliu41 in https://github.com/vllm-project/vllm/pull/20819
* [Misc] Add unit tests for MoE ModularKernel combinations + Profiling utility by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20449
* [Kernel] Basic tuned configs for NVFP4 CUTLASS dense GEMM by @mgoin in https://github.com/vllm-project/vllm/pull/20646
* [Docs] Data Parallel deployment documentation by @njhill in https://github.com/vllm-project/vllm/pull/20768
* [Bugfix] Fix OOM in language generation test by @Isotr0py in https://github.com/vllm-project/vllm/pull/20814
* Update kimi-k2 tool calling docs, enable unit tests by @MoyanZitto in https://github.com/vllm-project/vllm/pull/20821
* [CI Bug] Fix Async Engine, Inputs, Utils, Worker Test: 'State' object has no attribute 'enable_server_load_tracking' by @yewentao256 in https://github.com/vllm-project/vllm/pull/20845
* Integration SM100 FlashInfer fused allreduce RMSNorm by @ilmarkov in https://github.com/vllm-project/vllm/pull/20691
* Add pynccl all-gatherv and reducescatterv by @trevor-m in https://github.com/vllm-project/vllm/pull/20154
* [Misc] Restrict deep_gemm's log output by @jeejeelee in https://github.com/vllm-project/vllm/pull/20827
* [Bugfix] Lazy import fused_experts in BitsAndBytesMoEMethod to avoid break not-cuda-alike devices  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20822
* [Bugfix] Fix tensor parallel issue in Qwen3 reranker weight loading by @yurhett in https://github.com/vllm-project/vllm/pull/20682
* [CI/Build] Ensure compatability with Transformers v4.53 by @Isotr0py in https://github.com/vllm-project/vllm/pull/20541
* [Bugfix] : Fix typo - logger.warn_once -> logger.warning_once by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20852
* [Frontend] Abstract prompt and SpeechToTextConfig for transcriptions models by @NickLucche in https://github.com/vllm-project/vllm/pull/20637
* [Bugfix] Replace unavailable video url in multimodal test by @Isotr0py in https://github.com/vllm-project/vllm/pull/20854
* [Misc] Respect `no_use_tqdm_on_load` flag while capturing CUDA graph by @lk-chen in https://github.com/vllm-project/vllm/pull/20834
* [Bug] Fix DeepGemm for EP low latency case by @yewentao256 in https://github.com/vllm-project/vllm/pull/20833
* [Docs] Update basic.md by @luccafong in https://github.com/vllm-project/vllm/pull/20846
* [Bugfix] Fix torch.compile x LoRA for PyTorch 2.8  by @zou3519 in https://github.com/vllm-project/vllm/pull/20823
* [cold start time] add envs.VLLM_COMPILE_DEPYF to guard decompile by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20790
* Remove extra tensor on CPU by @maxdebayser in https://github.com/vllm-project/vllm/pull/20693
* Enable ModelOpt Llama4 fp8 checkpoint deployment by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/20419
* Revert "Use NVCC --compress-mode to reduce binary size by 30% #20694" by @mgoin in https://github.com/vllm-project/vllm/pull/20853
* [Model] New model support for microsoft/Phi-4-mini-flash-reasoning by @congcongchen123 in https://github.com/vllm-project/vllm/pull/20702
* [Bugfix] Fix Tensor Parallelism Padding Consistency in Granite Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/20843
* [docs] convert supported configs to table by @reidliu41 in https://github.com/vllm-project/vllm/pull/20858
* [Bugfix] Restrict Machete to only run on Hopper by @mgoin in https://github.com/vllm-project/vllm/pull/20830
* [Sched] Enhance the logic to remove stopped requests from queues by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20739
* [Perf] Use Triton instead of Torch for DeepGEMM Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/20841
* [Bugfix] Fix a couple PPLX+CUTLASS MoE bugs by @ElizaWszola in https://github.com/vllm-project/vllm/pull/20825
* [Refactor] Change the way of import triton by @yewentao256 in https://github.com/vllm-project/vllm/pull/20774
* [Core] Support multiple tasks per model by @NickLucche in https://github.com/vllm-project/vllm/pull/20771
* Renable google/gemma-3-1b-it accuracy test. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20866
* Support for LlamaForSequenceClassification by @thechaos16 in https://github.com/vllm-project/vllm/pull/20807
* [Bugfix] Fix: add patch_rope_scaling after hf override by @Wangmerlyn in https://github.com/vllm-project/vllm/pull/20857
* [Bugfix] fix define of RerankDocument by @Liuchenlong in https://github.com/vllm-project/vllm/pull/20877
* [V1] [ROCm] [AITER] Upgrade AITER to commit `916bf3c` and bugfix APIs by @tjtanaa in https://github.com/vllm-project/vllm/pull/20880
* [V1] Hybrid allocator without prefix caching by @nopperl in https://github.com/vllm-project/vllm/pull/20661
* [Core] Add `update_config` RPC method by @22quinn in https://github.com/vllm-project/vllm/pull/20095
* [Prefix Cache] Add reproducible prefix-cache block hashing using SHA-256 + CBOR (64bit) by @vMaroon in https://github.com/vllm-project/vllm/pull/20511
* Removing redundant python version check by @Dannyso05 in https://github.com/vllm-project/vllm/pull/20888
* Fix: Add missing EOFError handling in CLI complete command by @reidliu41 in https://github.com/vllm-project/vllm/pull/20896
* [ROCm] [Bugfix] [Critical]: Fix mamba compilation bug by @tjtanaa in https://github.com/vllm-project/vllm/pull/20883
* [Quantization] add BNB for MixtralForCausalLM by @jeejeelee in https://github.com/vllm-project/vllm/pull/20893
* [Refactor][V1] Move outlines utils for V1 imports by @aarnphm in https://github.com/vllm-project/vllm/pull/20878
* [MISC] Move bind_kv_cache to worker module by @wangxiyuan in https://github.com/vllm-project/vllm/pull/20900
* [CI/Build] Fix OOM issue in Jina-VL test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20907
* [Bugfix] Bump up mistral_common to support v13 tokenizer by @22quinn in https://github.com/vllm-project/vllm/pull/20905
* [Misc] Remove unused function by @reidliu41 in https://github.com/vllm-project/vllm/pull/20909
* [Bugfix]: Fix messy code when using logprobs by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20910
* [Misc] Log the reason for falling back to FlexAttention by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20699
* [Model] Add Ling implementation by @ant-yy in https://github.com/vllm-project/vllm/pull/20680
* [CI] cc folks on changes to vllm/compilation by @zou3519 in https://github.com/vllm-project/vllm/pull/20925
* [CI] Update codeowner for compilation code by @houseroad in https://github.com/vllm-project/vllm/pull/20929
* [Misc] Clean up Aimv2 config registration in Ovis config by @Isotr0py in https://github.com/vllm-project/vllm/pull/20921
* [CI/Build] Add Transformers nightly tests in CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/20924
* Change default model to Qwen3-0.6B by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20335
* Add benchmark dataset for mlperf llama tasks by @mgoin in https://github.com/vllm-project/vllm/pull/20338
* [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20725
* [Misc] Relax translations tests by @NickLucche in https://github.com/vllm-project/vllm/pull/20856
* Fix overflow indexing in causal_conv1d kernel by @tdoublep in https://github.com/vllm-project/vllm/pull/20938
* [Docs] remove outdated performance benchmark by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20935
* Fall back if flashinfer comm module not found by @sarckk in https://github.com/vllm-project/vllm/pull/20936
* SM100 Cutlass MLA decode with unrestricted num_heads (< 128) for DeepSeek TP by @alexm-redhat in https://github.com/vllm-project/vllm/pull/20769
* [BugFix] VLLM_DISABLE_COMPILE_CACHE=1 should disable all reads and writes from the cache by @zou3519 in https://github.com/vllm-project/vllm/pull/20942
* [Bugfix] Fix incorrect dispatch for CutlassBlockScaledGroupedGemm and DeepGEMM by @mgoin in https://github.com/vllm-project/vllm/pull/20933
* [CI/Build] Split Entrypoints Test into LLM and API Server by @mgoin in https://github.com/vllm-project/vllm/pull/20945
* Use w8a8 quantized matmul Pallas kernel by @vanbasten23 in https://github.com/vllm-project/vllm/pull/19170
* [Docs] Add Kuberay to deployment integrations by @crypdick in https://github.com/vllm-project/vllm/pull/20592
* feat: add image zoom to improve image viewing experience by @reidliu41 in https://github.com/vllm-project/vllm/pull/20763
* [CI] Fix flaky `test_streaming_response` test by @NickLucche in https://github.com/vllm-project/vllm/pull/20913
* Enabled BnB NF4 inference on Gaudi by @rsshaik1 in https://github.com/vllm-project/vllm/pull/20172
* [Bugfix] Switch bailout logic for kv-cache-dtype with SM100 Flashinfer by @pavanimajety in https://github.com/vllm-project/vllm/pull/20934
* [Doc] Clearer mistral3 and pixtral model support description by @Isotr0py in https://github.com/vllm-project/vllm/pull/20926
* [cold start] replace VLLM_COMPILE_DEPYF with debug_dump_dir by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20940
* [Model] Add AutoWeightsLoader support for BERT, RoBERTa by @jennifurhe in https://github.com/vllm-project/vllm/pull/20534
* Implement Async Scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19970
* [Misc] Refactor AllReduceFusionPass. Remove parameter by @ilmarkov in https://github.com/vllm-project/vllm/pull/20918
* [frontend] Add --help=page option for paginated help output by @reidliu41 in https://github.com/vllm-project/vllm/pull/20961
* [Docs] Improve documentation for RLHF example by @crypdick in https://github.com/vllm-project/vllm/pull/20598
* [frontend] Refactor CLI Args for a better modular integration by @kouroshHakha in https://github.com/vllm-project/vllm/pull/20206
* [Docs] Improve documentation for ray cluster launcher helper script by @crypdick in https://github.com/vllm-project/vllm/pull/20602
* [TPU] Optimize kv cache update kernel by @tengyifei in https://github.com/vllm-project/vllm/pull/20415
* [V1] [Hybrid] Refactor mamba state shape calculation; enable V1 via cli  by @tdoublep in https://github.com/vllm-project/vllm/pull/20840
* [MISC] Add init files for python package by @Potabk in https://github.com/vllm-project/vllm/pull/20908
* [doc] Add more details for Ray-based DP by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20948
* [Deprecation] Remove `TokenizerPoolConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/20968
* [v1][core] Support for attention free models by @christian-pinto in https://github.com/vllm-project/vllm/pull/20811
* Voxtral by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/20970
* [CI/Build] Fix wrong path in Transformers Nightly Models Test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20994
* [Deprecation] Remove everything scheduled for removal in v0.10.0 by @hmellor in https://github.com/vllm-project/vllm/pull/20979
* Configure Gemini by @hmellor in https://github.com/vllm-project/vllm/pull/20971
* [Deprecation] Remove `nullable_kvs` by @hmellor in https://github.com/vllm-project/vllm/pull/20969
* Add full serve CLI reference back to docs by @hmellor in https://github.com/vllm-project/vllm/pull/20978
* [ROCm] warpSize is being made non constexpr in ROCm 7.0 by @gshtras in https://github.com/vllm-project/vllm/pull/20330
* [BugFix] fix 3 issues: (1) using metadata for causal-conv1d, (2) indexing overflow in v1 vLLM, and (3) init_states in v0 by @thoangtrvn in https://github.com/vllm-project/vllm/pull/20838
* [Frontend] Support cache_salt in /v1/completions and /v1/responses by @dr75 in https://github.com/vllm-project/vllm/pull/20981
* [Bug Fix] get_distributed_init_method should get the ip from get_ip iâ€¦ by @Relics in https://github.com/vllm-project/vllm/pull/20889
* [Nvidia] Integrate SM100 cudnn prefill API to MLA prefill by @elfiegg in https://github.com/vllm-project/vllm/pull/20411
* [Frontend] OpenAI Responses API supports input image by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20975
* [Frontend] Remove print left in FrontendArgs.add_cli_args by @mgoin in https://github.com/vllm-project/vllm/pull/21004
* [Model] Add ModelConfig class for GraniteMoeHybrid to override default max_seq_len_to_capture by @tdoublep in https://github.com/vllm-project/vllm/pull/20923
* [Misc] bump xgrammar version to v0.1.21 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20992
* [Chore] Remove outdated transformers check by @b8zhong in https://github.com/vllm-project/vllm/pull/20989
* [Misc] Refactor: Improve argument handling for `conda` command by @reidliu41 in https://github.com/vllm-project/vllm/pull/20481
* [Docs] Enhance Anyscale documentation, add quickstart links for vLLM by @crypdick in https://github.com/vllm-project/vllm/pull/21018
* [Bugfix] Correct per_act_token in CompressedTensorsW8A8Fp8MoECutlassMâ€¦ by @minosfuture in https://github.com/vllm-project/vllm/pull/20937
* Add Dockerfile argument for VLLM_USE_PRECOMPILED environment by @dougbtv in https://github.com/vllm-project/vllm/pull/20943
* [CI][HPU] update for v0 deprecate by switching to VLLM_TARGET_DEVICE=empty by @xuechendi in https://github.com/vllm-project/vllm/pull/21006
* [Bugfix] Fix Mistral3 support on SM100/SM120 by @mgoin in https://github.com/vllm-project/vllm/pull/20998
* [Doc] Remove duplicate docstring by @yewentao256 in https://github.com/vllm-project/vllm/pull/21012
* [Voxtral] Add more tests by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/21010
* Avoid direct comparison of floating point numbers by @maxdebayser in https://github.com/vllm-project/vllm/pull/21002
* [Meta] Llama4 EAGLE Support by @morgendave in https://github.com/vllm-project/vllm/pull/20591
* [TPU] fix kv_cache_update kernel block size choosing logic by @yaochengji in https://github.com/vllm-project/vllm/pull/21007
* [BugFix] Fix import error on non-blackwell machines by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21020
* Fix inadvertently silenced PP tests for `mp`, add DeepSeek V2/V3 model family to PP tests by @eicherseiji in https://github.com/vllm-project/vllm/pull/20831
* [Docs] Add intro and fix 1-2-3 list in frameworks/open-webui.md by @windsonsea in https://github.com/vllm-project/vllm/pull/19199
* [Model] Consolidate pooler implementations by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20927
* feat - add a new endpoint `get_tokenizer_info` to provide tokenizer/chat-template information by @m-misiura in https://github.com/vllm-project/vllm/pull/20575
* [fix] fix qwen image_embeds input by @h-avsha in https://github.com/vllm-project/vllm/pull/21049
* Remove Qwen Omni workaround that's no longer necessary by @hmellor in https://github.com/vllm-project/vllm/pull/21057
* [Model] Remove model sampler by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21059
* Support FP8 Quantization and Inference Run on Intel Gaudi (HPU) using INC (Intel Neural Compressor) by @nirda7 in https://github.com/vllm-project/vllm/pull/12010
* Remove torch_xla.tpu.version() from pallas.py. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21065
* Update PyTorch to `torch==2.7.1` for CUDA by @mgoin in https://github.com/vllm-project/vllm/pull/21011
* [Bugfix] weight loading use correct tp_group with patch_tensor_parallel_group by @Kevin-XiongC in https://github.com/vllm-project/vllm/pull/21024
* [Docker] Allow FlashInfer to be built in the ARM CUDA Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/21013
* [TPU] Start using python 3.12 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21000
* [Bugfix] Fix Machete zero point issue for GPTQ models on SM90 by @mgoin in https://github.com/vllm-project/vllm/pull/21066
* [Attention] Refactor attention metadata builder interface by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20466
* [V1][P/D]Enhance Performance and code readability for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/20906
* [V1] [KVConnector] Fix MultiprocExecutor worker output aggregation by @sdavidbd in https://github.com/vllm-project/vllm/pull/21048
* [Misc] Fix PhiMoE expert mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/21085
* [Bugfix]: Fix final_res_batch list index out of range error by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21055
* [Kernel] DeepGemm MoE : Integrate triton permute / unpermute kernels  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20903
* [Model] Add ToolParser and MoE Config for Hunyuan A13B  by @kzjeef in https://github.com/vllm-project/vllm/pull/20820
* [VLM] Add Nemotron-Nano-VL-8B-V1 support by @kylehh in https://github.com/vllm-project/vllm/pull/20349
* [Docs] Improve docstring formatting for `FusedMoEParallelConfig.make` by @hmellor in https://github.com/vllm-project/vllm/pull/21117
* [Misc] Avoid unnecessary import by @wangxiyuan in https://github.com/vllm-project/vllm/pull/21106
* [Docs] Move code block out of admonition now that it's short by @hmellor in https://github.com/vllm-project/vllm/pull/21118
* [Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/20762
* [Model] Update pooling model interface by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21058
* [Misc] Qwen MoE model supports LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/20932
* On environments where numa cannot be detected we get 0 by @ericcurtin in https://github.com/vllm-project/vllm/pull/21115
* [V0 deprecation] Remove V0 HPU backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21131
* [Log] Debugging Log with more Information by @yewentao256 in https://github.com/vllm-project/vllm/pull/20770
* [Bugfix] Fix the tensor non-contiguous issue for Flashinfer TRT-LLM backend attention kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/21133
* [Docs] Add minimal demo of Ray Data API usage by @crypdick in https://github.com/vllm-project/vllm/pull/21080
* [Docs] Update supported models documentation with missing models by @luccafong in https://github.com/vllm-project/vllm/pull/20844
* [Attention] Make local attention backend agnostic by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21093
* [Doc] Add inplace weights loading example by @22quinn in https://github.com/vllm-project/vllm/pull/19640
* [Core] FlashInfer CUTLASS fused MoE backend (NVFP4) by @wenscarl in https://github.com/vllm-project/vllm/pull/20037
* [Perf] Add swap_ab to SM90 FP8 non-block CUTLASS moe grouped gemm by @shixianc in https://github.com/vllm-project/vllm/pull/20911
* [Misc] Do not print async output warning for v1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21151
* [benchmark] Sending request strictly follows the random intervals by @Jialin in https://github.com/vllm-project/vllm/pull/21108
* [Misc] Make MM embedding merge interface explicit in model runner by @ywang96 in https://github.com/vllm-project/vllm/pull/21147
* [Model] Re-add the implicit conversion feature for as_seq_cls_model by @noooop in https://github.com/vllm-project/vllm/pull/21103
* [Bugfix] The special_tokens in tokenizer should also be controlled by do_lower_case in encoder_config. by @noooop in https://github.com/vllm-project/vllm/pull/20750
* [Doc] Fix typo in model name by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21178
* [Bugfix] Allocate less memory in non-batched CUTLASS MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/21121
* [Core] Set pooling params based on task and model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21128
* Let GraniteMoeAttention use YaRN by @tdoublep in https://github.com/vllm-project/vllm/pull/21174
* [CI] Update CODEOWNERS for vllm/compilation by @zou3519 in https://github.com/vllm-project/vllm/pull/21185
* [Kernel] Apply torch.Tag.needs_fixed_stride_order only for torch==2.6.0 by @zou3519 in https://github.com/vllm-project/vllm/pull/19346
* [Core] Avoid KVCacheBlock.__eq__ invocations in FreeKVCacheBlockQueue by @JialinOuyang-Meta in https://github.com/vllm-project/vllm/pull/21005
* [Bugfix] Voxtral on Blackwell GPUs (RTX 50 series) by @hax0r31337 in https://github.com/vllm-project/vllm/pull/21077
* Elastic Expert Parallel Initial Support by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20775
* [Quantization] Enable BNB support for more MoE models by @jeejeelee in https://github.com/vllm-project/vllm/pull/21100
* [Core] Support Local Chunked Attention for Hybrid KV Cache by @luccafong in https://github.com/vllm-project/vllm/pull/19351
* [Bugfix][Model] Fix LoRA for Mistral-Small-3.1-24B-Instruct-2503 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21183
* [V0 Deprecation] Remove V0 Spec Decode workers by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21152
* [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21193
* [BugFix][CPU] Fix `TorchSDPABackendImpl` doesn't have `use_irope`  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21200
* [Bug] DeepGemm: Fix TypeError: per_block_cast_to_fp8() missing 1 required positional argument: 'use_ue8m0' for SM100 by @yewentao256 in https://github.com/vllm-project/vllm/pull/21187
* [Model] EXAONE 4.0 model support by @Deepfocused in https://github.com/vllm-project/vllm/pull/21060
* [Misc][Tools][Benchmark] Add readme file for auto_tune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20779
* Fix a couple of Voxtral tests by @huydhn in https://github.com/vllm-project/vllm/pull/21218
* [V0 deprecation] Remove long context LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/21169
* [Bugfix] Fix ndarray video color from VideoAsset by @Isotr0py in https://github.com/vllm-project/vllm/pull/21064
* [BugFix] Fix potential cuda-graph IMA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21196
* Add torch golden impl for moe_align_block_size kernel test by @shixianc in https://github.com/vllm-project/vllm/pull/20653
* [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency by @kaixih in https://github.com/vllm-project/vllm/pull/20645
* [Bugfix][Frontend] Fix openai CLI arg `middleware` by @22quinn in https://github.com/vllm-project/vllm/pull/21220
* [bugfix] Fix auto thread-binding when world_size > 1 in CPU backend and refactor code by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21032
* Fix/remove some broken model executor tests by @rabi in https://github.com/vllm-project/vllm/pull/21224
* [CI/CD][bugfix]fix: error argument to loads has incompatible type by @llsj14 in https://github.com/vllm-project/vllm/pull/21223
* [Docs] Update the link to the 'Prometheus/Grafana' example by @1195343015 in https://github.com/vllm-project/vllm/pull/21225
* [BugFix] Make PD work with Ray by @kouroshHakha in https://github.com/vllm-project/vllm/pull/21072
* [V1] [Hybrid] Enable piecewise CUDA Graph for mamba layers  by @tdoublep in https://github.com/vllm-project/vllm/pull/21194
* [V0 Deprecation] Deprecate BlockSparse Attention & Phi3-Small by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21217
* [BugFix] Fix full cuda graph slot_mapping by @fhl2000 in https://github.com/vllm-project/vllm/pull/21228
* GLM-4 Update by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/20736
* [Docs] [V1] Update docs to remove enforce_eager limitation for hybrid models. by @tdoublep in https://github.com/vllm-project/vllm/pull/21233
* [TPU] support fp8 kv cache quantization by @yaochengji in https://github.com/vllm-project/vllm/pull/19292
* Enable v1 metrics tests by @eicherseiji in https://github.com/vllm-project/vllm/pull/20953
* [Model] use AutoWeightsLoader for bart by @calvin0327 in https://github.com/vllm-project/vllm/pull/18299
* [Model] Support VLMs with transformers backend by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/20543
* [bugfix] fix syntax warning caused by backslash by @1195343015 in https://github.com/vllm-project/vllm/pull/21251
* [CI] Cleanup modelscope version constraint in Dockerfile by @yankay in https://github.com/vllm-project/vllm/pull/21243
* [Docs] Add RFC Meeting to Issue Template by @simon-mo in https://github.com/vllm-project/vllm/pull/21279
* Add the instruction to run e2e validation manually before release by @huydhn in https://github.com/vllm-project/vllm/pull/21023
* [Bugfix] Fix missing placeholder in logger debug by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21280
* [Model][1/N] Support multiple poolers at model level by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21227
* [Docs] Fix hardcoded links in docs by @hmellor in https://github.com/vllm-project/vllm/pull/21287
* [Docs] Make tables more space efficient in `supported_models.md` by @hmellor in https://github.com/vllm-project/vllm/pull/21291
* [Misc] unify variable for LLM instance by @andyxning in https://github.com/vllm-project/vllm/pull/20996
* Add Nvidia ModelOpt config adaptation by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/19815
* [Misc] Add sliding window to flashinfer test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21282
* [CPU] Enable shared-memory based pipeline parallel for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21289
* [BugFix] make utils.current_stream thread-safety (#21252) by @simpx in https://github.com/vllm-project/vllm/pull/21253
* [Misc] Add dummy maverick test by @minosfuture in https://github.com/vllm-project/vllm/pull/21199
* [Attention] Clean up iRoPE in V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21188
* [DP] Fix Prometheus Logging by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/21257
* Fix bad lm-eval fork by @mgoin in https://github.com/vllm-project/vllm/pull/21318
* [perf] Speed up align sum kernels by @hj-mistral in https://github.com/vllm-project/vllm/pull/21079
* [v1][sampler] Inplace logprobs comparison to get the token rank by @houseroad in https://github.com/vllm-project/vllm/pull/21283
* [XPU] Enable external_launcher to serve as an executor via torchrun by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21021
* [Doc] Fix CPU doc format by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21316
* [Intel GPU] Ray Compiled Graph avoid NCCL for Intel GPU by @ratnampa in https://github.com/vllm-project/vllm/pull/21338
* Revert "[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) by @minosfuture in https://github.com/vllm-project/vllm/pull/21334
* [Core] Minimize number of dict lookup in _maybe_evict_cached_block by @Jialin in https://github.com/vllm-project/vllm/pull/21281
* [V1] [Hybrid] Add new test to verify that hybrid views into KVCacheTensor are compatible by @tdoublep in https://github.com/vllm-project/vllm/pull/21300
* [Refactor] Fix Compile Warning #1444-D by @yewentao256 in https://github.com/vllm-project/vllm/pull/21208
* Fix kv_cache_dtype handling for out-of-tree HPU plugin by @kzawora-intel in https://github.com/vllm-project/vllm/pull/21302
* [Misc] DeepEPHighThroughtput - Enable Inductor pass by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21311
* [Bug] DeepGemm: Fix Cuda Init Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/21312
* Update fp4 quantize API by @wenscarl in https://github.com/vllm-project/vllm/pull/21327
* [Feature][eplb] add verify ep or tp or dp by @lengrongfu in https://github.com/vllm-project/vllm/pull/21102
* Add arcee model by @alyosha-swamy in https://github.com/vllm-project/vllm/pull/21296
* [Bugfix] Fix eviction cached blocked logic by @simon-mo in https://github.com/vllm-project/vllm/pull/21357
* [Misc] Remove deprecated args in v0.10 by @kebe7jun in https://github.com/vllm-project/vllm/pull/21349
* [Core] Optimize update checks in LogitsProcessor by @Jialin in https://github.com/vllm-project/vllm/pull/21245
* [benchmark] Port benchmark request sent optimization to benchmark_serving by @Jialin in https://github.com/vllm-project/vllm/pull/21209
* [Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool by @Jialin in https://github.com/vllm-project/vllm/pull/21222
* [Misc] unify variable for LLM instance v2 by @andyxning in https://github.com/vllm-project/vllm/pull/21356
* [perf] Add fused MLA QKV + strided layernorm by @mickaelseznec in https://github.com/vllm-project/vllm/pull/21116
* [feat]: add SM100 support for cutlass FP8 groupGEMM by @djmmoss in https://github.com/vllm-project/vllm/pull/20447
* [Perf] Cuda Kernel for Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/21083
* Adds parallel model weight loading for runai_streamer by @bbartels in https://github.com/vllm-project/vllm/pull/21330
* [feat] Enable mm caching for transformers backend by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/21358
* Revert "[Refactor] Fix Compile Warning #1444-D (#21208)" by @yewentao256 in https://github.com/vllm-project/vllm/pull/21384
* Add tokenization_kwargs to encode for embedding model truncation by @Receiling in https://github.com/vllm-project/vllm/pull/21033
* [Bugfix] Decode Tokenized IDs to Strings for `hf_processor` in `llm.chat()` with `model_impl=transformers` by @ariG23498 in https://github.com/vllm-project/vllm/pull/21353
* [CI/Build] Fix test failure due to updated model repo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21375
* Fix Flashinfer Allreduce+Norm enable disable calculation based on `fi_allreduce_fusion_max_token_num` by @xinli-git in https://github.com/vllm-project/vllm/pull/21325
* [Model] Add Qwen3CoderToolParser by @ranpox in https://github.com/vllm-project/vllm/pull/21396
* [Misc] Copy HF_TOKEN env var to Ray workers by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21406
* [BugFix] Fix ray import error mem cleanup bug by @joerunde in https://github.com/vllm-project/vllm/pull/21381
* [CI/Build] Fix model executor tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21387
* [Bugfix][ROCm][Build] Fix build regression on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/21393
* Simplify weight loading in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21382
* [BugFix] Update python to python3 calls for image; fix prefix & input calculations. by @ericehanley in https://github.com/vllm-project/vllm/pull/21391
* [BUGFIX] deepseek-v2-lite failed due to fused_qkv_a_proj name update by @xuechendi in https://github.com/vllm-project/vllm/pull/21414
* [Bugfix][CUDA] fixes CUDA FP8 kv cache dtype supported by @elvischenv in https://github.com/vllm-project/vllm/pull/21420
* Changing "amdproduction" allocation. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/21409
* [Bugfix] Fix nightly transformers CI failure by @Isotr0py in https://github.com/vllm-project/vllm/pull/21427
* [Core] Add basic unit test for maybe_evict_cached_block by @Jialin in https://github.com/vllm-project/vllm/pull/21400
* [Cleanup] Only log MoE DP setup warning if DP is enabled by @mgoin in https://github.com/vllm-project/vllm/pull/21315
* add clear messages for deprecated models by @youkaichao in https://github.com/vllm-project/vllm/pull/21424
* [Bugfix] ensure tool_choice is popped when `tool_choice:null` is passed in json payload by @gcalmettes in https://github.com/vllm-project/vllm/pull/19679
* Fixed typo in profiling logs by @sergiopaniego in https://github.com/vllm-project/vllm/pull/21441
* [Docs] Fix bullets and grammars in tool_calling.md by @windsonsea in https://github.com/vllm-project/vllm/pull/21440
* [Sampler] Introduce logprobs mode for logging by @houseroad in https://github.com/vllm-project/vllm/pull/21398
* Mamba V2 Test not Asserting Failures.  by @fabianlim in https://github.com/vllm-project/vllm/pull/21379
* [Misc] fixed nvfp4_moe test failures due to invalid kwargs by @chenyang78 in https://github.com/vllm-project/vllm/pull/21246
* [Docs] Clean up v1/metrics.md by @windsonsea in https://github.com/vllm-project/vllm/pull/21449
* [Model] add Hunyuan V1 Dense Model support. by @kzjeef in https://github.com/vllm-project/vllm/pull/21368
* [V1] Check all pooling tasks during profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21299
* [Bugfix][Qwen][DCA] fixes bug in dual-chunk-flash-attn backend for qwen 1m models. by @sighingnow in https://github.com/vllm-project/vllm/pull/21364
* [Tests] Add tests for headless internal DP LB by @njhill in https://github.com/vllm-project/vllm/pull/21450
* [Core][Model] PrithviMAE Enablement on vLLM v1 engine by @christian-pinto in https://github.com/vllm-project/vllm/pull/20577
* Add test case for compiling multiple graphs by @sarckk in https://github.com/vllm-project/vllm/pull/21044
* [TPU][TEST] Fix the downloading issue in TPU v1 test 11.  by @QiliangCui in https://github.com/vllm-project/vllm/pull/21418
* [Core] Add `reload_weights` RPC method by @22quinn in https://github.com/vllm-project/vllm/pull/20096
* [V1] Fix local chunked attention always disabled by @sarckk in https://github.com/vllm-project/vllm/pull/21419
* [V0 Deprecation] Remove Prompt Adapters by @mgoin in https://github.com/vllm-project/vllm/pull/20588
* [Core] Freeze gc during cuda graph capture to speed up init by @mgoin in https://github.com/vllm-project/vllm/pull/21146
* feat(gguf_loader): accept HF repo paths & URLs for GGUF by @hardikkgupta in https://github.com/vllm-project/vllm/pull/20793
* [Frontend] Set MAX_AUDIO_CLIP_FILESIZE_MB via env var instead of hardcoding by @deven-labovitch in https://github.com/vllm-project/vllm/pull/21374
* [Misc] Add dummy maverick test to CI by @minosfuture in https://github.com/vllm-project/vllm/pull/21324
* [XPU][UT] increase intel xpu CI test scope by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/21492
* [Bugfix] Fix casing warning by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/21468
* [Bugfix] Fix example disagg_example_p2p_nccl_xpyd.sh zombie process by @david6666666 in https://github.com/vllm-project/vllm/pull/21437
* [BugFix]: Batch generation from prompt_embeds fails for long prompts by @KazusatoOoko in https://github.com/vllm-project/vllm/pull/21390
* [BugFix] Fix KVConnector TP worker aggregation by @njhill in https://github.com/vllm-project/vllm/pull/21473
* [DP] Internal Load Balancing Per Node [`one-pod-per-node`] by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/21238
* Dump input metadata on crash for async scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21258
* [BugFix] Set CUDA_VISIBLE_DEVICES before spawning the subprocesses by @yinghai in https://github.com/vllm-project/vllm/pull/21211
* Add think chunk by @juliendenize in https://github.com/vllm-project/vllm/pull/21333

## New Contributors
* @py-andy-c made their first contribution in https://github.com/vllm-project/vllm/pull/19399
* @2niuhe made their first contribution in https://github.com/vllm-project/vllm/pull/19394
* @leopardracer made their first contribution in https://github.com/vllm-project/vllm/pull/19442
* @artetaout made their first contribution in https://github.com/vllm-project/vllm/pull/19085
* @runzhen made their first contribution in https://github.com/vllm-project/vllm/pull/19453
* @strutive07 made their first contribution in https://github.com/vllm-project/vllm/pull/19522
* @mobicham made their first contribution in https://github.com/vllm-project/vllm/pull/19265
* @kouroshHakha made their first contribution in https://github.com/vllm-project/vllm/pull/19378
* @BoyuanFeng made their first contribution in https://github.com/vllm-project/vllm/pull/19587
* @sahelib25 made their first contribution in https://github.com/vllm-project/vllm/pull/18354
* @jiahanc made their first contribution in https://github.com/vllm-project/vllm/pull/19500
* @quanliu1991 made their first contribution in https://github.com/vllm-project/vllm/pull/18957
* @f14-bertolotti made their first contribution in https://github.com/vllm-project/vllm/pull/19564
* @Navanit-git made their first contribution in https://github.com/vllm-project/vllm/pull/19557
* @nguyenhoangthuan99 made their first contribution in https://github.com/vllm-project/vllm/pull/19597
* @diliu0349 made their first contribution in https://github.com/vllm-project/vllm/pull/19600
* @Zzz9990 made their first contribution in https://github.com/vllm-project/vllm/pull/18596
* @yhtang made their first contribution in https://github.com/vllm-project/vllm/pull/19788
* @zsolt-borbely-htec made their first contribution in https://github.com/vllm-project/vllm/pull/19803
* @zuxin666 made their first contribution in https://github.com/vllm-project/vllm/pull/17148
* @NekoMimiUnagi made their first contribution in https://github.com/vllm-project/vllm/pull/19301
* @xzbdmw made their first contribution in https://github.com/vllm-project/vllm/pull/19735
* @Xerxes-cn made their first contribution in https://github.com/vllm-project/vllm/pull/19860
* @nie3e made their first contribution in https://github.com/vllm-project/vllm/pull/19663
* @vladmihailescu made their first contribution in https://github.com/vllm-project/vllm/pull/18777
* @rabinadk1 made their first contribution in https://github.com/vllm-project/vllm/pull/19910
* @amitm02 made their first contribution in https://github.com/vllm-project/vllm/pull/19057
* @jinqinn made their first contribution in https://github.com/vllm-project/vllm/pull/19544
* @Flink-ddd made their first contribution in https://github.com/vllm-project/vllm/pull/19643
* @Jun-Howie made their first contribution in https://github.com/vllm-project/vllm/pull/19395
* @seemethere made their first contribution in https://github.com/vllm-project/vllm/pull/20032
* @h-avsha made their first contribution in https://github.com/vllm-project/vllm/pull/19984
* @max-wittig made their first contribution in https://github.com/vllm-project/vllm/pull/19695
* @lsz05 made their first contribution in https://github.com/vllm-project/vllm/pull/20067
* @kyolebu made their first contribution in https://github.com/vllm-project/vllm/pull/20135
* @lihaoyang-amd made their first contribution in https://github.com/vllm-project/vllm/pull/19744
* @Yazan-Sharaya made their first contribution in https://github.com/vllm-project/vllm/pull/19946
* @ilyal-cerebras made their first contribution in https://github.com/vllm-project/vllm/pull/20065
* @fabiendupont made their first contribution in https://github.com/vllm-project/vllm/pull/18064
* @SHA-4096 made their first contribution in https://github.com/vllm-project/vllm/pull/19700
* @1195343015 made their first contribution in https://github.com/vllm-project/vllm/pull/20185
* @redmoe-moutain made their first contribution in https://github.com/vllm-project/vllm/pull/18254
* @noiji made their first contribution in https://github.com/vllm-project/vllm/pull/19598
* @chewong made their first contribution in https://github.com/vllm-project/vllm/pull/15897
* @sakogan made their first contribution in https://github.com/vllm-project/vllm/pull/18768
* @czhu-cohere made their first contribution in https://github.com/vllm-project/vllm/pull/20268
* @aiyiwang2025 made their first contribution in https://github.com/vllm-project/vllm/pull/20114
* @zhoutianzi666 made their first contribution in https://github.com/vllm-project/vllm/pull/20236
* @Liangliang-Ma made their first contribution in https://github.com/vllm-project/vllm/pull/20169
* @yyzxw made their first contribution in https://github.com/vllm-project/vllm/pull/20315
* @Kwai-Keye made their first contribution in https://github.com/vllm-project/vllm/pull/20126
* @CSWYF3634076 made their first contribution in https://github.com/vllm-project/vllm/pull/20220
* @kaln27 made their first contribution in https://github.com/vllm-project/vllm/pull/17280
* @huaqiangwang made their first contribution in https://github.com/vllm-project/vllm/pull/20322
* @zichongli5 made their first contribution in https://github.com/vllm-project/vllm/pull/20286
* @cronoik-inceptionai made their first contribution in https://github.com/vllm-project/vllm/pull/20373
* @sangbumlikeagod made their first contribution in https://github.com/vllm-project/vllm/pull/18809
* @djmmoss made their first contribution in https://github.com/vllm-project/vllm/pull/19757
* @GuyStone made their first contribution in https://github.com/vllm-project/vllm/pull/20497
* @bottler made their first contribution in https://github.com/vllm-project/vllm/pull/20487
* @dbyoung18 made their first contribution in https://github.com/vllm-project/vllm/pull/19410
* @Abirdcfly made their first contribution in https://github.com/vllm-project/vllm/pull/18878
* @Dekakhrone made their first contribution in https://github.com/vllm-project/vllm/pull/14047
* @viravera made their first contribution in https://github.com/vllm-project/vllm/pull/20618
* @wenxin0319 made their first contribution in https://github.com/vllm-project/vllm/pull/20142
* @ratnampa made their first contribution in https://github.com/vllm-project/vllm/pull/20596
* @dvrogozh made their first contribution in https://github.com/vllm-project/vllm/pull/20649
* @thoangtrvn made their first contribution in https://github.com/vllm-project/vllm/pull/18218
* @jmanning-stackav made their first contribution in https://github.com/vllm-project/vllm/pull/19818
* @Missmiaom made their first contribution in https://github.com/vllm-project/vllm/pull/19487
* @orozery made their first contribution in https://github.com/vllm-project/vllm/pull/19555
* @nishith-fujitsu made their first contribution in https://github.com/vllm-project/vllm/pull/14129
* @kzjeef made their first contribution in https://github.com/vllm-project/vllm/pull/20625
* @shineran96 made their first contribution in https://github.com/vllm-project/vllm/pull/20260
* @unaidedelf8777 made their first contribution in https://github.com/vllm-project/vllm/pull/15975
* @MoyanZitto made their first contribution in https://github.com/vllm-project/vllm/pull/20789
* @nopperl made their first contribution in https://github.com/vllm-project/vllm/pull/20660
* @trevor-m made their first contribution in https://github.com/vllm-project/vllm/pull/20154
* @yurhett made their first contribution in https://github.com/vllm-project/vllm/pull/20682
* @thechaos16 made their first contribution in https://github.com/vllm-project/vllm/pull/20807
* @Wangmerlyn made their first contribution in https://github.com/vllm-project/vllm/pull/20857
* @Liuchenlong made their first contribution in https://github.com/vllm-project/vllm/pull/20877
* @vMaroon made their first contribution in https://github.com/vllm-project/vllm/pull/20511
* @Dannyso05 made their first contribution in https://github.com/vllm-project/vllm/pull/20888
* @ant-yy made their first contribution in https://github.com/vllm-project/vllm/pull/20680
* @rsshaik1 made their first contribution in https://github.com/vllm-project/vllm/pull/20172
* @jennifurhe made their first contribution in https://github.com/vllm-project/vllm/pull/20534
* @tengyifei made their first contribution in https://github.com/vllm-project/vllm/pull/20415
* @Relics made their first contribution in https://github.com/vllm-project/vllm/pull/20889
* @dougbtv made their first contribution in https://github.com/vllm-project/vllm/pull/20943
* @morgendave made their first contribution in https://github.com/vllm-project/vllm/pull/20591
* @m-misiura made their first contribution in https://github.com/vllm-project/vllm/pull/20575
* @nirda7 made their first contribution in https://github.com/vllm-project/vllm/pull/12010
* @Kevin-XiongC made their first contribution in https://github.com/vllm-project/vllm/pull/21024
* @ericcurtin made their first contribution in https://github.com/vllm-project/vllm/pull/21115
* @shixianc made their first contribution in https://github.com/vllm-project/vllm/pull/20911
* @Jialin made their first contribution in https://github.com/vllm-project/vllm/pull/21108
* @JialinOuyang-Meta made their first contribution in https://github.com/vllm-project/vllm/pull/21005
* @hax0r31337 made their first contribution in https://github.com/vllm-project/vllm/pull/21077
* @Deepfocused made their first contribution in https://github.com/vllm-project/vllm/pull/21060
* @fhl2000 made their first contribution in https://github.com/vllm-project/vllm/pull/21228
* @alyosha-swamy made their first contribution in https://github.com/vllm-project/vllm/pull/21296
* @bbartels made their first contribution in https://github.com/vllm-project/vllm/pull/21330
* @Receiling made their first contribution in https://github.com/vllm-project/vllm/pull/21033
* @ariG23498 made their first contribution in https://github.com/vllm-project/vllm/pull/21353
* @xinli-git made their first contribution in https://github.com/vllm-project/vllm/pull/21325
* @ranpox made their first contribution in https://github.com/vllm-project/vllm/pull/21396
* @ericehanley made their first contribution in https://github.com/vllm-project/vllm/pull/21391
* @sergiopaniego made their first contribution in https://github.com/vllm-project/vllm/pull/21441
* @hardikkgupta made their first contribution in https://github.com/vllm-project/vllm/pull/20793
* @deven-labovitch made their first contribution in https://github.com/vllm-project/vllm/pull/21374
* @MatthewBonanni made their first contribution in https://github.com/vllm-project/vllm/pull/21468
* @david6666666 made their first contribution in https://github.com/vllm-project/vllm/pull/21437
* @KazusatoOoko made their first contribution in https://github.com/vllm-project/vllm/pull/21390

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.1...v0.10.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.0)

---

## v0.10.0rc2: v0.10.0rc2
**Published:** 2025-07-24
**Pre-release**

## What's Changed
* [Model] use AutoWeightsLoader for bart by @calvin0327 in https://github.com/vllm-project/vllm/pull/18299
* [Model] Support VLMs with transformers backend by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/20543
* [bugfix] fix syntax warning caused by backslash by @1195343015 in https://github.com/vllm-project/vllm/pull/21251
* [CI] Cleanup modelscope version constraint in Dockerfile by @yankay in https://github.com/vllm-project/vllm/pull/21243
* [Docs] Add RFC Meeting to Issue Template by @simon-mo in https://github.com/vllm-project/vllm/pull/21279
* Add the instruction to run e2e validation manually before release by @huydhn in https://github.com/vllm-project/vllm/pull/21023
* [Bugfix] Fix missing placeholder in logger debug by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21280
* [Model][1/N] Support multiple poolers at model level by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21227
* [Docs] Fix hardcoded links in docs by @hmellor in https://github.com/vllm-project/vllm/pull/21287
* [Docs] Make tables more space efficient in `supported_models.md` by @hmellor in https://github.com/vllm-project/vllm/pull/21291
* [Misc] unify variable for LLM instance by @andyxning in https://github.com/vllm-project/vllm/pull/20996
* Add Nvidia ModelOpt config adaptation by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/19815
* [Misc] Add sliding window to flashinfer test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21282
* [CPU] Enable shared-memory based pipeline parallel for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21289
* [BugFix] make utils.current_stream thread-safety (#21252) by @simpx in https://github.com/vllm-project/vllm/pull/21253
* [Misc] Add dummy maverick test by @minosfuture in https://github.com/vllm-project/vllm/pull/21199
* [Attention] Clean up iRoPE in V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21188
* [DP] Fix Prometheus Logging by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/21257
* Fix bad lm-eval fork by @mgoin in https://github.com/vllm-project/vllm/pull/21318
* [perf] Speed up align sum kernels by @hj-mistral in https://github.com/vllm-project/vllm/pull/21079
* [v1][sampler] Inplace logprobs comparison to get the token rank by @houseroad in https://github.com/vllm-project/vllm/pull/21283
* [XPU] Enable external_launcher to serve as an executor via torchrun by @chaojun-zhang in https://github.com/vllm-project/vllm/pull/21021
* [Doc] Fix CPU doc format by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21316
* [Intel GPU] Ray Compiled Graph avoid NCCL for Intel GPU by @ratnampa in https://github.com/vllm-project/vllm/pull/21338
* Revert "[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) by @minosfuture in https://github.com/vllm-project/vllm/pull/21334
* [Core] Minimize number of dict lookup in _maybe_evict_cached_block by @Jialin in https://github.com/vllm-project/vllm/pull/21281
* [V1] [Hybrid] Add new test to verify that hybrid views into KVCacheTensor are compatible by @tdoublep in https://github.com/vllm-project/vllm/pull/21300
* [Refactor] Fix Compile Warning #1444-D by @yewentao256 in https://github.com/vllm-project/vllm/pull/21208
* Fix kv_cache_dtype handling for out-of-tree HPU plugin by @kzawora-intel in https://github.com/vllm-project/vllm/pull/21302
* [Misc] DeepEPHighThroughtput - Enable Inductor pass by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21311
* [Bug] DeepGemm: Fix Cuda Init Error by @yewentao256 in https://github.com/vllm-project/vllm/pull/21312
* Update fp4 quantize API by @wenscarl in https://github.com/vllm-project/vllm/pull/21327
* [Feature][eplb] add verify ep or tp or dp by @lengrongfu in https://github.com/vllm-project/vllm/pull/21102
* Add arcee model by @alyosha-swamy in https://github.com/vllm-project/vllm/pull/21296
* [Bugfix] Fix eviction cached blocked logic by @simon-mo in https://github.com/vllm-project/vllm/pull/21357
* [Misc] Remove deprecated args in v0.10 by @kebe7jun in https://github.com/vllm-project/vllm/pull/21349
* [Core] Optimize update checks in LogitsProcessor by @Jialin in https://github.com/vllm-project/vllm/pull/21245
* [benchmark] Port benchmark request sent optimization to benchmark_serving by @Jialin in https://github.com/vllm-project/vllm/pull/21209
* [Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool by @Jialin in https://github.com/vllm-project/vllm/pull/21222
* [Misc] unify variable for LLM instance v2 by @andyxning in https://github.com/vllm-project/vllm/pull/21356
* [perf] Add fused MLA QKV + strided layernorm by @mickaelseznec in https://github.com/vllm-project/vllm/pull/21116
* [feat]: add SM100 support for cutlass FP8 groupGEMM by @djmmoss in https://github.com/vllm-project/vllm/pull/20447
* [Perf] Cuda Kernel for Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/21083
* Adds parallel model weight loading for runai_streamer by @bbartels in https://github.com/vllm-project/vllm/pull/21330
* [feat] Enable mm caching for transformers backend by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/21358
* Revert "[Refactor] Fix Compile Warning #1444-D (#21208)" by @yewentao256 in https://github.com/vllm-project/vllm/pull/21384
* Add tokenization_kwargs to encode for embedding model truncation by @Receiling in https://github.com/vllm-project/vllm/pull/21033
* [Bugfix] Decode Tokenized IDs to Strings for `hf_processor` in `llm.chat()` with `model_impl=transformers` by @ariG23498 in https://github.com/vllm-project/vllm/pull/21353
* [CI/Build] Fix test failure due to updated model repo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21375
* Fix Flashinfer Allreduce+Norm enable disable calculation based on `fi_allreduce_fusion_max_token_num` by @xinli-git in https://github.com/vllm-project/vllm/pull/21325
* [Model] Add Qwen3CoderToolParser by @ranpox in https://github.com/vllm-project/vllm/pull/21396
* [Misc] Copy HF_TOKEN env var to Ray workers by @ruisearch42 in https://github.com/vllm-project/vllm/pull/21406
* [BugFix] Fix ray import error mem cleanup bug by @joerunde in https://github.com/vllm-project/vllm/pull/21381
* [CI/Build] Fix model executor tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21387
* [Bugfix][ROCm][Build] Fix build regression on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/21393
* Simplify weight loading in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/21382
* [BugFix] Update python to python3 calls for image; fix prefix & input calculations. by @ericehanley in https://github.com/vllm-project/vllm/pull/21391
* [BUGFIX] deepseek-v2-lite failed due to fused_qkv_a_proj name update by @xuechendi in https://github.com/vllm-project/vllm/pull/21414
* [Bugfix][CUDA] fixes CUDA FP8 kv cache dtype supported by @elvischenv in https://github.com/vllm-project/vllm/pull/21420
* Changing "amdproduction" allocation. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/21409
* [Bugfix] Fix nightly transformers CI failure by @Isotr0py in https://github.com/vllm-project/vllm/pull/21427
* [Core] Add basic unit test for maybe_evict_cached_block by @Jialin in https://github.com/vllm-project/vllm/pull/21400
* [Cleanup] Only log MoE DP setup warning if DP is enabled by @mgoin in https://github.com/vllm-project/vllm/pull/21315
* add clear messages for deprecated models by @youkaichao in https://github.com/vllm-project/vllm/pull/21424
* [Bugfix] ensure tool_choice is popped when `tool_choice:null` is passed in json payload by @gcalmettes in https://github.com/vllm-project/vllm/pull/19679
* Fixed typo in profiling logs by @sergiopaniego in https://github.com/vllm-project/vllm/pull/21441
* [Docs] Fix bullets and grammars in tool_calling.md by @windsonsea in https://github.com/vllm-project/vllm/pull/21440
* [Sampler] Introduce logprobs mode for logging by @houseroad in https://github.com/vllm-project/vllm/pull/21398
* Mamba V2 Test not Asserting Failures.  by @fabianlim in https://github.com/vllm-project/vllm/pull/21379
* [Misc] fixed nvfp4_moe test failures due to invalid kwargs by @chenyang78 in https://github.com/vllm-project/vllm/pull/21246
* [Docs] Clean up v1/metrics.md by @windsonsea in https://github.com/vllm-project/vllm/pull/21449
* [Model] add Hunyuan V1 Dense Model support. by @kzjeef in https://github.com/vllm-project/vllm/pull/21368
* [V1] Check all pooling tasks during profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21299
* [Bugfix][Qwen][DCA] fixes bug in dual-chunk-flash-attn backend for qwen 1m models. by @sighingnow in https://github.com/vllm-project/vllm/pull/21364
* [Tests] Add tests for headless internal DP LB by @njhill in https://github.com/vllm-project/vllm/pull/21450
* [Core][Model] PrithviMAE Enablement on vLLM v1 engine by @christian-pinto in https://github.com/vllm-project/vllm/pull/20577
* Add test case for compiling multiple graphs by @sarckk in https://github.com/vllm-project/vllm/pull/21044
* [TPU][TEST] Fix the downloading issue in TPU v1 test 11.  by @QiliangCui in https://github.com/vllm-project/vllm/pull/21418
* [Core] Add `reload_weights` RPC method by @22quinn in https://github.com/vllm-project/vllm/pull/20096
* [V1] Fix local chunked attention always disabled by @sarckk in https://github.com/vllm-project/vllm/pull/21419
* [V0 Deprecation] Remove Prompt Adapters by @mgoin in https://github.com/vllm-project/vllm/pull/20588
* [Core] Freeze gc during cuda graph capture to speed up init by @mgoin in https://github.com/vllm-project/vllm/pull/21146
* feat(gguf_loader): accept HF repo paths & URLs for GGUF by @hardikkgupta in https://github.com/vllm-project/vllm/pull/20793
* [Frontend] Set MAX_AUDIO_CLIP_FILESIZE_MB via env var instead of hardcoding by @deven-labovitch in https://github.com/vllm-project/vllm/pull/21374
* [Misc] Add dummy maverick test to CI by @minosfuture in https://github.com/vllm-project/vllm/pull/21324
* [XPU][UT] increase intel xpu CI test scope by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/21492
* [Bugfix] Fix casing warning by @MatthewBonanni in https://github.com/vllm-project/vllm/pull/21468
* [Bugfix] Fix example disagg_example_p2p_nccl_xpyd.sh zombie process by @david6666666 in https://github.com/vllm-project/vllm/pull/21437
* [BugFix]: Batch generation from prompt_embeds fails for long prompts by @KazusatoOoko in https://github.com/vllm-project/vllm/pull/21390
* [BugFix] Fix KVConnector TP worker aggregation by @njhill in https://github.com/vllm-project/vllm/pull/21473
* [DP] Internal Load Balancing Per Node [`one-pod-per-node`] by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/21238
* Dump input metadata on crash for async scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21258
* [BugFix] Set CUDA_VISIBLE_DEVICES before spawning the subprocesses by @yinghai in https://github.com/vllm-project/vllm/pull/21211
* Add think chunk by @juliendenize in https://github.com/vllm-project/vllm/pull/21333

## New Contributors
* @chaojun-zhang made their first contribution in https://github.com/vllm-project/vllm/pull/21021
* @alyosha-swamy made their first contribution in https://github.com/vllm-project/vllm/pull/21296
* @bbartels made their first contribution in https://github.com/vllm-project/vllm/pull/21330
* @Receiling made their first contribution in https://github.com/vllm-project/vllm/pull/21033
* @ariG23498 made their first contribution in https://github.com/vllm-project/vllm/pull/21353
* @xinli-git made their first contribution in https://github.com/vllm-project/vllm/pull/21325
* @ranpox made their first contribution in https://github.com/vllm-project/vllm/pull/21396
* @ericehanley made their first contribution in https://github.com/vllm-project/vllm/pull/21391
* @sergiopaniego made their first contribution in https://github.com/vllm-project/vllm/pull/21441
* @hardikkgupta made their first contribution in https://github.com/vllm-project/vllm/pull/20793
* @deven-labovitch made their first contribution in https://github.com/vllm-project/vllm/pull/21374
* @MatthewBonanni made their first contribution in https://github.com/vllm-project/vllm/pull/21468
* @david6666666 made their first contribution in https://github.com/vllm-project/vllm/pull/21437
* @KazusatoOoko made their first contribution in https://github.com/vllm-project/vllm/pull/21390

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.10.0rc1...v0.10.0rc2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.0rc2)

---

## v0.10.0rc1: v0.10.0rc1
**Published:** 2025-07-20
**Pre-release**

## What's Changed
* [Kernel] Enable fp8 support for pplx and BatchedTritonExperts. by @bnellnm in https://github.com/vllm-project/vllm/pull/18864
* [Misc] Fix `Unable to detect current VLLM config. Defaulting to NHD kv cache layout` warning by @NickLucche in https://github.com/vllm-project/vllm/pull/20400
* [Bugfix] Register reducer even if transformers_modules not available by @eicherseiji in https://github.com/vllm-project/vllm/pull/19510
* Change warn_for_unimplemented_methods to debug by @mgoin in https://github.com/vllm-project/vllm/pull/20455
* [Platform] Add custom default max tokens by @gmarinho2 in https://github.com/vllm-project/vllm/pull/18557
* Add ignore consolidated file in mistral example code by @princepride in https://github.com/vllm-project/vllm/pull/20420
* [Misc] small update by @reidliu41 in https://github.com/vllm-project/vllm/pull/20462
* [Structured Outputs][V1] Skipping with models doesn't contain tokenizers by @aarnphm in https://github.com/vllm-project/vllm/pull/20365
* [Perf] Optimize Vectorization Utils for Int 8 Quantization Kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/20331
* [Misc] Add SPDX-FileCopyrightText by @jeejeelee in https://github.com/vllm-project/vllm/pull/20428
* Support Llama 4 for fused_marlin_moe by @mgoin in https://github.com/vllm-project/vllm/pull/20457
* [Bug][Frontend] Fix structure of transcription's decoder_prompt by @sangbumlikeagod in https://github.com/vllm-project/vllm/pull/18809
* [Model][3/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20168
* [Doc] Fix classification table in list of supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20489
* [CI] add kvcache-connector dependency definition and add into CI build by @panpan0000 in https://github.com/vllm-project/vllm/pull/18193
* [Misc] Small: Remove global media connector. Each test should have its own test connector object. by @huachenheli in https://github.com/vllm-project/vllm/pull/20395
* Enable V1 for Hybrid SSM/Attention Models by @tdoublep in https://github.com/vllm-project/vllm/pull/20016
* [feat]: CUTLASS block scaled group gemm for SM100 by @djmmoss in https://github.com/vllm-project/vllm/pull/19757
* [CI Bugfix] Fix pre-commit failures on main by @mgoin in https://github.com/vllm-project/vllm/pull/20502
* [Doc] fix mutltimodal_inputs.md gh examples link by @GuyStone in https://github.com/vllm-project/vllm/pull/20497
* [Misc] Add security warning for development mode endpoints by @reidliu41 in https://github.com/vllm-project/vllm/pull/20508
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/20506
* [Misc] Remove the unused LoRA test code by @jeejeelee in https://github.com/vllm-project/vllm/pull/20494
* Fix unknown attribute of topk_indices_dtype in CompressedTensorsW8A8Fp8MoECutlassMethod by @luccafong in https://github.com/vllm-project/vllm/pull/20507
* [v1] Re-add fp32 support to v1 engine through FlexAttention by @Isotr0py in https://github.com/vllm-project/vllm/pull/19754
* [Misc] Add logger.exception for TPU information collection failures by @reidliu41 in https://github.com/vllm-project/vllm/pull/20510
* [Misc] remove unused import by @reidliu41 in https://github.com/vllm-project/vllm/pull/20517
* test_attention compat with coming xformers change by @bottler in https://github.com/vllm-project/vllm/pull/20487
* [BUG] Fix #20484. Support empty sequence in cuda penalty kernel by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20491
* [Bugfix] Fix missing per_act_token parameter in compressed_tensors_moe by @luccafong in https://github.com/vllm-project/vllm/pull/20509
* [BugFix] Fix: ImportError when building on hopper systems by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20513
* [TPU][Bugfix] fix the MoE OOM issue by @yaochengji in https://github.com/vllm-project/vllm/pull/20339
* [Frontend] Support image object in llm.chat by @sfeng33 in https://github.com/vllm-project/vllm/pull/19635
* [Benchmark] Add support for multiple batch size benchmark through CLI in `benchmark_moe.py` + Add Triton Fused MoE kernel config for FP8 E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/20516
* [Misc] call the pre-defined func by @reidliu41 in https://github.com/vllm-project/vllm/pull/20518
* [V0 deprecation] Remove V0 CPU/XPU/TPU backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20412
* [V1] Support any head size for FlexAttention backend by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20467
* [BugFix][Spec Decode] Fix spec token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20530
* [Bugfix] Add `use_cross_encoder` flag to use correct activation in `ClassifierPooler` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20527
* Implement OpenAI Responses API [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20504
* [Misc] add a tip for pre-commit by @reidliu41 in https://github.com/vllm-project/vllm/pull/20536
* [Refactor]Abstract Platform Interface for Distributed Backend and Add xccl Support for Intel XPU by @dbyoung18 in https://github.com/vllm-project/vllm/pull/19410
* [CI/Build] Enable phi2 lora test by @jeejeelee in https://github.com/vllm-project/vllm/pull/20540
* [XPU][CI] add v1/core test in xpu hardware ci by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20537
* Add docstrings to url_schemes.py to improve readability by @windsonsea in https://github.com/vllm-project/vllm/pull/20545
* [XPU] log clean up for XPU platform by @yma11 in https://github.com/vllm-project/vllm/pull/20553
* [Docs] Clean up tables in supported_models.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20552
* [Misc] remove unused jinaai_serving_reranking by @Abirdcfly in https://github.com/vllm-project/vllm/pull/18878
* [Misc] Set the minimum openai version by @jeejeelee in https://github.com/vllm-project/vllm/pull/20539
* [Doc] Remove extra whitespace from CI failures doc by @hmellor in https://github.com/vllm-project/vllm/pull/20565
* [Doc] Use `gh-pr` and `gh-issue` everywhere we can in the docs by @hmellor in https://github.com/vllm-project/vllm/pull/20564
* [Doc] Fix internal links so they don't always point to latest by @hmellor in https://github.com/vllm-project/vllm/pull/20563
* [Doc] Add outline for content tabs by @hmellor in https://github.com/vllm-project/vllm/pull/20571
* [Doc] Fix some MkDocs snippets used in the installation docs by @hmellor in https://github.com/vllm-project/vllm/pull/20572
* [Model][Last/4] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/19675
* [Bugfix] Prevent IndexError for cached requests when pipeline parallelism is disabled by @panpan0000 in https://github.com/vllm-project/vllm/pull/20486
* [Feature] microbatch tokenization by @ztang2370 in https://github.com/vllm-project/vllm/pull/19334
* [DP] Copy environment variables to Ray DPEngineCoreActors by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20344
* [Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/20308
* [Misc] Add fully interleaved support for multimodal 'string' content format by @Dekakhrone in https://github.com/vllm-project/vllm/pull/14047
* [Misc] feat output content in stream response by @lengrongfu in https://github.com/vllm-project/vllm/pull/19608
* Fix links in multi-modal model contributing page by @hmellor in https://github.com/vllm-project/vllm/pull/18615
* [Config] Refactor mistral configs  by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/20570
* [Misc] Improve logging for dynamic shape cache compilation by @kyolebu in https://github.com/vllm-project/vllm/pull/20573
* [Bugfix] Fix Maverick correctness by filling zero to cache space in cutlass_moe by @minosfuture in https://github.com/vllm-project/vllm/pull/20167
* [Optimize] Don't send token ids when kv connector is not used by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20586
* Make distinct `code` and `console` admonitions so readers are less likely to miss them by @hmellor in https://github.com/vllm-project/vllm/pull/20585
* [Bugfix]: Fix messy code when using logprobs by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19209
* [Doc] Syntax highlight request responses as JSON instead of bash by @hmellor in https://github.com/vllm-project/vllm/pull/20582
* [Docs] Rewrite offline inference guide by @crypdick in https://github.com/vllm-project/vllm/pull/20594
* [Docs] Improve docstring for ray data llm example by @crypdick in https://github.com/vllm-project/vllm/pull/20597
* [Docs] Add Ray Serve LLM section to openai compatible server guide by @crypdick in https://github.com/vllm-project/vllm/pull/20595
* [Docs] Add Anyscale to frameworks by @crypdick in https://github.com/vllm-project/vllm/pull/20590
* [Misc] improve error msg by @reidliu41 in https://github.com/vllm-project/vllm/pull/20604
* [CI/Build][CPU] Fix CPU CI and remove all CPU V0 files by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20560
* [TPU] Temporary fix vmem oom for long model len by reducing page size by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20278
* [Frontend] [Core] Integrate Tensorizer in to S3 loading machinery, allow passing arbitrary arguments during save/load by @sangstar in https://github.com/vllm-project/vllm/pull/19619
* [PD][Nixl] Remote consumer READ timeout for clearing request blocks  by @NickLucche in https://github.com/vllm-project/vllm/pull/20139
* [Docs] Improve documentation for Deepseek R1 on Ray Serve LLM by @crypdick in https://github.com/vllm-project/vllm/pull/20601
* Remove unnecessary explicit title anchors and use relative links instead by @hmellor in https://github.com/vllm-project/vllm/pull/20620
* Stop using title frontmatter and fix doc that can only be reached by search by @hmellor in https://github.com/vllm-project/vllm/pull/20623
* [xpu]feat: support multi-lora on xpu by @yma11 in https://github.com/vllm-project/vllm/pull/20616
* Update torch/xla pin to 20250703 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/20589
* [Model] Implement missing `get_language_model` for Keye-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20631
* Revert invalid spellchecker fix on deepseek_vl2 by @viravera in https://github.com/vllm-project/vllm/pull/20618
* [CI] Increase the threshold of the MTEB RERANK tests by @noooop in https://github.com/vllm-project/vllm/pull/20615
* [Bugfix] Fix topk_ids indices_type for CUTLASS w8a8 FP8 MoE by @minosfuture in https://github.com/vllm-project/vllm/pull/20166
* [Core] Rename `get_max_tokens_per_item` for backward compatibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20630
* [Bugfix] Fix GLM-4.1-V video prompt update by @Isotr0py in https://github.com/vllm-project/vllm/pull/20635
* [TPU][Bugfix] disable phi-3 test by @QiliangCui in https://github.com/vllm-project/vllm/pull/20632
* Replace `multiply_add` with `homogeneous_multiply_add` to Address Clang Template Parameter Issue by @wenxin0319 in https://github.com/vllm-project/vllm/pull/20142
* [misc]refactor `Platform.set_device` method by @jikunshang in https://github.com/vllm-project/vllm/pull/20262
* [tech debt] Revisit lora request model checker by @kouroshHakha in https://github.com/vllm-project/vllm/pull/20636
* [BugFix][Intel GPU] Use refactored API for dist_backend in V1 worker by @ratnampa in https://github.com/vllm-project/vllm/pull/20596
* [Docs] Improve documentation for multi-node service helper script by @crypdick in https://github.com/vllm-project/vllm/pull/20600
* [Hardware][PPC64LE] Enable V1 for ppc64le and ARM by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/20554
* [Bugfix] set default set cuda_graph_sizes to min(self.max_num_seqs * 2, 512) by @izhuhaoran in https://github.com/vllm-project/vllm/pull/20628
* [feat] enable SM100 CUTLASS block scaled group gemm for smaller batch sizes by @djmmoss in https://github.com/vllm-project/vllm/pull/20640
* Fix bullets in incremental_build.md by @mgoin in https://github.com/vllm-project/vllm/pull/20642
* [Misc] Fix the size of batched_dummy_mm_inputs in profile_run by @B-201 in https://github.com/vllm-project/vllm/pull/20434
* [XPU] Use spawn with XPU multiprocessing by @dvrogozh in https://github.com/vllm-project/vllm/pull/20649
* [Intel GPU] support ray as distributed executor backend for XPU. by @jikunshang in https://github.com/vllm-project/vllm/pull/20659
* [Docs] fix minimax tool_calling docs error by @qscqesze in https://github.com/vllm-project/vllm/pull/20667
* [Bugfix] Fix the issue where `reasoning_content` is `None` when Thinkng is enabled and `tool_choice` is set to `'required'`. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20662
* [V1] [Doc] Update V1 docs for Mamba models by @tdoublep in https://github.com/vllm-project/vllm/pull/20499
* [Doc] Update notes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20668
* [Benchmark] Parameterization of streaming loading of multimodal datasets by @Potabk in https://github.com/vllm-project/vllm/pull/20528
* [Docs] Improve docs for RLHF co-location example by @crypdick in https://github.com/vllm-project/vllm/pull/20599
* [doc] update doc format by @reidliu41 in https://github.com/vllm-project/vllm/pull/20673
* [Bugfix] Fix handling of Tensorizer arguments for LoadConfig by @sangstar in https://github.com/vllm-project/vllm/pull/20643
* [TPU][Bugfix] fix test_pallas by @yaochengji in https://github.com/vllm-project/vllm/pull/20666
* [XPU][CI] enhance xpu test support by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20652
* [Bench] Add NVFP4 GEMM benchmark script by @mgoin in https://github.com/vllm-project/vllm/pull/20578
* [Doc] Update CPU doc by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20676
* Remove heading from installation `inc.md` file by @hmellor in https://github.com/vllm-project/vllm/pull/20697
* [CI/Build] Enlarge tolerance for a CPU multi-modal test by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20684
* Support Llama 4 for cutlass_moe_fp4 by @mgoin in https://github.com/vllm-project/vllm/pull/20453
* [Kernel] Triton implementation of causal-conv1d for Mamba-based models by @thoangtrvn in https://github.com/vllm-project/vllm/pull/18218
* [Kernel] Add Conch backend for mixed-precision linear layer by @jmanning-stackav in https://github.com/vllm-project/vllm/pull/19818
* [Feature][Quantization] MXFP4 support for MOE models by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/17888
* [BugFix]: Properly set engine_id when using multi connector by @Missmiaom in https://github.com/vllm-project/vllm/pull/19487
* [Misc] Simplify the prefix caching logic on draft tokens by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20701
* [CI/Build] Fix FlashInfer double build in Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/20651
* [Misc] DP : Add ExpertTokensMetadata by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20332
* Use NVCC `--compress-mode` to reduce binary size by 30% by @mgoin in https://github.com/vllm-project/vllm/pull/20694
* Correct PPMissingLayer handling in Deepseek-V2-Lite PP deployment by @eicherseiji in https://github.com/vllm-project/vllm/pull/20665
* [Frontend] Support Tool Calling with both `tool_choice='required'` and `$defs`. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20629
* [BugFix][CPU] Fix CPU worker dependency on cumem_allocator by @njhill in https://github.com/vllm-project/vllm/pull/20696
* [BugFix] Fix `VllmConfig()` construction on all platforms by @njhill in https://github.com/vllm-project/vllm/pull/20695
* [TPU][Core]Make load weight exceed hbm error more instructive for customers by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20644
* [KVConnector] Aggregate finished requests on the scheduler by @orozery in https://github.com/vllm-project/vllm/pull/19555
* [Misc] loose new-model tagger conditions by @Isotr0py in https://github.com/vllm-project/vllm/pull/20747
* [CI/Build] Fix Basic Models Test by @jeejeelee in https://github.com/vllm-project/vllm/pull/20728
* [Bugfix][Build][Non-CUDA] Only referencing CMAKE_CUDA_COMPILER_VERSION on CUDA where it is defined by @gshtras in https://github.com/vllm-project/vllm/pull/20738
* [doc] fix ordered list by @reidliu41 in https://github.com/vllm-project/vllm/pull/20749
* [CI Bugfix] Skip failing Tensorizer+LoRA test by @mgoin in https://github.com/vllm-project/vllm/pull/20724
* Normalize lm-eval command between baseline and correctness test by @mgoin in https://github.com/vllm-project/vllm/pull/18560
* [Misc] Clean up mark to fork process in BNB tests by @Isotr0py in https://github.com/vllm-project/vllm/pull/20692
* [Doc] Add engine args back in to the docs by @hmellor in https://github.com/vllm-project/vllm/pull/20674
* Update Dockerfile FlashInfer to v0.2.8rc1 by @mgoin in https://github.com/vllm-project/vllm/pull/20718
* [Hardware][CPU] Vllm int8 quantization enablement for ARM CPU by @nishith-fujitsu in https://github.com/vllm-project/vllm/pull/14129
* [ROCm][Regression] Remove tensor creation that harms performance on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/20741
* [Model] Add reason parser for Hunyuan A13B Model. by @kzjeef in https://github.com/vllm-project/vllm/pull/20625
* [Model][VLM] Support JinaVL Reranker by @shineran96 in https://github.com/vllm-project/vllm/pull/20260
* Fix DeepSeek-R1-0528 chat template by @sfbemerk in https://github.com/vllm-project/vllm/pull/20717
* [Test] Remove docker build from test. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20542
* [Bugfix] [CI] Fix Tensorizer LoRA test by @sangstar in https://github.com/vllm-project/vllm/pull/20760
* [V0][V1][Core] Add outlines integration for V1, and update V0 integration. by @unaidedelf8777 in https://github.com/vllm-project/vllm/pull/15975
* [CI] Fix pre commit issue by @yewentao256 in https://github.com/vllm-project/vllm/pull/20782
* [Bugfix] Remove assertion of expert_map being None by @minosfuture in https://github.com/vllm-project/vllm/pull/20714
* [Core] Add Support for Default Modality Specific LoRAs [generate / chat completions] by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/19126
* [Bugfix] Fused MoE Modular Kernel chunking loop by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20392
* [KVConnector] Always call connector `clear_metadata()` at end of step by @njhill in https://github.com/vllm-project/vllm/pull/20756
* [Misc] MoE ModularKernel : Introduce TopKWeightAndReduce  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20648
* [Bugfix][Benchmark] Make sure the output length > 0 when testing prefill workload. by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20786
* [Docs] Lazy import gguf by @simon-mo in https://github.com/vllm-project/vllm/pull/20785
* [CI Bugfix] Specify same TORCH_CUDA_ARCH_LIST for flashinfer aot and install by @mgoin in https://github.com/vllm-project/vllm/pull/20772
* Add kimi-k2 tool parser by @MoyanZitto in https://github.com/vllm-project/vllm/pull/20789
* [fix]: disable cutlass block scaled group gemm for EP by @djmmoss in https://github.com/vllm-project/vllm/pull/20781
* [Model] Support HF format of minimax by @mgoin in https://github.com/vllm-project/vllm/pull/20211
* [Attention] MLA - Flashinfer Ragged Prefill by @alexm-redhat in https://github.com/vllm-project/vllm/pull/20034
* [Feature] Integrate SM100 DeepGEMM support by @yewentao256 in https://github.com/vllm-project/vllm/pull/20087
* [XPU] XCCL support enabled in torch 2.8.0.dev nightly builds by @ratnampa in https://github.com/vllm-project/vllm/pull/20705
* [Perf][fp8] Use CustomOp abstraction for fp8 quant for better perf by @ProExpertProg in https://github.com/vllm-project/vllm/pull/19830
* [V1] Enable Mamba2 layers other than MambaMixer2 in the v1 engine by @nopperl in https://github.com/vllm-project/vllm/pull/20660
* [doc] fold long code block by @reidliu41 in https://github.com/vllm-project/vllm/pull/20795
* [Bugfix] Upgrade depyf to 0.19 and streamline custom pass logging by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20777
* [Quantization][1/N] MoE support BNB-Inflight Quantization by @jeejeelee in https://github.com/vllm-project/vllm/pull/20061
* [Core] Add Flashinfer TRTLLM Backend for Flashinfer decode path (SM100).  by @pavanimajety in https://github.com/vllm-project/vllm/pull/19825
* [Bugfix] Refactor `/invocations` to be task-agnostic by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20764
* Temporarily suspend google/gemma-3-1b-it. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20722
* [Bugfix] Add missing field to TritonLanguagePlaceholder by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20812
* [doc] fix ordered list issue by @reidliu41 in https://github.com/vllm-project/vllm/pull/20819
* [Misc] Add unit tests for MoE ModularKernel combinations + Profiling utility by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20449
* [Kernel] Basic tuned configs for NVFP4 CUTLASS dense GEMM by @mgoin in https://github.com/vllm-project/vllm/pull/20646
* [Docs] Data Parallel deployment documentation by @njhill in https://github.com/vllm-project/vllm/pull/20768
* [Bugfix] Fix OOM in language generation test by @Isotr0py in https://github.com/vllm-project/vllm/pull/20814
* Update kimi-k2 tool calling docs, enable unit tests by @MoyanZitto in https://github.com/vllm-project/vllm/pull/20821
* [CI Bug] Fix Async Engine, Inputs, Utils, Worker Test: 'State' object has no attribute 'enable_server_load_tracking' by @yewentao256 in https://github.com/vllm-project/vllm/pull/20845
* Integration SM100 FlashInfer fused allreduce RMSNorm by @ilmarkov in https://github.com/vllm-project/vllm/pull/20691
* Add pynccl all-gatherv and reducescatterv by @trevor-m in https://github.com/vllm-project/vllm/pull/20154
* [Misc] Restrict deep_gemm's log output by @jeejeelee in https://github.com/vllm-project/vllm/pull/20827
* [Bugfix] Lazy import fused_experts in BitsAndBytesMoEMethod to avoid break not-cuda-alike devices  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20822
* [Bugfix] Fix tensor parallel issue in Qwen3 reranker weight loading by @yurhett in https://github.com/vllm-project/vllm/pull/20682
* [CI/Build] Ensure compatability with Transformers v4.53 by @Isotr0py in https://github.com/vllm-project/vllm/pull/20541
* [Bugfix] : Fix typo - logger.warn_once -> logger.warning_once by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20852
* [Frontend] Abstract prompt and SpeechToTextConfig for transcriptions models by @NickLucche in https://github.com/vllm-project/vllm/pull/20637
* [Bugfix] Replace unavailable video url in multimodal test by @Isotr0py in https://github.com/vllm-project/vllm/pull/20854
* [Misc] Respect `no_use_tqdm_on_load` flag while capturing CUDA graph by @lk-chen in https://github.com/vllm-project/vllm/pull/20834
* [Bug] Fix DeepGemm for EP low latency case by @yewentao256 in https://github.com/vllm-project/vllm/pull/20833
* [Docs] Update basic.md by @luccafong in https://github.com/vllm-project/vllm/pull/20846
* [Bugfix] Fix torch.compile x LoRA for PyTorch 2.8  by @zou3519 in https://github.com/vllm-project/vllm/pull/20823
* [cold start time] add envs.VLLM_COMPILE_DEPYF to guard decompile by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20790
* Remove extra tensor on CPU by @maxdebayser in https://github.com/vllm-project/vllm/pull/20693
* Enable ModelOpt Llama4 fp8 checkpoint deployment by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/20419
* Revert "Use NVCC --compress-mode to reduce binary size by 30% #20694" by @mgoin in https://github.com/vllm-project/vllm/pull/20853
* [Model] New model support for microsoft/Phi-4-mini-flash-reasoning by @congcongchen123 in https://github.com/vllm-project/vllm/pull/20702
* [Bugfix] Fix Tensor Parallelism Padding Consistency in Granite Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/20843
* [docs] convert supported configs to table by @reidliu41 in https://github.com/vllm-project/vllm/pull/20858
* [Bugfix] Restrict Machete to only run on Hopper by @mgoin in https://github.com/vllm-project/vllm/pull/20830
* [Sched] Enhance the logic to remove stopped requests from queues by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20739
* [Perf] Use Triton instead of Torch for DeepGEMM Per Token Group Quant by @yewentao256 in https://github.com/vllm-project/vllm/pull/20841
* [Bugfix] Fix a couple PPLX+CUTLASS MoE bugs by @ElizaWszola in https://github.com/vllm-project/vllm/pull/20825
* [Refactor] Change the way of import triton by @yewentao256 in https://github.com/vllm-project/vllm/pull/20774
* [Core] Support multiple tasks per model by @NickLucche in https://github.com/vllm-project/vllm/pull/20771
* Renable google/gemma-3-1b-it accuracy test. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20866
* Support for LlamaForSequenceClassification by @thechaos16 in https://github.com/vllm-project/vllm/pull/20807
* [Bugfix] Fix: add patch_rope_scaling after hf override by @Wangmerlyn in https://github.com/vllm-project/vllm/pull/20857
* [Bugfix] fix define of RerankDocument by @Liuchenlong in https://github.com/vllm-project/vllm/pull/20877
* [V1] [ROCm] [AITER] Upgrade AITER to commit `916bf3c` and bugfix APIs by @tjtanaa in https://github.com/vllm-project/vllm/pull/20880
* [V1] Hybrid allocator without prefix caching by @nopperl in https://github.com/vllm-project/vllm/pull/20661
* [Core] Add `update_config` RPC method by @22quinn in https://github.com/vllm-project/vllm/pull/20095
* [Prefix Cache] Add reproducible prefix-cache block hashing using SHA-256 + CBOR (64bit) by @vMaroon in https://github.com/vllm-project/vllm/pull/20511
* Removing redundant python version check by @Dannyso05 in https://github.com/vllm-project/vllm/pull/20888
* Fix: Add missing EOFError handling in CLI complete command by @reidliu41 in https://github.com/vllm-project/vllm/pull/20896
* [ROCm] [Bugfix] [Critical]: Fix mamba compilation bug by @tjtanaa in https://github.com/vllm-project/vllm/pull/20883
* [Quantization] add BNB for MixtralForCausalLM by @jeejeelee in https://github.com/vllm-project/vllm/pull/20893
* [Refactor][V1] Move outlines utils for V1 imports by @aarnphm in https://github.com/vllm-project/vllm/pull/20878
* [MISC] Move bind_kv_cache to worker module by @wangxiyuan in https://github.com/vllm-project/vllm/pull/20900
* [CI/Build] Fix OOM issue in Jina-VL test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20907
* [Bugfix] Bump up mistral_common to support v13 tokenizer by @22quinn in https://github.com/vllm-project/vllm/pull/20905
* [Misc] Remove unused function by @reidliu41 in https://github.com/vllm-project/vllm/pull/20909
* [Bugfix]: Fix messy code when using logprobs by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20910
* [Misc] Log the reason for falling back to FlexAttention by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20699
* [Model] Add Ling implementation by @ant-yy in https://github.com/vllm-project/vllm/pull/20680
* [CI] cc folks on changes to vllm/compilation by @zou3519 in https://github.com/vllm-project/vllm/pull/20925
* [CI] Update codeowner for compilation code by @houseroad in https://github.com/vllm-project/vllm/pull/20929
* [Misc] Clean up Aimv2 config registration in Ovis config by @Isotr0py in https://github.com/vllm-project/vllm/pull/20921
* [CI/Build] Add Transformers nightly tests in CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/20924
* Change default model to Qwen3-0.6B by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20335
* Add benchmark dataset for mlperf llama tasks by @mgoin in https://github.com/vllm-project/vllm/pull/20338
* [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20725
* [Misc] Relax translations tests by @NickLucche in https://github.com/vllm-project/vllm/pull/20856
* Fix overflow indexing in causal_conv1d kernel by @tdoublep in https://github.com/vllm-project/vllm/pull/20938
* [Docs] remove outdated performance benchmark by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20935
* Fall back if flashinfer comm module not found by @sarckk in https://github.com/vllm-project/vllm/pull/20936
* SM100 Cutlass MLA decode with unrestricted num_heads (< 128) for DeepSeek TP by @alexm-redhat in https://github.com/vllm-project/vllm/pull/20769
* [BugFix] VLLM_DISABLE_COMPILE_CACHE=1 should disable all reads and writes from the cache by @zou3519 in https://github.com/vllm-project/vllm/pull/20942
* [Bugfix] Fix incorrect dispatch for CutlassBlockScaledGroupedGemm and DeepGEMM by @mgoin in https://github.com/vllm-project/vllm/pull/20933
* [CI/Build] Split Entrypoints Test into LLM and API Server by @mgoin in https://github.com/vllm-project/vllm/pull/20945
* Use w8a8 quantized matmul Pallas kernel by @vanbasten23 in https://github.com/vllm-project/vllm/pull/19170
* [Docs] Add Kuberay to deployment integrations by @crypdick in https://github.com/vllm-project/vllm/pull/20592
* feat: add image zoom to improve image viewing experience by @reidliu41 in https://github.com/vllm-project/vllm/pull/20763
* [CI] Fix flaky `test_streaming_response` test by @NickLucche in https://github.com/vllm-project/vllm/pull/20913
* Enabled BnB NF4 inference on Gaudi by @rsshaik1 in https://github.com/vllm-project/vllm/pull/20172
* [Bugfix] Switch bailout logic for kv-cache-dtype with SM100 Flashinfer by @pavanimajety in https://github.com/vllm-project/vllm/pull/20934
* [Doc] Clearer mistral3 and pixtral model support description by @Isotr0py in https://github.com/vllm-project/vllm/pull/20926
* [cold start] replace VLLM_COMPILE_DEPYF with debug_dump_dir by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20940
* [Model] Add AutoWeightsLoader support for BERT, RoBERTa by @jennifurhe in https://github.com/vllm-project/vllm/pull/20534
* Implement Async Scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19970
* [Misc] Refactor AllReduceFusionPass. Remove parameter by @ilmarkov in https://github.com/vllm-project/vllm/pull/20918
* [frontend] Add --help=page option for paginated help output by @reidliu41 in https://github.com/vllm-project/vllm/pull/20961
* [Docs] Improve documentation for RLHF example by @crypdick in https://github.com/vllm-project/vllm/pull/20598
* [frontend] Refactor CLI Args for a better modular integration by @kouroshHakha in https://github.com/vllm-project/vllm/pull/20206
* [Docs] Improve documentation for ray cluster launcher helper script by @crypdick in https://github.com/vllm-project/vllm/pull/20602
* [TPU] Optimize kv cache update kernel by @tengyifei in https://github.com/vllm-project/vllm/pull/20415
* [V1] [Hybrid] Refactor mamba state shape calculation; enable V1 via cli  by @tdoublep in https://github.com/vllm-project/vllm/pull/20840
* [MISC] Add init files for python package by @Potabk in https://github.com/vllm-project/vllm/pull/20908
* [doc] Add more details for Ray-based DP by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20948
* [Deprecation] Remove `TokenizerPoolConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/20968
* [v1][core] Support for attention free models by @christian-pinto in https://github.com/vllm-project/vllm/pull/20811
* Voxtral by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/20970
* [CI/Build] Fix wrong path in Transformers Nightly Models Test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20994
* [Deprecation] Remove everything scheduled for removal in v0.10.0 by @hmellor in https://github.com/vllm-project/vllm/pull/20979
* Configure Gemini by @hmellor in https://github.com/vllm-project/vllm/pull/20971
* [Deprecation] Remove `nullable_kvs` by @hmellor in https://github.com/vllm-project/vllm/pull/20969
* Add full serve CLI reference back to docs by @hmellor in https://github.com/vllm-project/vllm/pull/20978
* [ROCm] warpSize is being made non constexpr in ROCm 7.0 by @gshtras in https://github.com/vllm-project/vllm/pull/20330
* [BugFix] fix 3 issues: (1) using metadata for causal-conv1d, (2) indexing overflow in v1 vLLM, and (3) init_states in v0 by @thoangtrvn in https://github.com/vllm-project/vllm/pull/20838
* [Frontend] Support cache_salt in /v1/completions and /v1/responses by @dr75 in https://github.com/vllm-project/vllm/pull/20981
* [Bug Fix] get_distributed_init_method should get the ip from get_ip iâ€¦ by @Relics in https://github.com/vllm-project/vllm/pull/20889
* [Nvidia] Integrate SM100 cudnn prefill API to MLA prefill by @elfiegg in https://github.com/vllm-project/vllm/pull/20411
* [Frontend] OpenAI Responses API supports input image by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20975
* [Frontend] Remove print left in FrontendArgs.add_cli_args by @mgoin in https://github.com/vllm-project/vllm/pull/21004
* [Model] Add ModelConfig class for GraniteMoeHybrid to override default max_seq_len_to_capture by @tdoublep in https://github.com/vllm-project/vllm/pull/20923
* [Misc] bump xgrammar version to v0.1.21 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/20992
* [Chore] Remove outdated transformers check by @b8zhong in https://github.com/vllm-project/vllm/pull/20989
* [Misc] Refactor: Improve argument handling for `conda` command by @reidliu41 in https://github.com/vllm-project/vllm/pull/20481
* [Docs] Enhance Anyscale documentation, add quickstart links for vLLM by @crypdick in https://github.com/vllm-project/vllm/pull/21018
* [Bugfix] Correct per_act_token in CompressedTensorsW8A8Fp8MoECutlassMâ€¦ by @minosfuture in https://github.com/vllm-project/vllm/pull/20937
* Add Dockerfile argument for VLLM_USE_PRECOMPILED environment by @dougbtv in https://github.com/vllm-project/vllm/pull/20943
* [CI][HPU] update for v0 deprecate by switching to VLLM_TARGET_DEVICE=empty by @xuechendi in https://github.com/vllm-project/vllm/pull/21006
* [Bugfix] Fix Mistral3 support on SM100/SM120 by @mgoin in https://github.com/vllm-project/vllm/pull/20998
* [Doc] Remove duplicate docstring by @yewentao256 in https://github.com/vllm-project/vllm/pull/21012
* [Voxtral] Add more tests by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/21010
* Avoid direct comparison of floating point numbers by @maxdebayser in https://github.com/vllm-project/vllm/pull/21002
* [Meta] Llama4 EAGLE Support by @morgendave in https://github.com/vllm-project/vllm/pull/20591
* [TPU] fix kv_cache_update kernel block size choosing logic by @yaochengji in https://github.com/vllm-project/vllm/pull/21007
* [BugFix] Fix import error on non-blackwell machines by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21020
* Fix inadvertently silenced PP tests for `mp`, add DeepSeek V2/V3 model family to PP tests by @eicherseiji in https://github.com/vllm-project/vllm/pull/20831
* [Docs] Add intro and fix 1-2-3 list in frameworks/open-webui.md by @windsonsea in https://github.com/vllm-project/vllm/pull/19199
* [Model] Consolidate pooler implementations by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20927
* feat - add a new endpoint `get_tokenizer_info` to provide tokenizer/chat-template information by @m-misiura in https://github.com/vllm-project/vllm/pull/20575
* [fix] fix qwen image_embeds input by @h-avsha in https://github.com/vllm-project/vllm/pull/21049
* Remove Qwen Omni workaround that's no longer necessary by @hmellor in https://github.com/vllm-project/vllm/pull/21057
* [Model] Remove model sampler by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21059
* Support FP8 Quantization and Inference Run on Intel Gaudi (HPU) using INC (Intel Neural Compressor) by @nirda7 in https://github.com/vllm-project/vllm/pull/12010
* Remove torch_xla.tpu.version() from pallas.py. by @QiliangCui in https://github.com/vllm-project/vllm/pull/21065
* Update PyTorch to `torch==2.7.1` for CUDA by @mgoin in https://github.com/vllm-project/vllm/pull/21011
* [Bugfix] weight loading use correct tp_group with patch_tensor_parallel_group by @Kevin-XiongC in https://github.com/vllm-project/vllm/pull/21024
* [Docker] Allow FlashInfer to be built in the ARM CUDA Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/21013
* [TPU] Start using python 3.12 by @vanbasten23 in https://github.com/vllm-project/vllm/pull/21000
* [Bugfix] Fix Machete zero point issue for GPTQ models on SM90 by @mgoin in https://github.com/vllm-project/vllm/pull/21066
* [Attention] Refactor attention metadata builder interface by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20466
* [V1][P/D]Enhance Performance and code readability for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/20906
* [V1] [KVConnector] Fix MultiprocExecutor worker output aggregation by @sdavidbd in https://github.com/vllm-project/vllm/pull/21048
* [Misc] Fix PhiMoE expert mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/21085
* [Bugfix]: Fix final_res_batch list index out of range error by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/21055
* [Kernel] DeepGemm MoE : Integrate triton permute / unpermute kernels  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20903
* [Model] Add ToolParser and MoE Config for Hunyuan A13B  by @kzjeef in https://github.com/vllm-project/vllm/pull/20820
* [VLM] Add Nemotron-Nano-VL-8B-V1 support by @kylehh in https://github.com/vllm-project/vllm/pull/20349
* [Docs] Improve docstring formatting for `FusedMoEParallelConfig.make` by @hmellor in https://github.com/vllm-project/vllm/pull/21117
* [Misc] Avoid unnecessary import by @wangxiyuan in https://github.com/vllm-project/vllm/pull/21106
* [Docs] Move code block out of admonition now that it's short by @hmellor in https://github.com/vllm-project/vllm/pull/21118
* [Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/20762
* [Model] Update pooling model interface by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21058
* [Misc] Qwen MoE model supports LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/20932
* On environments where numa cannot be detected we get 0 by @ericcurtin in https://github.com/vllm-project/vllm/pull/21115
* [V0 deprecation] Remove V0 HPU backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21131
* [Log] Debugging Log with more Information by @yewentao256 in https://github.com/vllm-project/vllm/pull/20770
* [Bugfix] Fix the tensor non-contiguous issue for Flashinfer TRT-LLM backend attention kernel by @elvischenv in https://github.com/vllm-project/vllm/pull/21133
* [Docs] Add minimal demo of Ray Data API usage by @crypdick in https://github.com/vllm-project/vllm/pull/21080
* [Docs] Update supported models documentation with missing models by @luccafong in https://github.com/vllm-project/vllm/pull/20844
* [Attention] Make local attention backend agnostic by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21093
* [Doc] Add inplace weights loading example by @22quinn in https://github.com/vllm-project/vllm/pull/19640
* [Core] FlashInfer CUTLASS fused MoE backend (NVFP4) by @wenscarl in https://github.com/vllm-project/vllm/pull/20037
* [Perf] Add swap_ab to SM90 FP8 non-block CUTLASS moe grouped gemm by @shixianc in https://github.com/vllm-project/vllm/pull/20911
* [Misc] Do not print async output warning for v1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21151
* [benchmark] Sending request strictly follows the random intervals by @Jialin in https://github.com/vllm-project/vllm/pull/21108
* [Misc] Make MM embedding merge interface explicit in model runner by @ywang96 in https://github.com/vllm-project/vllm/pull/21147
* [Model] Re-add the implicit conversion feature for as_seq_cls_model by @noooop in https://github.com/vllm-project/vllm/pull/21103
* [Bugfix] The special_tokens in tokenizer should also be controlled by do_lower_case in encoder_config. by @noooop in https://github.com/vllm-project/vllm/pull/20750
* [Doc] Fix typo in model name by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21178
* [Bugfix] Allocate less memory in non-batched CUTLASS MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/21121
* [Core] Set pooling params based on task and model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/21128
* Let GraniteMoeAttention use YaRN by @tdoublep in https://github.com/vllm-project/vllm/pull/21174
* [CI] Update CODEOWNERS for vllm/compilation by @zou3519 in https://github.com/vllm-project/vllm/pull/21185
* [Kernel] Apply torch.Tag.needs_fixed_stride_order only for torch==2.6.0 by @zou3519 in https://github.com/vllm-project/vllm/pull/19346
* [Core] Avoid KVCacheBlock.__eq__ invocations in FreeKVCacheBlockQueue by @JialinOuyang-Meta in https://github.com/vllm-project/vllm/pull/21005
* [Bugfix] Voxtral on Blackwell GPUs (RTX 50 series) by @hax0r31337 in https://github.com/vllm-project/vllm/pull/21077
* Elastic Expert Parallel Initial Support by @ruisearch42 in https://github.com/vllm-project/vllm/pull/20775
* [Quantization] Enable BNB support for more MoE models by @jeejeelee in https://github.com/vllm-project/vllm/pull/21100
* [Core] Support Local Chunked Attention for Hybrid KV Cache by @luccafong in https://github.com/vllm-project/vllm/pull/19351
* [Bugfix][Model] Fix LoRA for Mistral-Small-3.1-24B-Instruct-2503 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21183
* [V0 Deprecation] Remove V0 Spec Decode workers by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21152
* [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/21193
* [BugFix][CPU] Fix `TorchSDPABackendImpl` doesn't have `use_irope`  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21200
* [Bug] DeepGemm: Fix TypeError: per_block_cast_to_fp8() missing 1 required positional argument: 'use_ue8m0' for SM100 by @yewentao256 in https://github.com/vllm-project/vllm/pull/21187
* [Model] EXAONE 4.0 model support by @Deepfocused in https://github.com/vllm-project/vllm/pull/21060
* [Misc][Tools][Benchmark] Add readme file for auto_tune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/20779
* Fix a couple of Voxtral tests by @huydhn in https://github.com/vllm-project/vllm/pull/21218
* [V0 deprecation] Remove long context LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/21169
* [Bugfix] Fix ndarray video color from VideoAsset by @Isotr0py in https://github.com/vllm-project/vllm/pull/21064
* [BugFix] Fix potential cuda-graph IMA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/21196
* Add torch golden impl for moe_align_block_size kernel test by @shixianc in https://github.com/vllm-project/vllm/pull/20653
* [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency by @kaixih in https://github.com/vllm-project/vllm/pull/20645
* [Bugfix][Frontend] Fix openai CLI arg `middleware` by @22quinn in https://github.com/vllm-project/vllm/pull/21220
* [bugfix] Fix auto thread-binding when world_size > 1 in CPU backend and refactor code by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/21032
* Fix/remove some broken model executor tests by @rabi in https://github.com/vllm-project/vllm/pull/21224
* [CI/CD][bugfix]fix: error argument to loads has incompatible type by @llsj14 in https://github.com/vllm-project/vllm/pull/21223
* [Docs] Update the link to the 'Prometheus/Grafana' example by @1195343015 in https://github.com/vllm-project/vllm/pull/21225
* [BugFix] Make PD work with Ray by @kouroshHakha in https://github.com/vllm-project/vllm/pull/21072
* [V1] [Hybrid] Enable piecewise CUDA Graph for mamba layers  by @tdoublep in https://github.com/vllm-project/vllm/pull/21194
* [V0 Deprecation] Deprecate BlockSparse Attention & Phi3-Small by @WoosukKwon in https://github.com/vllm-project/vllm/pull/21217
* [BugFix] Fix full cuda graph slot_mapping by @fhl2000 in https://github.com/vllm-project/vllm/pull/21228
* GLM-4 Update by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/20736
* [Docs] [V1] Update docs to remove enforce_eager limitation for hybrid models. by @tdoublep in https://github.com/vllm-project/vllm/pull/21233
* [TPU] support fp8 kv cache quantization by @yaochengji in https://github.com/vllm-project/vllm/pull/19292
* Enable v1 metrics tests by @eicherseiji in https://github.com/vllm-project/vllm/pull/20953

## New Contributors
* @sangbumlikeagod made their first contribution in https://github.com/vllm-project/vllm/pull/18809
* @djmmoss made their first contribution in https://github.com/vllm-project/vllm/pull/19757
* @GuyStone made their first contribution in https://github.com/vllm-project/vllm/pull/20497
* @bottler made their first contribution in https://github.com/vllm-project/vllm/pull/20487
* @dbyoung18 made their first contribution in https://github.com/vllm-project/vllm/pull/19410
* @Abirdcfly made their first contribution in https://github.com/vllm-project/vllm/pull/18878
* @Dekakhrone made their first contribution in https://github.com/vllm-project/vllm/pull/14047
* @minosfuture made their first contribution in https://github.com/vllm-project/vllm/pull/20167
* @crypdick made their first contribution in https://github.com/vllm-project/vllm/pull/20594
* @viravera made their first contribution in https://github.com/vllm-project/vllm/pull/20618
* @wenxin0319 made their first contribution in https://github.com/vllm-project/vllm/pull/20142
* @ratnampa made their first contribution in https://github.com/vllm-project/vllm/pull/20596
* @dvrogozh made their first contribution in https://github.com/vllm-project/vllm/pull/20649
* @thoangtrvn made their first contribution in https://github.com/vllm-project/vllm/pull/18218
* @jmanning-stackav made their first contribution in https://github.com/vllm-project/vllm/pull/19818
* @Missmiaom made their first contribution in https://github.com/vllm-project/vllm/pull/19487
* @orozery made their first contribution in https://github.com/vllm-project/vllm/pull/19555
* @nishith-fujitsu made their first contribution in https://github.com/vllm-project/vllm/pull/14129
* @kzjeef made their first contribution in https://github.com/vllm-project/vllm/pull/20625
* @shineran96 made their first contribution in https://github.com/vllm-project/vllm/pull/20260
* @unaidedelf8777 made their first contribution in https://github.com/vllm-project/vllm/pull/15975
* @MoyanZitto made their first contribution in https://github.com/vllm-project/vllm/pull/20789
* @nopperl made their first contribution in https://github.com/vllm-project/vllm/pull/20660
* @trevor-m made their first contribution in https://github.com/vllm-project/vllm/pull/20154
* @yurhett made their first contribution in https://github.com/vllm-project/vllm/pull/20682
* @thechaos16 made their first contribution in https://github.com/vllm-project/vllm/pull/20807
* @Wangmerlyn made their first contribution in https://github.com/vllm-project/vllm/pull/20857
* @Liuchenlong made their first contribution in https://github.com/vllm-project/vllm/pull/20877
* @vMaroon made their first contribution in https://github.com/vllm-project/vllm/pull/20511
* @Dannyso05 made their first contribution in https://github.com/vllm-project/vllm/pull/20888
* @ant-yy made their first contribution in https://github.com/vllm-project/vllm/pull/20680
* @rsshaik1 made their first contribution in https://github.com/vllm-project/vllm/pull/20172
* @jennifurhe made their first contribution in https://github.com/vllm-project/vllm/pull/20534
* @tengyifei made their first contribution in https://github.com/vllm-project/vllm/pull/20415
* @Relics made their first contribution in https://github.com/vllm-project/vllm/pull/20889
* @dougbtv made their first contribution in https://github.com/vllm-project/vllm/pull/20943
* @morgendave made their first contribution in https://github.com/vllm-project/vllm/pull/20591
* @m-misiura made their first contribution in https://github.com/vllm-project/vllm/pull/20575
* @nirda7 made their first contribution in https://github.com/vllm-project/vllm/pull/12010
* @Kevin-XiongC made their first contribution in https://github.com/vllm-project/vllm/pull/21024
* @ericcurtin made their first contribution in https://github.com/vllm-project/vllm/pull/21115
* @elvischenv made their first contribution in https://github.com/vllm-project/vllm/pull/21133
* @shixianc made their first contribution in https://github.com/vllm-project/vllm/pull/20911
* @Jialin made their first contribution in https://github.com/vllm-project/vllm/pull/21108
* @JialinOuyang-Meta made their first contribution in https://github.com/vllm-project/vllm/pull/21005
* @hax0r31337 made their first contribution in https://github.com/vllm-project/vllm/pull/21077
* @Deepfocused made their first contribution in https://github.com/vllm-project/vllm/pull/21060
* @fhl2000 made their first contribution in https://github.com/vllm-project/vllm/pull/21228

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.2rc1...v0.10.0rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.10.0rc1)

---

## v0.9.2: v0.9.2
**Published:** 2025-07-07

## Highlights

This release contains 452 commits from 167 contributors (31 new!)

**NOTE: This is the last version where V0 engine code and features stay intact. We highly recommend migrating to V1 engine.**

### Engine Core
* Priority Scheduling is now implemented in V1 engine (#19057), embedding models in V1 (#16188),  Mamba2 in V1 (#19327). 
* Full CUDAâ€‘Graph execution is now available for all FlashAttention v3 (FA3) and FlashMLA paths, including prefixâ€‘caching. CUDA graph now has a live capture progress bar makes debugging easier (#20301, #18581, #19617, #19501).  
* FlexAttention update â€“ any head size, FP32 fallback (#20467, #19754).  
* Shared `CachedRequestData` objects and cached samplerâ€‘ID stores deliver perf enhancements (#20232, #20291).  

### Model Support
* New families: Ernie 4.5 (+MoE) (#20220), MiniMaxâ€‘M1 (#19677, #20297), Slimâ€‘MoE â€œPhiâ€‘tinyâ€‘MoEâ€‘instructâ€ (#20286), Tencent HunYuanâ€‘MoEâ€‘V1 (#20114), Keyeâ€‘VLâ€‘8Bâ€‘Preview (#20126), GLMâ€‘4.1 V (#19331), Gemmaâ€‘3 (textâ€‘only, #20134), Tarsier 2 (#19887), Qwen 3 Embedding & Reranker (#19260), dots1 (#18254), GPTâ€‘2 for Sequence Classification (#19663).  
* Granite hybrid MoE configurations with shared experts are fully supported (#19652).  

### Largeâ€‘Scale Serving & Engine Improvements
* Expertâ€‘Parallel Load Balancer (EPLB) has been added! (#18343, #19790, #19885).  
* Disaggregated serving enhancements: Avoid stranding blocks in P when aborted in D's waiting queue (#19223), let toy proxy handle /chat/completions (#19730) 
* Native xPyD P2P NCCL transport as a base case for native PD without external dependency (#18242, #20246).  

### Hardware & Performance
* NVIDIA Blackwell 
	* SM120: CUTLASS W8A8/FP8 kernels and related tuning, added to Dockerfile (#17280, #19566, #20071, #19794) 
	* SM100: blockâ€‘scaledâ€‘group GEMM, INT8/FP8 vectorization, deepâ€‘GEMM kernels, activationâ€‘chunking for MoE, and groupâ€‘size 64 for Machete (#19757, #19572, #19168, #19085, #20290, #20331).  
* Intel GPU (V1) backend with Flashâ€‘Attention support (#19560).  
* AMD ROCm: fullâ€‘graph capture for TritonAttention, quick Allâ€‘Reduce, and chunked preâ€‘fill (#19158, #19744, #18596).  
	* Splitâ€‘KV support landed in the unified Triton Attention kernel, boosting longâ€‘context throughput (#19152).  
	* Fullâ€‘graph mode enabled in ROCm AITER MLA V1 decode path (#20254).  
* TPU: dynamicâ€‘grid KVâ€‘cache updates, headâ€‘dim less than 128, tuned pagedâ€‘attention kernels, and KVâ€‘padding fixes (#19928, #20235, #19620, #19813, #20048, #20339).  
	* Add models and features supporting matrix. (#20230) 

### Quantization
* Calibrationâ€‘free RTN INT4/INT8 pipeline for effortless, accurate compression (#18768).  
* Compressedâ€‘Tensor NVFP4 (including MoE) + emulation; FP4 emulation removed on < SM100 devices (#19879, #19990, #19563).  
* Dynamic MoEâ€‘layer quant (Marlin/GPTQ) and INT8 vectorization primitives (#19395, #20331, #19233).  
* Bitsâ€‘andâ€‘Bytes 0.45 + with improved doubleâ€‘quant logic and AWQ quality (#20424, #20033, #19431, #20076).

### API Â· CLI Â· Frontend
* API Server: Eliminate api_key and x_request_id headers middleware overhead (#19946) 
* New OpenAIâ€‘compatible endpoints: `/v1/audio/translations` & revamped `/v1/audio/transcriptions` (#19615, #20179, #19597).  
* Tokenâ€‘level progress bar for `LLM.beam_search` and cached templateâ€‘resolution speedâ€‘ups (#19301, #20065).  
* Imageâ€‘object support in `llm.chat`, toolâ€‘choice expansion, and customâ€‘arg passthroughs enrich multiâ€‘modal agents (#19635, #17177, #16862).  
* CLI QoL: better parsing for `-O/--compilation-config`, batchâ€‘sizeâ€‘sweep benchmarking, richer `--help`, faster startup (#20156, #20516, #20430, #19941).
* Metrics: Deprecate metrics with gpu_ prefix for non GPU specific metrics (#18354), Export NaNs in logits to scheduler_stats if output is corrupted (#18777) 

### Platform & Deployment
* Noâ€‘privileged CPU / Docker / K8s mode (#19241) and custom default maxâ€‘tokens for hosted platforms (#18557).  
* Security hardening â€“ runtime (cloud)pickle imports forbidden (#18018).  
* Hermetic builds and wheel slimming (FA2 8.0 + PTX only) shrink supplyâ€‘chain surface (#18064, #19336).  



## What's Changed
* [Docs] Note that alternative structured output backends are supported by @russellb in https://github.com/vllm-project/vllm/pull/19426
* [ROCm][V1] Adding ROCm to the list of plaforms using V1 by default by @gshtras in https://github.com/vllm-project/vllm/pull/19440
* [Model] use AutoWeightsLoader for commandr by @py-andy-c in https://github.com/vllm-project/vllm/pull/19399
* Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B-FP8 by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19401
* [BugFix] Allow use_cudagraph to work with dynamic VLLM_USE_V1 by @zou3519 in https://github.com/vllm-project/vllm/pull/19390
* [New Model]: Support Qwen3 Embedding & Reranker  by @noooop in https://github.com/vllm-project/vllm/pull/19260
* [BugFix] Fix docker build cpu-dev image error by @2niuhe in https://github.com/vllm-project/vllm/pull/19394
* Fix test_max_model_len in tests/entrypoints/llm/test_generate.py by @houseroad in https://github.com/vllm-project/vllm/pull/19451
* [CI] Disable failing GGUF model test by @mgoin in https://github.com/vllm-project/vllm/pull/19454
* [Misc] Remove unused `MultiModalHasher.hash_prompt_mm_data` by @lgeiger in https://github.com/vllm-project/vllm/pull/19422
* Add fused MOE config for Qwen3 30B A3B on B200 by @0xjunhao in https://github.com/vllm-project/vllm/pull/19455
* Fix Typo in Documentation and Function Name by @leopardracer in https://github.com/vllm-project/vllm/pull/19442
* [ROCm] Add rules to automatically label ROCm related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19405
* [Kernel] Support deep_gemm for linear methods by @artetaout in https://github.com/vllm-project/vllm/pull/19085
* [Doc] Update V1 User Guide for Hardware and Models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19474
* [Doc] Fix quantization link titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19478
* [Doc] Support "important" and "announcement" admonitions by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19479
* [Misc] Reduce warning message introduced in env_override by @houseroad in https://github.com/vllm-project/vllm/pull/19476
* Support non-string values in JSON keys from CLI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19471
* Add cache to cuda get_device_capability by @mgoin in https://github.com/vllm-project/vllm/pull/19436
* Fix some typo by @Ximingwang-09 in https://github.com/vllm-project/vllm/pull/19475
* Support no privileged mode on CPU for docker and kubernetes deployments by @louie-tsai in https://github.com/vllm-project/vllm/pull/19241
* [Bugfix] Update the example code, make it work with the latest lmcache by @runzhen in https://github.com/vllm-project/vllm/pull/19453
* [CI] Update FlashInfer to 0.2.6.post1 by @mgoin in https://github.com/vllm-project/vllm/pull/19297
* [doc] fix "Other AI accelerators" getting started page by @davidxia in https://github.com/vllm-project/vllm/pull/19457
* [Misc] Fix  misleading ROCm warning by @jeejeelee in https://github.com/vllm-project/vllm/pull/19486
* [Docs] Remove WIP features in V1 guide by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19498
* [Kernels] Add activation chunking logic to FusedMoEModularKernel by @bnellnm in https://github.com/vllm-project/vllm/pull/19168
* [AMD] [Quantization] Add override flag for attention dtype instead of using kv_cache_dtype trigger by @rasmith in https://github.com/vllm-project/vllm/pull/17331
* [UX] Add Feedback During CUDAGraph Capture by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19501
* [CI/Build] Fix torch nightly CI dependencies by @zou3519 in https://github.com/vllm-project/vllm/pull/19505
* [CI] change spell checker from codespell to typos by @andyxning in https://github.com/vllm-project/vllm/pull/18711
* [BugFix] Force registration of w8a8_block_fp8_matmul_deepgemm via lazy import by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19514
* Add Triton Fused MoE kernel config for E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/19518
* [Frontend] Improve error message in tool_choice validation by @22quinn in https://github.com/vllm-project/vllm/pull/19239
* [BugFix] Work-around incremental detokenization edge case error by @njhill in https://github.com/vllm-project/vllm/pull/19449
* [BugFix] Handle missing sep_token for Qwen3-Reranker in Score API by @strutive07 in https://github.com/vllm-project/vllm/pull/19522
* [AMD][Kernel][BugFix] fix test_rocm_compressed_tensors_w8a8 for rocm by @rasmith in https://github.com/vllm-project/vllm/pull/19509
* Fix typo by @2niuhe in https://github.com/vllm-project/vllm/pull/19525
* [Security] Prevent new imports of (cloud)pickle by @russellb in https://github.com/vllm-project/vllm/pull/18018
* [Bugfix][V1] Allow manual FlashAttention for Blackwell by @mgoin in https://github.com/vllm-project/vllm/pull/19492
* [Bugfix] Respect num-gpu-blocks-override in v1 by @jmswen in https://github.com/vllm-project/vllm/pull/19503
* [Quantization] Improve AWQ logic by @jeejeelee in https://github.com/vllm-project/vllm/pull/19431
* [Doc] Add V1 column to supported models list by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19523
* [NixlConnector] Drop `num_blocks` check  by @NickLucche in https://github.com/vllm-project/vllm/pull/19532
* [Perf] Vectorize static / dynamic INT8 quant kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/19233
* Fix TorchAOConfig skip layers by @mobicham in https://github.com/vllm-project/vllm/pull/19265
* [torch.compile][ROCm] Fuse quantization onto attention using a torch.compile pass by @ProExpertProg in https://github.com/vllm-project/vllm/pull/16756
* [doc] Make top navigation sticky by @reidliu41 in https://github.com/vllm-project/vllm/pull/19540
* [Spec Decode][Benchmark] Generalize spec decode offline benchmark to more methods and datasets by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18847
* [Misc] Turn MOE_DP_CHUNK_SIZE into an env var by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19506
* [Bugfix] Enforce contiguous input for dynamic_per_token FP8/INT8 quant by @mgoin in https://github.com/vllm-project/vllm/pull/19452
* [Doc] Unify structured outputs examples by @aarnphm in https://github.com/vllm-project/vllm/pull/18196
* [V1] Resolve failed concurrent structred output requests by @russellb in https://github.com/vllm-project/vllm/pull/19565
* Revert "[Build/CI] Add tracing deps to vllm container image (#15224)" by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19378
* [BugFix] : Fix Batched DeepGemm Experts by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19515
* [Bugfix] Fix EAGLE vocab embedding for multimodal target model by @zixi-qi in https://github.com/vllm-project/vllm/pull/19570
* [Doc] uses absolute links for structured outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/19582
* [doc] fix incorrect link by @reidliu41 in https://github.com/vllm-project/vllm/pull/19586
* [Misc] Correct broken docs link by @Zerohertz in https://github.com/vllm-project/vllm/pull/19553
* [CPU] Refine default config for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19539
* [Fix] bump mistral common to support magistral by @princepride in https://github.com/vllm-project/vllm/pull/19533
* [Fix] The zip function in Python 3.9 does not have the strict argument by @princepride in https://github.com/vllm-project/vllm/pull/19549
* use base version for version comparison by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/19587
* [torch.compile] reorganize the cache directory to support compiling multiple models by @youkaichao in https://github.com/vllm-project/vllm/pull/19064
* [BugFix] Honor `enable_caching` in connector-delayed kvcache load case by @njhill in https://github.com/vllm-project/vllm/pull/19435
* [Model] Fix minimax model cache & lm_head precision by @qscqesze in https://github.com/vllm-project/vllm/pull/19592
* [Refactor] Remove unused variables in `moe_permute_unpermute_kernel.inl` by @yewentao256 in https://github.com/vllm-project/vllm/pull/19573
* [doc][mkdocs] fix the  duplicate Supported features sections in GPU docs by @reidliu41 in https://github.com/vllm-project/vllm/pull/19606
* [CUDA] Enable full cudagraph for FlashMLA by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18581
* [Doc] Add troubleshooting section to k8s deployment by @annapendleton in https://github.com/vllm-project/vllm/pull/19377
* [torch.compile] Use custom ops when use_inductor=False by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19618
* Adding "AMD: Multi-step Tests" to amdproduction. by @Concurrensee in https://github.com/vllm-project/vllm/pull/19508
* [BugFix] Fix DP Coordinator incorrect debug log message by @njhill in https://github.com/vllm-project/vllm/pull/19624
* [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics. by @sahelib25 in https://github.com/vllm-project/vllm/pull/18354
* [Bugfix][1/n] Fix the speculative decoding test by setting the target dtype by @houseroad in https://github.com/vllm-project/vllm/pull/19633
* [Misc] Modularize CLI Argument Parsing in Benchmark Scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19593
* [Bugfix] Fix auto dtype casting for BatchFeature by @Isotr0py in https://github.com/vllm-project/vllm/pull/19316
* [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization by @jiahanc in https://github.com/vllm-project/vllm/pull/19500
* Only build CUTLASS MoE kernels on Hopper by @huydhn in https://github.com/vllm-project/vllm/pull/19648
* [Bugfix] Don't attempt to use triton if no driver is active by @kzawora-intel in https://github.com/vllm-project/vllm/pull/19561
* [Fix] Convert kv_transfer_config from dict to KVTransferConfig by @maobaolong in https://github.com/vllm-project/vllm/pull/19262
* [Perf] Further tunings for SM100 FP8 CUTLASS kernel by @ilmarkov in https://github.com/vllm-project/vllm/pull/19566
* [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness by @houseroad in https://github.com/vllm-project/vllm/pull/19644
* [Kernel] Raise verbose error and consolidate `num_heads/num_kv_heads` divisibility check by @22quinn in https://github.com/vllm-project/vllm/pull/19339
* [Benchmark] Refactor benchmark script for fp8 & int8 by @yewentao256 in https://github.com/vllm-project/vllm/pull/19627
* Enable prefix caching with full cuda graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19617
* [CI/Build] Fix torch nightly CI dependencies part 2 by @zou3519 in https://github.com/vllm-project/vllm/pull/19589
* [Misc] Remove duplicate multiproc method setting for CPU platform by @Isotr0py in https://github.com/vllm-project/vllm/pull/19649
* [MISC] Remove unused variableds in C++ by @houseroad in https://github.com/vllm-project/vllm/pull/19609
* [Bugfix][Core] Prefix caching causes incorrect outputs due to outdated ComputedBlocksTracker by @quanliu1991 in https://github.com/vllm-project/vllm/pull/18957
* [Misc][Frontend] passthrough `bad_words` by @f14-bertolotti in https://github.com/vllm-project/vllm/pull/19564
* [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19660
* [TPU] support attention head dim smaller than 128 by @yaochengji in https://github.com/vllm-project/vllm/pull/19620
* [MISC] typo fix by @andyxning in https://github.com/vllm-project/vllm/pull/19672
* [CI] Add mteb testing for rerank models by @noooop in https://github.com/vllm-project/vllm/pull/19344
* [Docs] Move multiproc doc to v1 dir by @russellb in https://github.com/vllm-project/vllm/pull/19651
* [Kernel] GGUF MMVQ kernel for multiple input vectors by @SzymonOzog in https://github.com/vllm-project/vllm/pull/18754
* [BugFix] Don't catch BaseException when dumping execute_model errors by @njhill in https://github.com/vllm-project/vllm/pull/19626
* [DOC] Add reasoning capability to vLLM streamlit code by @Navanit-git in https://github.com/vllm-project/vllm/pull/19557
* [Feature]:Allow for Granite MoE Hybrid models with _only_ shared experts. by @shawntan in https://github.com/vllm-project/vllm/pull/19652
* [Bugfix] Fix TP inference for Flex attention backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/19657
* [MISC] bump huggingface_hub pkg to 0.33.0 by @andyxning in https://github.com/vllm-project/vllm/pull/19547
* [Bugfix] fix missing 'finish_reason': null in streaming chat by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19662
* [Kernels] Use empty for modular MoE workspaces by @bnellnm in https://github.com/vllm-project/vllm/pull/19667
* [Model] Add support for MiniMaxM1ForCausalLM (shares architecture with MiniMaxText01ForCausalLM) by @qscqesze in https://github.com/vllm-project/vllm/pull/19677
* [V1] Change return type on get_multimodal_embeddings() by @russellb in https://github.com/vllm-project/vllm/pull/19446
* [Quantization] Remove FP4 emulation; Fall-back to marlin for device < 100 by @dsikka in https://github.com/vllm-project/vllm/pull/19563
* [Fix] Fall back to Gloo when NCCL backend is unavailable by @conroy-cheers in https://github.com/vllm-project/vllm/pull/19641
* [doc] add project flag to gcloud TPU command by @davidxia in https://github.com/vllm-project/vllm/pull/19664
* [Wheel Size] Only build FA2 8.0+PTX by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19336
* [Frontend] add chunking audio for > 30s audio by @nguyenhoangthuan99 in https://github.com/vllm-project/vllm/pull/19597
* [DOC] fix doc typos by @diliu0349 in https://github.com/vllm-project/vllm/pull/19600
* Fixes IMA for TP w/ flex-attention by @drisspg in https://github.com/vllm-project/vllm/pull/19712
* [Core] add remove_seq_from_computed_blocks_tracker to BlockSpaceManager by @quanliu1991 in https://github.com/vllm-project/vllm/pull/19686
* [Doc] Add missing llava family multi-image examples by @Isotr0py in https://github.com/vllm-project/vllm/pull/19698
* Add a doc on how to update PyTorch version by @huydhn in https://github.com/vllm-project/vllm/pull/19705
* [Kernel] Add Split-KV Support to Unified Triton Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/19152
* [doc][mkdocs] Add edit  button to documentation by @reidliu41 in https://github.com/vllm-project/vllm/pull/19637
* [doc] split "Other AI Accelerators" tabs by @davidxia in https://github.com/vllm-project/vllm/pull/19708
* [V1][Kernel] Flashinfer HND KV cache layout by @NickLucche in https://github.com/vllm-project/vllm/pull/19280
* [Mis] remove duplicate engine status checks by @googs1025 in https://github.com/vllm-project/vllm/pull/19647
* [Bugfix] Update multimodel models mapping to fit new checkpoint after Transformers v4.52 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19151
* [Perf] Optimize `moe_align_block_size` CUDA kernel by @yewentao256 in https://github.com/vllm-project/vllm/pull/19572
* Remove sm120 arch from sm100 cutlass kernel arch list by @mgoin in https://github.com/vllm-project/vllm/pull/19716
* [Misc] Update lmcache connector with the latest connector apis by @YaoJiayi in https://github.com/vllm-project/vllm/pull/19441
* [Bugfix] Fix faulty triton importing logic when using Ray for DP by @mgoin in https://github.com/vllm-project/vllm/pull/19734
* [Feature][ROCm] Add full graph capture support for TritonAttentionBackend by @charlifu in https://github.com/vllm-project/vllm/pull/19158
* [TPU] Update torch version to include paged attention kernel change by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19706
* [MISC] correct copy_blocks src_to_dists param type by @andyxning in https://github.com/vllm-project/vllm/pull/19696
* [MISC] correct DeviceConfig device field static type analysis by @andyxning in https://github.com/vllm-project/vllm/pull/19699
* [Misc] Add __str__ for RequestStatus by @lk-chen in https://github.com/vllm-project/vllm/pull/19780
* [V1] Add API docs for EncoderCacheManager by @russellb in https://github.com/vllm-project/vllm/pull/19294
* [V1][P/D] An native implementation of xPyD based on P2P NCCL by @Abatom in https://github.com/vllm-project/vllm/pull/18242
* [V1] Decouple GPU and TPU `InputBatch` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19778
* [Minor] Zero-initialize attn output buffer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19784
* [doc] fix the incorrect label by @reidliu41 in https://github.com/vllm-project/vllm/pull/19787
* [Platform] Allow platform use V1 Engine by default by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19792
* [Qwen] Add tagging rule for Qwen related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19799
* [Hardware][AMD] integrate aiter chunked prefill into vllm by @Zzz9990 in https://github.com/vllm-project/vllm/pull/18596
* [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19725
* [Docs] Add Huzaifa Sidhpurwala to vuln mgmt team doc by @russellb in https://github.com/vllm-project/vllm/pull/19808
* [v1] Support mamba2 by @heheda12345 in https://github.com/vllm-project/vllm/pull/19327
* docs: fix Slack bulletpoint in README by @nathan-weinberg in https://github.com/vllm-project/vllm/pull/19811
* Disable "Forbid direct 'import triton'" check for `vllm/triton_utils/importing.py` in an extensible way by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19783
* [Core] Do not copy array during hashing by @lgeiger in https://github.com/vllm-project/vllm/pull/19484
* [TPU] Update torch-xla version to include paged attention tuned block change by @QiliangCui in https://github.com/vllm-project/vllm/pull/19813
* [Core] More fixes to MultiModalEmbeddings type handling by @russellb in https://github.com/vllm-project/vllm/pull/19715
* [Multimodal] Use fast processor for Qwen2/2.5-VL by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19789
* [BugFix] Fix use_cudagraph=False by @zou3519 in https://github.com/vllm-project/vllm/pull/19612
* [Frontend] Expose custom args in OpenAI APIs by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16862
* Fix FA2 fallback for Blackwell V1 by @mgoin in https://github.com/vllm-project/vllm/pull/19781
* [Misc][ROCm] Enforce no unused variable in ROCm C++ files by @houseroad in https://github.com/vllm-project/vllm/pull/19796
* [Quantization] Modify the logic of BNB double quantization by @jeejeelee in https://github.com/vllm-project/vllm/pull/19742
* Support embedding models in V1 by @maxdebayser in https://github.com/vllm-project/vllm/pull/16188
* [Bugfix] Fix the linter by @houseroad in https://github.com/vllm-project/vllm/pull/19826
* [Bugfix] Add check_health to v1 async client. by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19821
* Mark invariant normalizer in Gemma as non-persistent by @yhtang in https://github.com/vllm-project/vllm/pull/19788
* [ROCm] [AITER] [Bugfix] Patch for AITER commit `648764942e552a8bb5fe16026703716a81f05374` by @tjtanaa in https://github.com/vllm-project/vllm/pull/18990
* [Misc] [ROCm] Prevent surplus tensor reshape by @zsolt-borbely-htec in https://github.com/vllm-project/vllm/pull/19803
* raise exception for pin_lora by @andyxning in https://github.com/vllm-project/vllm/pull/19809
* [Minor] Allow redirecting model path for HfRunner in test by @Isotr0py in https://github.com/vllm-project/vllm/pull/19795
* Add xLAM tool parser support by @zuxin666 in https://github.com/vllm-project/vllm/pull/17148
* [Frontend] Add optional token-level progress bar to `LLM.beam_search` by @NekoMimiUnagi in https://github.com/vllm-project/vllm/pull/19301
* Fixing Chunked Prefill Test. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/19762
* [Doc] Update V1 user guide for embedding models by @22quinn in https://github.com/vllm-project/vllm/pull/19842
* [CI][CPU] Improve dummy Triton interfaces and fix the CPU CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19838
* [Core][Bugfix] Fix Online MM Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/19688
* [Frontend] early return chat format resolution when specified by @xzbdmw in https://github.com/vllm-project/vllm/pull/19735
* [Benchmark][Bugfix] Fix Dataset Length Calculation by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19868
* [CI/Build][Bugfix] Fix deadlock on v1 engine test CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/19872
* [CI][Neuron] Fail and exit on first error by @elaineyz in https://github.com/vllm-project/vllm/pull/19622
* [Benchmark] Fix `Value of type "SampleRequest" is not indexable` by @b8zhong in https://github.com/vllm-project/vllm/pull/18032
* [Chore]: qwen3-moe-type-hints-mistake by @Xerxes-cn in https://github.com/vllm-project/vllm/pull/19860
* [Bugfix] Enable PP with AITER+V1 by @qli88 in https://github.com/vllm-project/vllm/pull/19822
* [Bugfix][Ray] Set the cuda context eagerly in the ray worker  by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19583
* [Misc] update cuda version by @reidliu41 in https://github.com/vllm-project/vllm/pull/19526
* [Misc] refactor example - openai_transcription_client by @reidliu41 in https://github.com/vllm-project/vllm/pull/19851
* [Kernel] correct cpu worker function parameter type by @andyxning in https://github.com/vllm-project/vllm/pull/19745
* [Fix] import regex instead of re by @tdoublep in https://github.com/vllm-project/vllm/pull/19875
* [Model] GPT2ForSequenceClassification model by @nie3e in https://github.com/vllm-project/vllm/pull/19663
* [custom_op][vllm-plugin] update custom_op class to use op_registry by @xuechendi in https://github.com/vllm-project/vllm/pull/19164
* Export NaNs in logits to scheduler_stats if output is corrupted by @vladmihailescu in https://github.com/vllm-project/vllm/pull/18777
* [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19901
* [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError by @andyxning in https://github.com/vllm-project/vllm/pull/19749
* [Misc] Clean up useless code by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19889
* Fix: Check the type of params to be a Sequence not list. by @rabinadk1 in https://github.com/vllm-project/vllm/pull/19910
* [Bugfix] Fix bnb 8bit model weights loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/19917
* [New model support]Support Tarsier2 by @princepride in https://github.com/vllm-project/vllm/pull/19887
* [doc] add contact us in community by @reidliu41 in https://github.com/vllm-project/vllm/pull/19922
* [Multimodal] Optimize Qwen2/2.5-VL startup time by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19756
* [Docs] Add GPT2ForSequenceClassification to supported models in docs by @nie3e in https://github.com/vllm-project/vllm/pull/19932
* [Misc] add vllm_config in __init__ by @andyxning in https://github.com/vllm-project/vllm/pull/19866
* [MISC] add cpu_kvcache_space_bytes to CacheConfig by @andyxning in https://github.com/vllm-project/vllm/pull/19812
* [Benchmark] fix request loss if "ping" is returned by @sywangyi in https://github.com/vllm-project/vllm/pull/19535
* [CI/Build] Auto tag perf benchmarks related PRs by @22quinn in https://github.com/vllm-project/vllm/pull/19943
* [doc] use snippets for contact us by @reidliu41 in https://github.com/vllm-project/vllm/pull/19944
* [Misc] Update model-specific PR tagging by @ywang96 in https://github.com/vllm-project/vllm/pull/19949
* [Misc] Simplify vllm bench cli subcommand implementation by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19948
* [Chore] dedup logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19955
* [BugFix] Add an env to disable moe chunking to work around compile incompatibility by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19642
* [Perf][CLI] Improve overall startup time by @aarnphm in https://github.com/vllm-project/vllm/pull/19941
* [Core] feat: Implement Priority Scheduling in V1 Engine by @amitm02 in https://github.com/vllm-project/vllm/pull/19057
* [Misc] Configurable timeout for execute_model RPC calls via env var by @jinqinn in https://github.com/vllm-project/vllm/pull/19544
* Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor by @Flink-ddd in https://github.com/vllm-project/vllm/pull/19643
* [doc] Fold long code blocks to improve readability by @reidliu41 in https://github.com/vllm-project/vllm/pull/19926
* [P/D][NixlConnector] Support `tp_size > num_kv_heads` deployments by @NickLucche in https://github.com/vllm-project/vllm/pull/19691
* [BugFix][P/D] Fix for cases where _recving_transfers can be cleaned up when *all* transfer done by @lk-chen in https://github.com/vllm-project/vllm/pull/19874
* [Doc] Update V1 status for decoder-only embedding models by @Isotr0py in https://github.com/vllm-project/vllm/pull/19952
* [doc] use MkDocs collapsible blocks - supplement by @reidliu41 in https://github.com/vllm-project/vllm/pull/19973
* [Bugfix] Fix CI bitsandbytes failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/19969
* [doc] improve readability for long commands by @reidliu41 in https://github.com/vllm-project/vllm/pull/19920
* [Docs] Fix syntax highlighting of shell commands by @lgeiger in https://github.com/vllm-project/vllm/pull/19870
* [EP+DP] Optimize the little operations in the DeepGEMM + DeepEP low latency case by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19885
* [Bugfix][v1] Fix step pooler implementation and step pooling usage in v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19956
* [Misc] Add type alias `ReqId` and `EngineId` for better readability by @lk-chen in https://github.com/vllm-project/vllm/pull/19880
* [Feature] Support sequence parallelism for static fp8 quantization by @cascade812 in https://github.com/vllm-project/vllm/pull/19181
* [CI/Build] Push latest tag for cpu and neuron docker image by @22quinn in https://github.com/vllm-project/vllm/pull/19897
* Feat Dynamic Quantization for MoE Layers in GPTQ Marlin Backend by @Jun-Howie in https://github.com/vllm-project/vllm/pull/19395
* [Bugfix][Benchmark] Fix Marlin benchmark by @22quinn in https://github.com/vllm-project/vllm/pull/19929
* [TPU] Fix tpu model runner test by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19995
* Update test case parameter to have the throughput above 8.0 by @QiliangCui in https://github.com/vllm-project/vllm/pull/19994
* [Misc][Tools][Benchmark] Add profile to autotune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19711
* [doc] Fix broken link in the installation for CPU by @yankay in https://github.com/vllm-project/vllm/pull/19980
* add some examples for other benchmark scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19893
* [PERF] Speedup of MRoPE prepare inputs by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/19939
* [Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20014
* refactor example - qwen3_reranker by @reidliu41 in https://github.com/vllm-project/vllm/pull/19847
* [Fix][V1] Remove --scheduling-policy oracle by @amitm02 in https://github.com/vllm-project/vllm/pull/20010
* [Perf] Improve/Fix-regression for FA3 in High QPS regimes by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19463
* [Misc][Benchmarking] Add variable request-rate ("ramp-up") to the benchmarking client. by @dtransposed in https://github.com/vllm-project/vllm/pull/19423
* [BugFix] Fix multi-node offline data parallel by @njhill in https://github.com/vllm-project/vllm/pull/19937
* [P/D] Asynchronously do _nixl_handshake by @lk-chen in https://github.com/vllm-project/vllm/pull/19836
* [Feature] Integrate new deepgemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/19820
* [Easy] Remove submodule added in #19463 by @b8zhong in https://github.com/vllm-project/vllm/pull/20039
* use .dev for version comparison with pytorch nightly release by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20031
* cmake: Update vllm_flash_attn for vllm_kernels by @seemethere in https://github.com/vllm-project/vllm/pull/20032
* [Llama4] Update `attn_temperature_tuning` by @b8zhong in https://github.com/vllm-project/vllm/pull/19997
* Revert "[Feature] Integrate new deepgemm (#19820)" by @yewentao256 in https://github.com/vllm-project/vllm/pull/20049
* Revert "Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor" by @Isotr0py in https://github.com/vllm-project/vllm/pull/20030
* Move to a faster base64 implementation by @h-avsha in https://github.com/vllm-project/vllm/pull/19984
* [Frontend] speed up import time of vllm.config by @davidxia in https://github.com/vllm-project/vllm/pull/18036
* [Refactor] Remove duplicate `ceil_div` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20023
* [Feat][CLI] enforce-include-usage by @max-wittig in https://github.com/vllm-project/vllm/pull/19695
* [Kernels][Bugfix] Use torch op for all kernels in FusedMoE forward.  Add additional testing for cudagraphs. by @bnellnm in https://github.com/vllm-project/vllm/pull/19717
* [Chore] debloat some initial logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19438
* [BugFix] Fix full-cuda-graph illegal memory access in FA3 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20057
* [doc] add reference link for Intel XPU by @reidliu41 in https://github.com/vllm-project/vllm/pull/20064
* [Doc] Guide for Incremental Compilation Workflow by @mgoin in https://github.com/vllm-project/vllm/pull/19109
* [V1][Speculative Decoding] Fix DeepSeek MTP by @cjackal in https://github.com/vllm-project/vllm/pull/20022
* [Frontend] Add `/v1/audio/translations` OpenAI API endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/19615
* [Quantization] Add compressed-tensors emulations support for NVFP4 by @dsikka in https://github.com/vllm-project/vllm/pull/19879
* [Fix] Support cls pooling in ModernBertPooler by @lsz05 in https://github.com/vllm-project/vllm/pull/20067
* static_scaled_fp8_quant should not run when scale.numel is not 1 by @eldarkurtic in https://github.com/vllm-project/vllm/pull/20076
* [PD] let toy proxy handle /chat/completions by @lk-chen in https://github.com/vllm-project/vllm/pull/19730
* [Misc] Add parallel state `node_count` function by @njhill in https://github.com/vllm-project/vllm/pull/20045
* Fix the path to the testing script. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20082
* [Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine by @izhuhaoran in https://github.com/vllm-project/vllm/pull/20062
* [TPU][Bugfix] fix kv cache padding by @yaochengji in https://github.com/vllm-project/vllm/pull/20048
* [P/D] Avoid stranding blocks in P when aborted in D's waiting queue by @njhill in https://github.com/vllm-project/vllm/pull/19223
* [TPU] Add TPU specific var VLLM_TPU_MOST_MODEL_LEN by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19919
* [CI] Add SM120 to the Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/19794
* [Bugfix] Fix Mistral tool-parser regex for nested JSON by @mgoin in https://github.com/vllm-project/vllm/pull/20093
* [PD] Skip `tp_size` exchange with rank0 by @NickLucche in https://github.com/vllm-project/vllm/pull/19413
* [Benchmark][Bug] Fix multiple bugs in bench and add args to spec_decode offline by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/20083
* [Bugfix] Allow `CUDA_VISIBLE_DEVICES=''` in `Platform.device_id_to_physical_device_id` by @eicherseiji in https://github.com/vllm-project/vllm/pull/18979
* [Doc] Update docs for New Model Implementation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20115
* [Refactor] Remove unused library by @yewentao256 in https://github.com/vllm-project/vllm/pull/20099
* [CPU] Fix torch version in x86 CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19258
* [Misc] Use collapsible blocks for benchmark examples. by @reidliu41 in https://github.com/vllm-project/vllm/pull/20017
* [Docs] Improve frameworks/helm.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20113
* [Bugfix][V1][ROCm] Fix AITER Flash Attention Backend (Fix API Break and Local Attention Logic: affecting Llama4) by @tjtanaa in https://github.com/vllm-project/vllm/pull/19904
* Revert "[Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine" by @mgoin in https://github.com/vllm-project/vllm/pull/20128
* [Bug Fix] Fix address/port already in use error for pplx test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20094
* [Doc] Automatically signed-off by PyCharm by @noooop in https://github.com/vllm-project/vllm/pull/20120
* [Doc] Auto sign-off for VSCode by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20132
* [Doc] Rename page titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20130
* Spam folks if config.py changes by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20131
* [Hardware][Intel GPU] Add v1 Intel GPU support with Flash attention backend. by @jikunshang in https://github.com/vllm-project/vllm/pull/19560
* [TPU] add kv cache update kernel by @yaochengji in https://github.com/vllm-project/vllm/pull/19928
* [Refactor] Rename commnication utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20091
* [Doc] correct LoRA capitalization by @kyolebu in https://github.com/vllm-project/vllm/pull/20135
* [Feature] Expert Parallelism Load Balancer (EPLB) by @abmfy in https://github.com/vllm-project/vllm/pull/18343
* [CI Failure] Fix OOM with test_oot_registration_embedding by @mgoin in https://github.com/vllm-project/vllm/pull/20144
* [Quantization] Bump to use latest `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/20033
* [Perf] SM100 FP8 GEMM Optimizations after cutlass_profiler by @ilmarkov in https://github.com/vllm-project/vllm/pull/20071
* [Bugfix] Build moe_data for both sm100 and sm90 by @mgoin in https://github.com/vllm-project/vllm/pull/20086
* [Feature][Rocm] add quick all reduce for rocm by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/19744
* [CI] Sync test dependency with test.in for torch nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/19632
* [Fix] Fix gemma CI test failing on main by @tdoublep in https://github.com/vllm-project/vllm/pull/20124
* [Model][1/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20012
* [Perf][Frontend]: eliminate api_key and x_request_id headers middleware overhead by @Yazan-Sharaya in https://github.com/vllm-project/vllm/pull/19946
* Quick Fix by adding conditional import for flash_attn_varlen_func in flash_attn by @xuechendi in https://github.com/vllm-project/vllm/pull/20143
* Gemma3n (Text-only) by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/20134
* [Bugfix] Fix flaky failure when getting DP ports by @mgoin in https://github.com/vllm-project/vllm/pull/20151
* [Perf][Frontend] Cached resolution for resolving chat templates by @ilyal-cerebras in https://github.com/vllm-project/vllm/pull/20065
* [Fix][ROCm] Remove unused variables to fix build error on GFX11/12 by @hyoon1 in https://github.com/vllm-project/vllm/pull/19891
* [Fix][torch.compile] Enable custom ops by default when Inductor off by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20102
* [Bugfix] Mark 'hidden_states' as mutable in moe_forward registration. by @bnellnm in https://github.com/vllm-project/vllm/pull/20152
* [Bugfix] Fix some narrowing conversion warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20141
* [CI/Build] Allow hermetic builds by @fabiendupont in https://github.com/vllm-project/vllm/pull/18064
* [CI Fix] Pin tests/models/registry.py MiniMaxText01ForCausalLM to revision due to model changes by @mgoin in https://github.com/vllm-project/vllm/pull/20199
* [Misc] Add type assertion of request_id for LLMEngine.add_request by @SHA-4096 in https://github.com/vllm-project/vllm/pull/19700
* Fix num_token_padding support for static per-tensor scaled_fp8_quant by @mgoin in https://github.com/vllm-project/vllm/pull/20188
* fix ci issue distributed 4 gpu test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20204
* [Bugfix] Properly reject requests with empty list guided_choice by @mgoin in https://github.com/vllm-project/vllm/pull/20195
* [BugFix] Fix the incorrect func name in the comments. (config.py) by @1195343015 in https://github.com/vllm-project/vllm/pull/20185
* [CI/Build] Add new CI job to validate Hybrid Models for every PR  by @tdoublep in https://github.com/vllm-project/vllm/pull/20147
* [Frontend] Generalize `v1/audio/transcriptions` endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/20179
* [Bugfix] Correct behavior of GraniteMoeHybrid for TensorParallel execution by @s3woz in https://github.com/vllm-project/vllm/pull/20137
* [Refactor] Create a function util and cache the results for `has_deepgemm`, `has_deepep`, `has_pplx` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20187
* [CI Fix] Try fixing eagle e2e test OOM by reducing block allocation by @mgoin in https://github.com/vllm-project/vllm/pull/20213
* [Quantization] Add compressed-tensors NVFP4 MoE Support by @dsikka in https://github.com/vllm-project/vllm/pull/19990
* Fix cuda_archs_loose_intersection when handling sm_*a by @huydhn in https://github.com/vllm-project/vllm/pull/20207
* [Model] support dots1 by @redmoe-moutain in https://github.com/vllm-project/vllm/pull/18254
* [BUGFIX][DEEPSEEK][MODEL_LOAD] fix w13, w2 weight not initialized assert by @xuechendi in https://github.com/vllm-project/vllm/pull/20202
* [Misc] Fix import by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20233
* [doc] Add Slack and Forum to the top navigation by @reidliu41 in https://github.com/vllm-project/vllm/pull/20208
* [Bugfix] Skip loading extra parameters for modelopt Qwen3 MoE model by @noiji in https://github.com/vllm-project/vllm/pull/19598
* [Bugfix] Fix processor initialization in transformers 4.53.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/20244
* [Quantization] Improve BitsAndBytesModelLoader by @jeejeelee in https://github.com/vllm-project/vllm/pull/20242
* [Docs] Fix 1-2-3 list in v1/prefix_caching.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20243
* [Bugfix] fix quark ptpc by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/20251
* [Spec Decode] Refactor spec decoding into a separate function by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20238
* [Spec Decode] Clean up spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20240
* [Optimization] Use Shared `CachedRequestData` Instance Across All Requests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20232
* [Unit Test] Add unit test for deep gemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/20090
* [Core] [Bugfix] [Multimodal] Fix multimodal profiling and generation for SFT/PTQed models by @kylesayrs in https://github.com/vllm-project/vllm/pull/20058
* [Refactor] Remove useless pdb comment by @yewentao256 in https://github.com/vllm-project/vllm/pull/20266
* [Bugfix][V1][P/D]Fix the issue of occasional garbled output  for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/20263
* [CLI] Improve CLI arg parsing for `-O`/`--compilation-config` by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20156
* [Bugfix] Fix include prompt in stream response when echo=true by @fyuan1316 in https://github.com/vllm-project/vllm/pull/15233
* [Misc] Fix spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20296
* [Example] add one-click runnable example for P2P NCCL XpYd by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20246
* [CI][Intel Gaudi][vllm-Plugin]Add CI for hpu-plugin-v1-test by @xuechendi in https://github.com/vllm-project/vllm/pull/20196
* [Doc] add config and troubleshooting guide for NCCL & GPUDirect RDMA by @chewong in https://github.com/vllm-project/vllm/pull/15897
* [Feature] A calibration-free RTN-based quantization for accurate and accelerated INT4/INT8 inference by @sakogan in https://github.com/vllm-project/vllm/pull/18768
* [V1] Only print cudagraph tqdm on rank 0 with `is_global_first_rank` by @mgoin in https://github.com/vllm-project/vllm/pull/19516
* Fix `numel()` downcast in vllm/csrc/moe/moe_align_sum_kernels.cu +2 by @r-barnes in https://github.com/vllm-project/vllm/pull/17082
* [Misc] add xgrammar for arm64 by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/18359
* Enable ZP Support for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20268
* [CPU] Update custom ops for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20255
* [Bugfix] Fix deepep tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20288
* [Misc] remove redundant char by @kebe7jun in https://github.com/vllm-project/vllm/pull/20287
* [BugFix][V1][ROCm] Triton MLA uses V0 backend on V1 engine by @tywuAMD in https://github.com/vllm-project/vllm/pull/19067
* [doc] fix the incorrect logo in dark mode by @reidliu41 in https://github.com/vllm-project/vllm/pull/20289
* [Perf] Validate @config in pre-commit instead of dynamically by @lionelvillard in https://github.com/vllm-project/vllm/pull/20200
* [Quant] [Bugfix] Fix quantization config matching with `hf_to_vllm_mapper` by @kylesayrs in https://github.com/vllm-project/vllm/pull/20046
* [Misc] Minor refactor of NIXL background handshake by @NickLucche in https://github.com/vllm-project/vllm/pull/20068
* Add GLM4.1V model (Draft) by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/19331
* [Model]Add Tencent HunYuanMoEV1 Model Support by @aiyiwang2025 in https://github.com/vllm-project/vllm/pull/20114
* [Misc] Minor refactoring for scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20299
* [Docs] Update transcriptions API to use openai client with `stream=True`  by @NickLucche in https://github.com/vllm-project/vllm/pull/20271
* [CUDA graphs] Enable full cuda graphs with FA3 AoT scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20301
* [Frontend] Expand tools even if tool_choice="none" by @okdshin in https://github.com/vllm-project/vllm/pull/17177
* [V1] [ROCm] Enable EP with AITER Fused MoE by @tjtanaa in https://github.com/vllm-project/vllm/pull/20270
* [Optimization] Cache sampled token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20291
* remove unused variables in marlin_template.h by @zhoutianzi666 in https://github.com/vllm-project/vllm/pull/20236
* [Refactor] Refactor import utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20269
* Enable group size 64 for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20290
* [Kernel][Bugfix] Fixup some warnings in nvfp4_blockwise_moe when CUDA < 12.8 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20324
* [UT][intel GPU] use current_platform instead of device hardcode in v1 tests by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20169
* [Refactor] Remove duplicate `find_free_port` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20333
* [Refactor] Remove Unused Env `VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20334
* [Misc][Doc] Add missing comment for LLM by @draftbk in https://github.com/vllm-project/vllm/pull/20285
* [FIX][Intel GPU]fix ipex flash_attn_varlen_func api missing parameter by @jikunshang in https://github.com/vllm-project/vllm/pull/20348
* [Bugfix] Fix dynamic rotary embedding by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20343
* fix[Docs]: link anchor is incorrect #20309 by @yyzxw in https://github.com/vllm-project/vllm/pull/20315
* [Doc][TPU] Add models and features supporting matrix. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20230
* [TPU] kv cache update kernel supports dynamic grid by @yaochengji in https://github.com/vllm-project/vllm/pull/20235
* [Frontend] Support configurable mm placeholder strings & flexible video sampling policies via CLI flags. by @huachenheli in https://github.com/vllm-project/vllm/pull/20105
* [Model][VLM] Support Keye-VL-8B-Preview by @Kwai-Keye in https://github.com/vllm-project/vllm/pull/20126
* [Bugfix] Keye-VL compatibility with `tok_kwargs` (#20058) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20353
* [Docs] Fix indentations for 2-level items in deprecation_policy.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20352
* [Docs] Make TPU ref prettier in google_tpu.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20356
* [Model] Add Ernie4.5 and Ernie4.5MoE Model Support by @CSWYF3634076 in https://github.com/vllm-project/vllm/pull/20220
* [Build/CI] Automatically tag DeepSeek related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/20370
* [NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120) by @kaln27 in https://github.com/vllm-project/vllm/pull/17280
* [Bugfix] Fix the max_seq_len limit of 16384 for DeepSeek models by @huaqiangwang in https://github.com/vllm-project/vllm/pull/20322
* [Model] Adds support for SlimMoE models Phi-tiny-MoE-instruct by @zichongli5 in https://github.com/vllm-project/vllm/pull/20286
* Documentation update tool_calling: mapping back to function from response by @cronoik-inceptionai in https://github.com/vllm-project/vllm/pull/20373
* [Kernels] MoE refactor by @bnellnm in https://github.com/vllm-project/vllm/pull/19636
* [V1] LogitsProcessor programming model by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16728
* [Minor] Clean up incorrect comment in test by @njhill in https://github.com/vllm-project/vllm/pull/20382
* [Misc] add handler HF_TOKEN is emptry string by @lengrongfu in https://github.com/vllm-project/vllm/pull/20369
* [ROCm][FEAT] Enable Full Graph Mode in AITER MLA V1 Attn Backend (Decode Phase only) by @vllmellm in https://github.com/vllm-project/vllm/pull/20254
* [DP] Support external DP Load Balancer mode by @njhill in https://github.com/vllm-project/vllm/pull/19790
* [Docs] Update EAGLE example by @NickLucche in https://github.com/vllm-project/vllm/pull/20375
* [Bugfix] Fixes for FlashInfer's TORCH_CUDA_ARCH_LIST by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20136
* [BugFix] Fix DP headless mode arg validation by @njhill in https://github.com/vllm-project/vllm/pull/20398
* Enable CPU nightly performance benchmark and its Markdown report by @louie-tsai in https://github.com/vllm-project/vllm/pull/18444
* [Bugfix] Fix import of CutlassExpertsFp8 in compressed_tensors_moe.py by @bnellnm in https://github.com/vllm-project/vllm/pull/20381
* [Misc] Small: Fix video loader return type annotations. by @huachenheli in https://github.com/vllm-project/vllm/pull/20389
* [Bugfix][CI/CD][CPU] Fix CPU CI tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20383
* [TPU] Add a case to cover RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8 by @QiliangCui in https://github.com/vllm-project/vllm/pull/20385
* [Feature] Support MiniMax-M1 function calls features by @qscqesze in https://github.com/vllm-project/vllm/pull/20297
* [Tests] Update online DP tests to verify that requests are balanced by @njhill in https://github.com/vllm-project/vllm/pull/20157
* [Misc] Add rules to label Speculative Decoding Related PRs by @draftbk in https://github.com/vllm-project/vllm/pull/20406
* [doc] fix link by @reidliu41 in https://github.com/vllm-project/vllm/pull/20417
* [Docs] Replace two list with tables in intel_gaudi.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20414
* [Core] Move multimodal placeholder from chat utils to model definition by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20355
* [Kernel] refactor cpu worker v0 cache dtype by @andyxning in https://github.com/vllm-project/vllm/pull/20080
* [CI/Build][CPU] Enable cross compilation in CPU release pipeline by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20423
* [Quantization] Bump to use latest bitsandbytes by @jeejeelee in https://github.com/vllm-project/vllm/pull/20424
* [Model][2/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/19978
* [Misc] Automatically tag PRs to add new models by @Isotr0py in https://github.com/vllm-project/vllm/pull/20222
* [Frontend] improve vllm bench <bench_type> --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/20430
* [Bugfix] Fix flaky `test_streaming_response` test by @NickLucche in https://github.com/vllm-project/vllm/pull/20363
* [Frontend] fix duplicate output for bench subcmd by @reidliu41 in https://github.com/vllm-project/vllm/pull/20446
* [CI] Trimming some failing test groups from AMDPRODUCTION. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/20390
* [Misc] Clean up InternVL family config registration by @Isotr0py in https://github.com/vllm-project/vllm/pull/19992
* [Misc] adjust for ipv6 for mookcacke url parse by @andyxning in https://github.com/vllm-project/vllm/pull/20107
* [Misc] Remove _maybe_ignore_quant_config from GLM4.1v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/20432
* [Kernel] Enable fp8 support for pplx and BatchedTritonExperts. by @bnellnm in https://github.com/vllm-project/vllm/pull/18864
* [Misc] Fix `Unable to detect current VLLM config. Defaulting to NHD kv cache layout` warning by @NickLucche in https://github.com/vllm-project/vllm/pull/20400
* [Bugfix] Register reducer even if transformers_modules not available by @eicherseiji in https://github.com/vllm-project/vllm/pull/19510
* Change warn_for_unimplemented_methods to debug by @mgoin in https://github.com/vllm-project/vllm/pull/20455
* [Platform] Add custom default max tokens by @gmarinho2 in https://github.com/vllm-project/vllm/pull/18557
* Add ignore consolidated file in mistral example code by @princepride in https://github.com/vllm-project/vllm/pull/20420
* [Misc] small update by @reidliu41 in https://github.com/vllm-project/vllm/pull/20462
* [Structured Outputs][V1] Skipping with models doesn't contain tokenizers by @aarnphm in https://github.com/vllm-project/vllm/pull/20365
* [Perf] Optimize Vectorization Utils for Int 8 Quantization Kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/20331
* [Misc] Add SPDX-FileCopyrightText by @jeejeelee in https://github.com/vllm-project/vllm/pull/20428
* Support Llama 4 for fused_marlin_moe by @mgoin in https://github.com/vllm-project/vllm/pull/20457
* [Bug][Frontend] Fix structure of transcription's decoder_prompt by @sangbumlikeagod in https://github.com/vllm-project/vllm/pull/18809
* [Model][3/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20168
* [Doc] Fix classification table in list of supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20489
* [CI] add kvcache-connector dependency definition and add into CI build by @panpan0000 in https://github.com/vllm-project/vllm/pull/18193
* [Misc] Small: Remove global media connector. Each test should have its own test connector object. by @huachenheli in https://github.com/vllm-project/vllm/pull/20395
* Enable V1 for Hybrid SSM/Attention Models by @tdoublep in https://github.com/vllm-project/vllm/pull/20016
* [feat]: CUTLASS block scaled group gemm for SM100 by @djmmoss in https://github.com/vllm-project/vllm/pull/19757
* [CI Bugfix] Fix pre-commit failures on main by @mgoin in https://github.com/vllm-project/vllm/pull/20502
* [Doc] fix mutltimodal_inputs.md gh examples link by @GuyStone in https://github.com/vllm-project/vllm/pull/20497
* [Misc] Add security warning for development mode endpoints by @reidliu41 in https://github.com/vllm-project/vllm/pull/20508
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/20506
* [Misc] Remove the unused LoRA test code by @jeejeelee in https://github.com/vllm-project/vllm/pull/20494
* Fix unknown attribute of topk_indices_dtype in CompressedTensorsW8A8Fp8MoECutlassMethod by @luccafong in https://github.com/vllm-project/vllm/pull/20507
* [v1] Re-add fp32 support to v1 engine through FlexAttention by @Isotr0py in https://github.com/vllm-project/vllm/pull/19754
* [Misc] Add logger.exception for TPU information collection failures by @reidliu41 in https://github.com/vllm-project/vllm/pull/20510
* [Misc] remove unused import by @reidliu41 in https://github.com/vllm-project/vllm/pull/20517
* test_attention compat with coming xformers change by @bottler in https://github.com/vllm-project/vllm/pull/20487
* [BUG] Fix #20484. Support empty sequence in cuda penalty kernel by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20491
* [Bugfix] Fix missing per_act_token parameter in compressed_tensors_moe by @luccafong in https://github.com/vllm-project/vllm/pull/20509
* [BugFix] Fix: ImportError when building on hopper systems by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20513
* [TPU][Bugfix] fix the MoE OOM issue by @yaochengji in https://github.com/vllm-project/vllm/pull/20339
* [Frontend] Support image object in llm.chat by @sfeng33 in https://github.com/vllm-project/vllm/pull/19635
* [Benchmark] Add support for multiple batch size benchmark through CLI in `benchmark_moe.py` + Add Triton Fused MoE kernel config for FP8 E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/20516
* [Misc] call the pre-defined func by @reidliu41 in https://github.com/vllm-project/vllm/pull/20518
* [V0 deprecation] Remove V0 CPU/XPU/TPU backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20412
* [V1] Support any head size for FlexAttention backend by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20467
* [BugFix][Spec Decode] Fix spec token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20530
* [Bugfix] Add `use_cross_encoder` flag to use correct activation in `ClassifierPooler` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20527

## New Contributors
* @py-andy-c made their first contribution in https://github.com/vllm-project/vllm/pull/19399
* @2niuhe made their first contribution in https://github.com/vllm-project/vllm/pull/19394
* @leopardracer made their first contribution in https://github.com/vllm-project/vllm/pull/19442
* @artetaout made their first contribution in https://github.com/vllm-project/vllm/pull/19085
* @runzhen made their first contribution in https://github.com/vllm-project/vllm/pull/19453
* @strutive07 made their first contribution in https://github.com/vllm-project/vllm/pull/19522
* @yewentao256 made their first contribution in https://github.com/vllm-project/vllm/pull/19233
* @mobicham made their first contribution in https://github.com/vllm-project/vllm/pull/19265
* @kouroshHakha made their first contribution in https://github.com/vllm-project/vllm/pull/19378
* @BoyuanFeng made their first contribution in https://github.com/vllm-project/vllm/pull/19587
* @sahelib25 made their first contribution in https://github.com/vllm-project/vllm/pull/18354
* @jiahanc made their first contribution in https://github.com/vllm-project/vllm/pull/19500
* @quanliu1991 made their first contribution in https://github.com/vllm-project/vllm/pull/18957
* @f14-bertolotti made their first contribution in https://github.com/vllm-project/vllm/pull/19564
* @Navanit-git made their first contribution in https://github.com/vllm-project/vllm/pull/19557
* @nguyenhoangthuan99 made their first contribution in https://github.com/vllm-project/vllm/pull/19597
* @diliu0349 made their first contribution in https://github.com/vllm-project/vllm/pull/19600
* @Zzz9990 made their first contribution in https://github.com/vllm-project/vllm/pull/18596
* @yhtang made their first contribution in https://github.com/vllm-project/vllm/pull/19788
* @zsolt-borbely-htec made their first contribution in https://github.com/vllm-project/vllm/pull/19803
* @zuxin666 made their first contribution in https://github.com/vllm-project/vllm/pull/17148
* @NekoMimiUnagi made their first contribution in https://github.com/vllm-project/vllm/pull/19301
* @xzbdmw made their first contribution in https://github.com/vllm-project/vllm/pull/19735
* @Xerxes-cn made their first contribution in https://github.com/vllm-project/vllm/pull/19860
* @nie3e made their first contribution in https://github.com/vllm-project/vllm/pull/19663
* @vladmihailescu made their first contribution in https://github.com/vllm-project/vllm/pull/18777
* @rabinadk1 made their first contribution in https://github.com/vllm-project/vllm/pull/19910
* @amitm02 made their first contribution in https://github.com/vllm-project/vllm/pull/19057
* @jinqinn made their first contribution in https://github.com/vllm-project/vllm/pull/19544
* @Flink-ddd made their first contribution in https://github.com/vllm-project/vllm/pull/19643
* @Jun-Howie made their first contribution in https://github.com/vllm-project/vllm/pull/19395
* @seemethere made their first contribution in https://github.com/vllm-project/vllm/pull/20032
* @h-avsha made their first contribution in https://github.com/vllm-project/vllm/pull/19984
* @max-wittig made their first contribution in https://github.com/vllm-project/vllm/pull/19695
* @lsz05 made their first contribution in https://github.com/vllm-project/vllm/pull/20067
* @kyolebu made their first contribution in https://github.com/vllm-project/vllm/pull/20135
* @lihaoyang-amd made their first contribution in https://github.com/vllm-project/vllm/pull/19744
* @Yazan-Sharaya made their first contribution in https://github.com/vllm-project/vllm/pull/19946
* @ilyal-cerebras made their first contribution in https://github.com/vllm-project/vllm/pull/20065
* @fabiendupont made their first contribution in https://github.com/vllm-project/vllm/pull/18064
* @SHA-4096 made their first contribution in https://github.com/vllm-project/vllm/pull/19700
* @1195343015 made their first contribution in https://github.com/vllm-project/vllm/pull/20185
* @redmoe-moutain made their first contribution in https://github.com/vllm-project/vllm/pull/18254
* @noiji made their first contribution in https://github.com/vllm-project/vllm/pull/19598
* @chewong made their first contribution in https://github.com/vllm-project/vllm/pull/15897
* @sakogan made their first contribution in https://github.com/vllm-project/vllm/pull/18768
* @czhu-cohere made their first contribution in https://github.com/vllm-project/vllm/pull/20268
* @aiyiwang2025 made their first contribution in https://github.com/vllm-project/vllm/pull/20114
* @okdshin made their first contribution in https://github.com/vllm-project/vllm/pull/17177
* @zhoutianzi666 made their first contribution in https://github.com/vllm-project/vllm/pull/20236
* @yyzxw made their first contribution in https://github.com/vllm-project/vllm/pull/20315
* @Kwai-Keye made their first contribution in https://github.com/vllm-project/vllm/pull/20126
* @CSWYF3634076 made their first contribution in https://github.com/vllm-project/vllm/pull/20220
* @kaln27 made their first contribution in https://github.com/vllm-project/vllm/pull/17280
* @huaqiangwang made their first contribution in https://github.com/vllm-project/vllm/pull/20322
* @zichongli5 made their first contribution in https://github.com/vllm-project/vllm/pull/20286
* @cronoik-inceptionai made their first contribution in https://github.com/vllm-project/vllm/pull/20373
* @sangbumlikeagod made their first contribution in https://github.com/vllm-project/vllm/pull/18809
* @djmmoss made their first contribution in https://github.com/vllm-project/vllm/pull/19757
* @GuyStone made their first contribution in https://github.com/vllm-project/vllm/pull/20497
* @bottler made their first contribution in https://github.com/vllm-project/vllm/pull/20487

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.1...v0.9.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.2)

---

## v0.9.2rc2: v0.9.2rc2
**Published:** 2025-07-06
**Pre-release**

## What's Changed
* [Kernel] Enable fp8 support for pplx and BatchedTritonExperts. by @bnellnm in https://github.com/vllm-project/vllm/pull/18864
* [Misc] Fix `Unable to detect current VLLM config. Defaulting to NHD kv cache layout` warning by @NickLucche in https://github.com/vllm-project/vllm/pull/20400
* [Bugfix] Register reducer even if transformers_modules not available by @eicherseiji in https://github.com/vllm-project/vllm/pull/19510
* Change warn_for_unimplemented_methods to debug by @mgoin in https://github.com/vllm-project/vllm/pull/20455
* [Platform] Add custom default max tokens by @gmarinho2 in https://github.com/vllm-project/vllm/pull/18557
* Add ignore consolidated file in mistral example code by @princepride in https://github.com/vllm-project/vllm/pull/20420
* [Misc] small update by @reidliu41 in https://github.com/vllm-project/vllm/pull/20462
* [Structured Outputs][V1] Skipping with models doesn't contain tokenizers by @aarnphm in https://github.com/vllm-project/vllm/pull/20365
* [Perf] Optimize Vectorization Utils for Int 8 Quantization Kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/20331
* [Misc] Add SPDX-FileCopyrightText by @jeejeelee in https://github.com/vllm-project/vllm/pull/20428
* Support Llama 4 for fused_marlin_moe by @mgoin in https://github.com/vllm-project/vllm/pull/20457
* [Bug][Frontend] Fix structure of transcription's decoder_prompt by @sangbumlikeagod in https://github.com/vllm-project/vllm/pull/18809
* [Model][3/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20168
* [Doc] Fix classification table in list of supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20489
* [CI] add kvcache-connector dependency definition and add into CI build by @panpan0000 in https://github.com/vllm-project/vllm/pull/18193
* [Misc] Small: Remove global media connector. Each test should have its own test connector object. by @huachenheli in https://github.com/vllm-project/vllm/pull/20395
* Enable V1 for Hybrid SSM/Attention Models by @tdoublep in https://github.com/vllm-project/vllm/pull/20016
* [feat]: CUTLASS block scaled group gemm for SM100 by @djmmoss in https://github.com/vllm-project/vllm/pull/19757
* [CI Bugfix] Fix pre-commit failures on main by @mgoin in https://github.com/vllm-project/vllm/pull/20502
* [Doc] fix mutltimodal_inputs.md gh examples link by @GuyStone in https://github.com/vllm-project/vllm/pull/20497
* [Misc] Add security warning for development mode endpoints by @reidliu41 in https://github.com/vllm-project/vllm/pull/20508
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/20506
* [Misc] Remove the unused LoRA test code by @jeejeelee in https://github.com/vllm-project/vllm/pull/20494
* Fix unknown attribute of topk_indices_dtype in CompressedTensorsW8A8Fp8MoECutlassMethod by @luccafong in https://github.com/vllm-project/vllm/pull/20507
* [v1] Re-add fp32 support to v1 engine through FlexAttention by @Isotr0py in https://github.com/vllm-project/vllm/pull/19754
* [Misc] Add logger.exception for TPU information collection failures by @reidliu41 in https://github.com/vllm-project/vllm/pull/20510
* [Misc] remove unused import by @reidliu41 in https://github.com/vllm-project/vllm/pull/20517
* test_attention compat with coming xformers change by @bottler in https://github.com/vllm-project/vllm/pull/20487
* [BUG] Fix #20484. Support empty sequence in cuda penalty kernel by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/20491
* [Bugfix] Fix missing per_act_token parameter in compressed_tensors_moe by @luccafong in https://github.com/vllm-project/vllm/pull/20509
* [BugFix] Fix: ImportError when building on hopper systems by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20513
* [TPU][Bugfix] fix the MoE OOM issue by @yaochengji in https://github.com/vllm-project/vllm/pull/20339
* [Frontend] Support image object in llm.chat by @sfeng33 in https://github.com/vllm-project/vllm/pull/19635
* [Benchmark] Add support for multiple batch size benchmark through CLI in `benchmark_moe.py` + Add Triton Fused MoE kernel config for FP8 E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/20516
* [Misc] call the pre-defined func by @reidliu41 in https://github.com/vllm-project/vllm/pull/20518
* [V0 deprecation] Remove V0 CPU/XPU/TPU backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20412
* [V1] Support any head size for FlexAttention backend by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20467
* [BugFix][Spec Decode] Fix spec token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20530
* [Bugfix] Add `use_cross_encoder` flag to use correct activation in `ClassifierPooler` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20527

## New Contributors
* @sangbumlikeagod made their first contribution in https://github.com/vllm-project/vllm/pull/18809
* @djmmoss made their first contribution in https://github.com/vllm-project/vllm/pull/19757
* @GuyStone made their first contribution in https://github.com/vllm-project/vllm/pull/20497
* @bottler made their first contribution in https://github.com/vllm-project/vllm/pull/20487

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.2rc1...v0.9.2rc2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.2rc2)

---

## v0.9.2rc1: v0.9.2rc1
**Published:** 2025-07-03
**Pre-release**

## What's Changed
* [Docs] Note that alternative structured output backends are supported by @russellb in https://github.com/vllm-project/vllm/pull/19426
* [ROCm][V1] Adding ROCm to the list of plaforms using V1 by default by @gshtras in https://github.com/vllm-project/vllm/pull/19440
* [Model] use AutoWeightsLoader for commandr by @py-andy-c in https://github.com/vllm-project/vllm/pull/19399
* Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B-FP8 by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19401
* [BugFix] Allow use_cudagraph to work with dynamic VLLM_USE_V1 by @zou3519 in https://github.com/vllm-project/vllm/pull/19390
* [New Model]: Support Qwen3 Embedding & Reranker  by @noooop in https://github.com/vllm-project/vllm/pull/19260
* [BugFix] Fix docker build cpu-dev image error by @2niuhe in https://github.com/vllm-project/vllm/pull/19394
* Fix test_max_model_len in tests/entrypoints/llm/test_generate.py by @houseroad in https://github.com/vllm-project/vllm/pull/19451
* [CI] Disable failing GGUF model test by @mgoin in https://github.com/vllm-project/vllm/pull/19454
* [Misc] Remove unused `MultiModalHasher.hash_prompt_mm_data` by @lgeiger in https://github.com/vllm-project/vllm/pull/19422
* Add fused MOE config for Qwen3 30B A3B on B200 by @0xjunhao in https://github.com/vllm-project/vllm/pull/19455
* Fix Typo in Documentation and Function Name by @leopardracer in https://github.com/vllm-project/vllm/pull/19442
* [ROCm] Add rules to automatically label ROCm related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19405
* [Kernel] Support deep_gemm for linear methods by @artetaout in https://github.com/vllm-project/vllm/pull/19085
* [Doc] Update V1 User Guide for Hardware and Models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19474
* [Doc] Fix quantization link titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19478
* [Doc] Support "important" and "announcement" admonitions by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19479
* [Misc] Reduce warning message introduced in env_override by @houseroad in https://github.com/vllm-project/vllm/pull/19476
* Support non-string values in JSON keys from CLI by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19471
* Add cache to cuda get_device_capability by @mgoin in https://github.com/vllm-project/vllm/pull/19436
* Fix some typo by @Ximingwang-09 in https://github.com/vllm-project/vllm/pull/19475
* Support no privileged mode on CPU for docker and kubernetes deployments by @louie-tsai in https://github.com/vllm-project/vllm/pull/19241
* [Bugfix] Update the example code, make it work with the latest lmcache by @runzhen in https://github.com/vllm-project/vllm/pull/19453
* [CI] Update FlashInfer to 0.2.6.post1 by @mgoin in https://github.com/vllm-project/vllm/pull/19297
* [doc] fix "Other AI accelerators" getting started page by @davidxia in https://github.com/vllm-project/vllm/pull/19457
* [Misc] Fix  misleading ROCm warning by @jeejeelee in https://github.com/vllm-project/vllm/pull/19486
* [Docs] Remove WIP features in V1 guide by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19498
* [Kernels] Add activation chunking logic to FusedMoEModularKernel by @bnellnm in https://github.com/vllm-project/vllm/pull/19168
* [AMD] [Quantization] Add override flag for attention dtype instead of using kv_cache_dtype trigger by @rasmith in https://github.com/vllm-project/vllm/pull/17331
* [UX] Add Feedback During CUDAGraph Capture by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19501
* [CI/Build] Fix torch nightly CI dependencies by @zou3519 in https://github.com/vllm-project/vllm/pull/19505
* [CI] change spell checker from codespell to typos by @andyxning in https://github.com/vllm-project/vllm/pull/18711
* [BugFix] Force registration of w8a8_block_fp8_matmul_deepgemm via lazy import by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19514
* Add Triton Fused MoE kernel config for E=16 on B200 by @b8zhong in https://github.com/vllm-project/vllm/pull/19518
* [Frontend] Improve error message in tool_choice validation by @22quinn in https://github.com/vllm-project/vllm/pull/19239
* [BugFix] Work-around incremental detokenization edge case error by @njhill in https://github.com/vllm-project/vllm/pull/19449
* [BugFix] Handle missing sep_token for Qwen3-Reranker in Score API by @strutive07 in https://github.com/vllm-project/vllm/pull/19522
* [AMD][Kernel][BugFix] fix test_rocm_compressed_tensors_w8a8 for rocm by @rasmith in https://github.com/vllm-project/vllm/pull/19509
* Fix typo by @2niuhe in https://github.com/vllm-project/vllm/pull/19525
* [Security] Prevent new imports of (cloud)pickle by @russellb in https://github.com/vllm-project/vllm/pull/18018
* [Bugfix][V1] Allow manual FlashAttention for Blackwell by @mgoin in https://github.com/vllm-project/vllm/pull/19492
* [Bugfix] Respect num-gpu-blocks-override in v1 by @jmswen in https://github.com/vllm-project/vllm/pull/19503
* [Quantization] Improve AWQ logic by @jeejeelee in https://github.com/vllm-project/vllm/pull/19431
* [Doc] Add V1 column to supported models list by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19523
* [NixlConnector] Drop `num_blocks` check  by @NickLucche in https://github.com/vllm-project/vllm/pull/19532
* [Perf] Vectorize static / dynamic INT8 quant kernels by @yewentao256 in https://github.com/vllm-project/vllm/pull/19233
* Fix TorchAOConfig skip layers by @mobicham in https://github.com/vllm-project/vllm/pull/19265
* [torch.compile][ROCm] Fuse quantization onto attention using a torch.compile pass by @ProExpertProg in https://github.com/vllm-project/vllm/pull/16756
* [doc] Make top navigation sticky by @reidliu41 in https://github.com/vllm-project/vllm/pull/19540
* [Spec Decode][Benchmark] Generalize spec decode offline benchmark to more methods and datasets by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18847
* [Misc] Turn MOE_DP_CHUNK_SIZE into an env var by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19506
* [Bugfix] Enforce contiguous input for dynamic_per_token FP8/INT8 quant by @mgoin in https://github.com/vllm-project/vllm/pull/19452
* [Doc] Unify structured outputs examples by @aarnphm in https://github.com/vllm-project/vllm/pull/18196
* [V1] Resolve failed concurrent structred output requests by @russellb in https://github.com/vllm-project/vllm/pull/19565
* Revert "[Build/CI] Add tracing deps to vllm container image (#15224)" by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19378
* [BugFix] : Fix Batched DeepGemm Experts by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19515
* [Bugfix] Fix EAGLE vocab embedding for multimodal target model by @zixi-qi in https://github.com/vllm-project/vllm/pull/19570
* [Doc] uses absolute links for structured outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/19582
* [doc] fix incorrect link by @reidliu41 in https://github.com/vllm-project/vllm/pull/19586
* [Misc] Correct broken docs link by @Zerohertz in https://github.com/vllm-project/vllm/pull/19553
* [CPU] Refine default config for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19539
* [Fix] bump mistral common to support magistral by @princepride in https://github.com/vllm-project/vllm/pull/19533
* [Fix] The zip function in Python 3.9 does not have the strict argument by @princepride in https://github.com/vllm-project/vllm/pull/19549
* use base version for version comparison by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/19587
* [torch.compile] reorganize the cache directory to support compiling multiple models by @youkaichao in https://github.com/vllm-project/vllm/pull/19064
* [BugFix] Honor `enable_caching` in connector-delayed kvcache load case by @njhill in https://github.com/vllm-project/vllm/pull/19435
* [Model] Fix minimax model cache & lm_head precision by @qscqesze in https://github.com/vllm-project/vllm/pull/19592
* [Refactor] Remove unused variables in `moe_permute_unpermute_kernel.inl` by @yewentao256 in https://github.com/vllm-project/vllm/pull/19573
* [doc][mkdocs] fix the  duplicate Supported features sections in GPU docs by @reidliu41 in https://github.com/vllm-project/vllm/pull/19606
* [CUDA] Enable full cudagraph for FlashMLA by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18581
* [Doc] Add troubleshooting section to k8s deployment by @annapendleton in https://github.com/vllm-project/vllm/pull/19377
* [torch.compile] Use custom ops when use_inductor=False by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19618
* Adding "AMD: Multi-step Tests" to amdproduction. by @Concurrensee in https://github.com/vllm-project/vllm/pull/19508
* [BugFix] Fix DP Coordinator incorrect debug log message by @njhill in https://github.com/vllm-project/vllm/pull/19624
* [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics. by @sahelib25 in https://github.com/vllm-project/vllm/pull/18354
* [Bugfix][1/n] Fix the speculative decoding test by setting the target dtype by @houseroad in https://github.com/vllm-project/vllm/pull/19633
* [Misc] Modularize CLI Argument Parsing in Benchmark Scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19593
* [Bugfix] Fix auto dtype casting for BatchFeature by @Isotr0py in https://github.com/vllm-project/vllm/pull/19316
* [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization by @jiahanc in https://github.com/vllm-project/vllm/pull/19500
* Only build CUTLASS MoE kernels on Hopper by @huydhn in https://github.com/vllm-project/vllm/pull/19648
* [Bugfix] Don't attempt to use triton if no driver is active by @kzawora-intel in https://github.com/vllm-project/vllm/pull/19561
* [Fix] Convert kv_transfer_config from dict to KVTransferConfig by @maobaolong in https://github.com/vllm-project/vllm/pull/19262
* [Perf] Further tunings for SM100 FP8 CUTLASS kernel by @ilmarkov in https://github.com/vllm-project/vllm/pull/19566
* [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness by @houseroad in https://github.com/vllm-project/vllm/pull/19644
* [Kernel] Raise verbose error and consolidate `num_heads/num_kv_heads` divisibility check by @22quinn in https://github.com/vllm-project/vllm/pull/19339
* [Benchmark] Refactor benchmark script for fp8 & int8 by @yewentao256 in https://github.com/vllm-project/vllm/pull/19627
* Enable prefix caching with full cuda graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19617
* [CI/Build] Fix torch nightly CI dependencies part 2 by @zou3519 in https://github.com/vllm-project/vllm/pull/19589
* [Misc] Remove duplicate multiproc method setting for CPU platform by @Isotr0py in https://github.com/vllm-project/vllm/pull/19649
* [MISC] Remove unused variableds in C++ by @houseroad in https://github.com/vllm-project/vllm/pull/19609
* [Bugfix][Core] Prefix caching causes incorrect outputs due to outdated ComputedBlocksTracker by @quanliu1991 in https://github.com/vllm-project/vllm/pull/18957
* [Misc][Frontend] passthrough `bad_words` by @f14-bertolotti in https://github.com/vllm-project/vllm/pull/19564
* [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19660
* [TPU] support attention head dim smaller than 128 by @yaochengji in https://github.com/vllm-project/vllm/pull/19620
* [MISC] typo fix by @andyxning in https://github.com/vllm-project/vllm/pull/19672
* [CI] Add mteb testing for rerank models by @noooop in https://github.com/vllm-project/vllm/pull/19344
* [Docs] Move multiproc doc to v1 dir by @russellb in https://github.com/vllm-project/vllm/pull/19651
* [Kernel] GGUF MMVQ kernel for multiple input vectors by @SzymonOzog in https://github.com/vllm-project/vllm/pull/18754
* [BugFix] Don't catch BaseException when dumping execute_model errors by @njhill in https://github.com/vllm-project/vllm/pull/19626
* [DOC] Add reasoning capability to vLLM streamlit code by @Navanit-git in https://github.com/vllm-project/vllm/pull/19557
* [Feature]:Allow for Granite MoE Hybrid models with _only_ shared experts. by @shawntan in https://github.com/vllm-project/vllm/pull/19652
* [Bugfix] Fix TP inference for Flex attention backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/19657
* [MISC] bump huggingface_hub pkg to 0.33.0 by @andyxning in https://github.com/vllm-project/vllm/pull/19547
* [Bugfix] fix missing 'finish_reason': null in streaming chat by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19662
* [Kernels] Use empty for modular MoE workspaces by @bnellnm in https://github.com/vllm-project/vllm/pull/19667
* [Model] Add support for MiniMaxM1ForCausalLM (shares architecture with MiniMaxText01ForCausalLM) by @qscqesze in https://github.com/vllm-project/vllm/pull/19677
* [V1] Change return type on get_multimodal_embeddings() by @russellb in https://github.com/vllm-project/vllm/pull/19446
* [Quantization] Remove FP4 emulation; Fall-back to marlin for device < 100 by @dsikka in https://github.com/vllm-project/vllm/pull/19563
* [Fix] Fall back to Gloo when NCCL backend is unavailable by @conroy-cheers in https://github.com/vllm-project/vllm/pull/19641
* [doc] add project flag to gcloud TPU command by @davidxia in https://github.com/vllm-project/vllm/pull/19664
* [Wheel Size] Only build FA2 8.0+PTX by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19336
* [Frontend] add chunking audio for > 30s audio by @nguyenhoangthuan99 in https://github.com/vllm-project/vllm/pull/19597
* [DOC] fix doc typos by @diliu0349 in https://github.com/vllm-project/vllm/pull/19600
* Fixes IMA for TP w/ flex-attention by @drisspg in https://github.com/vllm-project/vllm/pull/19712
* [Core] add remove_seq_from_computed_blocks_tracker to BlockSpaceManager by @quanliu1991 in https://github.com/vllm-project/vllm/pull/19686
* [Doc] Add missing llava family multi-image examples by @Isotr0py in https://github.com/vllm-project/vllm/pull/19698
* Add a doc on how to update PyTorch version by @huydhn in https://github.com/vllm-project/vllm/pull/19705
* [Kernel] Add Split-KV Support to Unified Triton Attention Kernel by @jvlunteren in https://github.com/vllm-project/vllm/pull/19152
* [doc][mkdocs] Add edit  button to documentation by @reidliu41 in https://github.com/vllm-project/vllm/pull/19637
* [doc] split "Other AI Accelerators" tabs by @davidxia in https://github.com/vllm-project/vllm/pull/19708
* [V1][Kernel] Flashinfer HND KV cache layout by @NickLucche in https://github.com/vllm-project/vllm/pull/19280
* [Mis] remove duplicate engine status checks by @googs1025 in https://github.com/vllm-project/vllm/pull/19647
* [Bugfix] Update multimodel models mapping to fit new checkpoint after Transformers v4.52 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19151
* [Perf] Optimize `moe_align_block_size` CUDA kernel by @yewentao256 in https://github.com/vllm-project/vllm/pull/19572
* Remove sm120 arch from sm100 cutlass kernel arch list by @mgoin in https://github.com/vllm-project/vllm/pull/19716
* [Misc] Update lmcache connector with the latest connector apis by @YaoJiayi in https://github.com/vllm-project/vllm/pull/19441
* [Bugfix] Fix faulty triton importing logic when using Ray for DP by @mgoin in https://github.com/vllm-project/vllm/pull/19734
* [Feature][ROCm] Add full graph capture support for TritonAttentionBackend by @charlifu in https://github.com/vllm-project/vllm/pull/19158
* [TPU] Update torch version to include paged attention kernel change by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19706
* [MISC] correct copy_blocks src_to_dists param type by @andyxning in https://github.com/vllm-project/vllm/pull/19696
* [MISC] correct DeviceConfig device field static type analysis by @andyxning in https://github.com/vllm-project/vllm/pull/19699
* [Misc] Add __str__ for RequestStatus by @lk-chen in https://github.com/vllm-project/vllm/pull/19780
* [V1] Add API docs for EncoderCacheManager by @russellb in https://github.com/vllm-project/vllm/pull/19294
* [V1][P/D] An native implementation of xPyD based on P2P NCCL by @Abatom in https://github.com/vllm-project/vllm/pull/18242
* [V1] Decouple GPU and TPU `InputBatch` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19778
* [Minor] Zero-initialize attn output buffer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19784
* [doc] fix the incorrect label by @reidliu41 in https://github.com/vllm-project/vllm/pull/19787
* [Platform] Allow platform use V1 Engine by default by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19792
* [Qwen] Add tagging rule for Qwen related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/19799
* [Hardware][AMD] integrate aiter chunked prefill into vllm by @Zzz9990 in https://github.com/vllm-project/vllm/pull/18596
* [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19725
* [Docs] Add Huzaifa Sidhpurwala to vuln mgmt team doc by @russellb in https://github.com/vllm-project/vllm/pull/19808
* [v1] Support mamba2 by @heheda12345 in https://github.com/vllm-project/vllm/pull/19327
* docs: fix Slack bulletpoint in README by @nathan-weinberg in https://github.com/vllm-project/vllm/pull/19811
* Disable "Forbid direct 'import triton'" check for `vllm/triton_utils/importing.py` in an extensible way by @afeldman-nm in https://github.com/vllm-project/vllm/pull/19783
* [Core] Do not copy array during hashing by @lgeiger in https://github.com/vllm-project/vllm/pull/19484
* [TPU] Update torch-xla version to include paged attention tuned block change by @QiliangCui in https://github.com/vllm-project/vllm/pull/19813
* [Core] More fixes to MultiModalEmbeddings type handling by @russellb in https://github.com/vllm-project/vllm/pull/19715
* [Multimodal] Use fast processor for Qwen2/2.5-VL by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19789
* [BugFix] Fix use_cudagraph=False by @zou3519 in https://github.com/vllm-project/vllm/pull/19612
* [Frontend] Expose custom args in OpenAI APIs by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16862
* Fix FA2 fallback for Blackwell V1 by @mgoin in https://github.com/vllm-project/vllm/pull/19781
* [Misc][ROCm] Enforce no unused variable in ROCm C++ files by @houseroad in https://github.com/vllm-project/vllm/pull/19796
* [Quantization] Modify the logic of BNB double quantization by @jeejeelee in https://github.com/vllm-project/vllm/pull/19742
* Support embedding models in V1 by @maxdebayser in https://github.com/vllm-project/vllm/pull/16188
* [Bugfix] Fix the linter by @houseroad in https://github.com/vllm-project/vllm/pull/19826
* [Bugfix] Add check_health to v1 async client. by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19821
* Mark invariant normalizer in Gemma as non-persistent by @yhtang in https://github.com/vllm-project/vllm/pull/19788
* [ROCm] [AITER] [Bugfix] Patch for AITER commit `648764942e552a8bb5fe16026703716a81f05374` by @tjtanaa in https://github.com/vllm-project/vllm/pull/18990
* [Misc] [ROCm] Prevent surplus tensor reshape by @zsolt-borbely-htec in https://github.com/vllm-project/vllm/pull/19803
* raise exception for pin_lora by @andyxning in https://github.com/vllm-project/vllm/pull/19809
* [Minor] Allow redirecting model path for HfRunner in test by @Isotr0py in https://github.com/vllm-project/vllm/pull/19795
* Add xLAM tool parser support by @zuxin666 in https://github.com/vllm-project/vllm/pull/17148
* [Frontend] Add optional token-level progress bar to `LLM.beam_search` by @NekoMimiUnagi in https://github.com/vllm-project/vllm/pull/19301
* Fixing Chunked Prefill Test. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/19762
* [Doc] Update V1 user guide for embedding models by @22quinn in https://github.com/vllm-project/vllm/pull/19842
* [CI][CPU] Improve dummy Triton interfaces and fix the CPU CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19838
* [Core][Bugfix] Fix Online MM Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/19688
* [Frontend] early return chat format resolution when specified by @xzbdmw in https://github.com/vllm-project/vllm/pull/19735
* [Benchmark][Bugfix] Fix Dataset Length Calculation by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19868
* [CI/Build][Bugfix] Fix deadlock on v1 engine test CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/19872
* [CI][Neuron] Fail and exit on first error by @elaineyz in https://github.com/vllm-project/vllm/pull/19622
* [Benchmark] Fix `Value of type "SampleRequest" is not indexable` by @b8zhong in https://github.com/vllm-project/vllm/pull/18032
* [Chore]: qwen3-moe-type-hints-mistake by @Xerxes-cn in https://github.com/vllm-project/vllm/pull/19860
* [Bugfix] Enable PP with AITER+V1 by @qli88 in https://github.com/vllm-project/vllm/pull/19822
* [Bugfix][Ray] Set the cuda context eagerly in the ray worker  by @kouroshHakha in https://github.com/vllm-project/vllm/pull/19583
* [Misc] update cuda version by @reidliu41 in https://github.com/vllm-project/vllm/pull/19526
* [Misc] refactor example - openai_transcription_client by @reidliu41 in https://github.com/vllm-project/vllm/pull/19851
* [Kernel] correct cpu worker function parameter type by @andyxning in https://github.com/vllm-project/vllm/pull/19745
* [Fix] import regex instead of re by @tdoublep in https://github.com/vllm-project/vllm/pull/19875
* [Model] GPT2ForSequenceClassification model by @nie3e in https://github.com/vllm-project/vllm/pull/19663
* [custom_op][vllm-plugin] update custom_op class to use op_registry by @xuechendi in https://github.com/vllm-project/vllm/pull/19164
* Export NaNs in logits to scheduler_stats if output is corrupted by @vladmihailescu in https://github.com/vllm-project/vllm/pull/18777
* [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19901
* [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError by @andyxning in https://github.com/vllm-project/vllm/pull/19749
* [Misc] Clean up useless code by @wangxiyuan in https://github.com/vllm-project/vllm/pull/19889
* Fix: Check the type of params to be a Sequence not list. by @rabinadk1 in https://github.com/vllm-project/vllm/pull/19910
* [Bugfix] Fix bnb 8bit model weights loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/19917
* [New model support]Support Tarsier2 by @princepride in https://github.com/vllm-project/vllm/pull/19887
* [doc] add contact us in community by @reidliu41 in https://github.com/vllm-project/vllm/pull/19922
* [Multimodal] Optimize Qwen2/2.5-VL startup time by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19756
* [Docs] Add GPT2ForSequenceClassification to supported models in docs by @nie3e in https://github.com/vllm-project/vllm/pull/19932
* [Misc] add vllm_config in __init__ by @andyxning in https://github.com/vllm-project/vllm/pull/19866
* [MISC] add cpu_kvcache_space_bytes to CacheConfig by @andyxning in https://github.com/vllm-project/vllm/pull/19812
* [Benchmark] fix request loss if "ping" is returned by @sywangyi in https://github.com/vllm-project/vllm/pull/19535
* [CI/Build] Auto tag perf benchmarks related PRs by @22quinn in https://github.com/vllm-project/vllm/pull/19943
* [doc] use snippets for contact us by @reidliu41 in https://github.com/vllm-project/vllm/pull/19944
* [Misc] Update model-specific PR tagging by @ywang96 in https://github.com/vllm-project/vllm/pull/19949
* [Misc] Simplify vllm bench cli subcommand implementation by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19948
* [Chore] dedup logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19955
* [BugFix] Add an env to disable moe chunking to work around compile incompatibility by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19642
* [Perf][CLI] Improve overall startup time by @aarnphm in https://github.com/vllm-project/vllm/pull/19941
* [Core] feat: Implement Priority Scheduling in V1 Engine by @amitm02 in https://github.com/vllm-project/vllm/pull/19057
* [Misc] Configurable timeout for execute_model RPC calls via env var by @jinqinn in https://github.com/vllm-project/vllm/pull/19544
* Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor by @Flink-ddd in https://github.com/vllm-project/vllm/pull/19643
* [doc] Fold long code blocks to improve readability by @reidliu41 in https://github.com/vllm-project/vllm/pull/19926
* [P/D][NixlConnector] Support `tp_size > num_kv_heads` deployments by @NickLucche in https://github.com/vllm-project/vllm/pull/19691
* [BugFix][P/D] Fix for cases where _recving_transfers can be cleaned up when *all* transfer done by @lk-chen in https://github.com/vllm-project/vllm/pull/19874
* [Doc] Update V1 status for decoder-only embedding models by @Isotr0py in https://github.com/vllm-project/vllm/pull/19952
* [doc] use MkDocs collapsible blocks - supplement by @reidliu41 in https://github.com/vllm-project/vllm/pull/19973
* [Bugfix] Fix CI bitsandbytes failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/19969
* [doc] improve readability for long commands by @reidliu41 in https://github.com/vllm-project/vllm/pull/19920
* [Docs] Fix syntax highlighting of shell commands by @lgeiger in https://github.com/vllm-project/vllm/pull/19870
* [EP+DP] Optimize the little operations in the DeepGEMM + DeepEP low latency case by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19885
* [Bugfix][v1] Fix step pooler implementation and step pooling usage in v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/19956
* [Misc] Add type alias `ReqId` and `EngineId` for better readability by @lk-chen in https://github.com/vllm-project/vllm/pull/19880
* [Feature] Support sequence parallelism for static fp8 quantization by @cascade812 in https://github.com/vllm-project/vllm/pull/19181
* [CI/Build] Push latest tag for cpu and neuron docker image by @22quinn in https://github.com/vllm-project/vllm/pull/19897
* Feat Dynamic Quantization for MoE Layers in GPTQ Marlin Backend by @Jun-Howie in https://github.com/vllm-project/vllm/pull/19395
* [Bugfix][Benchmark] Fix Marlin benchmark by @22quinn in https://github.com/vllm-project/vllm/pull/19929
* [TPU] Fix tpu model runner test by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19995
* Update test case parameter to have the throughput above 8.0 by @QiliangCui in https://github.com/vllm-project/vllm/pull/19994
* [Misc][Tools][Benchmark] Add profile to autotune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19711
* [doc] Fix broken link in the installation for CPU by @yankay in https://github.com/vllm-project/vllm/pull/19980
* add some examples for other benchmark scripts by @reidliu41 in https://github.com/vllm-project/vllm/pull/19893
* [PERF] Speedup of MRoPE prepare inputs by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/19939
* [Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20014
* refactor example - qwen3_reranker by @reidliu41 in https://github.com/vllm-project/vllm/pull/19847
* [Fix][V1] Remove --scheduling-policy oracle by @amitm02 in https://github.com/vllm-project/vllm/pull/20010
* [Perf] Improve/Fix-regression for FA3 in High QPS regimes by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/19463
* [Misc][Benchmarking] Add variable request-rate ("ramp-up") to the benchmarking client. by @dtransposed in https://github.com/vllm-project/vllm/pull/19423
* [BugFix] Fix multi-node offline data parallel by @njhill in https://github.com/vllm-project/vllm/pull/19937
* [P/D] Asynchronously do _nixl_handshake by @lk-chen in https://github.com/vllm-project/vllm/pull/19836
* [Feature] Integrate new deepgemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/19820
* [Easy] Remove submodule added in #19463 by @b8zhong in https://github.com/vllm-project/vllm/pull/20039
* use .dev for version comparison with pytorch nightly release by @BoyuanFeng in https://github.com/vllm-project/vllm/pull/20031
* cmake: Update vllm_flash_attn for vllm_kernels by @seemethere in https://github.com/vllm-project/vllm/pull/20032
* [Llama4] Update `attn_temperature_tuning` by @b8zhong in https://github.com/vllm-project/vllm/pull/19997
* Revert "[Feature] Integrate new deepgemm (#19820)" by @yewentao256 in https://github.com/vllm-project/vllm/pull/20049
* Revert "Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor" by @Isotr0py in https://github.com/vllm-project/vllm/pull/20030
* Move to a faster base64 implementation by @h-avsha in https://github.com/vllm-project/vllm/pull/19984
* [Frontend] speed up import time of vllm.config by @davidxia in https://github.com/vllm-project/vllm/pull/18036
* [Refactor] Remove duplicate `ceil_div` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20023
* [Feat][CLI] enforce-include-usage by @max-wittig in https://github.com/vllm-project/vllm/pull/19695
* [Kernels][Bugfix] Use torch op for all kernels in FusedMoE forward.  Add additional testing for cudagraphs. by @bnellnm in https://github.com/vllm-project/vllm/pull/19717
* [Chore] debloat some initial logs by @aarnphm in https://github.com/vllm-project/vllm/pull/19438
* [BugFix] Fix full-cuda-graph illegal memory access in FA3 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/20057
* [doc] add reference link for Intel XPU by @reidliu41 in https://github.com/vllm-project/vllm/pull/20064
* [Doc] Guide for Incremental Compilation Workflow by @mgoin in https://github.com/vllm-project/vllm/pull/19109
* [V1][Speculative Decoding] Fix DeepSeek MTP by @cjackal in https://github.com/vllm-project/vllm/pull/20022
* [Frontend] Add `/v1/audio/translations` OpenAI API endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/19615
* [Quantization] Add compressed-tensors emulations support for NVFP4 by @dsikka in https://github.com/vllm-project/vllm/pull/19879
* [Fix] Support cls pooling in ModernBertPooler by @lsz05 in https://github.com/vllm-project/vllm/pull/20067
* static_scaled_fp8_quant should not run when scale.numel is not 1 by @eldarkurtic in https://github.com/vllm-project/vllm/pull/20076
* [PD] let toy proxy handle /chat/completions by @lk-chen in https://github.com/vllm-project/vllm/pull/19730
* [Misc] Add parallel state `node_count` function by @njhill in https://github.com/vllm-project/vllm/pull/20045
* Fix the path to the testing script. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20082
* [Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine by @izhuhaoran in https://github.com/vllm-project/vllm/pull/20062
* [TPU][Bugfix] fix kv cache padding by @yaochengji in https://github.com/vllm-project/vllm/pull/20048
* [P/D] Avoid stranding blocks in P when aborted in D's waiting queue by @njhill in https://github.com/vllm-project/vllm/pull/19223
* [TPU] Add TPU specific var VLLM_TPU_MOST_MODEL_LEN by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19919
* [CI] Add SM120 to the Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/19794
* [Bugfix] Fix Mistral tool-parser regex for nested JSON by @mgoin in https://github.com/vllm-project/vllm/pull/20093
* [PD] Skip `tp_size` exchange with rank0 by @NickLucche in https://github.com/vllm-project/vllm/pull/19413
* [Benchmark][Bug] Fix multiple bugs in bench and add args to spec_decode offline by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/20083
* [Bugfix] Allow `CUDA_VISIBLE_DEVICES=''` in `Platform.device_id_to_physical_device_id` by @eicherseiji in https://github.com/vllm-project/vllm/pull/18979
* [Doc] Update docs for New Model Implementation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20115
* [Refactor] Remove unused library by @yewentao256 in https://github.com/vllm-project/vllm/pull/20099
* [CPU] Fix torch version in x86 CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19258
* [Misc] Use collapsible blocks for benchmark examples. by @reidliu41 in https://github.com/vllm-project/vllm/pull/20017
* [Docs] Improve frameworks/helm.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20113
* [Bugfix][V1][ROCm] Fix AITER Flash Attention Backend (Fix API Break and Local Attention Logic: affecting Llama4) by @tjtanaa in https://github.com/vllm-project/vllm/pull/19904
* Revert "[Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine" by @mgoin in https://github.com/vllm-project/vllm/pull/20128
* [Bug Fix] Fix address/port already in use error for pplx test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20094
* [Doc] Automatically signed-off by PyCharm by @noooop in https://github.com/vllm-project/vllm/pull/20120
* [Doc] Auto sign-off for VSCode by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20132
* [Doc] Rename page titles by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20130
* Spam folks if config.py changes by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20131
* [Hardware][Intel GPU] Add v1 Intel GPU support with Flash attention backend. by @jikunshang in https://github.com/vllm-project/vllm/pull/19560
* [TPU] add kv cache update kernel by @yaochengji in https://github.com/vllm-project/vllm/pull/19928
* [Refactor] Rename commnication utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20091
* [Doc] correct LoRA capitalization by @kyolebu in https://github.com/vllm-project/vllm/pull/20135
* [Feature] Expert Parallelism Load Balancer (EPLB) by @abmfy in https://github.com/vllm-project/vllm/pull/18343
* [CI Failure] Fix OOM with test_oot_registration_embedding by @mgoin in https://github.com/vllm-project/vllm/pull/20144
* [Quantization] Bump to use latest `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/20033
* [Perf] SM100 FP8 GEMM Optimizations after cutlass_profiler by @ilmarkov in https://github.com/vllm-project/vllm/pull/20071
* [Bugfix] Build moe_data for both sm100 and sm90 by @mgoin in https://github.com/vllm-project/vllm/pull/20086
* [Feature][Rocm] add quick all reduce for rocm by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/19744
* [CI] Sync test dependency with test.in for torch nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/19632
* [Fix] Fix gemma CI test failing on main by @tdoublep in https://github.com/vllm-project/vllm/pull/20124
* [Model][1/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/20012
* [Perf][Frontend]: eliminate api_key and x_request_id headers middleware overhead by @Yazan-Sharaya in https://github.com/vllm-project/vllm/pull/19946
* Quick Fix by adding conditional import for flash_attn_varlen_func in flash_attn by @xuechendi in https://github.com/vllm-project/vllm/pull/20143
* Gemma3n (Text-only) by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/20134
* [Bugfix] Fix flaky failure when getting DP ports by @mgoin in https://github.com/vllm-project/vllm/pull/20151
* [Perf][Frontend] Cached resolution for resolving chat templates by @ilyal-cerebras in https://github.com/vllm-project/vllm/pull/20065
* [Fix][ROCm] Remove unused variables to fix build error on GFX11/12 by @hyoon1 in https://github.com/vllm-project/vllm/pull/19891
* [Fix][torch.compile] Enable custom ops by default when Inductor off by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20102
* [Bugfix] Mark 'hidden_states' as mutable in moe_forward registration. by @bnellnm in https://github.com/vllm-project/vllm/pull/20152
* [Bugfix] Fix some narrowing conversion warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20141
* [CI/Build] Allow hermetic builds by @fabiendupont in https://github.com/vllm-project/vllm/pull/18064
* [CI Fix] Pin tests/models/registry.py MiniMaxText01ForCausalLM to revision due to model changes by @mgoin in https://github.com/vllm-project/vllm/pull/20199
* [Misc] Add type assertion of request_id for LLMEngine.add_request by @SHA-4096 in https://github.com/vllm-project/vllm/pull/19700
* Fix num_token_padding support for static per-tensor scaled_fp8_quant by @mgoin in https://github.com/vllm-project/vllm/pull/20188
* fix ci issue distributed 4 gpu test by @yewentao256 in https://github.com/vllm-project/vllm/pull/20204
* [Bugfix] Properly reject requests with empty list guided_choice by @mgoin in https://github.com/vllm-project/vllm/pull/20195
* [BugFix] Fix the incorrect func name in the comments. (config.py) by @1195343015 in https://github.com/vllm-project/vllm/pull/20185
* [CI/Build] Add new CI job to validate Hybrid Models for every PR  by @tdoublep in https://github.com/vllm-project/vllm/pull/20147
* [Frontend] Generalize `v1/audio/transcriptions` endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/20179
* [Bugfix] Correct behavior of GraniteMoeHybrid for TensorParallel execution by @s3woz in https://github.com/vllm-project/vllm/pull/20137
* [Refactor] Create a function util and cache the results for `has_deepgemm`, `has_deepep`, `has_pplx` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20187
* [CI Fix] Try fixing eagle e2e test OOM by reducing block allocation by @mgoin in https://github.com/vllm-project/vllm/pull/20213
* [Quantization] Add compressed-tensors NVFP4 MoE Support by @dsikka in https://github.com/vllm-project/vllm/pull/19990
* Fix cuda_archs_loose_intersection when handling sm_*a by @huydhn in https://github.com/vllm-project/vllm/pull/20207
* [Model] support dots1 by @redmoe-moutain in https://github.com/vllm-project/vllm/pull/18254
* [BUGFIX][DEEPSEEK][MODEL_LOAD] fix w13, w2 weight not initialized assert by @xuechendi in https://github.com/vllm-project/vllm/pull/20202
* [Misc] Fix import by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20233
* [doc] Add Slack and Forum to the top navigation by @reidliu41 in https://github.com/vllm-project/vllm/pull/20208
* [Bugfix] Skip loading extra parameters for modelopt Qwen3 MoE model by @noiji in https://github.com/vllm-project/vllm/pull/19598
* [Bugfix] Fix processor initialization in transformers 4.53.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/20244
* [Quantization] Improve BitsAndBytesModelLoader by @jeejeelee in https://github.com/vllm-project/vllm/pull/20242
* [Docs] Fix 1-2-3 list in v1/prefix_caching.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20243
* [Bugfix] fix quark ptpc by @lihaoyang-amd in https://github.com/vllm-project/vllm/pull/20251
* [Spec Decode] Refactor spec decoding into a separate function by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20238
* [Spec Decode] Clean up spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20240
* [Optimization] Use Shared `CachedRequestData` Instance Across All Requests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20232
* [Unit Test] Add unit test for deep gemm by @yewentao256 in https://github.com/vllm-project/vllm/pull/20090
* [Core] [Bugfix] [Multimodal] Fix multimodal profiling and generation for SFT/PTQed models by @kylesayrs in https://github.com/vllm-project/vllm/pull/20058
* [Refactor] Remove useless pdb comment by @yewentao256 in https://github.com/vllm-project/vllm/pull/20266
* [Bugfix][V1][P/D]Fix the issue of occasional garbled output  for P2pNcclConnector by @Abatom in https://github.com/vllm-project/vllm/pull/20263
* [CLI] Improve CLI arg parsing for `-O`/`--compilation-config` by @ProExpertProg in https://github.com/vllm-project/vllm/pull/20156
* [Bugfix] Fix include prompt in stream response when echo=true by @fyuan1316 in https://github.com/vllm-project/vllm/pull/15233
* [Misc] Fix spec decode example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20296
* [Example] add one-click runnable example for P2P NCCL XpYd by @KuntaiDu in https://github.com/vllm-project/vllm/pull/20246
* [CI][Intel Gaudi][vllm-Plugin]Add CI for hpu-plugin-v1-test by @xuechendi in https://github.com/vllm-project/vllm/pull/20196
* [Doc] add config and troubleshooting guide for NCCL & GPUDirect RDMA by @chewong in https://github.com/vllm-project/vllm/pull/15897
* [Feature] A calibration-free RTN-based quantization for accurate and accelerated INT4/INT8 inference by @sakogan in https://github.com/vllm-project/vllm/pull/18768
* [V1] Only print cudagraph tqdm on rank 0 with `is_global_first_rank` by @mgoin in https://github.com/vllm-project/vllm/pull/19516
* Fix `numel()` downcast in vllm/csrc/moe/moe_align_sum_kernels.cu +2 by @r-barnes in https://github.com/vllm-project/vllm/pull/17082
* [Misc] add xgrammar for arm64 by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/18359
* Enable ZP Support for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20268
* [CPU] Update custom ops for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20255
* [Bugfix] Fix deepep tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/20288
* [Misc] remove redundant char by @kebe7jun in https://github.com/vllm-project/vllm/pull/20287
* [BugFix][V1][ROCm] Triton MLA uses V0 backend on V1 engine by @tywuAMD in https://github.com/vllm-project/vllm/pull/19067
* [doc] fix the incorrect logo in dark mode by @reidliu41 in https://github.com/vllm-project/vllm/pull/20289
* [Perf] Validate @config in pre-commit instead of dynamically by @lionelvillard in https://github.com/vllm-project/vllm/pull/20200
* [Quant] [Bugfix] Fix quantization config matching with `hf_to_vllm_mapper` by @kylesayrs in https://github.com/vllm-project/vllm/pull/20046
* [Misc] Minor refactor of NIXL background handshake by @NickLucche in https://github.com/vllm-project/vllm/pull/20068
* Add GLM4.1V model (Draft) by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/19331
* [Model]Add Tencent HunYuanMoEV1 Model Support by @aiyiwang2025 in https://github.com/vllm-project/vllm/pull/20114
* [Misc] Minor refactoring for scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20299
* [Docs] Update transcriptions API to use openai client with `stream=True`  by @NickLucche in https://github.com/vllm-project/vllm/pull/20271
* [CUDA graphs] Enable full cuda graphs with FA3 AoT scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20301
* [Frontend] Expand tools even if tool_choice="none" by @okdshin in https://github.com/vllm-project/vllm/pull/17177
* [V1] [ROCm] Enable EP with AITER Fused MoE by @tjtanaa in https://github.com/vllm-project/vllm/pull/20270
* [Optimization] Cache sampled token ids in model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/20291
* remove unused variables in marlin_template.h by @zhoutianzi666 in https://github.com/vllm-project/vllm/pull/20236
* [Refactor] Refactor import utils by @yewentao256 in https://github.com/vllm-project/vllm/pull/20269
* Enable group size 64 for Machete by @czhu-cohere in https://github.com/vllm-project/vllm/pull/20290
* [Kernel][Bugfix] Fixup some warnings in nvfp4_blockwise_moe when CUDA < 12.8 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20324
* [UT][intel GPU] use current_platform instead of device hardcode in v1 tests by @Liangliang-Ma in https://github.com/vllm-project/vllm/pull/20169
* [Refactor] Remove duplicate `find_free_port` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20333
* [Refactor] Remove Unused Env `VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON` by @yewentao256 in https://github.com/vllm-project/vllm/pull/20334
* [Misc][Doc] Add missing comment for LLM by @draftbk in https://github.com/vllm-project/vllm/pull/20285
* [FIX][Intel GPU]fix ipex flash_attn_varlen_func api missing parameter by @jikunshang in https://github.com/vllm-project/vllm/pull/20348
* [Bugfix] Fix dynamic rotary embedding by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20343
* fix[Docs]: link anchor is incorrect #20309 by @yyzxw in https://github.com/vllm-project/vllm/pull/20315
* [Doc][TPU] Add models and features supporting matrix. by @QiliangCui in https://github.com/vllm-project/vllm/pull/20230
* [TPU] kv cache update kernel supports dynamic grid by @yaochengji in https://github.com/vllm-project/vllm/pull/20235
* [Frontend] Support configurable mm placeholder strings & flexible video sampling policies via CLI flags. by @huachenheli in https://github.com/vllm-project/vllm/pull/20105
* [Model][VLM] Support Keye-VL-8B-Preview by @Kwai-Keye in https://github.com/vllm-project/vllm/pull/20126
* [Bugfix] Keye-VL compatibility with `tok_kwargs` (#20058) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20353
* [Docs] Fix indentations for 2-level items in deprecation_policy.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20352
* [Docs] Make TPU ref prettier in google_tpu.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20356
* [Model] Add Ernie4.5 and Ernie4.5MoE Model Support by @CSWYF3634076 in https://github.com/vllm-project/vllm/pull/20220
* [Build/CI] Automatically tag DeepSeek related PRs by @houseroad in https://github.com/vllm-project/vllm/pull/20370
* [NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120) by @kaln27 in https://github.com/vllm-project/vllm/pull/17280
* [Bugfix] Fix the max_seq_len limit of 16384 for DeepSeek models by @huaqiangwang in https://github.com/vllm-project/vllm/pull/20322
* [Model] Adds support for SlimMoE models Phi-tiny-MoE-instruct by @zichongli5 in https://github.com/vllm-project/vllm/pull/20286
* Documentation update tool_calling: mapping back to function from response by @cronoik-inceptionai in https://github.com/vllm-project/vllm/pull/20373
* [Kernels] MoE refactor by @bnellnm in https://github.com/vllm-project/vllm/pull/19636
* [V1] LogitsProcessor programming model by @afeldman-nm in https://github.com/vllm-project/vllm/pull/16728
* [Minor] Clean up incorrect comment in test by @njhill in https://github.com/vllm-project/vllm/pull/20382
* [Misc] add handler HF_TOKEN is emptry string by @lengrongfu in https://github.com/vllm-project/vllm/pull/20369
* [ROCm][FEAT] Enable Full Graph Mode in AITER MLA V1 Attn Backend (Decode Phase only) by @vllmellm in https://github.com/vllm-project/vllm/pull/20254
* [DP] Support external DP Load Balancer mode by @njhill in https://github.com/vllm-project/vllm/pull/19790
* [Docs] Update EAGLE example by @NickLucche in https://github.com/vllm-project/vllm/pull/20375
* [Bugfix] Fixes for FlashInfer's TORCH_CUDA_ARCH_LIST by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/20136
* [BugFix] Fix DP headless mode arg validation by @njhill in https://github.com/vllm-project/vllm/pull/20398
* Enable CPU nightly performance benchmark and its Markdown report by @louie-tsai in https://github.com/vllm-project/vllm/pull/18444
* [Bugfix] Fix import of CutlassExpertsFp8 in compressed_tensors_moe.py by @bnellnm in https://github.com/vllm-project/vllm/pull/20381
* [Misc] Small: Fix video loader return type annotations. by @huachenheli in https://github.com/vllm-project/vllm/pull/20389
* [Bugfix][CI/CD][CPU] Fix CPU CI tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20383
* [TPU] Add a case to cover RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8 by @QiliangCui in https://github.com/vllm-project/vllm/pull/20385
* [Feature] Support MiniMax-M1 function calls features by @qscqesze in https://github.com/vllm-project/vllm/pull/20297
* [Tests] Update online DP tests to verify that requests are balanced by @njhill in https://github.com/vllm-project/vllm/pull/20157
* [Misc] Add rules to label Speculative Decoding Related PRs by @draftbk in https://github.com/vllm-project/vllm/pull/20406
* [doc] fix link by @reidliu41 in https://github.com/vllm-project/vllm/pull/20417
* [Docs] Replace two list with tables in intel_gaudi.md by @windsonsea in https://github.com/vllm-project/vllm/pull/20414
* [Core] Move multimodal placeholder from chat utils to model definition by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/20355
* [Kernel] refactor cpu worker v0 cache dtype by @andyxning in https://github.com/vllm-project/vllm/pull/20080
* [CI/Build][CPU] Enable cross compilation in CPU release pipeline by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/20423
* [Quantization] Bump to use latest bitsandbytes by @jeejeelee in https://github.com/vllm-project/vllm/pull/20424
* [Model][2/N] Automatic conversion of CrossEncoding model by @noooop in https://github.com/vllm-project/vllm/pull/19978
* [Misc] Automatically tag PRs to add new models by @Isotr0py in https://github.com/vllm-project/vllm/pull/20222
* [Frontend] improve vllm bench <bench_type> --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/20430
* [Bugfix] Fix flaky `test_streaming_response` test by @NickLucche in https://github.com/vllm-project/vllm/pull/20363
* [Frontend] fix duplicate output for bench subcmd by @reidliu41 in https://github.com/vllm-project/vllm/pull/20446
* [CI] Trimming some failing test groups from AMDPRODUCTION. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/20390
* [Misc] Clean up InternVL family config registration by @Isotr0py in https://github.com/vllm-project/vllm/pull/19992
* [Misc] adjust for ipv6 for mookcacke url parse by @andyxning in https://github.com/vllm-project/vllm/pull/20107
* [Misc] Remove _maybe_ignore_quant_config from GLM4.1v by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/20432

## New Contributors
* @py-andy-c made their first contribution in https://github.com/vllm-project/vllm/pull/19399
* @2niuhe made their first contribution in https://github.com/vllm-project/vllm/pull/19394
* @leopardracer made their first contribution in https://github.com/vllm-project/vllm/pull/19442
* @artetaout made their first contribution in https://github.com/vllm-project/vllm/pull/19085
* @runzhen made their first contribution in https://github.com/vllm-project/vllm/pull/19453
* @strutive07 made their first contribution in https://github.com/vllm-project/vllm/pull/19522
* @yewentao256 made their first contribution in https://github.com/vllm-project/vllm/pull/19233
* @mobicham made their first contribution in https://github.com/vllm-project/vllm/pull/19265
* @kouroshHakha made their first contribution in https://github.com/vllm-project/vllm/pull/19378
* @BoyuanFeng made their first contribution in https://github.com/vllm-project/vllm/pull/19587
* @sahelib25 made their first contribution in https://github.com/vllm-project/vllm/pull/18354
* @jiahanc made their first contribution in https://github.com/vllm-project/vllm/pull/19500
* @quanliu1991 made their first contribution in https://github.com/vllm-project/vllm/pull/18957
* @f14-bertolotti made their first contribution in https://github.com/vllm-project/vllm/pull/19564
* @Navanit-git made their first contribution in https://github.com/vllm-project/vllm/pull/19557
* @nguyenhoangthuan99 made their first contribution in https://github.com/vllm-project/vllm/pull/19597
* @diliu0349 made their first contribution in https://github.com/vllm-project/vllm/pull/19600
* @Zzz9990 made their first contribution in https://github.com/vllm-project/vllm/pull/18596
* @yhtang made their first contribution in https://github.com/vllm-project/vllm/pull/19788
* @zsolt-borbely-htec made their first contribution in https://github.com/vllm-project/vllm/pull/19803
* @zuxin666 made their first contribution in https://github.com/vllm-project/vllm/pull/17148
* @NekoMimiUnagi made their first contribution in https://github.com/vllm-project/vllm/pull/19301
* @xzbdmw made their first contribution in https://github.com/vllm-project/vllm/pull/19735
* @Xerxes-cn made their first contribution in https://github.com/vllm-project/vllm/pull/19860
* @nie3e made their first contribution in https://github.com/vllm-project/vllm/pull/19663
* @vladmihailescu made their first contribution in https://github.com/vllm-project/vllm/pull/18777
* @rabinadk1 made their first contribution in https://github.com/vllm-project/vllm/pull/19910
* @amitm02 made their first contribution in https://github.com/vllm-project/vllm/pull/19057
* @jinqinn made their first contribution in https://github.com/vllm-project/vllm/pull/19544
* @Flink-ddd made their first contribution in https://github.com/vllm-project/vllm/pull/19643
* @Jun-Howie made their first contribution in https://github.com/vllm-project/vllm/pull/19395
* @seemethere made their first contribution in https://github.com/vllm-project/vllm/pull/20032
* @h-avsha made their first contribution in https://github.com/vllm-project/vllm/pull/19984
* @max-wittig made their first contribution in https://github.com/vllm-project/vllm/pull/19695
* @lsz05 made their first contribution in https://github.com/vllm-project/vllm/pull/20067
* @kyolebu made their first contribution in https://github.com/vllm-project/vllm/pull/20135
* @lihaoyang-amd made their first contribution in https://github.com/vllm-project/vllm/pull/19744
* @Yazan-Sharaya made their first contribution in https://github.com/vllm-project/vllm/pull/19946
* @ilyal-cerebras made their first contribution in https://github.com/vllm-project/vllm/pull/20065
* @fabiendupont made their first contribution in https://github.com/vllm-project/vllm/pull/18064
* @SHA-4096 made their first contribution in https://github.com/vllm-project/vllm/pull/19700
* @1195343015 made their first contribution in https://github.com/vllm-project/vllm/pull/20185
* @redmoe-moutain made their first contribution in https://github.com/vllm-project/vllm/pull/18254
* @noiji made their first contribution in https://github.com/vllm-project/vllm/pull/19598
* @chewong made their first contribution in https://github.com/vllm-project/vllm/pull/15897
* @sakogan made their first contribution in https://github.com/vllm-project/vllm/pull/18768
* @czhu-cohere made their first contribution in https://github.com/vllm-project/vllm/pull/20268
* @aiyiwang2025 made their first contribution in https://github.com/vllm-project/vllm/pull/20114
* @okdshin made their first contribution in https://github.com/vllm-project/vllm/pull/17177
* @zhoutianzi666 made their first contribution in https://github.com/vllm-project/vllm/pull/20236
* @Liangliang-Ma made their first contribution in https://github.com/vllm-project/vllm/pull/20169
* @yyzxw made their first contribution in https://github.com/vllm-project/vllm/pull/20315
* @Kwai-Keye made their first contribution in https://github.com/vllm-project/vllm/pull/20126
* @CSWYF3634076 made their first contribution in https://github.com/vllm-project/vllm/pull/20220
* @kaln27 made their first contribution in https://github.com/vllm-project/vllm/pull/17280
* @huaqiangwang made their first contribution in https://github.com/vllm-project/vllm/pull/20322
* @zichongli5 made their first contribution in https://github.com/vllm-project/vllm/pull/20286
* @cronoik-inceptionai made their first contribution in https://github.com/vllm-project/vllm/pull/20373

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.1...v0.9.2rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.2rc1)

---

## v0.9.1: v0.9.1
**Published:** 2025-06-10

## Highlights

This release features **274 commits, from 123 contributors (27 new contributors!)**

* Progress in large scale serving
	* DP Attention + Expert Parallelism: CUDA graph support (#18724), DeepEP dispatch-combine kernel (#18434), batched/masked DeepGEMM kernel (#19111), CUTLASS MoE kernel with PPLX (#18762)
	* Heterogeneous TP (#18833), NixlConnector Enable FlashInfer backend (#19090)
	* DP: API-server scaleout with many-to-many server-engine comms (#17546), Support DP with Ray (#18779), allow AsyncLLMEngine.generate to target a specific DP rank (#19102), data parallel rank to KVEventBatch (#18925)
	* Tooling: Simplify EP kernels installation (#19412)
* RLHF workflow: Support inplace model weights loading (#18745)
* Initial full support for Hybrid Memory Allocator (#17996), support cross-layer KV sharing (#18212)
* Add FlexAttention to vLLM V1 (#16078)
* Various production hardening related to full cuda graph mode (#19171, #19106, #19321)

### Model Support
* Support Magistral (#19193), LoRA support for InternVL (#18842), minicpm eagle support (#18943), NemotronH support (#18863, #19249)
* Enable data parallel for Llama4 vision encoder (#18368)
* Add DeepSeek-R1-0528 function call chat template (#18874)

### Hardware Support & Performance Optimizations
* Add H20-3e fused MoE kernel tuning configs for DeepSeek-R1/V3 (#19205), Qwen3-235B-A22B (#19315)
* Blackwell: Add Cutlass MLA backend (#17625), Tunings for SM100 FP8 CUTLASS kernel (#18778), Use FlashInfer by default on Blackwell GPUs (#19118), Tune `scaled_fp8_quant` by increasing vectorization (#18844)
* FP4: Add compressed-tensors NVFP4 support (#18312), FP4 MoE kernel optimization (#19110)
* CPU: V1 support for the CPU backend (#16441)
* ROCm: Add AITER grouped topk for DeepSeekV2 (#18825)
* POWER: Add IBM POWER11 Support to CPU Extension Detection (#19082)
* TPU: Initial support of model parallelism with single worker using SPMD (#18011), Multi-LoRA Optimizations for the V1 TPU backend (#15655)
* Neuron: Add multi-LoRA support for Neuron. (#18284), Add Multi-Modal model support for Neuron (#18921), Support quantization on neuron (#18283)
* Platform: Make torch distributed process group extendable (#18763)

### Engine features
* Add Lora Support to Beam Search (#18346)
* Add rerank support to run_batch endpoint (#16278)
* CLI: add run batch (#18804)
* Server: custom logging (#18403), allowed_token_ids in ChatCompletionRequest (#19143)
* `LLM` API: make use_tqdm accept a callable for custom progress bars (#19357)
* perf: [KERNEL] Sampler. CUDA kernel for applying repetition penalty (#18437)

### API Deprecations
* Disallow pos-args other than `model` when initializing `LLM` (#18802)
* Remove `inputs` arg fallback in Engine classes (#18799)
* Remove fallbacks for Embeddings API (#18795)
* Remove mean pooling default for `Qwen2EmbeddingModel` (#18913)
* Require overriding `get_dummy_text` and `get_dummy_mm_data` (#18796)
* Remove metrics that were deprecated in 0.8 (#18837)

### Documentation
* Add CLI doc (#18871)
* Update SECURITY.md with link to our security guide (#18961), Add security warning to bug report template (#19365)


## What's Changed
* [CI/Build] [TPU] Fix TPU CI exit code by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18282
* [Neuron] Support quantization on neuron by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18283
* Support datasets in `vllm bench serve` and sync with benchmark_[serving,datasets].py by @mgoin in https://github.com/vllm-project/vllm/pull/18566
* [Bugfix] Disable prefix caching by default for benchmark by @cascade812 in https://github.com/vllm-project/vllm/pull/18771
* [Build] Fixes for CMake install by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18570
* [Core] Improve Tensor serialisation by @lgeiger in https://github.com/vllm-project/vllm/pull/18774
* [rocm] Fix wrong attention log by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/18764
* [Bugfix] Fix nomic max_model_len by @noooop in https://github.com/vllm-project/vllm/pull/18755
* [Bugfix]: correctly propagate errors message caught at the chat_templating step to the client by @gcalmettes in https://github.com/vllm-project/vllm/pull/18769
* [V1] fix torch profiling for V1 offline scenarios by @divakar-amd in https://github.com/vllm-project/vllm/pull/18445
* [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (2) by @RonaldBXu in https://github.com/vllm-project/vllm/pull/18781
* [Bugfix][FailingTest]Fix test_model_load_with_params.py by @rabi in https://github.com/vllm-project/vllm/pull/18758
* [Deprecation] Require overriding `get_dummy_text` and `get_dummy_mm_data` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18796
* [Deprecation] Remove unused sync methods in `async_timeout` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18792
* [Deprecation] Remove fallbacks for Embeddings API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18795
* [CI] improve embed testing by @noooop in https://github.com/vllm-project/vllm/pull/18747
* Fix PiecewiseCompileInterpreter by @zou3519 in https://github.com/vllm-project/vllm/pull/17338
* [BugFix] FA2 MLA Accuracy Issue by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/18807
* [Platform][Dist] Make torch distributed process group extendable by @MengqingCao in https://github.com/vllm-project/vllm/pull/18763
* Enable Pydantic mypy checks and convert configs to Pydantic dataclasses by @hmellor in https://github.com/vllm-project/vllm/pull/17599
* [Frontend] add run batch to CLI by @reidliu41 in https://github.com/vllm-project/vllm/pull/18804
* decrement server_load on listen for disconnect by @daniel-salib in https://github.com/vllm-project/vllm/pull/18784
* [Core] Add Lora Support to Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/18346
* [Chore] update ty configuration by @aarnphm in https://github.com/vllm-project/vllm/pull/18839
* [Misc] fix olmoe model layer for TP > 1 by @lengrongfu in https://github.com/vllm-project/vllm/pull/18828
* [V1][Metrics] Remove metrics that were deprecated in 0.8 by @markmc in https://github.com/vllm-project/vllm/pull/18837
* [Chore][Spec Decode] Update check NoneType instead of assigning variables by @aarnphm in https://github.com/vllm-project/vllm/pull/18836
* [Hardware][TPU][V1] Multi-LoRA Optimisations for the V1 TPU backend by @Akshat-Tripathi in https://github.com/vllm-project/vllm/pull/15655
* Remove checks for `None` for fields which should never be `None` by @hmellor in https://github.com/vllm-project/vllm/pull/17985
* [Core] Enable CUDA graphs for DP + All2All kernels  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/18724
* [Bugfix][ROCm] fix the power of 2 exception from triton_unified_attention.py when running llama4 models and unit test fix by @hongxiayang in https://github.com/vllm-project/vllm/pull/18100
* Prevent the cross-encoder logic from being applied to classification tasks by @maxdebayser in https://github.com/vllm-project/vllm/pull/18838
* Add ability to use CUDAGraphs with use_inductor=False by @zou3519 in https://github.com/vllm-project/vllm/pull/17345
* [Bugfix][TPU] fix moe custom kernel import by @yaochengji in https://github.com/vllm-project/vllm/pull/18853
* [Doc][Neuron] Update documentation for Neuron by @elaineyz in https://github.com/vllm-project/vllm/pull/18868
* Skip device and quant Pydantic validation to make plugin device work by @Yikun in https://github.com/vllm-project/vllm/pull/18843
* Fixes a dead link in nightly benchmark readme by @nerdalert in https://github.com/vllm-project/vllm/pull/18856
* [Neuron] Add multi-LoRA support for Neuron. by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18284
* [LoRA] Add LoRA support for InternVL  by @jeejeelee in https://github.com/vllm-project/vllm/pull/18842
* [Doc] Remove redundant spaces from compatibility_matrix.md by @windsonsea in https://github.com/vllm-project/vllm/pull/18891
* [doc] add CLI doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/18871
* [Bugfix] Fix misleading information in the documentation by @jeejeelee in https://github.com/vllm-project/vllm/pull/18845
* [Misc] Replace TODO in serving transcription by @NickLucche in https://github.com/vllm-project/vllm/pull/18895
* [Bugfix] Ensure tensors are contiguous during serialisation by @lgeiger in https://github.com/vllm-project/vllm/pull/18860
* [BugFix] Update pydantic to fix error on python 3.10 by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18852
* Fix an error in dummy weight loading for quantization models by @Chenyaaang in https://github.com/vllm-project/vllm/pull/18855
* [Misc][Tools][Benchmark] Add benchmark_serving supports for llama.cpp.  by @Duyi-Wang in https://github.com/vllm-project/vllm/pull/18692
* [Doc]  Fix codeblocks formatting in LoRA adapters documentation by @Zerohertz in https://github.com/vllm-project/vllm/pull/18907
* [Bugfix] Fix the failing gte embedding test by @Isotr0py in https://github.com/vllm-project/vllm/pull/18720
* [Attention][V1] Toggle for v1 attention backend by @gshtras in https://github.com/vllm-project/vllm/pull/18275
* [ROCm][V0][Attention] Revert to the previous FA triton kernel by @gshtras in https://github.com/vllm-project/vllm/pull/18226
* [Deprecation] Disallow pos-args other than `model` when initializing `LLM` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18802
* [Misc] Remove duplicate init for self.vllm_config by @googs1025 in https://github.com/vllm-project/vllm/pull/18896
* [V1] Allocate kv_cache with stride order for V1 by @NickLucche in https://github.com/vllm-project/vllm/pull/18775
* [BugFix] Make DP work with connector-delayed new requests by @njhill in https://github.com/vllm-project/vllm/pull/18559
* [P/D] NixlConnector DP fixes by @wseaton in https://github.com/vllm-project/vllm/pull/18903
* Use standalone_compile by default in torch >= 2.8.0 by @zou3519 in https://github.com/vllm-project/vllm/pull/18846
* [TPU] remove transpose ops in moe kernel by @yaochengji in https://github.com/vllm-project/vllm/pull/18923
* [Bugfix] Fix PP default fallback behavior for V1 by @mgoin in https://github.com/vllm-project/vllm/pull/18915
* [Misc] Update type annotation for rotary embedding `base` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18914
* [TPU][CI/CD] Clean up docker for TPU tests. by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18926
* improve the robustness of parsing vlms config in AutoRound by @wenhuach21 in https://github.com/vllm-project/vllm/pull/18894
* [Bugfix] Consistent ascii handling in tool parsers by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/18883
* [Model] Use AutoWeightsLoader for mamba2 by @jinyouzhi in https://github.com/vllm-project/vllm/pull/18918
* [docs] fix: fix markdown syntax by @eric-haibin-lin in https://github.com/vllm-project/vllm/pull/18927
* [ROCm] Remove unnecessary assertion of max_model_len in ROCM_AITER_MLA attention backend. by @vllmellm in https://github.com/vllm-project/vllm/pull/18938
* [Bugfix] Remove NVFP4 scales assertions to fix load_format=dummy by @mgoin in https://github.com/vllm-project/vllm/pull/18861
* [Deprecation] Remove mean pooling default for `Qwen2EmbeddingModel` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18913
* [Misc]Fix benchmarks/README.md for speculative decoding by @rabi in https://github.com/vllm-project/vllm/pull/18897
* [doc] add mkdocs doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/18930
* [Model] Use in-place adds in SigLIP by @lgeiger in https://github.com/vllm-project/vllm/pull/18922
* [Bugfix][Failing Test] Fix test_vllm_port.py by @rabi in https://github.com/vllm-project/vllm/pull/18618
* [Misc]Fix typo by @Always-Naive in https://github.com/vllm-project/vllm/pull/18947
* [Bugfix][TPU] Fix tpu model runner testcase failure by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18810
* [CI/Build] remove regex from build dependencies by @dtrifiro in https://github.com/vllm-project/vllm/pull/18945
* [Feature] minicpm eagle support by @huangyuxiang03 in https://github.com/vllm-project/vllm/pull/18943
* [doc] show the count for fork and watch by @reidliu41 in https://github.com/vllm-project/vllm/pull/18950
* [Docs] Update SECURITY.md with link to our security guide by @russellb in https://github.com/vllm-project/vllm/pull/18961
* Improve "failed to get the hash of the compiled graph" error by @zou3519 in https://github.com/vllm-project/vllm/pull/18956
* [Perf] API-server scaleout with many-to-many server-engine comms  by @njhill in https://github.com/vllm-project/vllm/pull/17546
* Benchmark script for fp8 vs bf16 gemm by @mgoin in https://github.com/vllm-project/vllm/pull/17126
* [VLM] Add PP support and fix GPTQ inference for Ovis models by @Isotr0py in https://github.com/vllm-project/vllm/pull/18958
* [Misc] add group_size is -1 in awq quantization by @lengrongfu in https://github.com/vllm-project/vllm/pull/18910
* Tool parser regex timeout handling by @wseaton in https://github.com/vllm-project/vllm/pull/18960
* [Docs] Correct multiprocessing design doc by @lgeiger in https://github.com/vllm-project/vllm/pull/18964
* create util function for batched arange by @yuguo68 in https://github.com/vllm-project/vllm/pull/18937
* [Frontend] Add rerank support to run_batch endpoint by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/16278
* [Misc] Fix estimated max model len msg by @sarckk in https://github.com/vllm-project/vllm/pull/18966
* [Bugfix]: Fix the incompatibility issue with Structured Outputs when Thinking is disabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/18879
* fix security issue of logging llm output by @luccafong in https://github.com/vllm-project/vllm/pull/18980
* [Neuron] Add Multi-Modal model support for Neuron by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18921
* [doc] fix the list rendering issue - security.md by @reidliu41 in https://github.com/vllm-project/vllm/pull/18982
* [BugFix] Pydantic part 2 by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18911
* [FEAT][ROCm] Add AITER grouped topk for DeepSeekV2 by @vllmellm in https://github.com/vllm-project/vllm/pull/18825
* [Bugfix] Fix for issue 17396 by @frreiss in https://github.com/vllm-project/vllm/pull/18773
* [ROCm][Kernel] Add gfx950 support for skinny gemms by @charlifu in https://github.com/vllm-project/vllm/pull/18010
* [P/D] NixlConnector use cache device index for memory registration by @ptarasiewiczNV in https://github.com/vllm-project/vllm/pull/18969
* [BugFix] Fix multi-node offline data-parallel by @njhill in https://github.com/vllm-project/vllm/pull/18981
* [Misc] add return token strs for tokenize by @reidliu41 in https://github.com/vllm-project/vllm/pull/18941
* [Misc][Benchmark] Add support for CustomDataset by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18511
* [Bugfix] Fix EAGLE3 broken logits by @benchislett in https://github.com/vllm-project/vllm/pull/18909
* [Core] Rework dtype resolution by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18751
* [LoRA] Support dynamically initialize `packed_modules_mapping` for VLM with arbitrary components by @Isotr0py in https://github.com/vllm-project/vllm/pull/18987
* [doc] small fix -  mkdocs by @reidliu41 in https://github.com/vllm-project/vllm/pull/18996
* Let max_num_batched_tokens use human_readable_int for large numbers by @mgoin in https://github.com/vllm-project/vllm/pull/18968
* [BugFix] fix data parallel construct ipv6 url addres by @lengrongfu in https://github.com/vllm-project/vllm/pull/18991
* [BugFix] Fix incorrect metrics shutdown error log message by @njhill in https://github.com/vllm-project/vllm/pull/18992
* [doc] wrong output by @reidliu41 in https://github.com/vllm-project/vllm/pull/19000
* [Misc] reuse num_tokens_across_dp of get_dp_padding to avoid unnecessary dp all reduce in set_forward_context by @izhuhaoran in https://github.com/vllm-project/vllm/pull/18935
* [Bugfix][Nixl] Fix DP Metadata Handshake by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19008
* [Core] Support inplace model weights loading by @22quinn in https://github.com/vllm-project/vllm/pull/18745
* [doc] add pytest tips by @reidliu41 in https://github.com/vllm-project/vllm/pull/19010
* [Model] enable data parallel for Llama4 vision encoder by @jennyyyyzhen in https://github.com/vllm-project/vllm/pull/18368
* [Frontend] enable custom logging for the uvicorn server (OpenAI API server) by @fpaupier in https://github.com/vllm-project/vllm/pull/18403
* [Bugfix][Model] Attempt to fix eagle in V0. by @gshtras in https://github.com/vllm-project/vllm/pull/18978
* add an absolute path for run.sh by @calvin0327 in https://github.com/vllm-project/vllm/pull/18258
* [Hardware][TPU] Initial support of model parallelism with single worker using SPMD by @lsy323 in https://github.com/vllm-project/vllm/pull/18011
* [Doc] Remove duplicate TOCs during MkDocs migration by @Zerohertz in https://github.com/vllm-project/vllm/pull/19021
* [Bugfix][EP+DP] Use pplx-kernel internode instead of intranode by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19034
* Adding "LoRA Test %N" to AMD production tests by @Concurrensee in https://github.com/vllm-project/vllm/pull/18929
* [CPU][CI] Re-enable the CPU CI tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19046
* [ROCm][Build] Clean up the ROCm build by @gshtras in https://github.com/vllm-project/vllm/pull/19040
* [V1] Support DP with Ray by @ruisearch42 in https://github.com/vllm-project/vllm/pull/18779
* Add tarsier model support by @princepride in https://github.com/vllm-project/vllm/pull/18985
* [bugfix] small fix logic issue by @reidliu41 in https://github.com/vllm-project/vllm/pull/18999
* Reduce logs in CLI scripts and plugin loader by @mgoin in https://github.com/vllm-project/vllm/pull/18970
* [Bugfix] Use cmake 3.26.1 instead of 3.26 to avoid build failure by @houseroad in https://github.com/vllm-project/vllm/pull/19019
* [v1][KVCacheManager] Rename BlockHashType to BlockHash by @heheda12345 in https://github.com/vllm-project/vllm/pull/19015
* Update docker docs with ARM CUDA cross-compile by @mgoin in https://github.com/vllm-project/vllm/pull/19037
* [Doc] Add InternVL LoRA support  by @jeejeelee in https://github.com/vllm-project/vllm/pull/19055
* [Misc] Update `WeightsMapper` for qwen2-vl/qwen2.5-vl by @Isotr0py in https://github.com/vllm-project/vllm/pull/19054
* [Doc] Update V1 user guide for embedding and enc-dec models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19060
* [doc] clarify windows support by @youkaichao in https://github.com/vllm-project/vllm/pull/19088
* [CI/Build] Remove V0 LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/19066
* Fix underscores in dict keys passed via CLI by @hmellor in https://github.com/vllm-project/vllm/pull/19030
* [Bugfix] disable processor cache  by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/19068
* [Doc] Improve the Pull Request template with key components by @houseroad in https://github.com/vllm-project/vllm/pull/19086
* [Misc] Add missing `_Backend` enums by @NickLucche in https://github.com/vllm-project/vllm/pull/19081
* [Misc] fix: add miss best_of param validation by @googs1025 in https://github.com/vllm-project/vllm/pull/18555
* [Misc] Add SPDX-FileCopyrightText  by @simon-mo in https://github.com/vllm-project/vllm/pull/19100
* [Doc] Readme standardization by @SorenDreano in https://github.com/vllm-project/vllm/pull/18695
* [doc] update docker version by @reidliu41 in https://github.com/vllm-project/vllm/pull/19074
* [Kernel] DeepEP dispatch-combine kernel integration by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/18434
* [V1] Support cross-layer KV sharing by @sarckk in https://github.com/vllm-project/vllm/pull/18212
* [Perf] Tune `scaled_fp8_quant` by increasing vectorization by @mgoin in https://github.com/vllm-project/vllm/pull/18844
* Fix interaction between `Optional` and `Annotated` in CLI typing by @hmellor in https://github.com/vllm-project/vllm/pull/19093
* [v1] Re-init input batch for multiple kv cache groups by @heheda12345 in https://github.com/vllm-project/vllm/pull/18654
* [V1][Spec Decode][Ngram] 1.35x gain -> 1.95x gain on InstructCoder with prompt fix by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18971
* [Bugfix] get_num_blocks_to_allocate with null_block by @heheda12345 in https://github.com/vllm-project/vllm/pull/19031
* [Bugfix]: Fix the incompatibility issue with tool_choice 'required' when Thinking is enabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19075
* [Bugfix][P/D] Fix Prefix Cache Bug by @NickLucche in https://github.com/vllm-project/vllm/pull/18411
* [Bugfix] Max concurrency estimation and check_enough_kv_cache_memory for models with sliding window layers by @heheda12345 in https://github.com/vllm-project/vllm/pull/19029
* feat: add data parallel rank to KVEventBatch by @PeaBrane in https://github.com/vllm-project/vllm/pull/18925
* [Misc] Fix path and python alias errors in disagg_prefill exmaples by @Jeffwan in https://github.com/vllm-project/vllm/pull/18919
* [Docs] Add developer doc about CI failures by @russellb in https://github.com/vllm-project/vllm/pull/18782
* [CPU] V1 support for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/16441
* [Core] Cast multimodal input in hf processor by @lgeiger in https://github.com/vllm-project/vllm/pull/18862
* [KERNEL] Sampler. CUDA kernel for applying repetition penalty by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/18437
* [Cleanup][v1]:remote guided-decoding-backend for example by @calvin0327 in https://github.com/vllm-project/vllm/pull/19059
* [NVIDIA] Add Cutlass MLA backend by @kaixih in https://github.com/vllm-project/vllm/pull/17625
* [Bugfix] Fix FA3 full cuda graph correctness by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19106
* Fix #19130 by @princepride in https://github.com/vllm-project/vllm/pull/19132
* [TPU] Skip hanging tests by @lsy323 in https://github.com/vllm-project/vllm/pull/19115
* Fix ValueError: Missing value for tag key(s): model_name,engine. by @eicherseiji in https://github.com/vllm-project/vllm/pull/19113
* [Misc] Add packages for benchmark as extra dependency by @Isotr0py in https://github.com/vllm-project/vllm/pull/19089
* Improve the output precision of embedding models by @noooop in https://github.com/vllm-project/vllm/pull/19092
* [CI/Build][Bugfix] Ensure compatibility with transformers 4.52 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18678
* Add DeepSeek-R1-0528 function call chat template by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/18874
* Sm100 blockwise fp8 swap ab by @IwakuraRein in https://github.com/vllm-project/vllm/pull/18564
* [Doc] Update V1 Guide for embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19141
* Allow AsyncLLMEngine.generate to target a specific DP rank by @jmswen in https://github.com/vllm-project/vllm/pull/19102
* [Bugfix][EP+DP] Fix internode check by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19112
* [Perf] Tunings for SM100 FP8 CUTLASS kernel by @mgoin in https://github.com/vllm-project/vllm/pull/18778
* [TPU] Update dynamo dump file name in compilation test by @lsy323 in https://github.com/vllm-project/vllm/pull/19108
* [Bugfix] fix v1 cpu worker fails on macOS by @kebe7jun in https://github.com/vllm-project/vllm/pull/19121
* [Kernel] Integrate batched/masked deepgemm kernel by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19111
* [Misc] refactor: simplify EngineCoreClient.make_async_mp_client in AsyncLLM by @googs1025 in https://github.com/vllm-project/vllm/pull/18817
* [P/D] Heterogeneous TP by @NickLucche in https://github.com/vllm-project/vllm/pull/18833
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/19167
* [Bugfix][Nixl] Fix full prefix cache hit bug by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/18632
* [Bugfix] Fix port handling in make_zmq_path by @mgoin in https://github.com/vllm-project/vllm/pull/19117
* [Torch Nightly]add missing dependency by @yangw-dev in https://github.com/vllm-project/vllm/pull/18770
* Handle non-serializable objects when dumping benchmark results by @huydhn in https://github.com/vllm-project/vllm/pull/19114
* [BugFix][Minor] Fix full cuda graph bug when max_num_seqs < 512 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19171
* [Bugfix]: Fix the incompatibility issue with stream when Thinking is disabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19135
* [Build] Annotate wheel and container path for release workflow by @simon-mo in https://github.com/vllm-project/vllm/pull/19162
* [Misc] Remove unnecessary fallback to prefill-decode attention by @vllmellm in https://github.com/vllm-project/vllm/pull/19138
* [Misc] Do not override NCCL_CUMEM_ENABLE if set explicitly by @22quinn in https://github.com/vllm-project/vllm/pull/19105
* [Frontend] improve vllm run-batch --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/19187
* [Bugfix] properly catch PIL-related errors for vision models when incorrect data urls are provided by @gcalmettes in https://github.com/vllm-project/vllm/pull/19202
* [mistral_common] Add v11 tokenizer by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/19193
* Add H20-3e fused MoE kernel tuning configs for DeepSeek-R1/V3 by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19205
* [Hardware][NVIDIA] FP4 MoE kernel optimization by @dubcyfor3 in https://github.com/vllm-project/vllm/pull/19110
* [MISC][Bugfix] Use less CPU when message queue has been empty for some time by @p12tic in https://github.com/vllm-project/vllm/pull/16226
* [P/D][NixlConnector] Enable FlashInfer backend by @NickLucche in https://github.com/vllm-project/vllm/pull/19090
* [Quantization] Skip Fp4 Test for `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/19217
* [V1] Use FlashInfer by default on Blackwell GPUs by @mgoin in https://github.com/vllm-project/vllm/pull/19118
* [Model] NemotronH support by @vegaluisjose in https://github.com/vllm-project/vllm/pull/18863
* Fix AOPerModuleConfig name changes by @jerryzh168 in https://github.com/vllm-project/vllm/pull/18869
* [Bugfix] Fix EAGLE vocab embedding construction for Llama 70B by @benchislett in https://github.com/vllm-project/vllm/pull/19033
* [v1] Hybrid Memory Allocator by @heheda12345 in https://github.com/vllm-project/vllm/pull/17996
* [TPU] update torch_xla pin by @yaochengji in https://github.com/vllm-project/vllm/pull/19231
* Support allowed_token_ids in ChatCompletionRequest by @xu-song in https://github.com/vllm-project/vllm/pull/19143
* [Chore] update CODEOWNERS by @aarnphm in https://github.com/vllm-project/vllm/pull/19247
* [v1][P/D] Fix a edge case in kv cache schedule by @KingsleyZhang123 in https://github.com/vllm-project/vllm/pull/19182
* [TPU] fix kv cache dtype in model runner by @yaochengji in https://github.com/vllm-project/vllm/pull/19244
* [Quantization] Bump compressed-tensors version; update NVFP4A16 test model by @dsikka in https://github.com/vllm-project/vllm/pull/19224
* [Docs] Improve V1 KVConnector interface documentation by @njhill in https://github.com/vllm-project/vllm/pull/19172
* Fix CompilationConfig repr by @zou3519 in https://github.com/vllm-project/vllm/pull/19091
* Unit Test for run_dp_sharded_vision_model by @cryptopic in https://github.com/vllm-project/vllm/pull/19103
* [Model] Optimize nemotron_h implementation by @jeejeelee in https://github.com/vllm-project/vllm/pull/19249
* [Core] Raise when non-multi-instance DP clients target a DP rank by @jmswen in https://github.com/vllm-project/vllm/pull/19227
* improve logits bias by @yuguo68 in https://github.com/vllm-project/vllm/pull/19041
* Fixed ppc build when it runs on non-RHEL based linux distros by @npanpaliya in https://github.com/vllm-project/vllm/pull/18422
* [BugFix] Fix MultiConnector test after HMA changes by @njhill in https://github.com/vllm-project/vllm/pull/19291
* [Bugfix][Core] Update cancellation logic in `generate()` to handle Generator exits by @Adolfo-Karim in https://github.com/vllm-project/vllm/pull/19225
* [Core] Fix abrupt request abort by @NickLucche in https://github.com/vllm-project/vllm/pull/18485
* [BugFix] Fix tpu_model_runner block_id concatenation by @njhill in https://github.com/vllm-project/vllm/pull/19228
* [Misc][Tools][Benchmark] Fix and improve auto tune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19163
* [Build][ROCm] Update Dockerfile.rocm by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/19296
* [Easy][Test] Simplify test_function_tool_use with multiple parametrizes by @houseroad in https://github.com/vllm-project/vllm/pull/19269
* [Kernel] Integrate CUTLASS MoE kernel with PPLX by @ElizaWszola in https://github.com/vllm-project/vllm/pull/18762
* [TPU][Test] Add script to run benchmark on TPU for buildkite by @QiliangCui in https://github.com/vllm-project/vllm/pull/19039
* [CI][PowerPC] Use a more appropriate way to select testcase in tests/models/language/pooling/test_embedding.py by @AaruniAggarwal in https://github.com/vllm-project/vllm/pull/19253
* Add FlexAttention to V1 by @drisspg in https://github.com/vllm-project/vllm/pull/16078
* [Misc] refactor context extension by @reidliu41 in https://github.com/vllm-project/vllm/pull/19246
* [CI/Build] Improve Llama GGUF test robustness by @Isotr0py in https://github.com/vllm-project/vllm/pull/19287
* [Nit][Benchmark]Fix example in benchmark_serving_structured_output.py by @draftbk in https://github.com/vllm-project/vllm/pull/19311
* [AMD] Update compatible packaging version by @pramenku in https://github.com/vllm-project/vllm/pull/19309
* [BugFix][V1] Fix memory profiling bug by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18974
* [Bugfix]: Fix TypeError: 'float' object cannot be interpreted as an integer by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19283
* [Bugfix] Re-enable use_cudagraph in vLLM v1 by @zou3519 in https://github.com/vllm-project/vllm/pull/19299
* [Misc] Change tests/compile to use VLLM_V1 by default by @zou3519 in https://github.com/vllm-project/vllm/pull/19302
* Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19315
* [Hardware][POWER] Add IBM POWER11 Support to CPU Extension Detection by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/19082
* [Quantization] Add compressed-tensors NVFP4 support by @dsikka in https://github.com/vllm-project/vllm/pull/18312
* [Multi Modal] Add an env var for message queue max chunk bytes  by @jennyyyyzhen in https://github.com/vllm-project/vllm/pull/19242
* [Bugfix] model_max_length should consider max_model_len in tokenizer_config by @noooop in https://github.com/vllm-project/vllm/pull/19201
* [Deprecation] Remove `inputs` arg fallback in Engine classes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18799
* [Misc] Add documentation update reminder to PR template by @Isotr0py in https://github.com/vllm-project/vllm/pull/19289
* [Frontend] Remove unreachable code from llm.py by @KsuParkhamchuk in https://github.com/vllm-project/vllm/pull/19288
* [Misc] Cleanup compilation tests by @zou3519 in https://github.com/vllm-project/vllm/pull/19343
* [doc] improve ci doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/19307
* [Doc] Fix description in the Automatic Prefix Caching design doc by @cr7258 in https://github.com/vllm-project/vllm/pull/19333
* [CI/Build] Fix LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/19350
* [Fix] Allow kernel compilation for CUDA capability 8.7 by @conroy-cheers in https://github.com/vllm-project/vllm/pull/19328
* [CI] Introduce rules for llama auto-label by @houseroad in https://github.com/vllm-project/vllm/pull/19323
* [Docs] Fix a bullet list in usage/security.md by @windsonsea in https://github.com/vllm-project/vllm/pull/19358
* [full_graph] Fix query_start_loc padding by @yinghai in https://github.com/vllm-project/vllm/pull/19321
* [v1] Add fp32 support to v1 engine through flex attn by @Isotr0py in https://github.com/vllm-project/vllm/pull/19319
* [Misc] Fixes and Optimizations for DeepEP + DeepGEMM combination. by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19298
* [Bugfix][Core] Prevent token lengths exceeding `max_model_len` in V0 by @22quinn in https://github.com/vllm-project/vllm/pull/19348
* [Quantization] Bump compressed-tensors version by @kylesayrs in https://github.com/vllm-project/vllm/pull/19295
* [Frontend] Make TIMEOUT_KEEP_ALIVE configurable through env var by @liusiqian-tal in https://github.com/vllm-project/vllm/pull/18472
* [TPU]Fix KV cache sharing tests by @lsy323 in https://github.com/vllm-project/vllm/pull/19371
* [HOT-FIX] Add `kv_sharing_target_layer_name` argument to cutlass_mla backend by @pavanimajety in https://github.com/vllm-project/vllm/pull/19374
* [Misc] Fix a config typo in disable_hybrid_kv_cache_manager configuration by @lsy323 in https://github.com/vllm-project/vllm/pull/19383
* [V1] Reuse V0's memory_profiling util for gpu worker memory profiling by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/19312
* [Bugfix] Fix benchmark_moe.py by @gty111 in https://github.com/vllm-project/vllm/pull/19016
* Use xla flag to improve the quantized model performance by @vanbasten23 in https://github.com/vllm-project/vllm/pull/19303
* Fix docs/mkdocs/hooks/remove_announcement.py by @hmellor in https://github.com/vllm-project/vllm/pull/19382
* [Frontend] Make use_tqdm accept a callable for custom progress bars by @reidliu41 in https://github.com/vllm-project/vllm/pull/19357
* [Core] Use tuple for kv cache group block ids by @njhill in https://github.com/vllm-project/vllm/pull/19175
* [Bugfix] Fix modelscope token passed in by @Potabk in https://github.com/vllm-project/vllm/pull/19389
* [Core] Batch multi modal input using pinned memory by @lgeiger in https://github.com/vllm-project/vllm/pull/19169
* Add security warning to bug report template by @russellb in https://github.com/vllm-project/vllm/pull/19365
* [Misc] refactor neuron_multimodal and profiling by @reidliu41 in https://github.com/vllm-project/vllm/pull/19397
* Add clear documentation around the impact of debugging flag by @annapendleton in https://github.com/vllm-project/vllm/pull/19369
* Automatically bind CPU OMP Threads of a rank to CPU ids of a NUMA node. by @louie-tsai in https://github.com/vllm-project/vllm/pull/17930
* Revert "[v1] Add fp32 support to v1 engine through flex attn" by @Isotr0py in https://github.com/vllm-project/vllm/pull/19404
* [BugFix][FlashInfer] Fix attention backend interface mismatch with unexpected keyword `use_irope` by @YUNQIUGUO in https://github.com/vllm-project/vllm/pull/19134
* [BugFix][CPU] Fix CPU CI by ignore collecting test_pixtral by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19411
* Simplify ep kernels installation by @youkaichao in https://github.com/vllm-project/vllm/pull/19412
* [Misc] Slight improvement of the BNB  by @jeejeelee in https://github.com/vllm-project/vllm/pull/19418

## New Contributors
* @nerdalert made their first contribution in https://github.com/vllm-project/vllm/pull/18856
* @Duyi-Wang made their first contribution in https://github.com/vllm-project/vllm/pull/18692
* @jinyouzhi made their first contribution in https://github.com/vllm-project/vllm/pull/18918
* @eric-haibin-lin made their first contribution in https://github.com/vllm-project/vllm/pull/18927
* @Always-Naive made their first contribution in https://github.com/vllm-project/vllm/pull/18947
* @yuguo68 made their first contribution in https://github.com/vllm-project/vllm/pull/18937
* @ptarasiewiczNV made their first contribution in https://github.com/vllm-project/vllm/pull/18969
* @izhuhaoran made their first contribution in https://github.com/vllm-project/vllm/pull/18935
* @jennyyyyzhen made their first contribution in https://github.com/vllm-project/vllm/pull/18368
* @zucchini-nlp made their first contribution in https://github.com/vllm-project/vllm/pull/19068
* @SorenDreano made their first contribution in https://github.com/vllm-project/vllm/pull/18695
* @PeaBrane made their first contribution in https://github.com/vllm-project/vllm/pull/18925
* @jmswen made their first contribution in https://github.com/vllm-project/vllm/pull/19102
* @dubcyfor3 made their first contribution in https://github.com/vllm-project/vllm/pull/19110
* @p12tic made their first contribution in https://github.com/vllm-project/vllm/pull/16226
* @KingsleyZhang123 made their first contribution in https://github.com/vllm-project/vllm/pull/19182
* @cryptopic made their first contribution in https://github.com/vllm-project/vllm/pull/19103
* @Adolfo-Karim made their first contribution in https://github.com/vllm-project/vllm/pull/19225
* @QiliangCui made their first contribution in https://github.com/vllm-project/vllm/pull/19039
* @draftbk made their first contribution in https://github.com/vllm-project/vllm/pull/19311
* @pramenku made their first contribution in https://github.com/vllm-project/vllm/pull/19309
* @KsuParkhamchuk made their first contribution in https://github.com/vllm-project/vllm/pull/19288
* @cr7258 made their first contribution in https://github.com/vllm-project/vllm/pull/19333
* @liusiqian-tal made their first contribution in https://github.com/vllm-project/vllm/pull/18472
* @annapendleton made their first contribution in https://github.com/vllm-project/vllm/pull/19369
* @louie-tsai made their first contribution in https://github.com/vllm-project/vllm/pull/17930
* @YUNQIUGUO made their first contribution in https://github.com/vllm-project/vllm/pull/19134

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.0...v0.9.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.1)

---

## v0.9.1rc1: v0.9.1rc1
**Published:** 2025-06-09
**Pre-release**

## What's Changed
* [CI/Build] [TPU] Fix TPU CI exit code by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18282
* [Neuron] Support quantization on neuron by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18283
* Support datasets in `vllm bench serve` and sync with benchmark_[serving,datasets].py by @mgoin in https://github.com/vllm-project/vllm/pull/18566
* [Bugfix] Disable prefix caching by default for benchmark by @cascade812 in https://github.com/vllm-project/vllm/pull/18771
* [Build] Fixes for CMake install by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18570
* [Core] Improve Tensor serialisation by @lgeiger in https://github.com/vllm-project/vllm/pull/18774
* [rocm] Fix wrong attention log by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/18764
* [Bugfix] Fix nomic max_model_len by @noooop in https://github.com/vllm-project/vllm/pull/18755
* [Bugfix]: correctly propagate errors message caught at the chat_templating step to the client by @gcalmettes in https://github.com/vllm-project/vllm/pull/18769
* [V1] fix torch profiling for V1 offline scenarios by @divakar-amd in https://github.com/vllm-project/vllm/pull/18445
* [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (2) by @RonaldBXu in https://github.com/vllm-project/vllm/pull/18781
* [Bugfix][FailingTest]Fix test_model_load_with_params.py by @rabi in https://github.com/vllm-project/vllm/pull/18758
* [Deprecation] Require overriding `get_dummy_text` and `get_dummy_mm_data` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18796
* [Deprecation] Remove unused sync methods in `async_timeout` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18792
* [Deprecation] Remove fallbacks for Embeddings API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18795
* [CI] improve embed testing by @noooop in https://github.com/vllm-project/vllm/pull/18747
* Fix PiecewiseCompileInterpreter by @zou3519 in https://github.com/vllm-project/vllm/pull/17338
* [BugFix] FA2 MLA Accuracy Issue by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/18807
* [Platform][Dist] Make torch distributed process group extendable by @MengqingCao in https://github.com/vllm-project/vllm/pull/18763
* Enable Pydantic mypy checks and convert configs to Pydantic dataclasses by @hmellor in https://github.com/vllm-project/vllm/pull/17599
* [Frontend] add run batch to CLI by @reidliu41 in https://github.com/vllm-project/vllm/pull/18804
* decrement server_load on listen for disconnect by @daniel-salib in https://github.com/vllm-project/vllm/pull/18784
* [Core] Add Lora Support to Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/18346
* [Chore] update ty configuration by @aarnphm in https://github.com/vllm-project/vllm/pull/18839
* [Misc] fix olmoe model layer for TP > 1 by @lengrongfu in https://github.com/vllm-project/vllm/pull/18828
* [V1][Metrics] Remove metrics that were deprecated in 0.8 by @markmc in https://github.com/vllm-project/vllm/pull/18837
* [Chore][Spec Decode] Update check NoneType instead of assigning variables by @aarnphm in https://github.com/vllm-project/vllm/pull/18836
* [Hardware][TPU][V1] Multi-LoRA Optimisations for the V1 TPU backend by @Akshat-Tripathi in https://github.com/vllm-project/vllm/pull/15655
* Remove checks for `None` for fields which should never be `None` by @hmellor in https://github.com/vllm-project/vllm/pull/17985
* [Core] Enable CUDA graphs for DP + All2All kernels  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/18724
* [Bugfix][ROCm] fix the power of 2 exception from triton_unified_attention.py when running llama4 models and unit test fix by @hongxiayang in https://github.com/vllm-project/vllm/pull/18100
* Prevent the cross-encoder logic from being applied to classification tasks by @maxdebayser in https://github.com/vllm-project/vllm/pull/18838
* Add ability to use CUDAGraphs with use_inductor=False by @zou3519 in https://github.com/vllm-project/vllm/pull/17345
* [Bugfix][TPU] fix moe custom kernel import by @yaochengji in https://github.com/vllm-project/vllm/pull/18853
* [Doc][Neuron] Update documentation for Neuron by @elaineyz in https://github.com/vllm-project/vllm/pull/18868
* Skip device and quant Pydantic validation to make plugin device work by @Yikun in https://github.com/vllm-project/vllm/pull/18843
* Fixes a dead link in nightly benchmark readme by @nerdalert in https://github.com/vllm-project/vllm/pull/18856
* [Neuron] Add multi-LoRA support for Neuron. by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18284
* [LoRA] Add LoRA support for InternVL  by @jeejeelee in https://github.com/vllm-project/vllm/pull/18842
* [Doc] Remove redundant spaces from compatibility_matrix.md by @windsonsea in https://github.com/vllm-project/vllm/pull/18891
* [doc] add CLI doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/18871
* [Bugfix] Fix misleading information in the documentation by @jeejeelee in https://github.com/vllm-project/vllm/pull/18845
* [Misc] Replace TODO in serving transcription by @NickLucche in https://github.com/vllm-project/vllm/pull/18895
* [Bugfix] Ensure tensors are contiguous during serialisation by @lgeiger in https://github.com/vllm-project/vllm/pull/18860
* [BugFix] Update pydantic to fix error on python 3.10 by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18852
* Fix an error in dummy weight loading for quantization models by @Chenyaaang in https://github.com/vllm-project/vllm/pull/18855
* [Misc][Tools][Benchmark] Add benchmark_serving supports for llama.cpp.  by @Duyi-Wang in https://github.com/vllm-project/vllm/pull/18692
* [Doc]  Fix codeblocks formatting in LoRA adapters documentation by @Zerohertz in https://github.com/vllm-project/vllm/pull/18907
* [Bugfix] Fix the failing gte embedding test by @Isotr0py in https://github.com/vllm-project/vllm/pull/18720
* [Attention][V1] Toggle for v1 attention backend by @gshtras in https://github.com/vllm-project/vllm/pull/18275
* [ROCm][V0][Attention] Revert to the previous FA triton kernel by @gshtras in https://github.com/vllm-project/vllm/pull/18226
* [Deprecation] Disallow pos-args other than `model` when initializing `LLM` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18802
* [Misc] Remove duplicate init for self.vllm_config by @googs1025 in https://github.com/vllm-project/vllm/pull/18896
* [V1] Allocate kv_cache with stride order for V1 by @NickLucche in https://github.com/vllm-project/vllm/pull/18775
* [BugFix] Make DP work with connector-delayed new requests by @njhill in https://github.com/vllm-project/vllm/pull/18559
* [P/D] NixlConnector DP fixes by @wseaton in https://github.com/vllm-project/vllm/pull/18903
* Use standalone_compile by default in torch >= 2.8.0 by @zou3519 in https://github.com/vllm-project/vllm/pull/18846
* [TPU] remove transpose ops in moe kernel by @yaochengji in https://github.com/vllm-project/vllm/pull/18923
* [Bugfix] Fix PP default fallback behavior for V1 by @mgoin in https://github.com/vllm-project/vllm/pull/18915
* [Misc] Update type annotation for rotary embedding `base` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18914
* [TPU][CI/CD] Clean up docker for TPU tests. by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18926
* improve the robustness of parsing vlms config in AutoRound by @wenhuach21 in https://github.com/vllm-project/vllm/pull/18894
* [Bugfix] Consistent ascii handling in tool parsers by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/18883
* [Model] Use AutoWeightsLoader for mamba2 by @jinyouzhi in https://github.com/vllm-project/vllm/pull/18918
* [docs] fix: fix markdown syntax by @eric-haibin-lin in https://github.com/vllm-project/vllm/pull/18927
* [ROCm] Remove unnecessary assertion of max_model_len in ROCM_AITER_MLA attention backend. by @vllmellm in https://github.com/vllm-project/vllm/pull/18938
* [Bugfix] Remove NVFP4 scales assertions to fix load_format=dummy by @mgoin in https://github.com/vllm-project/vllm/pull/18861
* [Deprecation] Remove mean pooling default for `Qwen2EmbeddingModel` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18913
* [Misc]Fix benchmarks/README.md for speculative decoding by @rabi in https://github.com/vllm-project/vllm/pull/18897
* [doc] add mkdocs doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/18930
* [Model] Use in-place adds in SigLIP by @lgeiger in https://github.com/vllm-project/vllm/pull/18922
* [Bugfix][Failing Test] Fix test_vllm_port.py by @rabi in https://github.com/vllm-project/vllm/pull/18618
* [Misc]Fix typo by @Always-Naive in https://github.com/vllm-project/vllm/pull/18947
* [Bugfix][TPU] Fix tpu model runner testcase failure by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/18810
* [CI/Build] remove regex from build dependencies by @dtrifiro in https://github.com/vllm-project/vllm/pull/18945
* [Feature] minicpm eagle support by @huangyuxiang03 in https://github.com/vllm-project/vllm/pull/18943
* [doc] show the count for fork and watch by @reidliu41 in https://github.com/vllm-project/vllm/pull/18950
* [Docs] Update SECURITY.md with link to our security guide by @russellb in https://github.com/vllm-project/vllm/pull/18961
* Improve "failed to get the hash of the compiled graph" error by @zou3519 in https://github.com/vllm-project/vllm/pull/18956
* [Perf] API-server scaleout with many-to-many server-engine comms  by @njhill in https://github.com/vllm-project/vllm/pull/17546
* Benchmark script for fp8 vs bf16 gemm by @mgoin in https://github.com/vllm-project/vllm/pull/17126
* [VLM] Add PP support and fix GPTQ inference for Ovis models by @Isotr0py in https://github.com/vllm-project/vllm/pull/18958
* [Misc] add group_size is -1 in awq quantization by @lengrongfu in https://github.com/vllm-project/vllm/pull/18910
* Tool parser regex timeout handling by @wseaton in https://github.com/vllm-project/vllm/pull/18960
* [Docs] Correct multiprocessing design doc by @lgeiger in https://github.com/vllm-project/vllm/pull/18964
* create util function for batched arange by @yuguo68 in https://github.com/vllm-project/vllm/pull/18937
* [Frontend] Add rerank support to run_batch endpoint by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/16278
* [Misc] Fix estimated max model len msg by @sarckk in https://github.com/vllm-project/vllm/pull/18966
* [Bugfix]: Fix the incompatibility issue with Structured Outputs when Thinking is disabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/18879
* fix security issue of logging llm output by @luccafong in https://github.com/vllm-project/vllm/pull/18980
* [Neuron] Add Multi-Modal model support for Neuron by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18921
* [doc] fix the list rendering issue - security.md by @reidliu41 in https://github.com/vllm-project/vllm/pull/18982
* [BugFix] Pydantic part 2 by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18911
* [FEAT][ROCm] Add AITER grouped topk for DeepSeekV2 by @vllmellm in https://github.com/vllm-project/vllm/pull/18825
* [Bugfix] Fix for issue 17396 by @frreiss in https://github.com/vllm-project/vllm/pull/18773
* [ROCm][Kernel] Add gfx950 support for skinny gemms by @charlifu in https://github.com/vllm-project/vllm/pull/18010
* [P/D] NixlConnector use cache device index for memory registration by @ptarasiewiczNV in https://github.com/vllm-project/vllm/pull/18969
* [BugFix] Fix multi-node offline data-parallel by @njhill in https://github.com/vllm-project/vllm/pull/18981
* [Misc] add return token strs for tokenize by @reidliu41 in https://github.com/vllm-project/vllm/pull/18941
* [Misc][Benchmark] Add support for CustomDataset by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18511
* [Bugfix] Fix EAGLE3 broken logits by @benchislett in https://github.com/vllm-project/vllm/pull/18909
* [Core] Rework dtype resolution by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18751
* [LoRA] Support dynamically initialize `packed_modules_mapping` for VLM with arbitrary components by @Isotr0py in https://github.com/vllm-project/vllm/pull/18987
* [doc] small fix -  mkdocs by @reidliu41 in https://github.com/vllm-project/vllm/pull/18996
* Let max_num_batched_tokens use human_readable_int for large numbers by @mgoin in https://github.com/vllm-project/vllm/pull/18968
* [BugFix] fix data parallel construct ipv6 url addres by @lengrongfu in https://github.com/vllm-project/vllm/pull/18991
* [BugFix] Fix incorrect metrics shutdown error log message by @njhill in https://github.com/vllm-project/vllm/pull/18992
* [doc] wrong output by @reidliu41 in https://github.com/vllm-project/vllm/pull/19000
* [Misc] reuse num_tokens_across_dp of get_dp_padding to avoid unnecessary dp all reduce in set_forward_context by @izhuhaoran in https://github.com/vllm-project/vllm/pull/18935
* [Bugfix][Nixl] Fix DP Metadata Handshake by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/19008
* [Core] Support inplace model weights loading by @22quinn in https://github.com/vllm-project/vllm/pull/18745
* [doc] add pytest tips by @reidliu41 in https://github.com/vllm-project/vllm/pull/19010
* [Model] enable data parallel for Llama4 vision encoder by @jennyyyyzhen in https://github.com/vllm-project/vllm/pull/18368
* [Frontend] enable custom logging for the uvicorn server (OpenAI API server) by @fpaupier in https://github.com/vllm-project/vllm/pull/18403
* [Bugfix][Model] Attempt to fix eagle in V0. by @gshtras in https://github.com/vllm-project/vllm/pull/18978
* add an absolute path for run.sh by @calvin0327 in https://github.com/vllm-project/vllm/pull/18258
* [Hardware][TPU] Initial support of model parallelism with single worker using SPMD by @lsy323 in https://github.com/vllm-project/vllm/pull/18011
* [Doc] Remove duplicate TOCs during MkDocs migration by @Zerohertz in https://github.com/vllm-project/vllm/pull/19021
* [Bugfix][EP+DP] Use pplx-kernel internode instead of intranode by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19034
* Adding "LoRA Test %N" to AMD production tests by @Concurrensee in https://github.com/vllm-project/vllm/pull/18929
* [CPU][CI] Re-enable the CPU CI tests by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/19046
* [ROCm][Build] Clean up the ROCm build by @gshtras in https://github.com/vllm-project/vllm/pull/19040
* [V1] Support DP with Ray by @ruisearch42 in https://github.com/vllm-project/vllm/pull/18779
* Add tarsier model support by @princepride in https://github.com/vllm-project/vllm/pull/18985
* [bugfix] small fix logic issue by @reidliu41 in https://github.com/vllm-project/vllm/pull/18999
* Reduce logs in CLI scripts and plugin loader by @mgoin in https://github.com/vllm-project/vllm/pull/18970
* [Bugfix] Use cmake 3.26.1 instead of 3.26 to avoid build failure by @houseroad in https://github.com/vllm-project/vllm/pull/19019
* [v1][KVCacheManager] Rename BlockHashType to BlockHash by @heheda12345 in https://github.com/vllm-project/vllm/pull/19015
* Update docker docs with ARM CUDA cross-compile by @mgoin in https://github.com/vllm-project/vllm/pull/19037
* [Doc] Add InternVL LoRA support  by @jeejeelee in https://github.com/vllm-project/vllm/pull/19055
* [Misc] Update `WeightsMapper` for qwen2-vl/qwen2.5-vl by @Isotr0py in https://github.com/vllm-project/vllm/pull/19054
* [Doc] Update V1 user guide for embedding and enc-dec models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19060
* [doc] clarify windows support by @youkaichao in https://github.com/vllm-project/vllm/pull/19088
* [CI/Build] Remove V0 LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/19066
* Fix underscores in dict keys passed via CLI by @hmellor in https://github.com/vllm-project/vllm/pull/19030
* [Bugfix] disable processor cache  by @zucchini-nlp in https://github.com/vllm-project/vllm/pull/19068
* [Doc] Improve the Pull Request template with key components by @houseroad in https://github.com/vllm-project/vllm/pull/19086
* [Misc] Add missing `_Backend` enums by @NickLucche in https://github.com/vllm-project/vllm/pull/19081
* [Misc] fix: add miss best_of param validation by @googs1025 in https://github.com/vllm-project/vllm/pull/18555
* [Misc] Add SPDX-FileCopyrightText  by @simon-mo in https://github.com/vllm-project/vllm/pull/19100
* [Doc] Readme standardization by @SorenDreano in https://github.com/vllm-project/vllm/pull/18695
* [doc] update docker version by @reidliu41 in https://github.com/vllm-project/vllm/pull/19074
* [Kernel] DeepEP dispatch-combine kernel integration by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/18434
* [V1] Support cross-layer KV sharing by @sarckk in https://github.com/vllm-project/vllm/pull/18212
* [Perf] Tune `scaled_fp8_quant` by increasing vectorization by @mgoin in https://github.com/vllm-project/vllm/pull/18844
* Fix interaction between `Optional` and `Annotated` in CLI typing by @hmellor in https://github.com/vllm-project/vllm/pull/19093
* [v1] Re-init input batch for multiple kv cache groups by @heheda12345 in https://github.com/vllm-project/vllm/pull/18654
* [V1][Spec Decode][Ngram] 1.35x gain -> 1.95x gain on InstructCoder with prompt fix by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18971
* [Bugfix] get_num_blocks_to_allocate with null_block by @heheda12345 in https://github.com/vllm-project/vllm/pull/19031
* [Bugfix]: Fix the incompatibility issue with tool_choice 'required' when Thinking is enabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19075
* [Bugfix][P/D] Fix Prefix Cache Bug by @NickLucche in https://github.com/vllm-project/vllm/pull/18411
* [Bugfix] Max concurrency estimation and check_enough_kv_cache_memory for models with sliding window layers by @heheda12345 in https://github.com/vllm-project/vllm/pull/19029
* feat: add data parallel rank to KVEventBatch by @PeaBrane in https://github.com/vllm-project/vllm/pull/18925
* [Misc] Fix path and python alias errors in disagg_prefill exmaples by @Jeffwan in https://github.com/vllm-project/vllm/pull/18919
* [Docs] Add developer doc about CI failures by @russellb in https://github.com/vllm-project/vllm/pull/18782
* [CPU] V1 support for the CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/16441
* [Core] Cast multimodal input in hf processor by @lgeiger in https://github.com/vllm-project/vllm/pull/18862
* [KERNEL] Sampler. CUDA kernel for applying repetition penalty by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/18437
* [Cleanup][v1]:remote guided-decoding-backend for example by @calvin0327 in https://github.com/vllm-project/vllm/pull/19059
* [NVIDIA] Add Cutlass MLA backend by @kaixih in https://github.com/vllm-project/vllm/pull/17625
* [Bugfix] Fix FA3 full cuda graph correctness by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19106
* Fix #19130 by @princepride in https://github.com/vllm-project/vllm/pull/19132
* [TPU] Skip hanging tests by @lsy323 in https://github.com/vllm-project/vllm/pull/19115
* Fix ValueError: Missing value for tag key(s): model_name,engine. by @eicherseiji in https://github.com/vllm-project/vllm/pull/19113
* [Misc] Add packages for benchmark as extra dependency by @Isotr0py in https://github.com/vllm-project/vllm/pull/19089
* Improve the output precision of embedding models by @noooop in https://github.com/vllm-project/vllm/pull/19092
* [CI/Build][Bugfix] Ensure compatibility with transformers 4.52 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18678
* Add DeepSeek-R1-0528 function call chat template by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/18874
* Sm100 blockwise fp8 swap ab by @IwakuraRein in https://github.com/vllm-project/vllm/pull/18564
* [Doc] Update V1 Guide for embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/19141
* Allow AsyncLLMEngine.generate to target a specific DP rank by @jmswen in https://github.com/vllm-project/vllm/pull/19102
* [Bugfix][EP+DP] Fix internode check by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/19112
* [Perf] Tunings for SM100 FP8 CUTLASS kernel by @mgoin in https://github.com/vllm-project/vllm/pull/18778
* [TPU] Update dynamo dump file name in compilation test by @lsy323 in https://github.com/vllm-project/vllm/pull/19108
* [Bugfix] fix v1 cpu worker fails on macOS by @kebe7jun in https://github.com/vllm-project/vllm/pull/19121
* [Kernel] Integrate batched/masked deepgemm kernel by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19111
* [Misc] refactor: simplify EngineCoreClient.make_async_mp_client in AsyncLLM by @googs1025 in https://github.com/vllm-project/vllm/pull/18817
* [P/D] Heterogeneous TP by @NickLucche in https://github.com/vllm-project/vllm/pull/18833
* [doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/19167
* [Bugfix][Nixl] Fix full prefix cache hit bug by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/18632
* [Bugfix] Fix port handling in make_zmq_path by @mgoin in https://github.com/vllm-project/vllm/pull/19117
* [Torch Nightly]add missing dependency by @yangw-dev in https://github.com/vllm-project/vllm/pull/18770
* Handle non-serializable objects when dumping benchmark results by @huydhn in https://github.com/vllm-project/vllm/pull/19114
* [BugFix][Minor] Fix full cuda graph bug when max_num_seqs < 512 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/19171
* [Bugfix]: Fix the incompatibility issue with stream when Thinking is disabled by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19135
* [Build] Annotate wheel and container path for release workflow by @simon-mo in https://github.com/vllm-project/vllm/pull/19162
* [Misc] Remove unnecessary fallback to prefill-decode attention by @vllmellm in https://github.com/vllm-project/vllm/pull/19138
* [Misc] Do not override NCCL_CUMEM_ENABLE if set explicitly by @22quinn in https://github.com/vllm-project/vllm/pull/19105
* [Frontend] improve vllm run-batch --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/19187
* [Bugfix] properly catch PIL-related errors for vision models when incorrect data urls are provided by @gcalmettes in https://github.com/vllm-project/vllm/pull/19202
* [mistral_common] Add v11 tokenizer by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/19193
* Add H20-3e fused MoE kernel tuning configs for DeepSeek-R1/V3 by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19205
* [Hardware][NVIDIA] FP4 MoE kernel optimization by @dubcyfor3 in https://github.com/vllm-project/vllm/pull/19110
* [MISC][Bugfix] Use less CPU when message queue has been empty for some time by @p12tic in https://github.com/vllm-project/vllm/pull/16226
* [P/D][NixlConnector] Enable FlashInfer backend by @NickLucche in https://github.com/vllm-project/vllm/pull/19090
* [Quantization] Skip Fp4 Test for `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/19217
* [V1] Use FlashInfer by default on Blackwell GPUs by @mgoin in https://github.com/vllm-project/vllm/pull/19118
* [Model] NemotronH support by @vegaluisjose in https://github.com/vllm-project/vllm/pull/18863
* Fix AOPerModuleConfig name changes by @jerryzh168 in https://github.com/vllm-project/vllm/pull/18869
* [Bugfix] Fix EAGLE vocab embedding construction for Llama 70B by @benchislett in https://github.com/vllm-project/vllm/pull/19033
* [v1] Hybrid Memory Allocator by @heheda12345 in https://github.com/vllm-project/vllm/pull/17996
* [TPU] update torch_xla pin by @yaochengji in https://github.com/vllm-project/vllm/pull/19231
* Support allowed_token_ids in ChatCompletionRequest by @xu-song in https://github.com/vllm-project/vllm/pull/19143
* [Chore] update CODEOWNERS by @aarnphm in https://github.com/vllm-project/vllm/pull/19247
* [v1][P/D] Fix a edge case in kv cache schedule by @KingsleyZhang123 in https://github.com/vllm-project/vllm/pull/19182
* [TPU] fix kv cache dtype in model runner by @yaochengji in https://github.com/vllm-project/vllm/pull/19244
* [Quantization] Bump compressed-tensors version; update NVFP4A16 test model by @dsikka in https://github.com/vllm-project/vllm/pull/19224
* [Docs] Improve V1 KVConnector interface documentation by @njhill in https://github.com/vllm-project/vllm/pull/19172
* Fix CompilationConfig repr by @zou3519 in https://github.com/vllm-project/vllm/pull/19091
* Unit Test for run_dp_sharded_vision_model by @cryptopic in https://github.com/vllm-project/vllm/pull/19103
* [Model] Optimize nemotron_h implementation by @jeejeelee in https://github.com/vllm-project/vllm/pull/19249
* [Core] Raise when non-multi-instance DP clients target a DP rank by @jmswen in https://github.com/vllm-project/vllm/pull/19227
* improve logits bias by @yuguo68 in https://github.com/vllm-project/vllm/pull/19041
* Fixed ppc build when it runs on non-RHEL based linux distros by @npanpaliya in https://github.com/vllm-project/vllm/pull/18422
* [BugFix] Fix MultiConnector test after HMA changes by @njhill in https://github.com/vllm-project/vllm/pull/19291
* [Bugfix][Core] Update cancellation logic in `generate()` to handle Generator exits by @Adolfo-Karim in https://github.com/vllm-project/vllm/pull/19225
* [Core] Fix abrupt request abort by @NickLucche in https://github.com/vllm-project/vllm/pull/18485
* [BugFix] Fix tpu_model_runner block_id concatenation by @njhill in https://github.com/vllm-project/vllm/pull/19228
* [Misc][Tools][Benchmark] Fix and improve auto tune script by @Chenyaaang in https://github.com/vllm-project/vllm/pull/19163
* [Build][ROCm] Update Dockerfile.rocm by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/19296
* [Easy][Test] Simplify test_function_tool_use with multiple parametrizes by @houseroad in https://github.com/vllm-project/vllm/pull/19269
* [Kernel] Integrate CUTLASS MoE kernel with PPLX by @ElizaWszola in https://github.com/vllm-project/vllm/pull/18762
* [TPU][Test] Add script to run benchmark on TPU for buildkite by @QiliangCui in https://github.com/vllm-project/vllm/pull/19039
* [CI][PowerPC] Use a more appropriate way to select testcase in tests/models/language/pooling/test_embedding.py by @AaruniAggarwal in https://github.com/vllm-project/vllm/pull/19253
* Add FlexAttention to V1 by @drisspg in https://github.com/vllm-project/vllm/pull/16078
* [Misc] refactor context extension by @reidliu41 in https://github.com/vllm-project/vllm/pull/19246
* [CI/Build] Improve Llama GGUF test robustness by @Isotr0py in https://github.com/vllm-project/vllm/pull/19287
* [Nit][Benchmark]Fix example in benchmark_serving_structured_output.py by @draftbk in https://github.com/vllm-project/vllm/pull/19311
* [AMD] Update compatible packaging version by @pramenku in https://github.com/vllm-project/vllm/pull/19309
* [BugFix][V1] Fix memory profiling bug by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18974
* [Bugfix]: Fix TypeError: 'float' object cannot be interpreted as an integer by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/19283
* [Bugfix] Re-enable use_cudagraph in vLLM v1 by @zou3519 in https://github.com/vllm-project/vllm/pull/19299
* [Misc] Change tests/compile to use VLLM_V1 by default by @zou3519 in https://github.com/vllm-project/vllm/pull/19302
* Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/19315
* [Hardware][POWER] Add IBM POWER11 Support to CPU Extension Detection by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/19082
* [Quantization] Add compressed-tensors NVFP4 support by @dsikka in https://github.com/vllm-project/vllm/pull/18312
* [Multi Modal] Add an env var for message queue max chunk bytes  by @jennyyyyzhen in https://github.com/vllm-project/vllm/pull/19242
* [Bugfix] model_max_length should consider max_model_len in tokenizer_config by @noooop in https://github.com/vllm-project/vllm/pull/19201
* [Deprecation] Remove `inputs` arg fallback in Engine classes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18799
* [Misc] Add documentation update reminder to PR template by @Isotr0py in https://github.com/vllm-project/vllm/pull/19289
* [Frontend] Remove unreachable code from llm.py by @KsuParkhamchuk in https://github.com/vllm-project/vllm/pull/19288
* [Misc] Cleanup compilation tests by @zou3519 in https://github.com/vllm-project/vllm/pull/19343
* [doc] improve ci doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/19307
* [Doc] Fix description in the Automatic Prefix Caching design doc by @cr7258 in https://github.com/vllm-project/vllm/pull/19333
* [CI/Build] Fix LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/19350
* [Fix] Allow kernel compilation for CUDA capability 8.7 by @conroy-cheers in https://github.com/vllm-project/vllm/pull/19328
* [CI] Introduce rules for llama auto-label by @houseroad in https://github.com/vllm-project/vllm/pull/19323
* [Docs] Fix a bullet list in usage/security.md by @windsonsea in https://github.com/vllm-project/vllm/pull/19358
* [full_graph] Fix query_start_loc padding by @yinghai in https://github.com/vllm-project/vllm/pull/19321
* [v1] Add fp32 support to v1 engine through flex attn by @Isotr0py in https://github.com/vllm-project/vllm/pull/19319
* [Misc] Fixes and Optimizations for DeepEP + DeepGEMM combination. by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/19298
* [Bugfix][Core] Prevent token lengths exceeding `max_model_len` in V0 by @22quinn in https://github.com/vllm-project/vllm/pull/19348
* [Quantization] Bump compressed-tensors version by @kylesayrs in https://github.com/vllm-project/vllm/pull/19295
* [Frontend] Make TIMEOUT_KEEP_ALIVE configurable through env var by @liusiqian-tal in https://github.com/vllm-project/vllm/pull/18472
* [TPU]Fix KV cache sharing tests by @lsy323 in https://github.com/vllm-project/vllm/pull/19371
* [HOT-FIX] Add `kv_sharing_target_layer_name` argument to cutlass_mla backend by @pavanimajety in https://github.com/vllm-project/vllm/pull/19374
* [Misc] Fix a config typo in disable_hybrid_kv_cache_manager configuration by @lsy323 in https://github.com/vllm-project/vllm/pull/19383

## New Contributors
* @nerdalert made their first contribution in https://github.com/vllm-project/vllm/pull/18856
* @Duyi-Wang made their first contribution in https://github.com/vllm-project/vllm/pull/18692
* @jinyouzhi made their first contribution in https://github.com/vllm-project/vllm/pull/18918
* @eric-haibin-lin made their first contribution in https://github.com/vllm-project/vllm/pull/18927
* @Always-Naive made their first contribution in https://github.com/vllm-project/vllm/pull/18947
* @yuguo68 made their first contribution in https://github.com/vllm-project/vllm/pull/18937
* @ptarasiewiczNV made their first contribution in https://github.com/vllm-project/vllm/pull/18969
* @izhuhaoran made their first contribution in https://github.com/vllm-project/vllm/pull/18935
* @jennyyyyzhen made their first contribution in https://github.com/vllm-project/vllm/pull/18368
* @zucchini-nlp made their first contribution in https://github.com/vllm-project/vllm/pull/19068
* @SorenDreano made their first contribution in https://github.com/vllm-project/vllm/pull/18695
* @PeaBrane made their first contribution in https://github.com/vllm-project/vllm/pull/18925
* @jmswen made their first contribution in https://github.com/vllm-project/vllm/pull/19102
* @dubcyfor3 made their first contribution in https://github.com/vllm-project/vllm/pull/19110
* @p12tic made their first contribution in https://github.com/vllm-project/vllm/pull/16226
* @KingsleyZhang123 made their first contribution in https://github.com/vllm-project/vllm/pull/19182
* @cryptopic made their first contribution in https://github.com/vllm-project/vllm/pull/19103
* @Adolfo-Karim made their first contribution in https://github.com/vllm-project/vllm/pull/19225
* @QiliangCui made their first contribution in https://github.com/vllm-project/vllm/pull/19039
* @draftbk made their first contribution in https://github.com/vllm-project/vllm/pull/19311
* @pramenku made their first contribution in https://github.com/vllm-project/vllm/pull/19309
* @KsuParkhamchuk made their first contribution in https://github.com/vllm-project/vllm/pull/19288
* @cr7258 made their first contribution in https://github.com/vllm-project/vllm/pull/19333
* @liusiqian-tal made their first contribution in https://github.com/vllm-project/vllm/pull/18472

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.0...v0.9.1rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.1rc1)

---

## v0.9.0.1: v0.9.0.1
**Published:** 2025-05-30

This patch release contains important bugfix for DeepSeek family of models on NVIDIA Ampere and below (#18807) 

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.9.0...v0.9.0.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.0.1)

---

## v0.9.0: v0.9.0
**Published:** 2025-05-15

## Highlights
This release features 649 commits, from 215 contributors (82 new contributors!)

* vLLM has upgraded to PyTorch 2.7!  (#16859) This is a breaking change for environment dependency.
	* The default wheel has been upgraded from CUDA 12.4 to CUDA 12.8. We will distribute CUDA 12.6 wheel on GitHub artifact. 
	* As a general rule of thumb, our CUDA version policy follow PyTorch's CUDA version policy. 
* Enhanced NVIDIA Blackwell support. vLLM now ships with initial set of optimized kernels on NVIDIA Blackwell with both attention and mlp. 
	* You can use our docker image or install FlashInfer nightly wheel `pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl` then set `VLLM_ATTENTION_BACKEND=FLASHINFER` for better performance.
	* Upgraded support for the new FlashInfer main branch. (#15777)
	* Please checkout https://github.com/vllm-project/vllm/issues/18153 for the full roadmap 
* Initial DP, EP, PD support for large scale inference
	* EP:
		* Permute and unpermute kernel for moe optimization (#14568)
		* Modularize fused experts and integrate PPLX kernels (#15956)
		* Refactor pplx init logic to make it modular (prepare for deepep) (#18200)
		* Add ep group and all2all interface (#18077)
	* DP:
		* Decouple engine process management and comms (#15977)
	* PD:
		* NIXL Integration (#17751)
		* Local attention optimization for NIXL (#18170)
		* Support multiple kv connectors (#17564)
* Migrate docs from Sphinx to MkDocs (#18145, #18610, #18614, #18616. #18622, #18626, #18627, #18635, #18637, #18657, #18663, #18666, #18713)

### Notable Changes
* Removal of CUDA 12.4 support due to PyTorch upgrade to 2.7. 
* Change `top_k` to be disabled with `0` (still accept `-1` for now) (#17773)
* The seed is now set to `0` by default for V1 Engine, meaning that different vLLM runs now yield the same outputs even if `temperature > 0`. This does not modify the random state in user code since workers are run in separate processes unless `VLLM_USE_V1_MULTIPROCESSING=0`. (#17929, #18741)

### Model Enhancements
* Support MiMo-7B (#17433), MiniMax-VL-01 (#16328), Ovis 1.6 (#17861), Ovis 2 (#15826), GraniteMoeHybrid 4.0 (#17497), FalconH1\* (#18406), LlamaGuard4 (#17315)
  * Please install the development version of `transformers` (from source) to use Falcon-H1.
* Embedding models: nomic-embed-text-v2-moe (#17785), new class of gte models (#17986)
* Progress in Hybrid Memory Allocator (#17394, #17479, #17474, #17483, #17193, #17946, #17945, #17999, #18001, #18593)
* DeepSeek: perf enhancement by moving more calls into cuda-graph region(#17484, #17668), Function Call (#17784), MTP in V1 (#18435)
* Qwen2.5-1M: Implements dual-chunk-flash-attn backend for dual chunk attention with sparse attention support (#11844)
* Qwen2.5-VL speed enhancement via rotary_emb optimization (#17973)
* InternVL models with Qwen2.5 backbone now support video inputs (#18499)

### Performance, Production and Scaling
* Support full cuda graph in v1 (#16072)
* Pipeline Parallelism: MultiprocExecutor support (#14219), `torchrun` (#17827)
* Support sequence parallelism combined with pipeline parallelism (#18243)
* Async tensor parallelism using compilation pass (#17882)
* Perf: Use small max_num_batched_tokens for A100 (#17885)
* Fast Model Loading: Tensorizer support for V1 and LoRA (#17926)
* Multi-modality: Automatically cast multi-modal input dtype before transferring device (#18756)

### Security
* Prevent side-channel attacks via cache salting (#17045)
* Fix image hash collision in certain edge cases (#17378)
* Add `VLLM_ALLOW_INSECURE_SERIALIZATION` env var (#17490)
* Migrate to REGEX Library to prevent catastrophic backtracking (#18454, #18750)

### Features
* CLI: `deprecated=True` (#17426)
* Frontend: progress bar for adding requests (#17525), `chat_template_kwargs` in `LLM.chat` (#17356), `/classify` endpoint (#17032), truncation control for embedding models (#14776), `cached_tokens` in response usage (#18149)
* LoRA: default local directory LoRA resolver plugin. (#16855)
* Metrics: kv event publishing (#16750), API for accessing in-memory Prometheus metrics (#17010)
* Quantization: `nvidia/DeepSeek-R1-FP4` (#16362), Quark MXFP4 format (#16943), AutoRound (#17850), torchao models with `AOPerModuleConfig` (#17826), CUDA Graph support for V1 GGUF support (#18646)
* Reasoning: deprecate `--enable-reasoning` (#17452)
* Spec Decode: EAGLE share input embedding (#17326), torch.compile & cudagraph to EAGLE (#17211), EAGLE3 (#17504), log accumulated metrics(#17913), Medusa (#17956)
* Structured Outputs: Thinking compatibility (#16577), Spec Decoding (#14702), Qwen3 reasoning parser (#17466), `tool_choice: required` for Xgrammar (#17845), Structural Tag with Guidance backend (#17333)
* Transformers backend: named parameters (#16868), interleaved sliding window attention (#18494)


### Hardwares
* NVIDIA: cutlass support for blackwell fp8 blockwise gemm (#14383)
* TPU: Multi-LoRA implementation(#14238), default max-num-batched-tokens (#17508), V1 backend by default (#17673), top-logprobs (#17072)
* Neuron: NeuronxDistributedInference support (#15970), Speculative Decoding, Dynamic on-device sampling (#16357), Mistral Model (#18222), Multi-LoRA (#18284)
* AMD: Enable FP8 KV cache on V1 (#17870), Tuned fused moe config for Qwen3 MoE on MI300X (#17535, #17530), AITER biased group topk (#17955), Block-Scaled GEMM (#14968), MLA (#17523), Radeon GPU use Custom Paged Attention (#17004), reduce the number of environment variables in command line (#17229)
* Extensibility: Make PiecewiseBackend pluggable and extendable (#18076)

### Documentation
* Update quickstart and install for cu128 using `--torch-backend=auto` (#18505)
* NVIDIA TensorRT Model Optimizer (#17561)
* Usage of Qwen3 thinking (#18291)

### Developer Facing 
* Benchmark: Add single turn MTBench to Serving Bench (#17202)
* Usability: Decrease import time of `vllm.multimodal` (#18031)
* Code Format: Code formatting using `ruff format` (#17656, #18068, #18400)
* Readability: 
	* Configuration and arguments unification is now complete! (#17130, #17453, #17562)
	* Update deprecated type hinting from Python 3.7 (#18056, #18130, #18132, #18129, #18073, #18072, #18126, #18128, #18057, #18058)
* Process:
	* Propose a deprecation policy for the project (#17063)
* Testing: expanding torch nightly tests (#18004)


## What's Changed
* Support loading transformers models with named parameters by @wuisawesome in https://github.com/vllm-project/vllm/pull/16868
* Add tuned triton fused_moe configs for Qwen3Moe by @mgoin in https://github.com/vllm-project/vllm/pull/17328
* [Benchmark] Add single turn MTBench to Serving Bench by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/17202
* [Optim] Compute multimodal hash only once per item by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17314
* implement Structural Tag with Guidance backend by @mmoskal in https://github.com/vllm-project/vllm/pull/17333
* [V1][Spec Decode] Make Eagle model arch config driven by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/17323
* [model] make llama4 compatible with pure dense layers by @luccafong in https://github.com/vllm-project/vllm/pull/17315
* [Bugfix] Fix `numel()` downcast in fused_layernorm_dynamic_per_token_quant.cu by @r-barnes in https://github.com/vllm-project/vllm/pull/17316
* Ignore `'<string>'` filepath by @zou3519 in https://github.com/vllm-project/vllm/pull/17330
* [Bugfix] Add contiguous call inside rope kernel wrapper by @timzsu in https://github.com/vllm-project/vllm/pull/17091
* [Misc] Add a Jinja template to support Mistral3 function calling by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17195
* [Model] support MiniMax-VL-01 model by @qscqesze in https://github.com/vllm-project/vllm/pull/16328
* [Misc] Move config fields to MultiModalConfig by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17343
* [Misc]Use a platform independent interface to obtain the device attributes by @ponix-j in https://github.com/vllm-project/vllm/pull/17100
* [Fix] Documentation spacing in compilation config help text by @Zerohertz in https://github.com/vllm-project/vllm/pull/17342
* [Build][Bugfix] Restrict setuptools version to <80 by @gshtras in https://github.com/vllm-project/vllm/pull/17320
* [Model] Ignore rotary embed load for Cohere model by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/17319
* Update docs requirements by @hmellor in https://github.com/vllm-project/vllm/pull/17379
* [Doc] Fix QWen3MOE info by @jeejeelee in https://github.com/vllm-project/vllm/pull/17381
* [Bugfix] Clean up MiniMax-VL and fix processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17354
* `pre-commit autoupdate` by @hmellor in https://github.com/vllm-project/vllm/pull/17380
* [Frontend] Support `chat_template_kwargs` in `LLM.chat` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17356
* Transformers backend tweaks by @hmellor in https://github.com/vllm-project/vllm/pull/17365
* Fix: Spelling of inference by @a2q1p in https://github.com/vllm-project/vllm/pull/17387
* Improve literal dataclass field conversion to argparse argument by @hmellor in https://github.com/vllm-project/vllm/pull/17391
* [V1] Remove num_input_tokens from attn_metadata by @heheda12345 in https://github.com/vllm-project/vllm/pull/17193
* [Bugfix] add qwen3 reasoning-parser fix content is None when disable â€¦ by @mofanke in https://github.com/vllm-project/vllm/pull/17369
* fix gemma3 results all zero by @mayuyuace in https://github.com/vllm-project/vllm/pull/17364
* [Misc][ROCm] Exclude `cutlass_mla_decode` for ROCm build by @tywuAMD in https://github.com/vllm-project/vllm/pull/17289
* Enabling multi-group kernel tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/17115
* [Docs] Propose a deprecation policy for the project by @russellb in https://github.com/vllm-project/vllm/pull/17063
* [Doc][Typo] Fixing label in new model requests link in overview.md by @casinca in https://github.com/vllm-project/vllm/pull/17400
* [TPU][V1][CI] Replace `python3 setup.py develop` with standard `pip install --e` on TPU by @NickLucche in https://github.com/vllm-project/vllm/pull/17374
* [CI] Uses Python 3.11 for TPU by @aarnphm in https://github.com/vllm-project/vllm/pull/17359
* [CI/Build] Add retry mechanism for add-apt-repository by @reidliu41 in https://github.com/vllm-project/vllm/pull/17107
* [Bugfix] Fix Minicpm-O-int4 GPTQ model inference by @Isotr0py in https://github.com/vllm-project/vllm/pull/17397
* Simplify (and fix) passing of guided decoding backend options by @hmellor in https://github.com/vllm-project/vllm/pull/17008
* Remove Falcon3 2x7B from CI by @hmellor in https://github.com/vllm-project/vllm/pull/17404
* Fix: Python package installation for opentelmetry by @dilipgb in https://github.com/vllm-project/vllm/pull/17049
* [V1][Spec Decode] Apply torch.compile & cudagraph to EAGLE by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/17211
* Remove Bamba 9B from CI by @hmellor in https://github.com/vllm-project/vllm/pull/17407
* [V1][Feature] Enable Speculative Decoding with Structured Outputs by @benchislett in https://github.com/vllm-project/vllm/pull/14702
* [release] Always git fetch all to get latest tag on TPU release by @khluu in https://github.com/vllm-project/vllm/pull/17322
* Truncation control for embedding models by @gmarinho2 in https://github.com/vllm-project/vllm/pull/14776
* Update PyTorch to 2.7.0 by @huydhn in https://github.com/vllm-project/vllm/pull/16859
* Improve configs - `ModelConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/17130
* Fix call to `logger.info_once` by @hmellor in https://github.com/vllm-project/vllm/pull/17416
* Fix some speculative decode tests with tl.dot by @huydhn in https://github.com/vllm-project/vllm/pull/17371
* Support LoRA for Mistral3 by @mgoin in https://github.com/vllm-project/vllm/pull/17428
* [Intel GPU] [CI]Fix XPU ci, setuptools >=80.0 have build issue by @jikunshang in https://github.com/vllm-project/vllm/pull/17298
* [Hardware][Intel GPU] Upgrade to torch 2.7 by @jikunshang in https://github.com/vllm-project/vllm/pull/17444
* [Bugfix] Fix AttributeError: 'State' object has no attribute 'engine_client' by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17434
* [MODEL ADDITION] Ovis2 Model Addition by @mlinmg in https://github.com/vllm-project/vllm/pull/15826
* Make the _apply_rotary_emb compatible with dynamo by @houseroad in https://github.com/vllm-project/vllm/pull/17435
* [Misc] Remove deprecated files by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17447
* [V1][Bugfix]: vllm v1 verison metric num_gpu_blocks is None by @lengrongfu in https://github.com/vllm-project/vllm/pull/15755
* [TPU][V1][CI] Update regression test baseline for v6 CI by @NickLucche in https://github.com/vllm-project/vllm/pull/17064
* [Core] Prevent side-channel attacks via cache salting by @dr75 in https://github.com/vllm-project/vllm/pull/17045
* [V1][Metrics] add support for kv event publishing by @alec-flowers in https://github.com/vllm-project/vllm/pull/16750
* [Feature] The Qwen3 reasoning parser supports  guided decoding by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17466
* [Docs] Add command for running mypy tests from CI by @russellb in https://github.com/vllm-project/vllm/pull/17475
* [Fix] Support passing args to logger by @aarnphm in https://github.com/vllm-project/vllm/pull/17425
* [Bugfix] Fixed mistral tokenizer path when pointing to file by @psav in https://github.com/vllm-project/vllm/pull/17457
* [V1] Allow turning off pickle fallback in vllm.v1.serial_utils by @russellb in https://github.com/vllm-project/vllm/pull/17427
* [Docs] Update optimization.md doc by @mgoin in https://github.com/vllm-project/vllm/pull/17482
* [BugFix] Fix authorization of openai_transcription_client.py by @hhy3 in https://github.com/vllm-project/vllm/pull/17321
* [Bugfix][ROCm] Restrict ray version due to a breaking release by @gshtras in https://github.com/vllm-project/vllm/pull/17480
* [doc] add install tips by @reidliu41 in https://github.com/vllm-project/vllm/pull/17373
* doc: fix bug report Github template formatting by @davidxia in https://github.com/vllm-project/vllm/pull/17486
* [v1][Spec Decode] Make sliding window compatible with eagle prefix caching by @heheda12345 in https://github.com/vllm-project/vllm/pull/17398
* Bump Compressed Tensors version to 0.9.4 by @rahul-tuli in https://github.com/vllm-project/vllm/pull/17478
* [Misc] Rename Audios -> Audio in Qwen2audio Processing by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/17507
* [CI][TPU] Skip Multimodal test by @lsy323 in https://github.com/vllm-project/vllm/pull/17488
* [Bugfix][ROCm] Fix import error on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/17495
* [Bugfix] Temporarily disable gptq_bitblas on ROCm by @nlzy in https://github.com/vllm-project/vllm/pull/17411
* [CI][TPU] Skip structured outputs+spec decode tests on TPU by @mgoin in https://github.com/vllm-project/vllm/pull/17510
* [CI][Bugfix] Fix failing V1 Test due to missing 'cache_salt' arg by @mgoin in https://github.com/vllm-project/vllm/pull/17500
* [CI/Build] Reorganize models tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17459
* FIxing the AMD test failures caused by PR#16457 by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/17511
* [Build] Require setuptools >= 77.0.3 for PEP 639 by @russellb in https://github.com/vllm-project/vllm/pull/17389
* [ROCm] Effort to reduce the number of environment variables in command line by @hongxiayang in https://github.com/vllm-project/vllm/pull/17229
* [BugFix] fix speculative decoding memory leak when speculation is disabled by @noyoshi in https://github.com/vllm-project/vllm/pull/15506
* [BugFix] Fix mla cpu - missing 3 required positional arguments by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17494
* Avoid overwriting vllm_compile_cache.py by @youngkent in https://github.com/vllm-project/vllm/pull/17418
* [Core] Enable IPv6 with vllm.utils.make_zmq_socket() by @russellb in https://github.com/vllm-project/vllm/pull/16506
* [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17515
* Improve configs - `ObservabilityConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/17453
* [Bugfix][Benchmarks] Allow benchmark of deepspeed-mii backend to select a model by @tishizaki in https://github.com/vllm-project/vllm/pull/17285
* [Frontend] Show progress bar for adding requests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17525
* [Misc] Clean up test docstrings and names by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17521
* [FEAT] [ROCm]: Add Qwen/Qwen3-30B-A3B-FP8 fused moe config for MI300X by @tjtanaa in https://github.com/vllm-project/vllm/pull/17530
* Fix more broken speculative decode tests by @huydhn in https://github.com/vllm-project/vllm/pull/17450
* [doc] add streamlit integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/17522
* [FEAT] [ROCm]: Add Qwen/Qwen3-235B-A22B-FP8 TP4 triton fused moe config by @tjtanaa in https://github.com/vllm-project/vllm/pull/17535
* [Feature][Frontend]: Deprecate --enable-reasoning by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17452
* [ROCm] remove unsupported archs from rocm triton flash-attention supported list by @hongxiayang in https://github.com/vllm-project/vllm/pull/17536
* [torch.compile] Add torch inductor pass for fusing silu_and_mul with subsequent scaled_fp8_quant operations by @SageMoore in https://github.com/vllm-project/vllm/pull/10867
* [Misc] refactor example - cpu_offload_lmcache by @reidliu41 in https://github.com/vllm-project/vllm/pull/17460
* [CI/Build] Remove `awscli` dependency by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17532
* Move the last arguments in `arg_utils.py` to be in their final groups by @hmellor in https://github.com/vllm-project/vllm/pull/17531
* [Model] Refactor Ovis2 to support original tokenizer by @Isotr0py in https://github.com/vllm-project/vllm/pull/17537
* [ROCm] update installation guide to include build aiter from source instructions by @hongxiayang in https://github.com/vllm-project/vllm/pull/17542
* [Misc]add configurable cuda graph size by @CXIAAAAA in https://github.com/vllm-project/vllm/pull/17201
* [Bugfix] Fix lint error by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17547
* [ROCM] Add gfx950 to the custom attention archs by @jpvillam-amd in https://github.com/vllm-project/vllm/pull/16034
* Remove duplicate code from dbrx.py by @sstamenk in https://github.com/vllm-project/vllm/pull/17550
* [Bug]change the position of cuda_graph_sizes in dataclasses by @CXIAAAAA in https://github.com/vllm-project/vllm/pull/17548
* [Misc][Tools][Benchmark] Publish script to auto tune server parameters by @Chenyaaang in https://github.com/vllm-project/vllm/pull/17207
* [V1][Spec Decode] Apply torch.compile & cudagraph to EAGLE3 by @zixi-qi in https://github.com/vllm-project/vllm/pull/17504
* [Bugfix] Disable gptq_bitblas for <SM80 to fix GPTQ on V100/T4 by @mgoin in https://github.com/vllm-project/vllm/pull/17541
* [Doc] note that not all unit tests pass on CPU platforms by @davidxia in https://github.com/vllm-project/vllm/pull/17554
* [Attention] MLA move o_proj q_proj into cuda-graph region by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17484
* [CI] Actually run tests/kv_transfer/test_disagg.py in CI by @mgoin in https://github.com/vllm-project/vllm/pull/17555
* Check if bitblas is installed during support check by @mgoin in https://github.com/vllm-project/vllm/pull/17572
* [Misc] Continue refactoring model tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17573
* Fix PixtralHF missing spatial_merge_size by @mgoin in https://github.com/vllm-project/vllm/pull/17571
* Add `pt_load_map_location` to allow loading to cuda by @jerryzh168 in https://github.com/vllm-project/vllm/pull/16869
* [Bugifx] Remove TritonPlaceholder from sys.modules by @Isotr0py in https://github.com/vllm-project/vllm/pull/17317
* [Core] [Bugfix] Add Input Embeddings by @qthequartermasterman in https://github.com/vllm-project/vllm/pull/15428
* [BugFix] Fix Memory Leak by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/17567
* [Misc] Rename assets for testing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17575
* add more pytorch related tests for torch nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/17422
* [doc] add the print result by @reidliu41 in https://github.com/vllm-project/vllm/pull/17584
* Automatically tell users that dict args must be valid JSON in CLI by @hmellor in https://github.com/vllm-project/vllm/pull/17577
* [Security] Fix image hash collision by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17378
* Support W8A8 INT8 MoE for compressed-tensors by @mgoin in https://github.com/vllm-project/vllm/pull/16745
* [doc] miss result by @reidliu41 in https://github.com/vllm-project/vllm/pull/17589
* [Misc] Clean up input processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17582
* [Bugfix] fix tmp_out and exp_sums dimensions by @hliuca in https://github.com/vllm-project/vllm/pull/17438
* [BugFix][Attention] Fix sliding window attention in V1 giving incorrect results by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17574
* permute/unpermute kernel for moe optimization by @CalebDu in https://github.com/vllm-project/vllm/pull/14568
* Add NVIDIA TensorRT Model Optimizer in vLLM documentation by @Edwardf0t1 in https://github.com/vllm-project/vllm/pull/17561
* [Hardware][AMD] Improve OAM device ID + llama4 Maverick MOE tuning by @xw285cornell in https://github.com/vllm-project/vllm/pull/16263
* [easy] Print number of needed GPUs in skip message by @zou3519 in https://github.com/vllm-project/vllm/pull/17594
* fix typo in logging by @ehartford in https://github.com/vllm-project/vllm/pull/17605
* [release] Add command to clean up Docker containers/images in TPU release machine by @khluu in https://github.com/vllm-project/vllm/pull/17606
* [Neuron][Build] Require setuptools >= 77.0.3 for PEP 639 by @liangfu in https://github.com/vllm-project/vllm/pull/17603
* Update test requirements to CUDA 12.8 by @22quinn in https://github.com/vllm-project/vllm/pull/17576
* [Quantizaton] [AMD] Add support for running DeepSeek int8 w8a8 MoE on ROCm by @rasmith in https://github.com/vllm-project/vllm/pull/17558
* [Frontend][TPU] Add TPU default max-num-batched-tokens based on device name  by @Chenyaaang in https://github.com/vllm-project/vllm/pull/17508
* [Build/CI] Upgrade CUTLASS to 3.9.1 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/17602
* [Bugfix][ROCm] Using device_type because on ROCm the API is still torch.cuda by @gshtras in https://github.com/vllm-project/vllm/pull/17601
* [Core] Gate `prompt_embeds` behind a feature flag by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17607
* [Bugfix] Fix broken Qwen2.5-omni tests by @Isotr0py in https://github.com/vllm-project/vllm/pull/17613
* [Misc] V0 fallback for `--enable-prompt-embeds` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17615
* Add full API docs and improve the UX of navigating them by @hmellor in https://github.com/vllm-project/vllm/pull/17485
* [Bugfix] Prioritize dtype in root config before checking text config by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17629
* [Bugfix][Easy] Fix whitespace in shm_broadcast.py logging by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/17635
* [Bugfix] fix KeyError on top logprobs are special tokens by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17637
* [Build/CI] Upgrade CUTLASS to 3.9.2 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/17641
* [Kernel] some optimizations for dense marlin and moe marlin by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/16850
* [Doc] Fix broken cuda installation doc rendering by @Isotr0py in https://github.com/vllm-project/vllm/pull/17654
* Use git-path commit in hook by @thomasjpfan in https://github.com/vllm-project/vllm/pull/17616
* [Benchmarks] Remove invalid option under V1 engine by @russellb in https://github.com/vllm-project/vllm/pull/17651
* [BugFix] Increase timeout for startup failure test by @njhill in https://github.com/vllm-project/vllm/pull/17642
* [TPU] Enable gemma3-27b with TP>1 on multi-chips. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/17335
* [TPU][V1] Add support for top-logprobs by @NickLucche in https://github.com/vllm-project/vllm/pull/17072
* [Bugfix] LoRA - Retire unused maxnreg LoRA kernel argument by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/17677
* Update nm to rht in doc links + refine fp8 doc by @mgoin in https://github.com/vllm-project/vllm/pull/17678
* [Model] Add GraniteMoeHybrid 4.0 model by @s3woz in https://github.com/vllm-project/vllm/pull/17497
* [easy] Fix logspam on PiecewiseBackend errors by @zou3519 in https://github.com/vllm-project/vllm/pull/17138
* [Bugfix] Fixed prompt length for random dataset by @Xarbirus in https://github.com/vllm-project/vllm/pull/17408
* [Doc] Update notes for H2O-VL and Gemma3 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17219
* [Misc] Fix ScalarType float4 naming  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17690
* Fix `dockerfilegraph` pre-commit hook by @hmellor in https://github.com/vllm-project/vllm/pull/17698
* [Bugfix] Fix triton import with local TritonPlaceholder by @MengqingCao in https://github.com/vllm-project/vllm/pull/17446
* [V1] Enable TPU V1 backend by default by @mgoin in https://github.com/vllm-project/vllm/pull/17673
* [V1][PP] Support PP for MultiprocExecutor by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/14219
* [v1] AttentionMetadata for each layer by @heheda12345 in https://github.com/vllm-project/vllm/pull/17394
* [Feat] Add deprecated=True to CLI args by @aarnphm in https://github.com/vllm-project/vllm/pull/17426
* [Docs] Use gh-file to add links to tool_calling.md by @windsonsea in https://github.com/vllm-project/vllm/pull/17709
* [v1] Introduce KVCacheBlocks as interface between Scheduler and KVCacheManager by @heheda12345 in https://github.com/vllm-project/vllm/pull/17479
* [doc] Add RAG Integration example by @reidliu41 in https://github.com/vllm-project/vllm/pull/17692
* [Bugfix] Fix modality limits in vision language example by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17721
* Make right sidebar more readable in "Supported Models" by @hmellor in https://github.com/vllm-project/vllm/pull/17723
* [TPU] Increase block size and reset block shapes by @bythew3i in https://github.com/vllm-project/vllm/pull/16458
* [Misc] Add Next Edit Prediction (NEP) datasets support in `benchmark_serving.py` by @dtransposed in https://github.com/vllm-project/vllm/pull/16839
* [Bugfix] Fix for the condition to accept empty encoder inputs for mllama by @gshtras in https://github.com/vllm-project/vllm/pull/17732
* [Kernel] Unified Triton kernel that doesn't distinguish between prefill + decode by @tdoublep in https://github.com/vllm-project/vllm/pull/16828
* Fix doc build performance by @hmellor in https://github.com/vllm-project/vllm/pull/17748
* [ROCm] fix num_stages for default moe config to avoid triton OutOfResource error by @hongxiayang in https://github.com/vllm-project/vllm/pull/17744
* Add logging for torch nightly version by @yangw-dev in https://github.com/vllm-project/vllm/pull/17669
* [Model] Mamba2 causal conv1d Refactor to Split Prefill and Decode Requests for Corresponding Kernels by @cyang49 in https://github.com/vllm-project/vllm/pull/17146
* Removed unused marlin cuda code by @mgoin in https://github.com/vllm-project/vllm/pull/17684
* [TPU] Add kernel test for moe_pallas by @mgoin in https://github.com/vllm-project/vllm/pull/17496
* Replace lm-eval bash script with pytest and use enforce_eager for faster CI by @mgoin in https://github.com/vllm-project/vllm/pull/17717
* [BugFix][Spec Decode] Fix hidden size mismatch between target and eagle head by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17740
* [Misc] Split model loader by @jeejeelee in https://github.com/vllm-project/vllm/pull/17712
* [Misc] Use `apply_rotary_emb` from vllm_flash_attn for Qwen2-VL vision RoPE by @Isotr0py in https://github.com/vllm-project/vllm/pull/17726
* [Kernel] GGUF MoeVec kernel by @SzymonOzog in https://github.com/vllm-project/vllm/pull/16780
* [Kernel] Use fused rmsnorm for some models like qwen3 series by @Eviannn in https://github.com/vllm-project/vllm/pull/17735
* [Misc] Remove  qlora_adapter_name_or_path by @jeejeelee in https://github.com/vllm-project/vllm/pull/17699
* Add NeuronxDistributedInference support, Speculative Decoding, Dynamic on-device sampling by @aws-satyajith in https://github.com/vllm-project/vllm/pull/16357
* [Frontend] Add missing chat templates for various MLLMs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17758
* Fix test_memory_usage_no_spec by @sarckk in https://github.com/vllm-project/vllm/pull/17754
* Make key optional for rotary embedding by @sarckk in https://github.com/vllm-project/vllm/pull/17566
* [doc] update the issue link by @reidliu41 in https://github.com/vllm-project/vllm/pull/17782
* [ROCm][FP8][Kernel] FP8 quantization fused into Custom Paged Attention by @gshtras in https://github.com/vllm-project/vllm/pull/17139
* Only depend on importlib-metadata for Python < 3.10 by @tiran in https://github.com/vllm-project/vllm/pull/17776
* [Bugfix] Fix Video IO error for short video by @Isotr0py in https://github.com/vllm-project/vllm/pull/17791
* Fix and simplify `deprecated=True` CLI `kwarg` by @hmellor in https://github.com/vllm-project/vllm/pull/17781
* [Bugfix] Fix missing lora name mapping for lora without prefix by @Isotr0py in https://github.com/vllm-project/vllm/pull/17793
* [Quantization] Quark MXFP4 format loading  by @BowenBao in https://github.com/vllm-project/vllm/pull/16943
* [Hardware][TPU][V1] Multi-LoRA implementation for the V1 TPU backend by @Akshat-Tripathi in https://github.com/vllm-project/vllm/pull/14238
* [BugFix] Avoid secondary missing `MultiprocExecutor.workers` error by @njhill in https://github.com/vllm-project/vllm/pull/17811
* [Core][Feature] Input metadata dump on crash by @wallashss in https://github.com/vllm-project/vllm/pull/13407
* [Chore][Doc] uses model id determined from OpenAI client by @aarnphm in https://github.com/vllm-project/vllm/pull/17815
* Don't call the venv `vllm` by @hmellor in https://github.com/vllm-project/vllm/pull/17810
* [BugFix] Fix `--disable-log-stats` in V1 server mode by @njhill in https://github.com/vllm-project/vllm/pull/17600
* [Core] Support full cuda graph in v1 by @chanh in https://github.com/vllm-project/vllm/pull/16072
* Improve exception reporting in MP engine by @vmarkovtsev in https://github.com/vllm-project/vllm/pull/17800
* [Installation] OpenTelemetry version update by @Xarbirus in https://github.com/vllm-project/vllm/pull/17771
* Only log non-default CLI args for online serving by @hmellor in https://github.com/vllm-project/vllm/pull/17803
* [V1] Add VLLM_ALLOW_INSECURE_SERIALIZATION env var by @russellb in https://github.com/vllm-project/vllm/pull/17490
* [Kernel][Hardware][AMD] Bf16 mfma opt for ROCm skinny GEMMs by @amd-hhashemi in https://github.com/vllm-project/vllm/pull/17071
* [Hardware][Power] Enable compressed tensor W8A8 INT8 quantization for POWER by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/17153
* [Hardware][Intel-Gaudi] Support Automatic Prefix Caching on HPU by @adobrzyn in https://github.com/vllm-project/vllm/pull/17648
* [Frontend] Chat template fallbacks for multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17805
* [Qwen3]add qwen3-235b-bf16 fused moe config on A100 by @Ximingwang-09 in https://github.com/vllm-project/vllm/pull/17715
* [Bugfix] Fix bad words for Mistral models by @qionghuang6 in https://github.com/vllm-project/vllm/pull/17753
* [Misc] support model prefix & add deepseek vl2 tiny fused moe config by @xsank in https://github.com/vllm-project/vllm/pull/17763
* [Bugfix] Fix tool call template validation for Mistral models by @RIckYuan999 in https://github.com/vllm-project/vllm/pull/17644
* [TPU] Fix the test_sampler by @bythew3i in https://github.com/vllm-project/vllm/pull/17820
* [Bugfix] Fix quark fp8 format loading on AMD GPUs by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/12612
* [Doc] Fix a typo in the file name by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17836
* [Easy] Eliminate c10::optional usage in vllm/csrc by @houseroad in https://github.com/vllm-project/vllm/pull/17819
* [Misc] add chatbox integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/17828
* Fix transient dependency error in docs build by @hmellor in https://github.com/vllm-project/vllm/pull/17848
* [Bugfix] `use_fast` failing to be propagated to Qwen2-VL image processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17838
* [Misc] Delete LoRA-related redundancy code by @jeejeelee in https://github.com/vllm-project/vllm/pull/17841
* [CI] Fix test_collective_rpc by @russellb in https://github.com/vllm-project/vllm/pull/17858
* [V1] Improve VLLM_ALLOW_INSECURE_SERIALIZATION logging by @russellb in https://github.com/vllm-project/vllm/pull/17860
* [Test] Attempt all TPU V1 tests, even if some of them fail. by @yarongmu-google in https://github.com/vllm-project/vllm/pull/17334
* [CI] Prune down lm-eval small tests by @mgoin in https://github.com/vllm-project/vllm/pull/17012
* Fix noisy warning for uncalibrated q_scale/p_scale by @mgoin in https://github.com/vllm-project/vllm/pull/17414
* Add cutlass support for blackwell fp8 blockwise gemm by @wenscarl in https://github.com/vllm-project/vllm/pull/14383
* [FEAT][ROCm]: Support AITER MLA on V1 Engine by @vllmellm in https://github.com/vllm-project/vllm/pull/17523
* [V1][Structured Output] Update llguidance (`>= 0.7.11`) to avoid AttributeError (no `StructTag`)  by @shen-shanshan in https://github.com/vllm-project/vllm/pull/17839
* [Attention] MLA move rotary embedding to cuda-graph region by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17668
* [BUGFIX]: return fast when request requires prompt logprobs by @andyxning in https://github.com/vllm-project/vllm/pull/17251
* [Docs] Add Slides from NYC Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/17879
* [Doc] Update several links in reasoning_outputs.md by @windsonsea in https://github.com/vllm-project/vllm/pull/17846
* [Doc] remove visible token in doc by @yma11 in https://github.com/vllm-project/vllm/pull/17884
* [Bugfix][ROCm] Fix AITER MLA V1 by @vllmellm in https://github.com/vllm-project/vllm/pull/17880
* [Bugfix][CPU] Fix broken AVX2 CPU TP support by @Isotr0py in https://github.com/vllm-project/vllm/pull/17252
* Fix Whisper crash caused by invalid``` max_num_batched_tokens``` config by @inkcherry in https://github.com/vllm-project/vllm/pull/17853
* Change `top_k` to be disabled with `0` (still accept `-1` for now) by @hmellor in https://github.com/vllm-project/vllm/pull/17773
* [Misc] add dify integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/17895
* [BugFix][AMD] Compatible patch for latest AITER(05/07/2025) by @qli88 in https://github.com/vllm-project/vllm/pull/17864
* [v1] Move block management logic from KVCacheManager to SpecializedManager by @heheda12345 in https://github.com/vllm-project/vllm/pull/17474
* [CI/Build] Automatically retry flaky tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17856
* Revert "[BugFix][AMD] Compatible patch for latest AITER(05/07/2025)" by @mgoin in https://github.com/vllm-project/vllm/pull/17910
* [Misc] Add references in ray_serve_deepseek example by @ruisearch42 in https://github.com/vllm-project/vllm/pull/17907
* [Misc] Auto fallback to float16 for pre-Ampere GPUs when detected bfloat16 config by @Isotr0py in https://github.com/vllm-project/vllm/pull/17265
* Update CT WNA16MarlinMoE integration by @mgoin in https://github.com/vllm-project/vllm/pull/16666
* Handle error when `str` passed to `/v1/audio/transcriptions` by @hmellor in https://github.com/vllm-project/vllm/pull/17909
* Add option to use torch._inductor.standalone_compile by @zou3519 in https://github.com/vllm-project/vllm/pull/17057
* [V1][Spec Decoding] Include bonus tokens in mean acceptance length by @markmc in https://github.com/vllm-project/vllm/pull/17908
* Improve configs - the rest! by @hmellor in https://github.com/vllm-project/vllm/pull/17562
* AMD conditional all test execution // new test groups by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/17556
* [Hardware/NVIDIA/Kernel] [Functional Enablement] [1/N] Enable nvidia/DeepSeek-R1-FP4 Model by @pavanimajety in https://github.com/vllm-project/vllm/pull/16362
* [V1][Spec Decoding] Log accumulated metrics after system goes idle by @markmc in https://github.com/vllm-project/vllm/pull/17913
* fix broken test vllm:test_kernels - test_attention_selector.py::test_flash_attn by @tracelogfb in https://github.com/vllm-project/vllm/pull/17873
* Add missing content type headers to /ping and /health (#17036) by @edrevo in https://github.com/vllm-project/vllm/pull/17786
* Don't default construct `ModelConfig` when default constructing `VllmConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/17943
* [Misc] remove --model from vllm serve usage by @reidliu41 in https://github.com/vllm-project/vllm/pull/17944
* [v1] Pass BlockTable and KVCacheSpec to AttentionMetadataBuilders by @heheda12345 in https://github.com/vllm-project/vllm/pull/17483
* [v1] Rename specialized_manager.py to single_type_kv_cache_manager.py by @heheda12345 in https://github.com/vllm-project/vllm/pull/17946
* [Kernel] fp4 marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/17687
* [Bugfix] Add revision to `transformers.Auto*.from_pretrained` processors by @xinli-centml in https://github.com/vllm-project/vllm/pull/17948
* [Perf] Use small max_num_batched_tokens for A100 by @KuntaiDu in https://github.com/vllm-project/vllm/pull/17885
* fix amd triton mla path by @842974287 in https://github.com/vllm-project/vllm/pull/17871
* [Bugfix]: v1 engine - consider lora adapters in allowed_token_ids by @bbrowning in https://github.com/vllm-project/vllm/pull/17855
* [doc] update lora doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/17936
* [Frontend] Add /classify endpoint by @frieda-huang in https://github.com/vllm-project/vllm/pull/17032
* [Misc] Add compressed-tensors NVFP4A16 emulation support by @dsikka in https://github.com/vllm-project/vllm/pull/17914
* [FP8][ROCm][Attention] Enable FP8 KV cache on ROCm for V1 by @gshtras in https://github.com/vllm-project/vllm/pull/17870
* [New Model]: nomic-embed-text-v2-moe by @noooop in https://github.com/vllm-project/vllm/pull/17785
* [Misc] not show --model in vllm serve --help by @reidliu41 in https://github.com/vllm-project/vllm/pull/16691
* [BugFix] [ROCm]: Bugfix and handle addition case of input for `rocm_aiter_rms_norm` by @tjtanaa in https://github.com/vllm-project/vllm/pull/17857
* [BUG] [ROCm] [MLA] Fix variable name bug due to change in variable name in PR #17483 by @tjtanaa in https://github.com/vllm-project/vllm/pull/17961
* [Model] Broadcast Ovis2 implementation to fit Ovis1.6 by @Isotr0py in https://github.com/vllm-project/vllm/pull/17861
* [misc] add instructions on how to install nvshmem/pplx/deepep by @youkaichao in https://github.com/vllm-project/vllm/pull/17964
* [Bugfix] validate grammar and throw 400 error instead of crashing the engine when xgrammar validation fails by @Jason-CKY in https://github.com/vllm-project/vllm/pull/17623
* [bugfix] fix the wrong parser by @reidliu41 in https://github.com/vllm-project/vllm/pull/17958
* [Bugfix] Fix pydantic.errors.PydanticUserError by @Potabk in https://github.com/vllm-project/vllm/pull/17962
* [Bugfix][TPU] Use np array when updating cache slot_mapping by @lsy323 in https://github.com/vllm-project/vllm/pull/17971
* [Fix] Benchmark `"EngineClient" has no attribute "model_config"` by @b8zhong in https://github.com/vllm-project/vllm/pull/17976
* [Feature] Support DeepSeekV3 Function Call by @Xu-Wenqing in https://github.com/vllm-project/vllm/pull/17784
* Correcting testcases in builkite job for IBM Power by @AaruniAggarwal in https://github.com/vllm-project/vllm/pull/17675
* [Misc] Improve modelscope  import error  by @jeejeelee in https://github.com/vllm-project/vllm/pull/17983
* Initialize the delta tool call fields explicitly by @maxdebayser in https://github.com/vllm-project/vllm/pull/17340
* [P/D] NIXL Integration by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/17751
* [Lora][Frontend]Add default local directory LoRA resolver plugin. by @jberkhahn in https://github.com/vllm-project/vllm/pull/16855
* Construct `KVTransferConfig` properly from Python instead of using JSON blobs without CLI by @hmellor in https://github.com/vllm-project/vllm/pull/17994
* [CI/Build] Fix TPU V1 Test mixed use of & and && across tests by @CAROLZXYZXY in https://github.com/vllm-project/vllm/pull/17968
* [Core] Use platform-agnostic device control for DP engine core by @jianzs in https://github.com/vllm-project/vllm/pull/17245
* Enabling "Weight Loading Multiple GPU Test - Large Models" by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/18020
* [v1][KVCacheManager] Change prefix caching metric from counting blocks to counting tokens by @heheda12345 in https://github.com/vllm-project/vllm/pull/18003
* [Chore] Remove unused method by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/18024
* Enable standard language model for torhc nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/18004
* [CI] Make JSON output tests less likely to fail by @russellb in https://github.com/vllm-project/vllm/pull/17859
* [V1][Spec Decode] Eagle unit tests by @wwl2755 in https://github.com/vllm-project/vllm/pull/17350
* [Bugfix] Fix FBGEMM integration by @mgoin in https://github.com/vllm-project/vllm/pull/18002
* [Model] Support MiMo-7B inference with MTP by @bwshen-mi in https://github.com/vllm-project/vllm/pull/17433
* Update some more deprecated type hinting by @hmellor in https://github.com/vllm-project/vllm/pull/17998
* Use NVFP4 Marlin for CompressedTensorsW4A16Fp4 by @mgoin in https://github.com/vllm-project/vllm/pull/18000
* Remove noisy warnings from `SchedulerConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/17995
* [ROCm] Skip tests for quantizations incompatible with ROCm by @hissu-hyvarinen in https://github.com/vllm-project/vllm/pull/17905
* Implements dual-chunk-flash-attn backend for dual chunk attention with sparse attention support by @sighingnow in https://github.com/vllm-project/vllm/pull/11844
* [Misc] Slight spelling modification by @jeejeelee in https://github.com/vllm-project/vllm/pull/18039
* [ROCm]: Fix build from source failure with gcc14 and ROCm 6.3 by @arjunkathuria in https://github.com/vllm-project/vllm/pull/13779
* [Bugfix] Fixes for new marlin moe usage by @mgoin in https://github.com/vllm-project/vllm/pull/18017
* [Bugfix] Avoid repeatedly creating dummy data during engine startup by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17935
* [Feature][V1]  Support `tool_choice: required` when using Xgrammar as the `StructuredOutputBackend`. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17845
* cleanup invalid prints by @calvin0327 in https://github.com/vllm-project/vllm/pull/18050
* [BugFix] Fix 4-GPU RLHF tests by @njhill in https://github.com/vllm-project/vllm/pull/18007
* Fix Broken macro for cutlass moe by @drisspg in https://github.com/vllm-project/vllm/pull/18049
* [v1][KVCacheManager] Avoid full cache hit by controlling max_length by @heheda12345 in https://github.com/vllm-project/vllm/pull/17999
* [Bugfix][V1] Only get input embeddings w/ multi-modal models if first PP by @jinhuang12 in https://github.com/vllm-project/vllm/pull/17916
* [BugFix] Set default random seed to 0 for V1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17929
* [Bugfix] Fix marlin moe fallback logic for llama4 by @mgoin in https://github.com/vllm-project/vllm/pull/18042
* [Benchmarks] Refactor run_structured_output_benchmarks.sh by @russellb in https://github.com/vllm-project/vllm/pull/17722
* Convert `.buildkite` to `ruff format` by @hmellor in https://github.com/vllm-project/vllm/pull/17656
* [Fix] check to make sure processor has chat templates by @aarnphm in https://github.com/vllm-project/vllm/pull/18047
* [doc] add download/list/delete HF model CLI usage by @reidliu41 in https://github.com/vllm-project/vllm/pull/17940
* Update deprecated type hinting in `model_executor/layers` by @hmellor in https://github.com/vllm-project/vllm/pull/18056
* Update deprecated type hinting in `vllm/profiler` by @hmellor in https://github.com/vllm-project/vllm/pull/18057
* Update deprecated type hinting in `vllm/transformers_utils` by @hmellor in https://github.com/vllm-project/vllm/pull/18058
* [CI] Set token permissions for reminder comment CI job by @russellb in https://github.com/vllm-project/vllm/pull/17728
* [CI] Add workflow permissions for helm CI job by @russellb in https://github.com/vllm-project/vllm/pull/17727
* [CI] Add token permissions for add-ready-label CI job by @russellb in https://github.com/vllm-project/vllm/pull/17730
* [CI] set token permissions for pre-commit CI job by @russellb in https://github.com/vllm-project/vllm/pull/17729
* [Bugfix] Fix entrypoints metrics tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18063
* Convert `benchmarks` to `ruff format` by @hmellor in https://github.com/vllm-project/vllm/pull/18068
* Give auto-merge label workflow permission to add labels to issues by @hmellor in https://github.com/vllm-project/vllm/pull/18078
* Update deprecated type hinting in `vllm/compilation` by @hmellor in https://github.com/vllm-project/vllm/pull/18072
* Update deprecated type hinting in `vllm/adapter_commons` by @hmellor in https://github.com/vllm-project/vllm/pull/18073
* [V1] DP scale-out (2/N): Decouple engine process management and comms by @njhill in https://github.com/vllm-project/vllm/pull/15977
* [Docs] Expand security doc with firewall info by @russellb in https://github.com/vllm-project/vllm/pull/18081
* [FEAT] [ROCm]: Add AITER Block-Scaled GEMM Feature by @vllmellm in https://github.com/vllm-project/vllm/pull/14968
* [v1][KVCacheManager] pass num_new_computed_tokens to kv cache manager by @heheda12345 in https://github.com/vllm-project/vllm/pull/18001
* [Fix] Support CUDAGraph capture for encoder-decoder on ROCm by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18104
* [Hardware/NVIDIA/Modelopt] Fix modelopt forward method for v1 torch.compile by @pavanimajety in https://github.com/vllm-project/vllm/pull/18101
* [P/D] Add some more debug logs to `NixlConnector` by @njhill in https://github.com/vllm-project/vllm/pull/18102
* [Misc] Remove unused numpy tensor by @ywang96 in https://github.com/vllm-project/vllm/pull/18084
* [Bug]: Fix S3 model/tokenizer path resolution by @gilljon in https://github.com/vllm-project/vllm/pull/18083
* [core][distributed] add ep group and all2all interface by @youkaichao in https://github.com/vllm-project/vllm/pull/18077
* [Bugfix] Fix FP8 Marlin MoE and enable for compressed-tensors models by @mgoin in https://github.com/vllm-project/vllm/pull/18026
* [Bugfix][V1] Fix FlashInfer V1 backend using the wrong VllmConfig by @mgoin in https://github.com/vllm-project/vllm/pull/18086
* [FEAT] [ROCm] [V1]: Add AITER biased group topk for DeepSeekV3 by @vllmellm in https://github.com/vllm-project/vllm/pull/17955
* [AMD][torch.compile] Enable silu+fp8_quant fusion for rocm by @charlifu in https://github.com/vllm-project/vllm/pull/18082
* [BugFix][AMD] Compatible patch for AITER lib after 04/20 by @qli88 in https://github.com/vllm-project/vllm/pull/17912
* Fix broken example: examples/offline_inference/profiling at scheduler_config  by @Ecthlion in https://github.com/vllm-project/vllm/pull/18117
* [Fix] Move "model_config" as keyword args in chat_utils.py by @lk-chen in https://github.com/vllm-project/vllm/pull/18098
* [Bugfix] fix moe marlin `topk_weight` loading by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/18080
* [Bugfix][Example] make lmcache v0 work. by @majianpeng in https://github.com/vllm-project/vllm/pull/18051
* [New Model]: support GTE NewModel by @noooop in https://github.com/vllm-project/vllm/pull/17986
* [Bugfix] Fix entrypoints audio test failure by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18111
* [Model] Add packed_modules_mapping for Qwen3-MOE by @jeejeelee in https://github.com/vllm-project/vllm/pull/18118
* [Misc] replace does not exist model by @lengrongfu in https://github.com/vllm-project/vllm/pull/18119
* [Bugfix] Fix QKVCrossParallelLinear::sync_weight_attrs for PyTorch compile by @anko-intel in https://github.com/vllm-project/vllm/pull/17844
* [FEAT] [ROCm]: Add AITER CK 2 Stages MoE support by @tjtanaa in https://github.com/vllm-project/vllm/pull/17110
* [Bugfix] Fix LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/18123
* [Model] GritLM supports other attention backends by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18109
* [doc] add missing import by @reidliu41 in https://github.com/vllm-project/vllm/pull/18133
* Update deprecated type hinting in `vllm/lora` by @hmellor in https://github.com/vllm-project/vllm/pull/18128
* Update deprecated type hinting in `vllm/device_allocator` and `vllm/distributed` by @hmellor in https://github.com/vllm-project/vllm/pull/18126
* Update deprecated type hinting in `platform`, `plugins`, `triton_utils`, `vllm_flash_attn` by @hmellor in https://github.com/vllm-project/vllm/pull/18129
* [Bugfix] Fix chat utils tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18139
* [KVConnector] Keep KVTransferParams as a dict by @njhill in https://github.com/vllm-project/vllm/pull/18033
* [Doc] Update prefix cache metrics to counting tokens by @heheda12345 in https://github.com/vllm-project/vllm/pull/18138
* [V1][Spec Decode] Share input embedding of target model with EAGLE draft model to free ~1GB for llama 3 model by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/17326
* Modularize fused experts and integrate PPLX kernels by @bnellnm in https://github.com/vllm-project/vllm/pull/15956
* [CI] Disable Failing Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/18165
* [Frontend] decrease import time of vllm.multimodal by @davidxia in https://github.com/vllm-project/vllm/pull/18031
* [Kernel] Have rotary embeddings support tensors by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/18046
* [V1] Structured Outputs + Thinking compatibility by @aarnphm in https://github.com/vllm-project/vllm/pull/16577
* Add support for loading torchao models with `AOPerModuleConfig` by @jerryzh168 in https://github.com/vllm-project/vllm/pull/17826
* [CI] Fix race condition in test_kv_cache_events test by @russellb in https://github.com/vllm-project/vllm/pull/18169
* [V1] Support multiple kv connectors by @mgoin in https://github.com/vllm-project/vllm/pull/17564
* Upload vllm index for the rc builds by @atalman in https://github.com/vllm-project/vllm/pull/18173
* [Bugfix]: make most of `test_openai_schema.py` pass by @davidxia in https://github.com/vllm-project/vllm/pull/17664
* [v1] Support multiple KV cache groups in GPU model runner by @heheda12345 in https://github.com/vllm-project/vllm/pull/17945
* [V1][Metrics] Remove unused code by @markmc in https://github.com/vllm-project/vllm/pull/18158
* [Chore] astral's ty by @aarnphm in https://github.com/vllm-project/vllm/pull/18116
* [Misc] add lobe-chat support by @reidliu41 in https://github.com/vllm-project/vllm/pull/18177
* [Fix][ROCm] Enforce eager for all encoder-decoder models on ROCm by @ProExpertProg in https://github.com/vllm-project/vllm/pull/18154
* Update deprecated type hinting in `models` by @hmellor in https://github.com/vllm-project/vllm/pull/18132
* [Bugfix] Fix fp8 tests for triton_unified_attention for Triton 3.3 by @tdoublep in https://github.com/vllm-project/vllm/pull/18013
* Support custom implementations of VideoLoader backends. by @huachenheli in https://github.com/vllm-project/vllm/pull/18091
* [UT] Add ut for none hash by @andyxning in https://github.com/vllm-project/vllm/pull/17892
* [Model] Allow the use of sliding window in Qwen2  by @inkcherry in https://github.com/vllm-project/vllm/pull/17772
* [Bugfix] Fix FusedMoEPrepareAndFinalize for cuda-disalike backends by @MengqingCao in https://github.com/vllm-project/vllm/pull/18178
* [CI] don't skip fixed `test_kv_cache_events()` by @davidxia in https://github.com/vllm-project/vllm/pull/18183
* [V1] Update zmq socket creation in nixl connector by @russellb in https://github.com/vllm-project/vllm/pull/18148
* fix: typos by @omahs in https://github.com/vllm-project/vllm/pull/18151
* Update deprecated type hinting in `model_loader` by @hmellor in https://github.com/vllm-project/vllm/pull/18130
* add tools into TokenizeChatRequest by @hustxiayang in https://github.com/vllm-project/vllm/pull/18187
* [Kernel] [V1] Fix performance regression for triton unified attention by @tdoublep in https://github.com/vllm-project/vllm/pull/18161
* Adding "Basic Models Test" and "Multi-Modal Models Test (Extended) 3" in AMD Pipeline by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/18106
* Improve examples rendering in docs and GitHub by @hmellor in https://github.com/vllm-project/vllm/pull/18203
* [Frontend] Fix chat template content format detection by @schoennenbeck in https://github.com/vllm-project/vllm/pull/18190
* [Bugfix]Change the exception thrown by call_hf_processor from RuntimeError to ValueError by @Abatom in https://github.com/vllm-project/vllm/pull/18181
* [Bugfix] [ROCm]: Remove assertion logic when using AITER fused moe in unquantizedMethod to reenable LLama4 BF16 by @tjtanaa in https://github.com/vllm-project/vllm/pull/18205
* [Misc] Avoid cuda graph log when sizes still match by @NickLucche in https://github.com/vllm-project/vllm/pull/18202
* Adding "AMD: Tensorizer Test" to amdproduction. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/18216
* [Bugfix] Fix test_eagle test by @luccafong in https://github.com/vllm-project/vllm/pull/18223
* [Build] Allow shipping PTX on a per-file basis by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/18155
* [Bugfix] fix rotary embedding test for _get_padded_tensor_shape by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/18229
* [Bugfix][ROCm] Use `chunked_prefill_paged_decode` as fallback for V1 attention on ROCm by @kliuae in https://github.com/vllm-project/vllm/pull/18093
* [Model] vLLM v1 supports Medusa by @skylee-01 in https://github.com/vllm-project/vllm/pull/17956
* Allow users to pass arbitrary JSON keys from CLI by @hmellor in https://github.com/vllm-project/vllm/pull/18208
* Throw better error for when running into k8s service discovery issue by @wseaton in https://github.com/vllm-project/vllm/pull/18209
* [Feature] Support Pipeline Parallism in torchrun SPMD offline inference for V1 by @luccafong in https://github.com/vllm-project/vllm/pull/17827
* [doc] fix multimodal example script by @davidxia in https://github.com/vllm-project/vllm/pull/18089
* [PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding constâ€¦ by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/17973
* [Misc] Add Ray Prometheus logger to V1 by @eicherseiji in https://github.com/vllm-project/vllm/pull/17925
* [Misc] Consolidate Audio tests into multimodal common generation tests by @Isotr0py in https://github.com/vllm-project/vllm/pull/18214
* use ceil_div in cutlass block scaling shape check by @IwakuraRein in https://github.com/vllm-project/vllm/pull/17918
* [Fix] Fix typo in `resolve_hf_chat_template` by @fxmarty-amd in https://github.com/vllm-project/vllm/pull/18259
* [Model] Use autoweightloader for dbrx by @learner0810 in https://github.com/vllm-project/vllm/pull/18251
* [Misc][MacOS] fix bfloat16 error by @reidliu41 in https://github.com/vllm-project/vllm/pull/18249
* [BugFix] Fix multi async save in MultiConnector by @njhill in https://github.com/vllm-project/vllm/pull/18246
* [BugFix] Fix ordering of KVConnector finished send/rcv sets by @njhill in https://github.com/vllm-project/vllm/pull/18211
* [CI] Assign reviewer to mergify with changes to Tensorizer files by @sangstar in https://github.com/vllm-project/vllm/pull/18278
* [Sampler] Adapt to FlashInfer 0.2.3 sampler API by @abmfy in https://github.com/vllm-project/vllm/pull/15777
* [Bugfix] fix `an illegal memory access was encountered` of marlin kernel + act_order  by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/18245
* [Spec Decode] Don't fall back to V0 when spec decoding is enabled by @WoosukKwon in https://github.com/vllm-project/vllm/pull/18265
* [V1][P/D] Local attention optimization for NIXL by @mgoin in https://github.com/vllm-project/vllm/pull/18170
* Move cli args docs to its own page (#18228) by @strangiato in https://github.com/vllm-project/vllm/pull/18264
* [Misc] reformat the collect-env output by @reidliu41 in https://github.com/vllm-project/vllm/pull/18285
* [BugFix] Correct max_model_len derivation from config.json for Mistral format  by @princepride in https://github.com/vllm-project/vllm/pull/17937
* [P/D][V1] Support dynamic loading of external KV connector implementations by @sdavidbd in https://github.com/vllm-project/vllm/pull/18142
* [Hardware][TPU] Optionally import for TPU backend by @lsy323 in https://github.com/vllm-project/vllm/pull/18269
* Update Dockerfile to build for Blackwell by @mgoin in https://github.com/vllm-project/vllm/pull/18095
* Fixed build on ppc64le due to openssl conflicts by @npanpaliya in https://github.com/vllm-project/vllm/pull/18262
* [Model] use AutoWeightsLoader for solar by @lengrongfu in https://github.com/vllm-project/vllm/pull/18113
* [MISC] fix typo by @andyxning in https://github.com/vllm-project/vllm/pull/18305
* Support sequence parallelism combined with pipeline parallelism by @cascade812 in https://github.com/vllm-project/vllm/pull/18243
* [doc] update reasoning doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/18306
* [Model] Use sigmoid for single-label classification by @22quinn in https://github.com/vllm-project/vllm/pull/18313
* Fix copy-paste error in phi4mm image processing by @lifuhuang in https://github.com/vllm-project/vllm/pull/18315
* [Misc] add litellm integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/18320
* [Doc] Add doc to explain the usage of Qwen3 thinking by @WangErXiao in https://github.com/vllm-project/vllm/pull/18291
* [Spec Decode][V0] Fix spec decode correctness test in V0 eagle/medusa by @wwl2755 in https://github.com/vllm-project/vllm/pull/18175
* Feature/vllm/input embedding completion api by @Nan2018 in https://github.com/vllm-project/vllm/pull/17590
* [Misc] extract parser.parse_args() by @reidliu41 in https://github.com/vllm-project/vllm/pull/18323
* [Build] Supports CUDA 12.6 and 11.8 after Blackwell Update by @simon-mo in https://github.com/vllm-project/vllm/pull/18316
* fix: Add type specifications for CLI arguments in tensorizer options by @googs1025 in https://github.com/vllm-project/vllm/pull/18314
* [BugFix] [Vul] Add missing `usedforsecurity=False` in MD5 hashing to enable FIPS by @shaoyuyoung in https://github.com/vllm-project/vllm/pull/18319
* [Doc] Fix prompt embedding examples by @Potabk in https://github.com/vllm-project/vllm/pull/18350
* [Doc] Move input-related docs to Features by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18353
* [BugFix] Fix handling of num_computed_tokens with connector by @njhill in https://github.com/vllm-project/vllm/pull/18232
* [Quantization] Pool model support bitsandbytes by @jeejeelee in https://github.com/vllm-project/vllm/pull/18087
* [Doc] Fix typo by @eladsegal in https://github.com/vllm-project/vllm/pull/18355
* [Frontend] add --quick option for vllm chat/complete by @reidliu41 in https://github.com/vllm-project/vllm/pull/18297
* [Feature]Add support for models quantized with AutoRound by @wenhuach21 in https://github.com/vllm-project/vllm/pull/17850
* Add files via uploadAdd fused MoE kernel tuning configs (fp8_w8a8) for DeepSeek V3/R1 on a single-node 8x NVIDIA H20 96GB setup by @sunyicode0012 in https://github.com/vllm-project/vllm/pull/18337
* [Misc] Fix typo by @Unprincess17 in https://github.com/vllm-project/vllm/pull/18330
* Neuron up mistral by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18222
* fix CUDA_check redefinition in #17918 by @luccafong in https://github.com/vllm-project/vllm/pull/18287
* [neuron] fix authorization issue by @liangfu in https://github.com/vllm-project/vllm/pull/18364
* [Misc] Allow `AutoWeightsLoader` to skip loading weights with specific substr in name by @Isotr0py in https://github.com/vllm-project/vllm/pull/18358
* [Core] [Bugfix]: tensor parallel with prompt embeds by @Nan2018 in https://github.com/vllm-project/vllm/pull/18171
* [release] Change dockerhub username for TPU release by @khluu in https://github.com/vllm-project/vllm/pull/18389
* [Bugfix] fix adding bias twice in ipex GPTQ quantization by @rand-fly in https://github.com/vllm-project/vllm/pull/18363
* [doc] update env variable export by @reidliu41 in https://github.com/vllm-project/vllm/pull/18391
* [Misc] Add LoRA code owner by @jeejeelee in https://github.com/vllm-project/vllm/pull/18387
* Update cpu.txt by @princepride in https://github.com/vllm-project/vllm/pull/18398
* [CI] Add mteb testing to test the accuracy of the embedding model by @noooop in https://github.com/vllm-project/vllm/pull/17175
* [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text by @wulipc in https://github.com/vllm-project/vllm/pull/18407
* [Misc] refactor prompt embedding examples by @reidliu41 in https://github.com/vllm-project/vllm/pull/18405
* [Minor] Rename quantization nvfp4 to modelopt_fp4 by @mgoin in https://github.com/vllm-project/vllm/pull/18356
* [Model] use AutoWeightsLoader for bloom by @calvin0327 in https://github.com/vllm-project/vllm/pull/18300
* [Kernel] update comment for KV shape in unified triton attn by @haochengxia in https://github.com/vllm-project/vllm/pull/18099
* fix:Build torch wheel inline rather than picking from nightly by @dilipgb in https://github.com/vllm-project/vllm/pull/18351
* [TPU] Re-enable the Pallas MoE kernel by @mgoin in https://github.com/vllm-project/vllm/pull/18025
* [Bugfix] config.head_dim is now explicitly set to None by @gshtras in https://github.com/vllm-project/vllm/pull/18432
* [Bug] Fix moe_sum signature by @bnellnm in https://github.com/vllm-project/vllm/pull/18440
* Revert "[Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text (#18407)" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18456
* [Bugfix][Failing Test] Fix nixl connector test when promt size < block size by @wwl2755 in https://github.com/vllm-project/vllm/pull/18429
* [Misc] MultiConnector._connectors type by @NickLucche in https://github.com/vllm-project/vllm/pull/18423
* [Frontend] deprecate `--device` arg by @kebe7jun in https://github.com/vllm-project/vllm/pull/18399
* [V1] Fix general plugins not loaded in engine for multiproc by @sarckk in https://github.com/vllm-project/vllm/pull/18326
* [Misc] refactor disaggregated-prefill-v1 example by @reidliu41 in https://github.com/vllm-project/vllm/pull/18474
* [Bugfix][Failing Test] Fix test_events.py by @rabi in https://github.com/vllm-project/vllm/pull/18460
* [MODEL] FalconH1 by @dhiaEddineRhaiem in https://github.com/vllm-project/vllm/pull/18406
* [Doc] fix arg docstring in linear layers by @giantcroc in https://github.com/vllm-project/vllm/pull/18410
* [Bugfix] Reduce moe_sum test size to avoid OOM by @bnellnm in https://github.com/vllm-project/vllm/pull/18484
* [Build] fix Dockerfile shell by @kebe7jun in https://github.com/vllm-project/vllm/pull/18402
* [Misc] Update deprecation message for `--enable-reasoning` by @Zerohertz in https://github.com/vllm-project/vllm/pull/18404
* [ROCm][Kernel][V1] Enable AMD Radeon GPU Custom Paged Attention on v1 by @hyoon1 in https://github.com/vllm-project/vllm/pull/17004
* Revert "[v1] Support multiple KV cache groups in GPU model runner (#17945) by @markmc in https://github.com/vllm-project/vllm/pull/18459
* [FEAT][ROCm] Upgrade AITER MLA v1 backend by @vllmellm in https://github.com/vllm-project/vllm/pull/18338
* [Bugfix] Consistent ascii handling in tool parsers by @schoennenbeck in https://github.com/vllm-project/vllm/pull/17704
* [FalconH1] Fix output dtype in RMSNorm fallback path for Falcon-H1 (e.g. 0.5B) by @dhiaEddineRhaiem in https://github.com/vllm-project/vllm/pull/18500
* [MISC] update project urls in pyproject.toml by @andyxning in https://github.com/vllm-project/vllm/pull/18519
* [CI] Fix race condition with StatelessProcessGroup.barrier by @russellb in https://github.com/vllm-project/vllm/pull/18506
* Intialize io_thread_pool attribute in the beginning. by @rabi in https://github.com/vllm-project/vllm/pull/18331
* [Bugfix] Inconsistent token calculation compared to HF in llava family by @cyr0930 in https://github.com/vllm-project/vllm/pull/18479
* [BugFix][DP] Send DP wave completion only from `dp_rank==0` by @njhill in https://github.com/vllm-project/vllm/pull/18502
* [Bugfix][Model] Make Olmo2Model weight loading return loaded weights by @2015aroras in https://github.com/vllm-project/vllm/pull/18504
* [Bugfix] Fix LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/18518
* [Doc] Fix invalid JSON in example args by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18527
* [Neuron] Update Dockerfile.neuron to use latest neuron release (2.23) by @aws-satyajith in https://github.com/vllm-project/vllm/pull/18512
* Update default neuron config for speculation by @elaineyz in https://github.com/vllm-project/vllm/pull/18274
* Order sequence ids + config update to support specifying custom quantization layers by @elaineyz in https://github.com/vllm-project/vllm/pull/18279
* [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text by @wulipc in https://github.com/vllm-project/vllm/pull/18526
* [Bugfix] Add kwargs to RequestOutput __init__ to be forward compatible by @lk-chen in https://github.com/vllm-project/vllm/pull/18513
* [CI/Build] Update bamba test model location by @hmellor in https://github.com/vllm-project/vllm/pull/18544
* [Doc] Support --stream arg in openai_completion_client.py script by @googs1025 in https://github.com/vllm-project/vllm/pull/18388
* [Bugfix] Use random hidden states in dummy sampler run by @abmfy in https://github.com/vllm-project/vllm/pull/18543
* [Doc] Add stream flag for chat completion example by @calvin0327 in https://github.com/vllm-project/vllm/pull/18524
* [BugFix][CPU] Fix x86 SHM distributed module initialization by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/18536
* [Misc] improve Automatic Prefix Caching example by @reidliu41 in https://github.com/vllm-project/vllm/pull/18554
* [Misc] Call `ndarray.tobytes()` directly instead of `ndarray.data.tobytes()` by @lgeiger in https://github.com/vllm-project/vllm/pull/18347
* [Bugfix] make `test_openai_schema.py` pass by @davidxia in https://github.com/vllm-project/vllm/pull/18224
* [Platform] Move platform check to right place by @wangxiyuan in https://github.com/vllm-project/vllm/pull/18470
* [Compile][Platform] Make PiecewiseBackend pluggable and extendable by @MengqingCao in https://github.com/vllm-project/vllm/pull/18076
* [Build/CI] Fix CUDA 11.8 build by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/17679
* [Tool] Add NIXL installation script by @lk-chen in https://github.com/vllm-project/vllm/pull/18172
* [V1][Spec Decode][Bugfix] Load quantize weights for EAGLE by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/18290
* [Frontend][Bug Fix] Update llama4 pythonic jinja template and llama4_pythonic parser by @wukaixingxp in https://github.com/vllm-project/vllm/pull/17917
* [Frontend] [Core] Add Tensorizer support for V1, LoRA adapter serialization and deserialization by @sangstar in https://github.com/vllm-project/vllm/pull/17926
* [AMD] [P/D] Compute num gpus for ROCm correctly in run_accuracy_test.sh by @rasmith in https://github.com/vllm-project/vllm/pull/18568
* Re-submit: Fix: Proper RGBA -> RGB conversion for PIL images. by @huachenheli in https://github.com/vllm-project/vllm/pull/18569
* [V1][Spec Decoding] Use model_loader.get_model() to load models by @markmc in https://github.com/vllm-project/vllm/pull/18273
* Enable interleaved sliding window attention models for Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/18494
* [Misc] refactor: simplify input validation and num_requests handling in _convert_v1_inputs by @googs1025 in https://github.com/vllm-project/vllm/pull/18482
* [BugFix] Increase TP execute_model timeout by @njhill in https://github.com/vllm-project/vllm/pull/18558
* [Bugfix] Set `KVTransferConfig.engine_id` in post_init by @lk-chen in https://github.com/vllm-project/vllm/pull/18576
* [Spec Decode] Make EAGLE3 draft token ID mapping optional by @benchislett in https://github.com/vllm-project/vllm/pull/18488
* [Neuron] Remove bypass on EAGLEConfig and add a test by @elaineyz in https://github.com/vllm-project/vllm/pull/18514
* [Bugfix][Benchmarks] Fix a benchmark of deepspeed-mii backend to use api_key by @tishizaki in https://github.com/vllm-project/vllm/pull/17291
* [Misc] Replace `cuda` hard code with `current_platform` by @shen-shanshan in https://github.com/vllm-project/vllm/pull/16983
* [Hardware] correct method signatures for HPU,ROCm,XPU by @andyxning in https://github.com/vllm-project/vllm/pull/18551
* [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal by @RonaldBXu in https://github.com/vllm-project/vllm/pull/18034
* [Feature]Add async tensor parallelism using compilation pass by @cascade812 in https://github.com/vllm-project/vllm/pull/17882
* [Doc] Update quickstart and install for cu128 using `--torch-backend=auto` by @mgoin in https://github.com/vllm-project/vllm/pull/18505
* [Feature][V1]: suupports cached_tokens in response usage by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/18149
* [Bugfix] Add half type support in reshape_and_cache_cpu_impl on x86 cpu platform by @zzzyq in https://github.com/vllm-project/vllm/pull/18430
* Migrate docs from Sphinx to MkDocs by @hmellor in https://github.com/vllm-project/vllm/pull/18145
* Revert "[V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (#18034)" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18600
* [Bugfix][Model] Fix baichuan model loader for tp by @MengqingCao in https://github.com/vllm-project/vllm/pull/18597
* [V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled by @shadeMe in https://github.com/vllm-project/vllm/pull/17731
* Add myself as docs code owner by @hmellor in https://github.com/vllm-project/vllm/pull/18605
* [Hardware][CPU] Update intel_extension_for_pytorch 2.7.0 and move to `requirements/cpu.txt`  by @yankay in https://github.com/vllm-project/vllm/pull/18542
* [CI] fix kv_cache_type argument by @andyxning in https://github.com/vllm-project/vllm/pull/18594
* [Doc] Fix indent of contributing to vllm by @Zerohertz in https://github.com/vllm-project/vllm/pull/18611
* Replace `{func}` with mkdocs style links by @hmellor in https://github.com/vllm-project/vllm/pull/18610
* [CI/Build] Fix V1 flag being set in entrypoints tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18598
* Fix examples with code blocks in docs by @hmellor in https://github.com/vllm-project/vllm/pull/18609
* [Bugfix] Fix transformers model impl ignored for mixtral quant by @tristanleclercq in https://github.com/vllm-project/vllm/pull/18602
* Include private attributes in API documentation by @hmellor in https://github.com/vllm-project/vllm/pull/18614
* [Misc] add Haystack integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/18601
* [Bugfix][Build/CI] Fixup CUDA compiler version check for CUDA_SUPPORTED_ARCHS by @simon-mo in https://github.com/vllm-project/vllm/pull/18579
* [Doc] Fix markdown list indentation for MkDocs rendering by @Zerohertz in https://github.com/vllm-project/vllm/pull/18620
* [Doc] Use a different color for the announcement by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18616
* Refactor pplx init logic to make it modular (prepare for deepep) by @youkaichao in https://github.com/vllm-project/vllm/pull/18200
* Fix figures in design doc by @hmellor in https://github.com/vllm-project/vllm/pull/18612
* [Docs] Change mkdocs to not use directory urls by @mgoin in https://github.com/vllm-project/vllm/pull/18622
* [v1] Redo "Support multiple KV cache groups in GPU model runner (#17945)" by @heheda12345 in https://github.com/vllm-project/vllm/pull/18593
* [Doc] fix list formatting by @davidxia in https://github.com/vllm-project/vllm/pull/18624
* [Doc] Fix top-level API links/docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18621
* [Doc] Avoid documenting dynamic / internal modules by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18626
* [Doc] Fix broken links and unlinked docs, add shortcuts to home sidebar by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18627
* [V1] Support Deepseek MTP by @YaoJiayi in https://github.com/vllm-project/vllm/pull/18435
* Use prebuilt FlashInfer x86_64 PyTorch 2.7 CUDA 12.8 wheel for CI by @huydhn in https://github.com/vllm-project/vllm/pull/18537
* [CI] Enable test_initialization to run on V1 by @mgoin in https://github.com/vllm-project/vllm/pull/16736
* [Doc] Update references to doc files by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18637
* [ModelOpt] Introduce VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE env var to control blockscale tensor allocation by @pavanimajety in https://github.com/vllm-project/vllm/pull/18160
* [Bugfix] Migrate to REGEX Library to prevent catastrophic backtracking by @Crucifixion-Fxl in https://github.com/vllm-project/vllm/pull/18454
* [Bugfix][Nixl] Fix Preemption Bug by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/18631
* config.py: Clarify that only local GGUF checkpoints are supported. by @MathieuBordere in https://github.com/vllm-project/vllm/pull/18623
* FIX MOE issue in AutoRound format by @wenhuach21 in https://github.com/vllm-project/vllm/pull/18586
* [V1][Spec Decode] Small refactors to improve eagle bookkeeping performance by @zixi-qi in https://github.com/vllm-project/vllm/pull/18424
* [Frontend] improve vllm serve --help display by @reidliu41 in https://github.com/vllm-project/vllm/pull/18643
* [Model] Add support for Qwen2.5-Omni-7B-AWQ (Qwen2_5OmniForConditionalGeneration) by @Nalkey in https://github.com/vllm-project/vllm/pull/18647
* [V1][Spec Decode] Support multi-layer eagle draft model by @zixi-qi in https://github.com/vllm-project/vllm/pull/18030
* [Doc] Update README links, mark external links by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18635
* [MISC][pre-commit] Add pre-commit check for triton import by @MengqingCao in https://github.com/vllm-project/vllm/pull/17716
* [Doc] Fix indentation problems in V0 Paged Attention docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18659
* [Doc] Add community links by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18657
* [Model] use AutoWeightsLoader for gpt2 by @ztang2370 in https://github.com/vllm-project/vllm/pull/18625
* [Doc] Reorganize user guide by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18661
* [CI/Build] `chmod +x` to `cleanup_pr_body.sh` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18650
* [MISC] typo fix and clean import by @andyxning in https://github.com/vllm-project/vllm/pull/18664
* [BugFix] Fix import error for fused_moe by @wangxiyuan in https://github.com/vllm-project/vllm/pull/18642
* [CI] enforce import regex instead of re by @aarnphm in https://github.com/vllm-project/vllm/pull/18665
* fix(regression): clone from reference items by @aarnphm in https://github.com/vllm-project/vllm/pull/18662
* [CI/Build] fix permission denied issue by @reidliu41 in https://github.com/vllm-project/vllm/pull/18645
* [BugFix][Spec Decode] Improve Prefix Caching Logic in Speculative Decoding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/18668
* [V1] Fix _pickle.PicklingError: Can't pickle <class 'transformers_modules.deepseek-ai.DeepSeek-V2-Lite... by @eicherseiji in https://github.com/vllm-project/vllm/pull/18640
* [MISC] correct signature for LoaderFunction by @andyxning in https://github.com/vllm-project/vllm/pull/18670
* [Misc]Replace `cuda` hard code with `current_platform` in Ray by @noemotiovon in https://github.com/vllm-project/vllm/pull/14668
* [Misc][ModelScope] Change to use runtime VLLM_USE_MODELSCOPE by @MengqingCao in https://github.com/vllm-project/vllm/pull/18655
* [VLM] Initialize video input support for InternVL models by @Isotr0py in https://github.com/vllm-project/vllm/pull/18499
* Speed up the `kernels/quantization/` tests by @mgoin in https://github.com/vllm-project/vllm/pull/18669
* [BUGFIX] catch subclass first for try...except by @andyxning in https://github.com/vllm-project/vllm/pull/18672
* [Misc] Reduce logs on startup by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18649
* [doc] fix broken links by @reidliu41 in https://github.com/vllm-project/vllm/pull/18671
* [doc] improve readability by @reidliu41 in https://github.com/vllm-project/vllm/pull/18675
* [Bugfix] Fix cpu usage and cache hit stats reporting on cpu environment by @zzzyq in https://github.com/vllm-project/vllm/pull/18674
* [CI/build] fix no regex by @reidliu41 in https://github.com/vllm-project/vllm/pull/18676
* [Misc] small improve by @reidliu41 in https://github.com/vllm-project/vllm/pull/18680
* [Bugfix] Fix profiling dummy data for Pixtral by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18677
* [Core][Multimodal] Convert PIL Image to array without data copy when hashing by @lgeiger in https://github.com/vllm-project/vllm/pull/18682
* [CI/Build][Doc] Update `gte-Qwen2-1.5B-instruct` usage by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18683
* [Misc] Fixed the abnormally high TTFT issue in the PD disaggregation example by @zhaohaidao in https://github.com/vllm-project/vllm/pull/18644
* refactor: simplify request handler, use positive condition check for handler assignment by @googs1025 in https://github.com/vllm-project/vllm/pull/18690
* [Bugfix] Fix the lm_head in gpt_bigcode in lora mode by @maxdebayser in https://github.com/vllm-project/vllm/pull/6357
* [CI] add missing argument by @andyxning in https://github.com/vllm-project/vllm/pull/18694
* [GH] Add issue template for reporting CI failures by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18696
* [Doc] Fix issue template format by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18699
* [Bugfix] Fix Mistral-format models with sliding window by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18693
* [CI/Build] Replace `math.isclose` with `pytest.approx` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18703
* [CI] fix dump_input for str type by @andyxning in https://github.com/vllm-project/vllm/pull/18697
* [Model] Add support for YARN in NemotronNAS models by @Naveassaf in https://github.com/vllm-project/vllm/pull/18427
* [CI/Build] Split pooling and generation extended language models tests in CI by @Isotr0py in https://github.com/vllm-project/vllm/pull/18705
* [Hardware][Intel-Gaudi] [CI/Build] Add tensor parallel size = 2 test to HPU CI by @ldurejko in https://github.com/vllm-project/vllm/pull/18709
* [Misc] add AutoGen integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/18712
* [Bugfix]: handle hf-xet CAS error when loading Qwen3 weights in vLLM by @YanWuHao in https://github.com/vllm-project/vllm/pull/18701
* [Doc] Improve API docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18713
* [Doc] Move examples and further reorganize user guide by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18666
* [Bugfix] Fix Llama GGUF initialization by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18717
* [V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs by @lgeiger in https://github.com/vllm-project/vllm/pull/18608
* Convert `examples` to `ruff-format` by @hmellor in https://github.com/vllm-project/vllm/pull/18400
* [Model][Gemma3] Simplify image input validation by @lgeiger in https://github.com/vllm-project/vllm/pull/18710
* [Misc] improve web section group title display by @reidliu41 in https://github.com/vllm-project/vllm/pull/18684
* [V1][Quantization] Add CUDA graph compatible v1 GGUF support by @Isotr0py in https://github.com/vllm-project/vllm/pull/18646
* [Model][Gemma3] Cast image pixel values already on CPU by @lgeiger in https://github.com/vllm-project/vllm/pull/18732
* [FEAT] [ROCm] Upgrade AITER Fused MoE kernels. by @vllmellm in https://github.com/vllm-project/vllm/pull/18271
* [Doc] Update OOT model docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18742
* [Doc] Update reproducibility doc and example by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18741
* [Misc] improve docs by @reidliu41 in https://github.com/vllm-project/vllm/pull/18734
* feat(rocm-support): support mamba2 on rocm by @almersawi in https://github.com/vllm-project/vllm/pull/18565
* [Hardware][Intel-Gaudi] [CI/Build] Fix multiple containers using the same name in run-hpu-test.sh by @ldurejko in https://github.com/vllm-project/vllm/pull/18752
* [Doc] cleanup deprecated flag for doc by @calvin0327 in https://github.com/vllm-project/vllm/pull/18715
* Minor fix about MooncakeStoreConnector by @maobaolong in https://github.com/vllm-project/vllm/pull/18721
* [Build] fix cpu build missing libtbbmalloc.so by @kebe7jun in https://github.com/vllm-project/vllm/pull/18744
* [BUG FIX] minicpm by @huangyuxiang03 in https://github.com/vllm-project/vllm/pull/18739
* [Doc]  Convert Sphinx directives ( `{class}`, `{meth}`, `{attr}`, ...) to MkDocs format for better documentation linking by @Zerohertz in https://github.com/vllm-project/vllm/pull/18663
* [CI/Build] Remove imports of built-in `re` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18750
* [V1][Metrics] Add API for accessing in-memory Prometheus metrics by @markmc in https://github.com/vllm-project/vllm/pull/17010
* Disable prefix cache by default for benchmark by @cascade812 in https://github.com/vllm-project/vllm/pull/18639
* optimize get_kv_cache_torch_dtype by @chunxiaozheng in https://github.com/vllm-project/vllm/pull/18531
* [Core] Automatically cast multi-modal input dtype by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/18756
* [Bugfix] Mistral tool calling when content is list by @mgoin in https://github.com/vllm-project/vllm/pull/18729

## New Contributors
* @r-barnes made their first contribution in https://github.com/vllm-project/vllm/pull/17316
* @qscqesze made their first contribution in https://github.com/vllm-project/vllm/pull/16328
* @ponix-j made their first contribution in https://github.com/vllm-project/vllm/pull/17100
* @Zerohertz made their first contribution in https://github.com/vllm-project/vllm/pull/17342
* @a2q1p made their first contribution in https://github.com/vllm-project/vllm/pull/17387
* @mofanke made their first contribution in https://github.com/vllm-project/vllm/pull/17369
* @mayuyuace made their first contribution in https://github.com/vllm-project/vllm/pull/17364
* @casinca made their first contribution in https://github.com/vllm-project/vllm/pull/17400
* @mlinmg made their first contribution in https://github.com/vllm-project/vllm/pull/15826
* @alec-flowers made their first contribution in https://github.com/vllm-project/vllm/pull/16750
* @psav made their first contribution in https://github.com/vllm-project/vllm/pull/17457
* @nlzy made their first contribution in https://github.com/vllm-project/vllm/pull/17411
* @noyoshi made their first contribution in https://github.com/vllm-project/vllm/pull/15506
* @tishizaki made their first contribution in https://github.com/vllm-project/vllm/pull/17285
* @sstamenk made their first contribution in https://github.com/vllm-project/vllm/pull/17550
* @qthequartermasterman made their first contribution in https://github.com/vllm-project/vllm/pull/15428
* @CalebDu made their first contribution in https://github.com/vllm-project/vllm/pull/14568
* @Edwardf0t1 made their first contribution in https://github.com/vllm-project/vllm/pull/17561
* @xw285cornell made their first contribution in https://github.com/vllm-project/vllm/pull/16263
* @ehartford made their first contribution in https://github.com/vllm-project/vllm/pull/17605
* @thomasjpfan made their first contribution in https://github.com/vllm-project/vllm/pull/17616
* @s3woz made their first contribution in https://github.com/vllm-project/vllm/pull/17497
* @Xarbirus made their first contribution in https://github.com/vllm-project/vllm/pull/17408
* @bythew3i made their first contribution in https://github.com/vllm-project/vllm/pull/16458
* @dtransposed made their first contribution in https://github.com/vllm-project/vllm/pull/16839
* @BowenBao made their first contribution in https://github.com/vllm-project/vllm/pull/16943
* @vmarkovtsev made their first contribution in https://github.com/vllm-project/vllm/pull/17800
* @amd-hhashemi made their first contribution in https://github.com/vllm-project/vllm/pull/17071
* @qionghuang6 made their first contribution in https://github.com/vllm-project/vllm/pull/17753
* @RIckYuan999 made their first contribution in https://github.com/vllm-project/vllm/pull/17644
* @fxmarty-amd made their first contribution in https://github.com/vllm-project/vllm/pull/12612
* @inkcherry made their first contribution in https://github.com/vllm-project/vllm/pull/17853
* @tracelogfb made their first contribution in https://github.com/vllm-project/vllm/pull/17873
* @edrevo made their first contribution in https://github.com/vllm-project/vllm/pull/17786
* @xinli-centml made their first contribution in https://github.com/vllm-project/vllm/pull/17948
* @bbrowning made their first contribution in https://github.com/vllm-project/vllm/pull/17855
* @frieda-huang made their first contribution in https://github.com/vllm-project/vllm/pull/17032
* @Xu-Wenqing made their first contribution in https://github.com/vllm-project/vllm/pull/17784
* @bwshen-mi made their first contribution in https://github.com/vllm-project/vllm/pull/17433
* @arjunkathuria made their first contribution in https://github.com/vllm-project/vllm/pull/13779
* @calvin0327 made their first contribution in https://github.com/vllm-project/vllm/pull/18050
* @jinhuang12 made their first contribution in https://github.com/vllm-project/vllm/pull/17916
* @gilljon made their first contribution in https://github.com/vllm-project/vllm/pull/18083
* @Ecthlion made their first contribution in https://github.com/vllm-project/vllm/pull/18117
* @majianpeng made their first contribution in https://github.com/vllm-project/vllm/pull/18051
* @anko-intel made their first contribution in https://github.com/vllm-project/vllm/pull/17844
* @huachenheli made their first contribution in https://github.com/vllm-project/vllm/pull/18091
* @omahs made their first contribution in https://github.com/vllm-project/vllm/pull/18151
* @hustxiayang made their first contribution in https://github.com/vllm-project/vllm/pull/18187
* @eicherseiji made their first contribution in https://github.com/vllm-project/vllm/pull/17925
* @IwakuraRein made their first contribution in https://github.com/vllm-project/vllm/pull/17918
* @learner0810 made their first contribution in https://github.com/vllm-project/vllm/pull/18251
* @strangiato made their first contribution in https://github.com/vllm-project/vllm/pull/18264
* @princepride made their first contribution in https://github.com/vllm-project/vllm/pull/17937
* @sdavidbd made their first contribution in https://github.com/vllm-project/vllm/pull/18142
* @Nan2018 made their first contribution in https://github.com/vllm-project/vllm/pull/17590
* @googs1025 made their first contribution in https://github.com/vllm-project/vllm/pull/18314
* @shaoyuyoung made their first contribution in https://github.com/vllm-project/vllm/pull/18319
* @eladsegal made their first contribution in https://github.com/vllm-project/vllm/pull/18355
* @wenhuach21 made their first contribution in https://github.com/vllm-project/vllm/pull/17850
* @sunyicode0012 made their first contribution in https://github.com/vllm-project/vllm/pull/18337
* @Unprincess17 made their first contribution in https://github.com/vllm-project/vllm/pull/18330
* @rand-fly made their first contribution in https://github.com/vllm-project/vllm/pull/18363
* @rabi made their first contribution in https://github.com/vllm-project/vllm/pull/18460
* @giantcroc made their first contribution in https://github.com/vllm-project/vllm/pull/18410
* @hyoon1 made their first contribution in https://github.com/vllm-project/vllm/pull/17004
* @cyr0930 made their first contribution in https://github.com/vllm-project/vllm/pull/18479
* @elaineyz made their first contribution in https://github.com/vllm-project/vllm/pull/18274
* @lgeiger made their first contribution in https://github.com/vllm-project/vllm/pull/18347
* @RonaldBXu made their first contribution in https://github.com/vllm-project/vllm/pull/18034
* @zzzyq made their first contribution in https://github.com/vllm-project/vllm/pull/18430
* @shadeMe made their first contribution in https://github.com/vllm-project/vllm/pull/17731
* @Crucifixion-Fxl made their first contribution in https://github.com/vllm-project/vllm/pull/18454
* @MathieuBordere made their first contribution in https://github.com/vllm-project/vllm/pull/18623
* @Nalkey made their first contribution in https://github.com/vllm-project/vllm/pull/18647
* @ztang2370 made their first contribution in https://github.com/vllm-project/vllm/pull/18625
* @zhaohaidao made their first contribution in https://github.com/vllm-project/vllm/pull/18644
* @ldurejko made their first contribution in https://github.com/vllm-project/vllm/pull/18709
* @YanWuHao made their first contribution in https://github.com/vllm-project/vllm/pull/18701
* @almersawi made their first contribution in https://github.com/vllm-project/vllm/pull/18565
* @huangyuxiang03 made their first contribution in https://github.com/vllm-project/vllm/pull/18739
* @chunxiaozheng made their first contribution in https://github.com/vllm-project/vllm/pull/18531

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.5.post1...v0.9.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.9.0)

---

## v0.8.5.post1: v0.8.5.post1
**Published:** 2025-05-02

This post release contains two bug fix for memory leak and model accuracy

* Fix Memory Leak in `_cached_reqs_data` (#17567)
* Fix sliding window attention in V1 giving incorrect results (#17574)

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.5...v0.8.5.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.5.post1)

---

## v0.8.5: v0.8.5
**Published:** 2025-04-28

This release contains 310 commits from 143 contributors (55 new contributors!). 

## Highlights
This release features important multi-modal bug fixes, day 0 support for Qwen3, and xgrammar's structure tag feature for tool calling.

### Model Support
* Day 0 support for Qwen3 and Qwen3MoE. This release fixes fp8 weight loading (#17318) and adds tuned MoE configs (#17328).
* Add ModernBERT (#16648)
* Add Granite Speech Support (#16246)
* Add PLaMo2 (#14323)
* Add Kimi-VL model support (#16387)
* Add Qwen2.5-Omni model support (thinker only) (#15130)
* Snowflake Arctic Embed (Family)  (#16649)
* Accuracy fixes for Llama4 Int4 (#16801), chat template for Llama 4 models (#16428), enhanced AMD support (#16674, #16847)

### V1 Engine
* Add `structural_tag` support using xgrammar (#17085)
* Disaggregated serving:
	* KV Connector API V1 (#15960)
	* Adding LMCache KV connector for v1 (#16625)
* Clean up: Remove Sampler from Model Code (#17084)
* MLA: Simplification to batch P/D reordering (#16673)
* Move usage stats to worker and start logging TPU hardware (#16211)
* Support FlashInfer Attention (#16684)
* Faster incremental detokenization (#15137)
* EAGLE-3 Support (#16937)

### Features
* Validate urls object for multimodal content parts (#16990)
* Prototype support sequence parallelism using compilation pass (#16155)
* Add sampling params to `v1/audio/transcriptions` endpoint (#16591)
* Enable vLLM to Dynamically Load LoRA from a Remote Server (#10546)
* Add `vllm bench [latency, throughput]` CLI commands (#16508)

### Performance
* Attention: 
	* FA3 decode perf improvement - single mma warp group support for head dim 128 (#16864)
	* Update to lastest FA3 code (#13111)
	* Support Cutlass MLA for Blackwell GPUs (#16032)
* MoE:
	* Add expert_map support to Cutlass FP8 MOE (#16861)
	* Add fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 on NVIDIA H20 (#16753)
* Support Microsoft Runtime Kernel Lib for our Low Precision Computation - BitBLAS (#6036)
* Optimize rotary_emb implementation to use Triton operator for improved performance (#16457)

### Hardwares
* TPU: 
	* Enable structured decoding on TPU V1 (#16499)
	* Capture multimodal encoder during model compilation (#15051)
	* Enable Top-P (#16843)
* AMD:
	* AITER Fused MOE V1 Support (#16752)
	* Integrate Paged Attention Kernel from AITER (#15001)
	* Support AITER MLA (#15893)
	* Upstream prefix prefill speed up for vLLM V1 (#13305)
	* Adding fp8 and variable length sequence support to Triton FAv2 kernel (#12591)
	* Add skinny gemms for unquantized linear on ROCm (#15830)
	* Follow-ups for Skinny Gemms on ROCm. (#17011)

### Documentation
* Add open-webui example (#16747)
* Document Matryoshka Representation Learning support (#16770)
* Add a security guide (#17230)
* Add example to run DeepSeek with Ray Serve LLM (#17134)
* Benchmarks for audio models (#16505)

### Security and Dependency Updates
* Don't bind tcp zmq socket to all interfaces (#17197)
* Use safe serialization and fix zmq setup for mooncake pipe (#17192)
* Bump Transformers to 4.51.3 (#17116)

### Build and testing
* Add property-based testing for vLLM endpoints using an API defined by an OpenAPI 3.1 schema (#16721)

### Breaking changes ðŸš¨
* `--enable-chunked-prefill`, `--multi-step-stream-outputs`, `--disable-chunked-mm-input` can no longer explicitly be set to `False`. Instead, add `no-` to the start of the argument (i.e. `--enable-chunked-prefill` and `--no-enable-chunked-prefill`) (https://github.com/vllm-project/vllm/pull/16533)

## What's Changed
* Improve configs - `SchedulerConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16533
* [Misc] remove warning if triton>=3.2.0 by @DefTruth in https://github.com/vllm-project/vllm/pull/16553
* [Misc] refactor examples by @reidliu41 in https://github.com/vllm-project/vllm/pull/16563
* [Misc] Update usage with mooncake lib for kv transfer by @ShangmingCai in https://github.com/vllm-project/vllm/pull/16523
* [fix]: Dockerfile.ppc64le fixes for opencv-python and hf-xet by @Shafi-Hussain in https://github.com/vllm-project/vllm/pull/16048
* [Bugfix] Multi-modal caches not acting like LRU caches by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16593
* [TPU][V1] Fix exponential padding when `max-num-batched-tokens` is not a power of 2 by @NickLucche in https://github.com/vllm-project/vllm/pull/16596
* Fix triton install condition on CPU by @hmellor in https://github.com/vllm-project/vllm/pull/16600
* s390x: Fix PyArrow build and add CPU test script for Buildkite CI by @Nash-123 in https://github.com/vllm-project/vllm/pull/16036
* [Model][VLM] Add Kimi-VL model support by @courage17340 in https://github.com/vllm-project/vllm/pull/16387
* [Hardware][TPU] Add torchvision to tpu dependency file by @lsy323 in https://github.com/vllm-project/vllm/pull/16616
* [DOC][TPU] Add core idea about avoiding recompilation after warmup by @yaochengji in https://github.com/vllm-project/vllm/pull/16614
* config check sleep mode support oot platforms by @celestialli in https://github.com/vllm-project/vllm/pull/16562
* [Core][Bugfix] Fix Offline MM Beam Search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/16390
* [Kernel] moe wna16 marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/14447
* [BugFix]: Update minimum `pyzmq` version by @taneem-ibrahim in https://github.com/vllm-project/vllm/pull/16549
* [Bugfix] Fix tests/kernels/test_mamba_ssm_ssd.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/16623
* [Bugfix] Fix broken GritLM model and tests (missing pooling_metadata) by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/16631
* Add `vllm bench [latency, throughput]` CLI commands by @mgoin in https://github.com/vllm-project/vllm/pull/16508
* Fix vLLM x torch.compile config caching by @zou3519 in https://github.com/vllm-project/vllm/pull/16491
* [Misc] refactor argument parsing in examples by @reidliu41 in https://github.com/vllm-project/vllm/pull/16635
* [CI/Build] Fix LoRA OOM by @jeejeelee in https://github.com/vllm-project/vllm/pull/16624
* Add "/server_info" endpoint in api_server to retrieve the vllm_config.Â  by @Cangxihui in https://github.com/vllm-project/vllm/pull/16572
* [Kernel] Remove redundant Exp calculations by @DefTruth in https://github.com/vllm-project/vllm/pull/16123
* [Misc] Update `compressed-tensors` WNA16 to support zero-points by @dsikka in https://github.com/vllm-project/vllm/pull/14211
* [Misc] Enable vLLM to Dynamically Load LoRA from a Remote Server by @angkywilliam in https://github.com/vllm-project/vllm/pull/10546
* [Model] Add PLaMo2 by @Alnusjaponica in https://github.com/vllm-project/vllm/pull/14323
* [Bugfix] fix gpu docker image mis benchmarks dir by @lengrongfu in https://github.com/vllm-project/vllm/pull/16628
* [Misc] Modify LRUCache touch by @jeejeelee in https://github.com/vllm-project/vllm/pull/16689
* Disable remote caching when calling compile_fx by @zou3519 in https://github.com/vllm-project/vllm/pull/16611
* [Feature] add model aware kv ops helper by @billishyahao in https://github.com/vllm-project/vllm/pull/16020
* [ROCM] Bind triton version to 3.2 in requirements-built.txt  by @SageMoore in https://github.com/vllm-project/vllm/pull/16664
* [V1][Structured Output] Move xgrammar related utils to `backend_xgrammar.py` by @shen-shanshan in https://github.com/vllm-project/vllm/pull/16578
* [CI] Cleanup `additional_dependencies: [toml]` for pre-commit yapf hook by @yankay in https://github.com/vllm-project/vllm/pull/16405
* [Misc] refactor examples series by @reidliu41 in https://github.com/vllm-project/vllm/pull/16708
* [Doc] Improve OOM troubleshooting by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16704
* [Bugfix][Kernel] fix potential cuda graph broken for merge_attn_states kernel by @DefTruth in https://github.com/vllm-project/vllm/pull/16693
* [Model] support modernbert  by @xsank in https://github.com/vllm-project/vllm/pull/16648
* [Hardware] Add processor inputs to platform validation by @joerunde in https://github.com/vllm-project/vllm/pull/16680
* Improve error for structured output backend selection by @hmellor in https://github.com/vllm-project/vllm/pull/16717
* [Misc] Remove redundant comment by @jianzs in https://github.com/vllm-project/vllm/pull/16703
* Help user create custom model for Transformers backend remote code models by @hmellor in https://github.com/vllm-project/vllm/pull/16719
* [V1][Performance] Implement custom serializaton for MultiModalKwargs [Rebased] by @p88h in https://github.com/vllm-project/vllm/pull/16432
* [V1][Spec Dec Bug Fix] Respect Spec Dec Method Specification by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/16636
* Adding vllm buildkite job for IBM Power by @AaruniAggarwal in https://github.com/vllm-project/vllm/pull/16679
* [V1][Frontend] Improve Shutdown And Logs by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11737
* [rocm][V0] fix selection logic for custom PA in V0 by @divakar-amd in https://github.com/vllm-project/vllm/pull/16426
* [Bugfix] Update Florence-2 tokenizer to make grounding tasks work by @Isotr0py in https://github.com/vllm-project/vllm/pull/16734
* [Bugfix] Revert max_prompt_len validation for decoder-only models. by @davidheineman in https://github.com/vllm-project/vllm/pull/16741
* [V1] Remove log noise when idle by @russellb in https://github.com/vllm-project/vllm/pull/16735
* [Ray] Improve documentation on batch inference by @richardliaw in https://github.com/vllm-project/vllm/pull/16609
* [misc] ignore marlin_moe_wna16 local gen codes by @DefTruth in https://github.com/vllm-project/vllm/pull/16760
* [Doc] Add more tips to avoid OOM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16765
* [doc] add open-webui example by @reidliu41 in https://github.com/vllm-project/vllm/pull/16747
* [Bugfix] Fix GLM4 model by @intervitens in https://github.com/vllm-project/vllm/pull/16618
* [Doc] Fix a 404 link in installation/cpu.md by @windsonsea in https://github.com/vllm-project/vllm/pull/16773
* [Misc] refactor examples series - lmcache by @reidliu41 in https://github.com/vllm-project/vllm/pull/16758
* Improve configs - `TokenizerPoolConfig` + `DeviceConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16603
* fix: hyperlink by @reidliu41 in https://github.com/vllm-project/vllm/pull/16778
* [Doc] Make sure to update vLLM when installing latest code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16781
* [Doc] Document Matryoshka Representation Learning support by @noooop in https://github.com/vllm-project/vllm/pull/16770
* [Doc] Changed explanation of generation_tokens_total and prompt_tokens_total counter type metrics to avoid confusion by @insukim1994 in https://github.com/vllm-project/vllm/pull/16784
* [V1][Perf] Faster incremental detokenization by @njhill in https://github.com/vllm-project/vllm/pull/15137
* [Bugfix]Fix index out of range error in api server log by @WangErXiao in https://github.com/vllm-project/vllm/pull/16787
* [Kernel] Add fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 on NVIDIA H20 by @Ximingwang-09 in https://github.com/vllm-project/vllm/pull/16753
* [Model] use AutoWeightsLoader for olmoe,opt,orion,persimmon,phi3_small by @lengrongfu in https://github.com/vllm-project/vllm/pull/16548
* [TPU][V1] Fix padding recompilation when `max-num-batched-tokens` is not even by @NickLucche in https://github.com/vllm-project/vllm/pull/16726
* [V1][TPU] Enable Top K by @NickLucche in https://github.com/vllm-project/vllm/pull/15489
* [ROCM] enable aiter fused moe kernel for llama4 bf16 checkpoints by @sijiac in https://github.com/vllm-project/vllm/pull/16674
* [V1][Metrics] Fix http metrics middleware by @markmc in https://github.com/vllm-project/vllm/pull/15894
* [MLA] Simplification to batch P/D reordering by @njhill in https://github.com/vllm-project/vllm/pull/16673
* [P/D][V1] KV Connector API V1 by @ApostaC in https://github.com/vllm-project/vllm/pull/15960
* [Attention] Update to lastest FA3 code by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13111
* Add property-based testing for vLLM endpoints using an API defined by an OpenAPI 3.1 schema by @tarukumar in https://github.com/vllm-project/vllm/pull/16721
* [Doc] Improve help examples for `--compilation-config` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16729
* [Misc] Update outdated note: LMCache now supports chunked prefill by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16697
* [V1][Structured Output] Minor modification to `_validate_structured_output()` by @shen-shanshan in https://github.com/vllm-project/vllm/pull/16748
* Add hardware print to TPU V1 test by @mgoin in https://github.com/vllm-project/vllm/pull/16792
* [BugFix] Accuracy fix for llama4 int4 - improperly casted scales by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/16801
* Improve configs - `MultiModalConfig` + `PoolerConfig` + `DecodingConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16789
* [Misc] add collect_env to cli and docker image by @lengrongfu in https://github.com/vllm-project/vllm/pull/16759
* [ROCm] [Attention] Cleanup ROCm output passing by @ProExpertProg in https://github.com/vllm-project/vllm/pull/16431
* [Bugfix] fix pp for llama4 by @luccafong in https://github.com/vllm-project/vllm/pull/16746
* [Doc] add podman setup instructions for official image by @nathan-weinberg in https://github.com/vllm-project/vllm/pull/16796
* [Docs] Fix a link and grammar issue in production-stack.md by @windsonsea in https://github.com/vllm-project/vllm/pull/16809
* [Model] use AutoWeightsLoader for BigCode, GPT-J by @jonghyunchoe in https://github.com/vllm-project/vllm/pull/16823
* [Misc] Clean up Kimi-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16833
* Fix `nullable_kvs` fallback by @hmellor in https://github.com/vllm-project/vllm/pull/16837
* [New Model]: Snowflake Arctic Embed (Family)  by @noooop in https://github.com/vllm-project/vllm/pull/16649
* [Misc] refactor examples series - Chat Completion Client With Tools by @reidliu41 in https://github.com/vllm-project/vllm/pull/16829
* [Doc] Updated Llama section in tool calling docs to have llama 3.2 config info by @jmho in https://github.com/vllm-project/vllm/pull/16857
* publish neuron docker image by @omrishiv in https://github.com/vllm-project/vllm/pull/16733
* [Model][VLM] Add Qwen2.5-Omni model support (thinker only) by @fyabc in https://github.com/vllm-project/vllm/pull/15130
* [rocm][MI300] llama4 maverick fp8 moe config tp8 by @divakar-amd in https://github.com/vllm-project/vllm/pull/16847
* [Frontend] Add sampling params to `v1/audio/transcriptions` endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/16591
* [Misc] Benchmarks for audio models by @NickLucche in https://github.com/vllm-project/vllm/pull/16505
* [V1][Misc] stop update prefix cache stats when logs_stats is disabled by @vie-serendipity in https://github.com/vllm-project/vllm/pull/16460
* [Model] Refactor Phi-4-multimodal to use merged processor and support V1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/15477
* [Model] Qwen2.5-Omni Cleanup  by @ywang96 in https://github.com/vllm-project/vllm/pull/16872
* [VLM] Clean up models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16873
* [doc] update hyperlink by @reidliu41 in https://github.com/vllm-project/vllm/pull/16877
* Log how much time loading a compiled artifact takes by @zou3519 in https://github.com/vllm-project/vllm/pull/16848
* Serialize tensors using int8 views by @p88h in https://github.com/vllm-project/vllm/pull/16866
* Improve configs - `CacheConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16835
* [easy] Pass compile_fx only the config patches by @zou3519 in https://github.com/vllm-project/vllm/pull/16845
* [Bugfix] Fix v1/spec_decode/test_ngram.py by @zixi-qi in https://github.com/vllm-project/vllm/pull/16895
* [CI/CD][V1] Add spec decode tests to CI by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16900
* [Bugfix] Fix distributed bug in Qwen2.5-VL & Qwen2.5-Omni by @fyabc in https://github.com/vllm-project/vllm/pull/16907
* [Doc] Split dummy_processor_inputs() in Multimodal Docs by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/16915
* Restore buffers when wake up from level 2 sleep (#16564) by @fingertap in https://github.com/vllm-project/vllm/pull/16889
* [Misc] fix collect_env version parse by @wangxiyuan in https://github.com/vllm-project/vllm/pull/15267
* [Misc] Refactor platform to get device specific stream and event by @shen-shanshan in https://github.com/vllm-project/vllm/pull/14411
* [Bugfix] Fix GLM rotary_dim issue and support v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/16912
* Raise error for data-parallel with benchmark_throughput by @kartikx in https://github.com/vllm-project/vllm/pull/16737
* [XPU][Bugfix] minor fix for XPU by @yma11 in https://github.com/vllm-project/vllm/pull/15591
* [doc] install required python3-dev apt package by @davidxia in https://github.com/vllm-project/vllm/pull/16888
* [Doc] mention how to install in CPU editable mode by @davidxia in https://github.com/vllm-project/vllm/pull/16923
* [Core] Speed up decode by remove synchronizing operation in sampler by @chanh in https://github.com/vllm-project/vllm/pull/16436
* [V1][Spec Decode] Handle draft tokens beyond max_model_len by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16087
* [TPU][V1] Implicitly adjust page size when there's SMEM OOM by @yaochengji in https://github.com/vllm-project/vllm/pull/16871
* Update Qwen1.5-MoE-W4A16-compressed-tensors.yaml by @mgoin in https://github.com/vllm-project/vllm/pull/16946
* [TPU][V1] Capture multimodal encoder during model compilation by @NickLucche in https://github.com/vllm-project/vllm/pull/15051
* [V1] V1 FlashInfer Attention by @mgoin in https://github.com/vllm-project/vllm/pull/16684
* [TPU][V1] Enable Top-P by @NickLucche in https://github.com/vllm-project/vllm/pull/16843
* [Doc] Remove unnecessary V1 flag by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16924
* [BugFix][Spec Decode] No in-place update to draft probs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16952
* [Bugfix]: fix issue with n>1 sampling on v1 requests overriding each other by @jeffrey-dot-li in https://github.com/vllm-project/vllm/pull/16863
* [ROCm] Add aiter tkw1 kernel for Llama4 fp8 by @kliuae in https://github.com/vllm-project/vllm/pull/16727
* [Misc] Remove the chunked prefill warning for LoRA  by @jeejeelee in https://github.com/vllm-project/vllm/pull/16925
* [Kernel] Add expert_map support to Cutlass FP8 MOE by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/16861
* [V1] Remove additional_config check by @wangxiyuan in https://github.com/vllm-project/vllm/pull/16710
* [Performance][ROCm] Add skinny gemms for unquantized linear on ROCm by @charlifu in https://github.com/vllm-project/vllm/pull/15830
* Support S3 Sharded loading with RunAI Model Streamer by @omer-dayan in https://github.com/vllm-project/vllm/pull/16317
* [Bugfix] Fix f-string for Python 3.9-3.11 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16962
* [Doc] Update ai_accelerator/hpu-gaudi.inc.md by @windsonsea in https://github.com/vllm-project/vllm/pull/16956
* [Perf] Optimize `_update_states` for GPU model runner by @SnowCharmQ in https://github.com/vllm-project/vllm/pull/16910
* [Bugfix] Fix the issue where llm.generate cannot be called repeatedly after setting GuidedDecodingParams by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16767
* [Model] Use autoweightloader for mamba by @sfeng33 in https://github.com/vllm-project/vllm/pull/16950
* [V1] Remove pre-allocation for KV cache by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16941
* [Kernel] Support Microsoft Runtime Kernel Lib for our Low Precision Computation - BitBLAS by @LeiWang1999 in https://github.com/vllm-project/vllm/pull/6036
* [BugFix] Fix incremental detokenization perf issue by @njhill in https://github.com/vllm-project/vllm/pull/16963
* [Doc] Improve documentation for multimodal CLI args by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16960
* [FEAT][ROCm] Integrate Paged Attention Kernel from AITER by @vllmellm in https://github.com/vllm-project/vllm/pull/15001
* [Misc] refactor example series by @reidliu41 in https://github.com/vllm-project/vllm/pull/16972
* [Bugfix] Fix distributed bug again in Qwen2.5-VL & Qwen2.5-Omni by @fyabc in https://github.com/vllm-project/vllm/pull/16974
* Improve configs - `SpeculativeConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16971
* [BugFix] Pass in correct VLLM config in FlashInfer backend (#13207) by @timzsu in https://github.com/vllm-project/vllm/pull/16973
* [Misc] Add S3 environment variables for better support of MinIO. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16977
* [frontend] enhance tool_calls type check by @reidliu41 in https://github.com/vllm-project/vllm/pull/16882
* [FEAT][ROCm]: Support AITER MLA by @vllmellm in https://github.com/vllm-project/vllm/pull/15893
* Add assertion for no objects while hashing hf_config by @zou3519 in https://github.com/vllm-project/vllm/pull/16930
* Fencing Kernels Tests for enabling on AMD by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/16929
* [BugFix] Remove default multiproc executor `collective_rpc` timeout by @njhill in https://github.com/vllm-project/vllm/pull/17000
* [Core][V1][TPU] Enable structured decoding on TPU V1 by @Chenyaaang in https://github.com/vllm-project/vllm/pull/16499
* [Bugfix] validate urls object for multimodal content parts by @gcalmettes in https://github.com/vllm-project/vllm/pull/16990
* add Dockerfile build vllm against torch nightly by @yangw-dev in https://github.com/vllm-project/vllm/pull/16936
* [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 by @maleksan85 in https://github.com/vllm-project/vllm/pull/13305
* [V1][DP] More robust DP/EP dummy request coordination by @njhill in https://github.com/vllm-project/vllm/pull/16277
* [BugFix] Revert ROCm Custom Paged Attention Env Flag Check by @vllmellm in https://github.com/vllm-project/vllm/pull/17022
* Revert "[Misc] Add S3 environment variables for better support of MinIO." by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/17021
* [misc] tune some env vars for GB200 by @youkaichao in https://github.com/vllm-project/vllm/pull/16992
* [INTEL-HPU][v0] Port delayed sampling to upstream by @xuechendi in https://github.com/vllm-project/vllm/pull/16949
* [doc] add download path tips by @reidliu41 in https://github.com/vllm-project/vllm/pull/17013
* [Bugfix] Triton FA function takes no keyword arguments by @vllmellm in https://github.com/vllm-project/vllm/pull/16902
* [V1] Avoid socket errors during shutdown when requests are in in-flight by @njhill in https://github.com/vllm-project/vllm/pull/16807
* [BugFix] llama4 fa3 fix - RuntimeError: scheduler_metadata must have shape (metadata_size) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/16998
* [Misc] Improve readability of get_open_port function. by @gitover22 in https://github.com/vllm-project/vllm/pull/17024
* [Bugfix] Fix AssertionError: skip_special_tokens=False is not supported for Mistral tokenizers by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16964
* [CI] Run v1/test_serial_utils.py in CI by @russellb in https://github.com/vllm-project/vllm/pull/16996
* Mistral-format support for compressed-tensors by @mgoin in https://github.com/vllm-project/vllm/pull/16803
* Categorize `tests/kernels/` based on kernel type by @mgoin in https://github.com/vllm-project/vllm/pull/16799
* [Doc] Add top anchor and a note to quantization/bitblas.md by @windsonsea in https://github.com/vllm-project/vllm/pull/17042
* Ensure that `pid` passed to `kill_process_tree` is `int` for `mypy` by @hmellor in https://github.com/vllm-project/vllm/pull/17051
* [CI] Update structured-output label automation by @russellb in https://github.com/vllm-project/vllm/pull/17055
* Improve Transformers backend model loading QoL by @hmellor in https://github.com/vllm-project/vllm/pull/17039
* `CacheConfig.block_size` should always be `int` when used by @hmellor in https://github.com/vllm-project/vllm/pull/17052
* Use `@property` and private field for `data_parallel_rank_local` by @hmellor in https://github.com/vllm-project/vllm/pull/17053
* [Frontend] Support guidance:no-additional-properties for compatibility with xgrammar by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/15949
* [BugFix][V1] Fix int32 token index overflow when preparing input ids by @sarckk in https://github.com/vllm-project/vllm/pull/16806
* [V1][Spec Decode] Always use argmax for sampling draft tokens  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16899
* [CI/Build] workaround for CI build failure by @csy1204 in https://github.com/vllm-project/vllm/pull/17070
* [Quantization]add prefix for commandA quantized model by @CXIAAAAA in https://github.com/vllm-project/vllm/pull/17017
* [Minor] Use larger batch sizes for A100/B100/B200/MI300x by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17073
* [Bugfix] Enable V1 usage stats by @mgoin in https://github.com/vllm-project/vllm/pull/16986
* More informative error when using Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/16988
* Addendum Fix to support FIPS enabled machines with MD5 hashing by @sydarb in https://github.com/vllm-project/vllm/pull/17043
* [Bugfix][Core] add seq_id_to_seq_group clearing to avoid memory leak when sâ€¦ by @zhangyuygss in https://github.com/vllm-project/vllm/pull/16472
* [V1] Update structured output by @reidliu41 in https://github.com/vllm-project/vllm/pull/16812
* [doc] update to hyperlink by @reidliu41 in https://github.com/vllm-project/vllm/pull/17096
* Add docs for runai_streamer_sharded by @omer-dayan in https://github.com/vllm-project/vllm/pull/17093
* [Chore] Remove Sampler from Model Code by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17084
* Disable enforce_eager for V1 TPU sampler and structured output tests by @mgoin in https://github.com/vllm-project/vllm/pull/17016
* Simplify `TokenizerGroup` by @hmellor in https://github.com/vllm-project/vllm/pull/16790
* Fix OOT registration test by @hmellor in https://github.com/vllm-project/vllm/pull/17099
* [V1][PP] Optimization: continue scheduling prefill chunks by @ruisearch42 in https://github.com/vllm-project/vllm/pull/17080
* [Misc] Remove OLMo2 config copy by @Isotr0py in https://github.com/vllm-project/vllm/pull/17066
* Improve static type checking in `LoRAModelRunnerMixin` by @hmellor in https://github.com/vllm-project/vllm/pull/17104
* [V1][Structured Output] Clear xgrammar compiler object when engine core shut down to avoid nanobind leaked warning by @shen-shanshan in https://github.com/vllm-project/vllm/pull/16954
* [Frontend] Using matryoshka_dimensions control the allowed output dimensions. by @noooop in https://github.com/vllm-project/vllm/pull/16970
* Add missing rocm_skinny_gemms kernel test to CI by @mgoin in https://github.com/vllm-project/vllm/pull/17060
* [Misc] refactor example series - structured outputs by @reidliu41 in https://github.com/vllm-project/vllm/pull/17040
* [V1][Spec Decoding] Add num_drafts and num_accepted_tokens_per_position metrics by @markmc in https://github.com/vllm-project/vllm/pull/16665
* [CI] Add automation for the `tool-calling` github label by @russellb in https://github.com/vllm-project/vllm/pull/17118
* Updating builkite job for IBM Power  by @AaruniAggarwal in https://github.com/vllm-project/vllm/pull/17111
* existing torch installation pip command fix for docs by @atilla00 in https://github.com/vllm-project/vllm/pull/17059
* Molmo Requirements by @Eyshika in https://github.com/vllm-project/vllm/pull/17026
* Add `:markdownhelp:` to `EngineArgs` docs so markdown docstrings render properly by @hmellor in https://github.com/vllm-project/vllm/pull/17124
* Improve configs - `LoRAConfig` + `PromptAdapterConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16980
* [Docs] Generate correct github links for decorated functions by @russellb in https://github.com/vllm-project/vllm/pull/17125
* Add collective_rpc to llm engine by @yinghai in https://github.com/vllm-project/vllm/pull/16999
* Add chat template for Llama 4 models by @maxdebayser in https://github.com/vllm-project/vllm/pull/16428
* [Misc] Add example to run DeepSeek with Ray Serve LLM by @ruisearch42 in https://github.com/vllm-project/vllm/pull/17134
* Better error message for missing mistral params.json by @mgoin in https://github.com/vllm-project/vllm/pull/17132
* Use custom address for listening socket by @jglaser in https://github.com/vllm-project/vllm/pull/15988
* [FEAT] [ROCm]: AITER Fused MOE V1 Support by @vllmellm in https://github.com/vllm-project/vllm/pull/16752
* [Attention] FA3 decode perf improvement - single mma warp group support for head dim 128 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/16864
* fix float16 support for kimi-vl by @zhouzaida in https://github.com/vllm-project/vllm/pull/17156
* [Doc] V1 : Update LoRA status by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/17133
* [Docs] Fix True->true in supported_models.md by @mgoin in https://github.com/vllm-project/vllm/pull/17141
* Move missed `SchedulerConfig` args into scheduler config group in `EngineArgs` by @hmellor in https://github.com/vllm-project/vllm/pull/17131
* [Misc] Clean up redundant code in uniproc_executor.py by @lifuhuang in https://github.com/vllm-project/vllm/pull/16762
* [Bugfix][Misc] Use TritonPlaceholderModule to defensively import triton by @MengqingCao in https://github.com/vllm-project/vllm/pull/15099
* [Misc] Benchmark Serving Script Support Appending Results by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17028
* [Perf]Optimize rotary_emb implementation to use Triton operator for improved inference performance by @cynthieye in https://github.com/vllm-project/vllm/pull/16457
* [Bugfix] remove fallback in guided_json (int range, patterns) by @csy1204 in https://github.com/vllm-project/vllm/pull/16725
* [Quantization][FP8] Add support for FP8 models with input_scale for output projection and QK quantization by @rasmith in https://github.com/vllm-project/vllm/pull/15734
* [Doc] Add headings to improve gptqmodel.md by @windsonsea in https://github.com/vllm-project/vllm/pull/17164
* Only turn on FastIncrementalDetokenizer when tokenizers >= 0.21.1 by @houseroad in https://github.com/vllm-project/vllm/pull/17158
* [Doc] Add two links to disagg_prefill.md by @windsonsea in https://github.com/vllm-project/vllm/pull/17168
* [Doc] Move todo out of beam search docstring by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/17183
* [Bugfix] Fix mistral model tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17181
* [Bugfix] Fix Mistral ChatCompletionRequest Body Exception by @JasmondL in https://github.com/vllm-project/vllm/pull/16769
* Bump Transformers to 4.51.3 by @hmellor in https://github.com/vllm-project/vllm/pull/17116
* Use Transformers helper `get_text_config()` instead of checking for `text_config` by @hmellor in https://github.com/vllm-project/vllm/pull/17105
* [doc] update wrong hf model links by @reidliu41 in https://github.com/vllm-project/vllm/pull/17184
* [Misc] Inline Molmo requirements by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17190
* [Security] Use safe serialization and fix zmq setup for mooncake pipe by @russellb in https://github.com/vllm-project/vllm/pull/17192
* [V1] Move usage stats to worker and start logging TPU hardware by @dyli-google in https://github.com/vllm-project/vllm/pull/16211
* [Bugfix] Fix hybrid model tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17182
* Fix Python packaging edge cases by @tiran in https://github.com/vllm-project/vllm/pull/17159
* [BugFix][Frontend] Fix `LLM.chat()` tokenization by @njhill in https://github.com/vllm-project/vllm/pull/16081
* [V1][Spec Decode] EAGLE-3 Support by @benchislett in https://github.com/vllm-project/vllm/pull/16937
* [Misc] Refine ray_serve_deepseek example by @ruisearch42 in https://github.com/vllm-project/vllm/pull/17204
* [Bugfix] gemma[2,3] interleaved attention when sliding window is disabled by @heheda12345 in https://github.com/vllm-project/vllm/pull/17180
* [AMD][FP8][BugFix] Remove V1 check in arg_utils.py for FP8 since it is not necessary by @rasmith in https://github.com/vllm-project/vllm/pull/17215
* [v1] [P/D] Adding LMCache KV connector for v1 by @ApostaC in https://github.com/vllm-project/vllm/pull/16625
* [Bugfix] [pytorch] Patch AOTAutogradCache._get_shape_env by @jamesjwu in https://github.com/vllm-project/vllm/pull/17142
* [MISC][AMD] Add unused annotation to rocm kernel file by @houseroad in https://github.com/vllm-project/vllm/pull/17097
* [doc] add Anything LLM integration by @reidliu41 in https://github.com/vllm-project/vllm/pull/17216
* [Minor][Spec Decode] Add use_eagle to SpeculativeConfig by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17213
* [Doc] Minor fix for the vLLM TPU setup page by @yarongmu-google in https://github.com/vllm-project/vllm/pull/17206
* [Minor][Models] Fix Return Types of Llama & Eagle by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17220
* Allocate kv_cache with stride order by @wenscarl in https://github.com/vllm-project/vllm/pull/16605
* [ROCm][Misc] Follow-ups for Skinny Gemms on ROCm. by @charlifu in https://github.com/vllm-project/vllm/pull/17011
* [V1][Metrics] Allow V1 AsyncLLM to use custom logger by @liuzijing2014 in https://github.com/vllm-project/vllm/pull/14661
* [BugFix] Avoid race conditions in zero-copy tensor transmission by @njhill in https://github.com/vllm-project/vllm/pull/17203
* [CI/test] Fix Eagle Correctness Test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17209
* [Core] Remove prompt string from engine core data structures by @njhill in https://github.com/vllm-project/vllm/pull/17214
* [Bugfix] Fix missing int type for `-n` in multi-image example by @Isotr0py in https://github.com/vllm-project/vllm/pull/17223
* [Bugfix] Fix standard models tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17217
* [Hardware][Intel-Gaudi] Update hpu-extension and update bucketing system for HPU device by @adobrzyn in https://github.com/vllm-project/vllm/pull/17186
* [V1] Add `structural_tag` support using xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/17085
* [BUGFIX] use random for NONE_HASH only when PYTHONHASHSEED not set by @andyxning in https://github.com/vllm-project/vllm/pull/17088
* [Chore] added stubs for `vllm_flash_attn` during development mode by @aarnphm in https://github.com/vllm-project/vllm/pull/17228
* [Docs] Update structured output doc for V1 by @russellb in https://github.com/vllm-project/vllm/pull/17135
* [Bugfix] fix error due to an uninitialized tokenizer when using `skip_tokenizer_init` with `num_scheduler_steps` by @junstar92 in https://github.com/vllm-project/vllm/pull/9276
* Disable the torch.compile cache checks when VLLM_DISABLE_COMPILE_CACHE=1 by @houseroad in https://github.com/vllm-project/vllm/pull/16573
* [MISC] rename interval to max_recent_requests by @andyxning in https://github.com/vllm-project/vllm/pull/14285
* [Bugfix] Fix Qwen2.5-Omni M-RoPE position ids generation by @imkero in https://github.com/vllm-project/vllm/pull/16878
* [Minor] Fix lint error in main branch by @WoosukKwon in https://github.com/vllm-project/vllm/pull/17233
* [CI/Build] remove -t for run-lm-eval-gsm-hf-baseline.sh by @reidliu41 in https://github.com/vllm-project/vllm/pull/16271
* Update test_flash_attn.py by @ShuaibinLi in https://github.com/vllm-project/vllm/pull/17102
* [Kernel][Triton][FP8] Adding fp8 and variable length sequence support to Triton FAv2 kernel by @rasmith in https://github.com/vllm-project/vllm/pull/12591
* [Misc] Make cached tokenizer pickle-compatible by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17048
* [Bugfix] Fix QWen2 VL multimodal mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/17240
* [Bugfix] Get a specific type of layer from forward context by @heheda12345 in https://github.com/vllm-project/vllm/pull/17222
* [MISC] Use string annotation types for class definitions by @jianzs in https://github.com/vllm-project/vllm/pull/17244
* [Misc] Change buckets of histogram_iteration_tokens to [1, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8096] to represent number of tokens by @sfc-gh-zhwang in https://github.com/vllm-project/vllm/pull/17033
* [Bugfix] Fix Lora Name Parsing by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/17196
* [NVIDIA] Support Cutlass MLA for Blackwell GPUs by @kaixih in https://github.com/vllm-project/vllm/pull/16032
* [Feature] support sequence parallelism using compilation pass by @cascade812 in https://github.com/vllm-project/vllm/pull/16155
* [doc] Add feature status legend by @reidliu41 in https://github.com/vllm-project/vllm/pull/17257
* [Metrics] Fix minor inconsistencies in bucket progression by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17262
* [V1][Spec Decode] Make eagle compatible with prefix caching. by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/17137
* [BugFix] Fix vllm_flash_attn install issues by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17267
* [Bugfix] Fix missing ARG in Dockerfile for arm64 platforms by @lkm-schulz in https://github.com/vllm-project/vllm/pull/17261
* [Bugfix] Fix cutlass dispatch for fp8/int8 to properly invoke M<=16 câ€¦ by @Ther-LF in https://github.com/vllm-project/vllm/pull/16751
* [Bugfix] Fix Mistral3 spatial merge error by @mgoin in https://github.com/vllm-project/vllm/pull/17270
* [Doc] Fix wrong github link in LMCache examples by @KuntaiDu in https://github.com/vllm-project/vllm/pull/17274
* [Doc] small fix by @reidliu41 in https://github.com/vllm-project/vllm/pull/17277
* [Misc] Validate `stop_token_ids` contents by @njhill in https://github.com/vllm-project/vllm/pull/17268
* [Minor][Models] Pass partial_rotary_factor parameter to rope by @Eviannn in https://github.com/vllm-project/vllm/pull/17266
* [Core] Remove legacy input mapper/processor from V0 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15686
* [Model] Add Granite Speech Support by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/16246
* Update tpu_worker.py 's typo by @idouba in https://github.com/vllm-project/vllm/pull/17288
* Add missing class docstring for `PromptAdapterConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/17302
* [Bugfix] Add missing `get_language_model` to new MLLMs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17300
* [doc] update wrong model id by @reidliu41 in https://github.com/vllm-project/vllm/pull/17287
* [Misc] Minor typo/grammar in `platforms/interface.py` by @NickLucche in https://github.com/vllm-project/vllm/pull/17307
* [Misc] Clean up Qwen2.5-Omni code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/17301
* [Docs] Add a security guide by @russellb in https://github.com/vllm-project/vllm/pull/17230
* Improve conversion from dataclass configs to argparse arguments by @hmellor in https://github.com/vllm-project/vllm/pull/17303
* Make name of `compressed-tensors` quant method consistent across vLLM by @hmellor in https://github.com/vllm-project/vllm/pull/17255
* Explicitly explain quant method override ordering and ensure all overrides are ordered by @hmellor in https://github.com/vllm-project/vllm/pull/17256
* [Security] Don't bind tcp zmq socket to all interfaces by @russellb in https://github.com/vllm-project/vllm/pull/17197
* [Chore] cleanup license indicators in light of SPDX by @aarnphm in https://github.com/vllm-project/vllm/pull/17259
* [BugFix] Fix cascade attention - RuntimeError: scheduler_metadata must have shape (metadata_size) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/17283
* [Bugfix] Fix moe weight losing all extra attrs after `process_weights_after_loading`. by @charlifu in https://github.com/vllm-project/vllm/pull/16854
* [Model] Qwen3 Dense FP8 Compat Fixes by @simon-mo in https://github.com/vllm-project/vllm/pull/17318

## New Contributors
* @Nash-123 made their first contribution in https://github.com/vllm-project/vllm/pull/16036
* @celestialli made their first contribution in https://github.com/vllm-project/vllm/pull/16562
* @taneem-ibrahim made their first contribution in https://github.com/vllm-project/vllm/pull/16549
* @Cangxihui made their first contribution in https://github.com/vllm-project/vllm/pull/16572
* @angkywilliam made their first contribution in https://github.com/vllm-project/vllm/pull/10546
* @Alnusjaponica made their first contribution in https://github.com/vllm-project/vllm/pull/14323
* @xsank made their first contribution in https://github.com/vllm-project/vllm/pull/16648
* @jianzs made their first contribution in https://github.com/vllm-project/vllm/pull/16703
* @p88h made their first contribution in https://github.com/vllm-project/vllm/pull/16432
* @AaruniAggarwal made their first contribution in https://github.com/vllm-project/vllm/pull/16679
* @davidheineman made their first contribution in https://github.com/vllm-project/vllm/pull/16741
* @richardliaw made their first contribution in https://github.com/vllm-project/vllm/pull/16609
* @intervitens made their first contribution in https://github.com/vllm-project/vllm/pull/16618
* @windsonsea made their first contribution in https://github.com/vllm-project/vllm/pull/16773
* @insukim1994 made their first contribution in https://github.com/vllm-project/vllm/pull/16784
* @Ximingwang-09 made their first contribution in https://github.com/vllm-project/vllm/pull/16753
* @sijiac made their first contribution in https://github.com/vllm-project/vllm/pull/16674
* @tarukumar made their first contribution in https://github.com/vllm-project/vllm/pull/16721
* @nathan-weinberg made their first contribution in https://github.com/vllm-project/vllm/pull/16796
* @jmho made their first contribution in https://github.com/vllm-project/vllm/pull/16857
* @vie-serendipity made their first contribution in https://github.com/vllm-project/vllm/pull/16460
* @zixi-qi made their first contribution in https://github.com/vllm-project/vllm/pull/16895
* @fingertap made their first contribution in https://github.com/vllm-project/vllm/pull/16889
* @kartikx made their first contribution in https://github.com/vllm-project/vllm/pull/16737
* @davidxia made their first contribution in https://github.com/vllm-project/vllm/pull/16888
* @chanh made their first contribution in https://github.com/vllm-project/vllm/pull/16436
* @jeffrey-dot-li made their first contribution in https://github.com/vllm-project/vllm/pull/16863
* @sfeng33 made their first contribution in https://github.com/vllm-project/vllm/pull/16950
* @LeiWang1999 made their first contribution in https://github.com/vllm-project/vllm/pull/6036
* @timzsu made their first contribution in https://github.com/vllm-project/vllm/pull/16973
* @yangw-dev made their first contribution in https://github.com/vllm-project/vllm/pull/16936
* @gitover22 made their first contribution in https://github.com/vllm-project/vllm/pull/17024
* @csy1204 made their first contribution in https://github.com/vllm-project/vllm/pull/17070
* @sydarb made their first contribution in https://github.com/vllm-project/vllm/pull/17043
* @zhangyuygss made their first contribution in https://github.com/vllm-project/vllm/pull/16472
* @atilla00 made their first contribution in https://github.com/vllm-project/vllm/pull/17059
* @Eyshika made their first contribution in https://github.com/vllm-project/vllm/pull/17026
* @yinghai made their first contribution in https://github.com/vllm-project/vllm/pull/16999
* @jglaser made their first contribution in https://github.com/vllm-project/vllm/pull/15988
* @zhouzaida made their first contribution in https://github.com/vllm-project/vllm/pull/17156
* @lifuhuang made their first contribution in https://github.com/vllm-project/vllm/pull/16762
* @JasmondL made their first contribution in https://github.com/vllm-project/vllm/pull/16769
* @tiran made their first contribution in https://github.com/vllm-project/vllm/pull/17159
* @jamesjwu made their first contribution in https://github.com/vllm-project/vllm/pull/17142
* @wenscarl made their first contribution in https://github.com/vllm-project/vllm/pull/16605
* @liuzijing2014 made their first contribution in https://github.com/vllm-project/vllm/pull/14661
* @adobrzyn made their first contribution in https://github.com/vllm-project/vllm/pull/17186
* @andyxning made their first contribution in https://github.com/vllm-project/vllm/pull/17088
* @junstar92 made their first contribution in https://github.com/vllm-project/vllm/pull/9276
* @ShuaibinLi made their first contribution in https://github.com/vllm-project/vllm/pull/17102
* @cascade812 made their first contribution in https://github.com/vllm-project/vllm/pull/16155
* @lkm-schulz made their first contribution in https://github.com/vllm-project/vllm/pull/17261
* @Ther-LF made their first contribution in https://github.com/vllm-project/vllm/pull/16751
* @Eviannn made their first contribution in https://github.com/vllm-project/vllm/pull/17266
* @idouba made their first contribution in https://github.com/vllm-project/vllm/pull/17288

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.4...v0.8.5

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.5)

---

## v0.8.4: v0.8.4
**Published:** 2025-04-14

This release contains 180 commits from 84 contributors (25 new contributors!). 

## Highlights
This release includes important accuracy fixes for Llama4 models, if you are using it, we highly recommend you to update. 

### Model

* Llama4 (#16113,#16509) bug fix and enhancements:
	* qknorm should be not shared across head (#16311)
	* Enable attention temperature tuning by default for long context (>32k) (#16439)
	* Index Error When Single Request Near Max Context (#16209)
	* Add tuned FusedMoE kernel config for Llama4 Scout, TP=8 on H100  (#16488)
	* Update to transformers==4.51.1 (#16257)
	* Added chat templates for LLaMa4 pythonic tool calling (#16463)
	* Optimized topk for topk=1(#16512)
	* Add warning for Attention backends that do not support irope yet (#16212)
* Support Qwen3 and Qwen3MoE (#15289), smolvlm (#16017), jinaai/jina-embeddings-v3 (#16120), InternVL3 (#16495), GLM-4-0414 (#16338)

### API
* Estimate max-model-len use available KV cache memory. The error message nows hints at how to set `--max-model-len` (#16168)
* Add hf_token to EngineArgs (#16093)
* Enable regex support with xgrammar in V0 engine (#13228)
* Support matryoshka representation / support embedding API dimensions (#16331)
* Add bucket for `request_latency`, `time_to_first_token` and `time_per_output_token` (#15202)
* Support for TorchAO quantization (#14231)

### Hardware
* Intel-Gaudi: Multi-step scheduling implementation for HPU (#12779)
* TPU:
	* Make @support_torch_compile work for XLA backend (#15782)
	* Use `language_model` interface for getting text backbone in MM (#16410)

### Performance
* DeepSeek MLA: a new merge_attn_states CUDA kernel, 3x speedup (#16173)
* MoE: Support W8A8 channel-wise weights and per-token activations in triton fused_moe_kernel (#16366)
* Add support to modelopt quantization of Mixtral model (#15961)
* Enable PTPC FP8 for CompressedTensorsW8A8Fp8MoEMethod (triton fused_moe) (#16537)

### V1 Engine Core
* Enable multi-input by default (#15799)
* Scatter and gather placeholders in the model runner (#16076)
* Set structured output backend to `auto` by default (#15724)
* Zero-copy tensor/ndarray serialization/transmission (#13790)
* Eagle Model loading (#16035)
* KV cache slots for eagle heads (#16370)
* Add `supports_structured_output()` method to Platform (#16148)


### Developer Facing
* Add sampling parameters to benchmark_serving. (#16022)
* AutoWeightsLoader refacotring (#16383, #16325, #16088, #16203, #16103)
* Unifieid configuration with engine args: `LoadConfig` (#16422), `ParallelConfig` (#16332)





## What's Changed
* [Misc] Auto detect bitsandbytes pre-quantized models by @tristanleclercq in https://github.com/vllm-project/vllm/pull/16027
* [CI] Fix benchmark script level by @khluu in https://github.com/vllm-project/vllm/pull/16089
* fix: support clang17 for macos and fix the real libomp by @yihong0618 in https://github.com/vllm-project/vllm/pull/16086
* [doc] fix 404 by @reidliu41 in https://github.com/vllm-project/vllm/pull/16082
* Revert "doc: add info for macos clang errors (#16049)" by @yihong0618 in https://github.com/vllm-project/vllm/pull/16091
* Fix some capitalisations in generated examples doc titles by @hmellor in https://github.com/vllm-project/vllm/pull/16094
* [Misc] format output for encoder_decoder.py by @reidliu41 in https://github.com/vllm-project/vllm/pull/16095
* [Misc] Remove redundant code by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16098
* [Bugfix] fix use_atomic_add support of marlin kernel when using v1 engine by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/15946
* [Model] use AutoWeightsLoader for phi, gemma, deepseek by @jonghyunchoe in https://github.com/vllm-project/vllm/pull/16088
* [Model] fix model testing for TeleChat2ForCausalLM and V0 llama4 by @luccafong in https://github.com/vllm-project/vllm/pull/16112
* [Benchmark] Add sampling parameters to benchmark_serving. by @hyeygit in https://github.com/vllm-project/vllm/pull/16022
* [Frontend] Fix typo in tool chat templates for llama3.2 and toolace by @bjj in https://github.com/vllm-project/vllm/pull/14501
* [CI][V1] Fix passing `tokenizer` as kwarg to `validate_guidance_grammar` by @ywang96 in https://github.com/vllm-project/vllm/pull/16117
* [Misc] refactor example eagle by @reidliu41 in https://github.com/vllm-project/vllm/pull/16100
* [Doc][Bugfix] Add missing EOF in k8s deploy doc by @psschwei in https://github.com/vllm-project/vllm/pull/16025
* [Misc] Improve model redirect to accept json dictionary by @Isotr0py in https://github.com/vllm-project/vllm/pull/16119
* [Model] use AutoWeightsLoader for stablelm,starcoder2,zamba2 by @lengrongfu in https://github.com/vllm-project/vllm/pull/16103
* [Bugfix] LoRA : Fix the order in which the kernels process LoRAs  by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/16040
* [Bugfix] add hf_token to EngineArgs by @paolovic in https://github.com/vllm-project/vllm/pull/16093
* [Misc] update requires-python in pyproject.toml by @reidliu41 in https://github.com/vllm-project/vllm/pull/16116
* [TPU] Update PyTorch/XLA by @yaochengji in https://github.com/vllm-project/vllm/pull/16130
* [V1][Minor] Optimize get_cached_block by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16135
* Fix requires-python by @martinhoyer in https://github.com/vllm-project/vllm/pull/16132
* [Metrics] Add bucket for `request_latency`, `time_to_first_token` and `time_per_output_token` by @yankay in https://github.com/vllm-project/vllm/pull/15202
* [V1][Minor] Minor simplification for get_computed_blocks  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/16139
* [Misc] Update Mistral-3.1 example by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16147
* [Bugfix] Make dummy encoder prompt padding alternative and add missing warnings by @Isotr0py in https://github.com/vllm-project/vllm/pull/16129
* [CI] Set max transformers version for Ultravox model test  by @ywang96 in https://github.com/vllm-project/vllm/pull/16149
* doc: fix some typos in doc by @yihong0618 in https://github.com/vllm-project/vllm/pull/16154
* [VLM] Florence-2 supports online serving by @Isotr0py in https://github.com/vllm-project/vllm/pull/16164
* [V1][Structured Output] Add `supports_structured_output()` method to Platform by @shen-shanshan in https://github.com/vllm-project/vllm/pull/16148
* [Model] Add Qwen3 and Qwen3MoE by @YamPengLi in https://github.com/vllm-project/vllm/pull/15289
* [Misc] improve example mlpspeculator and llm_engine_example by @reidliu41 in https://github.com/vllm-project/vllm/pull/16175
* [Doc]Update image to latest version by @WangErXiao in https://github.com/vllm-project/vllm/pull/16186
* Upstream Llama4 Support to Main by @houseroad in https://github.com/vllm-project/vllm/pull/16113
* [Bugfix] Re-enable support for `ChatGLMForConditionalGeneration` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16187
* [V1] Revert the default `max_num_seqs` to V0 values for most hardware by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16158
* [Misc] Print encoder seq len to short warning only once by @gshtras in https://github.com/vllm-project/vllm/pull/16193
* [Misc] Human-readable `max-model-len` cli arg by @NickLucche in https://github.com/vllm-project/vllm/pull/16181
* [Misc] Move Llama 4 projector call into encoder execution by @ywang96 in https://github.com/vllm-project/vllm/pull/16201
* [Bugfix] Fix guidance backend for Qwen models by @benchislett in https://github.com/vllm-project/vllm/pull/16210
* [V1][BugFix] Exit properly if engine core fails during startup by @njhill in https://github.com/vllm-project/vllm/pull/16137
* [Misc] add description attribute in CLI by @reidliu41 in https://github.com/vllm-project/vllm/pull/15921
* [Bugfix][V0] XGrammar structured output supports Enum by @leon-seidel in https://github.com/vllm-project/vllm/pull/15878
* Torchao by @drisspg in https://github.com/vllm-project/vllm/pull/14231
* [ROCm][Bugfix][FP8] Make fp8 quant respect fused modules mapping by @mgoin in https://github.com/vllm-project/vllm/pull/16031
* [core] do not send error across process by @youkaichao in https://github.com/vllm-project/vllm/pull/16174
* [Misc] Update compressed-tensors to version 0.9.3 by @mlsw in https://github.com/vllm-project/vllm/pull/16196
* Update BASE_IMAGE to 2.22 release of Neuron by @aws-satyajith in https://github.com/vllm-project/vllm/pull/16218
* [V1] Scatter and gather placeholders in the model runner by @ywang96 in https://github.com/vllm-project/vllm/pull/16076
* [Bugfix] fix use-ep bug to enable ep by dp/tp size > 1 by @zxfan-cpu in https://github.com/vllm-project/vllm/pull/16161
* Add warning for Attention backends that do not support irope yet by @sarckk in https://github.com/vllm-project/vllm/pull/16212
* [Bugfix] Do not skip "empty" parts of chats that are parsable by @mgoin in https://github.com/vllm-project/vllm/pull/16219
* [Bugfix] Fix and reorganize broken GGUF tests and bump gguf version by @Isotr0py in https://github.com/vllm-project/vllm/pull/16194
* [torch.compile][TPU] Make @support_torch_compile work for XLA backend by @lsy323 in https://github.com/vllm-project/vllm/pull/15782
* [V1] Add `disable_chunked_mm_input` arg to disable partial mm input prefill by @mgoin in https://github.com/vllm-project/vllm/pull/15837
* [Misc] Merge the logs of pp layers partitions by @kebe7jun in https://github.com/vllm-project/vllm/pull/16225
* [Docs] Add Slides from Singapore Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/16213
* [Misc] format and refactor some examples by @reidliu41 in https://github.com/vllm-project/vllm/pull/16252
* [Misc] Add warning for multimodal data in LLM.beam_search by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/16241
* [Model] use AutoWeightsLoader for phimoe,qwen2_moe,qwen3_moe by @lengrongfu in https://github.com/vllm-project/vllm/pull/16203
* [BugFix][ROCm] Fix GGUF MoE Dispatch Block_Dim for ROCm by @tywuAMD in https://github.com/vllm-project/vllm/pull/16247
* [Bugfix] Remove triton do_bench fast_flush arg by @kebe7jun in https://github.com/vllm-project/vllm/pull/16256
* Update to transformers==4.51.1 by @hmellor in https://github.com/vllm-project/vllm/pull/16257
* [New Model]: jinaai/jina-embeddings-v3 by @noooop in https://github.com/vllm-project/vllm/pull/16120
* [Misc] Avoid stripping meaningful whitespace from `nvidia-smi topo -m` output in collect_env.py by @imkero in https://github.com/vllm-project/vllm/pull/16272
* [Bugfix] Proper input validation for multi-modal encoder-decoder models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16156
* [Bugfix] Handle `process_weights_after_loading` for `QKVCrossParallelLinear` by @Isotr0py in https://github.com/vllm-project/vllm/pull/15328
* Add warning that content below line in template will be removed by @hmellor in https://github.com/vllm-project/vllm/pull/16276
* [BugFix] Fix Llama4 - Index Error When Single Request Near Max Context by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/16209
* [Bugfix] fix deepseek fp16 scale bug by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/14809
* [V1] Update structured output offline inference example by @russellb in https://github.com/vllm-project/vllm/pull/15721
* [CI/Build] Fix CI LoRA failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/16270
* Add support to modelopt quantization of Mixtral model by @yueshen2016 in https://github.com/vllm-project/vllm/pull/15961
* [Model] Add smolvlm support by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/16017
* [Bug] [ROCm] Fix Llama 4 Enablement Bug on ROCm: V0 ROCmFlashAttentionImpl and Triton Fused MoE bugs by @tjtanaa in https://github.com/vllm-project/vllm/pull/16198
* [Bugfix] fix gettid method is not define by @lengrongfu in https://github.com/vllm-project/vllm/pull/16084
* [Feature] Estimate max-model-len use available KV cache memory by @lengrongfu in https://github.com/vllm-project/vllm/pull/16168
* [Core] Upgrade to xgrammar 0.1.18, add cache size limit by @russellb in https://github.com/vllm-project/vllm/pull/16283
* [CI][Bugfix] Fix bad tolerance for test_batch_base64_embedding by @mgoin in https://github.com/vllm-project/vllm/pull/16221
* [TPU] Update PyTorch/XLA by @yaochengji in https://github.com/vllm-project/vllm/pull/16288
* [BugFix] Fix fusion test and add them to CI by @ProExpertProg in https://github.com/vllm-project/vllm/pull/16287
* [Misc] Fix test_sharded_state_loader.py(#16004) by @Accelerator1996 in https://github.com/vllm-project/vllm/pull/16005
* [Bugfix] Avoid transferring cached multi-modal items from P0 to P1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16273
* Update label-tpu mergify and remove removal bot by @mgoin in https://github.com/vllm-project/vllm/pull/16298
* [BugFix] logger is not callable by @yihong0618 in https://github.com/vllm-project/vllm/pull/16312
* [BugFix] llama4 qknorm should be not shared across head by @luccafong in https://github.com/vllm-project/vllm/pull/16311
* update neuron config by @ajayvohra2005 in https://github.com/vllm-project/vllm/pull/16289
* [BugFix] fix some typos found by typos. by @yihong0618 in https://github.com/vllm-project/vllm/pull/16314
* [Model] Add `SupportsMultiModal.get_language_model` interface by @NickLucche in https://github.com/vllm-project/vllm/pull/16007
* [Bugfix][Frontend] respect provided default guided decoding backend by @gcalmettes in https://github.com/vllm-project/vllm/pull/15476
* Revert "Update label-tpu mergify and remove removal bot" by @mgoin in https://github.com/vllm-project/vllm/pull/16350
* [Bugfix] Fix profiling.py by @hhy3 in https://github.com/vllm-project/vllm/pull/16202
* [Bugfix] catch AssertionError in MistralTokenizer as ValueError by @gcalmettes in https://github.com/vllm-project/vllm/pull/16344
* [CI]Fix hpu docker and numpy version for CI by @xuechendi in https://github.com/vllm-project/vllm/pull/16355
* Fix `benchmark_throughput.py --backend=hf` by @mgoin in https://github.com/vllm-project/vllm/pull/16352
* [Build/CI] Add tracing deps to vllm container image by @russellb in https://github.com/vllm-project/vllm/pull/15224
* [Hardware] add platform-specific request validation api by @joerunde in https://github.com/vllm-project/vllm/pull/16291
* [Misc] refactor Structured Outputs example by @reidliu41 in https://github.com/vllm-project/vllm/pull/16322
* [TPU][V1] Refine tpu_model_runner to mitigate future recompilation issues by @yaochengji in https://github.com/vllm-project/vllm/pull/16275
* Add GLM-4-0414 support by @zRzRzRzRzRzRzR in https://github.com/vllm-project/vllm/pull/16338
* [Bugfix]: do not shutdown server if `skip_special_use=False` for MistralTokenizer by @gcalmettes in https://github.com/vllm-project/vllm/pull/14094
* [Model] use AutoWeightsLoader for granite, granitemoe, granitemoeshared, grok1, mixtral by @aaron-ang in https://github.com/vllm-project/vllm/pull/16325
* [TPU] Fix dummy loading OOM by @yaochengji in https://github.com/vllm-project/vllm/pull/16372
* [bugfix] Avoid the time consumption caused by creating dummy videos. by @Jintao-Huang in https://github.com/vllm-project/vllm/pull/16371
* [CI][Bugfix] Pin triton version for CPU by @ywang96 in https://github.com/vllm-project/vllm/pull/16384
* [misc] use tqdm.auto where appropriate by @BKitor in https://github.com/vllm-project/vllm/pull/16290
* [Bugfix][TPU] Fix TPU validate_request by @mgoin in https://github.com/vllm-project/vllm/pull/16369
* fix sonnet dataset sample when prefix len is very small by @Chenyaaang in https://github.com/vllm-project/vllm/pull/16379
* [Model] use AutoWeightsLoader for deepseek_v2, internlm2 by @aaron-ang in https://github.com/vllm-project/vllm/pull/16383
* [Misc] Update transformers version limits of multi-modal tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16381
* [Bugfix] Fix validation error for text-only Mllama 3.2 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16377
* [Kernel] Use moe_wna16 kernel for compressed tensors wna16 moe models by @mgoin in https://github.com/vllm-project/vllm/pull/16038
* [doc] add download model tips by @reidliu41 in https://github.com/vllm-project/vllm/pull/16389
* Update Numba to 0.61.2 by @cyyever in https://github.com/vllm-project/vllm/pull/16376
* [Model] Remove image mm limit for LLaMa4  by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/16365
* [doc] update the wrong link by @reidliu41 in https://github.com/vllm-project/vllm/pull/16401
* [CI] Add auto update workflow for Dockerfile graph by @WineChord in https://github.com/vllm-project/vllm/pull/11879
* Fix the torch version parsing logic by @houseroad in https://github.com/vllm-project/vllm/pull/15857
* [VLM] Remove `BaseProcessingInfo.get_mm_max_tokens_per_item` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16408
* [TPU][V1] Use `language_model` interface for getting text backbone in MM by @NickLucche in https://github.com/vllm-project/vllm/pull/16410
* Improve configs - `ParallelConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16332
* [V1] Set structured output backend to `auto` by default by @russellb in https://github.com/vllm-project/vllm/pull/15724
* [V1][Spec Decode] Eagle Model loading by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/16035
* [Bugfix] Fix bug when dataset is json by @Chenyaaang in https://github.com/vllm-project/vllm/pull/15899
* [Model] Reduce redundant computations in mamba2 blocks for Bamba-9B by @cyang49 in https://github.com/vllm-project/vllm/pull/15423
* [V1] Zero-copy tensor/ndarray serialization/transmission by @njhill in https://github.com/vllm-project/vllm/pull/13790
* [VLM] Avoid unnecessary dummy multimodal data during processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16416
* [Bugfix] Fix output token length check logic by @eeslook in https://github.com/vllm-project/vllm/pull/16419
* [TPU][V1] Disable per-request seed/Generator by @NickLucche in https://github.com/vllm-project/vllm/pull/16172
* Fix range_ratio Bug in RandomDataset by @jadewang21 in https://github.com/vllm-project/vllm/pull/16126
* check input length of sonnet samples by @alexey-belyakov in https://github.com/vllm-project/vllm/pull/16423
* update benchmark_serving_structured_output to include auto backend by @Chenyaaang in https://github.com/vllm-project/vllm/pull/16438
* [Llama4] Enable attention temperature tuning by default for long context (>32k) by @sarckk in https://github.com/vllm-project/vllm/pull/16439
* Update supported_hardware.md for TPU INT8 by @mgoin in https://github.com/vllm-project/vllm/pull/16437
* [Bugfix][VLM] Fix failing Phi-4-MM multi-images tests and add vision-speech test by @Isotr0py in https://github.com/vllm-project/vllm/pull/16424
* [CPU][Bugfix] Fix CPU docker issues by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/16454
* [Bugfix] Don't set an upper bound on repetition penalty by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/16403
* Revert "[Model] use AutoWeightsLoader for deepseek_v2, internlm2" by @DefTruth in https://github.com/vllm-project/vllm/pull/16453
* [Core][LoRA][1/N] Add LoRA for EncoderDecoderModelRunner by @jeejeelee in https://github.com/vllm-project/vllm/pull/15990
* Enforce valid max_num_batched_tokens when disable_chunked_mm_input=True by @mgoin in https://github.com/vllm-project/vllm/pull/16447
* [Misc] Raise error for V1 not supporting Long LoRA. by @jeejeelee in https://github.com/vllm-project/vllm/pull/16415
* [Misc] update api_client example by @reidliu41 in https://github.com/vllm-project/vllm/pull/16459
* Don't install triton on `ppc64le` platform by @hmellor in https://github.com/vllm-project/vllm/pull/16470
* [Kernel] support merge_attn_states CUDA kernel, 3x speedup by @DefTruth in https://github.com/vllm-project/vllm/pull/16173
* [Bugfix] Fix bugs of running Quark quantized models by @cha557 in https://github.com/vllm-project/vllm/pull/16236
* [Hardware][Intel-Gaudi] Multi-step scheduling implementation for HPU by @tzielinski-habana in https://github.com/vllm-project/vllm/pull/12779
* Fix erroneous "model doesn't support compile" warning by @zou3519 in https://github.com/vllm-project/vllm/pull/16486
* [TPU][V1] Make `--disable_chunked_mm_input` mandatory for serving MM models by @NickLucche in https://github.com/vllm-project/vllm/pull/16483
* [Kernel] Support W8A8 channel-wise weights and per-token activations in triton fused_moe_kernel by @mgoin in https://github.com/vllm-project/vllm/pull/16366
* [Doc] Document InternVL3 support by @Isotr0py in https://github.com/vllm-project/vllm/pull/16495
* [Bugfix] handle alignment of encoder_seq_lens in mllama.py by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/14784
* Improve configs - `LoadConfig` by @hmellor in https://github.com/vllm-project/vllm/pull/16422
* [Frontend] Added chat templates for LLaMa4 pythonic tool calling by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/16463
* [Kernel] Add tuned FusedMoE kernel config for Llama4 Scout, TP=8 on H100  by @sarckk in https://github.com/vllm-project/vllm/pull/16488
* Update openai_compatible_server.md by @Chr1st1anSears in https://github.com/vllm-project/vllm/pull/16507
* [Bugfix] clean up duplicated code by @lengrongfu in https://github.com/vllm-project/vllm/pull/16485
* Bugfix for PixtralHF models without spatial_merge_size by @mgoin in https://github.com/vllm-project/vllm/pull/16513
* [Doc] Fix link to vLLM blog by @terrytangyuan in https://github.com/vllm-project/vllm/pull/16519
* [CI][Bugfix] Add mistral_tool_use to Ci by @mgoin in https://github.com/vllm-project/vllm/pull/16517
* [BugFix] Handle non-contiguous tensors properly when serializing by @njhill in https://github.com/vllm-project/vllm/pull/16492
* [Doc] Update Llama4 Model Names in Supported Models by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/16509
* Optimized topk for topk=1 (Llama-4) by @mgoin in https://github.com/vllm-project/vllm/pull/16512
* [Feature][V1] Add xgrammar to support minLength, maxLength with test by @leon-seidel in https://github.com/vllm-project/vllm/pull/16516
* [Frontend] support matryoshka representation / support embedding API dimensions by @noooop in https://github.com/vllm-project/vllm/pull/16331
* fix: spelling by @ezhoureal in https://github.com/vllm-project/vllm/pull/16466
* [Misc] Update chat utils tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/16520
* [Misc] Openai transcription client example use same Whisper model by @NickLucche in https://github.com/vllm-project/vllm/pull/16487
* [V1] Enable multi-input by default by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15799
* [MISC] Make GroupCoordinator compatible with out-of-tree devices by @ji-huazhong in https://github.com/vllm-project/vllm/pull/16464
* [Misc] Delete redundant code by @jeejeelee in https://github.com/vllm-project/vllm/pull/16530
* Fix syntaxWarning: invalid escape sequence '\s' by @DamonFool in https://github.com/vllm-project/vllm/pull/16532
* [Perf] Optimize Preparing Inputs for GPU Model Runner by @SnowCharmQ in https://github.com/vllm-project/vllm/pull/16484
* [Bugfix] Validate logit biases to prevent out of vocab ids crashing engine by @rymc in https://github.com/vllm-project/vllm/pull/16529
* [V1][Spec Decode] KV cache slots for eagle heads by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/16370
* Enable PTPC FP8 for CompressedTensorsW8A8Fp8MoEMethod (triton fused_moe) by @mgoin in https://github.com/vllm-project/vllm/pull/16537
* [Benchmark][Bugfix] Fix SonnetDataset default values in benchmark_throughput.py by @JenZhao in https://github.com/vllm-project/vllm/pull/16556
* [Core][V0] Enable regex support with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/13228

## New Contributors
* @bjj made their first contribution in https://github.com/vllm-project/vllm/pull/14501
* @psschwei made their first contribution in https://github.com/vllm-project/vllm/pull/16025
* @paolovic made their first contribution in https://github.com/vllm-project/vllm/pull/16093
* @YamPengLi made their first contribution in https://github.com/vllm-project/vllm/pull/15289
* @leon-seidel made their first contribution in https://github.com/vllm-project/vllm/pull/15878
* @drisspg made their first contribution in https://github.com/vllm-project/vllm/pull/14231
* @mlsw made their first contribution in https://github.com/vllm-project/vllm/pull/16196
* @aws-satyajith made their first contribution in https://github.com/vllm-project/vllm/pull/16218
* @zxfan-cpu made their first contribution in https://github.com/vllm-project/vllm/pull/16161
* @sarckk made their first contribution in https://github.com/vllm-project/vllm/pull/16212
* @yueshen2016 made their first contribution in https://github.com/vllm-project/vllm/pull/15961
* @Accelerator1996 made their first contribution in https://github.com/vllm-project/vllm/pull/16005
* @hhy3 made their first contribution in https://github.com/vllm-project/vllm/pull/16202
* @zRzRzRzRzRzRzR made their first contribution in https://github.com/vllm-project/vllm/pull/16338
* @aaron-ang made their first contribution in https://github.com/vllm-project/vllm/pull/16325
* @Jintao-Huang made their first contribution in https://github.com/vllm-project/vllm/pull/16371
* @WineChord made their first contribution in https://github.com/vllm-project/vllm/pull/11879
* @eeslook made their first contribution in https://github.com/vllm-project/vllm/pull/16419
* @jadewang21 made their first contribution in https://github.com/vllm-project/vllm/pull/16126
* @alexey-belyakov made their first contribution in https://github.com/vllm-project/vllm/pull/16423
* @tzielinski-habana made their first contribution in https://github.com/vllm-project/vllm/pull/12779
* @Chr1st1anSears made their first contribution in https://github.com/vllm-project/vllm/pull/16507
* @ezhoureal made their first contribution in https://github.com/vllm-project/vllm/pull/16466
* @SnowCharmQ made their first contribution in https://github.com/vllm-project/vllm/pull/16484
* @rymc made their first contribution in https://github.com/vllm-project/vllm/pull/16529

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.3...v0.8.4

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.4)

---

## v0.8.3: v0.8.3
**Published:** 2025-04-06

## Highlights

This release features 260 commits, 109 contributors, 38 new contributors.

* We are excited to announce Day 0 Support for Llama 4 Scout and Maverick (#16104). Please [see our blog for detailed user guide](https://blog.vllm.ai/2025/04/05/llama4). 
  * Please note that Llama4 is only supported in V1 engine only for now. 
* V1 engine now supports native sliding window attention (#14097) with the hybrid memory allocator.

### Cluster Scale Serving
* Single node data parallel with API server support (#13923)
* Multi-node offline DP+EP example (#15484)
* Expert parallelism enhancements
	* CUTLASS grouped gemm fp8 MoE kernel (#13972)
	* Fused experts refactor (#15914)
	* Fp8 Channelwise Dynamic Per Token GroupedGEMM (#15587)
	* Adding support for fp8 gemm layer input in fp8 (#14578)
	* Add option to use DeepGemm contiguous grouped gemm kernel for fused MoE operations. (#13932)
* Support XpYd disaggregated prefill with MooncakeStore (#12957)

### Model Supports
* Llama 4 (#16104), Aya Vision (#15441), MiniMaxText01(#13454), Skywork-R1V (#15397), jina-reranker-v2 (#15876)
* Add Reasoning Parser for Granite Models (#14202)
* Add Phi-4-mini function calling support (#14886)

### V1 Engine
* Collective RPC (#15444)
* Faster top-k only implementation (#15478)
* BitsAndBytes support (#15611)
* Speculative Decoding: metrics (#15151), Eagle Proposer (#15729), n-gram interface update (#15750), EAGLE Architecture with Proper RMS Norms (#14990)

### Features

#### API
* Support Enum for xgrammar based structured output in V1. (#15594, #15757)
* A new tags parameter for `wake_up`  (#15500)
* V1 LoRA support CPU offload (#15843)
* Prefix caching support: FIPS enabled machines with MD5 hashing (#15299), SHA256 as alternative hashing algorithm (#15297)
* Addition of http service metrics (#15657)

#### Performance
* LoRA Scheduler optimization bridging V1 and V0 performance (#15422).

#### Hardwares
* AMD:
	* Add custom allreduce support for ROCM (#14125)
	* Quark quantization documentation (#15861)
	* AITER integration:  int8 scaled gemm kernel (#15433), fused moe (#14967)
	* Paged attention for V1 (#15720)
* CPU:
	* CPU MLA (#14744)
* TPU
	* Improve Memory Usage Estimation (#15671)
	* Optimize the all-reduce performance (#15903)
	* Support sliding window and logit soft capping in the paged attention kernel. (#15732)
	* TPU-optimized top-p implementation (avoids scattering). (#15736)

### Doc, Build, Ecosystem

* V1 user guide update: fp8 kv cache support (#15585),  multi-modality (#15460)
* Recommend developing with Python 3.12 in developer guide (#15811)
* Clean up: move dockerfiles into their own directory (#14549)
* Add minimum version for `huggingface_hub` to enable Xet downloads (#15873)
* TPU CI: Add basic perf regression test (#15414)

## What's Changed
* Fix CUDA kernel index data type in vllm/csrc/quantization/gptq_marlin/awq_marlin_repack.cu +10 by @houseroad in https://github.com/vllm-project/vllm/pull/15160
* [Hardware][TPU][Bugfix] Fix v1 mp profiler by @lsy323 in https://github.com/vllm-project/vllm/pull/15409
* [Kernel][CPU] CPU MLA by @gau-nernst in https://github.com/vllm-project/vllm/pull/14744
* Dockerfile.ppc64le changes to move to UBI by @Shafi-Hussain in https://github.com/vllm-project/vllm/pull/15402
* [Misc] Clean up MiniCPM-V/O code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15337
* [Misc] Remove redundant `num_embeds` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15443
* [Doc] Update V1 user guide for multi-modality by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15460
* [Kernel] Fix conflicting macro names for gguf kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/15456
* [bugfix] fix inductor cache on max_position_embeddings by @youkaichao in https://github.com/vllm-project/vllm/pull/15436
* [CI/Build] Add tests for the V1 tpu_model_runner. by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14843
* [Bugfix] Support triton==3.3.0+git95326d9f for RTX 5090 (Unsloth + vLLM compatibility) by @oteroantoniogom in https://github.com/vllm-project/vllm/pull/15471
* [bugfix] add supports_v1 platform interface by @joerunde in https://github.com/vllm-project/vllm/pull/15417
* Add workaround for shared field_names in pydantic model class by @maxdebayser in https://github.com/vllm-project/vllm/pull/13925
* [TPU][V1] Fix Sampler recompilation by @NickLucche in https://github.com/vllm-project/vllm/pull/15309
* [V1][Minor] Use `SchedulerInterface` type for engine scheduler field by @njhill in https://github.com/vllm-project/vllm/pull/15499
* [V1] Support long_prefill_token_threshold in v1 scheduler by @houseroad in https://github.com/vllm-project/vllm/pull/15419
* [core] add bucket padding to tpu_model_runner by @Chenyaaang in https://github.com/vllm-project/vllm/pull/14995
* [Core] LoRA: V1 Scheduler optimization by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15422
* [CI/Build] LoRA: Delete long context tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15503
* Transformers backend already supports V1 by @hmellor in https://github.com/vllm-project/vllm/pull/15463
* [Model] Support multi-image for Molmo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15438
* [Misc] Warn about v0 in benchmark_paged_attn.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/15495
* [BugFix] Fix nightly MLA failure (FA2 + MLA chunked prefill, i.e. V1, producing bad results) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/15492
* [misc] LoRA - Skip LoRA kernels when not required by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15152
* Fix raw_request extraction in load_aware_call decorator by @daniel-salib in https://github.com/vllm-project/vllm/pull/15382
* [Feature] Enhance EAGLE Architecture with Proper RMS Norms by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/14990
* [FEAT][ROCm] Integrate Fused MoE Kernels from AITER by @vllmellm in https://github.com/vllm-project/vllm/pull/14967
* [Misc] Enhance warning information to user-defined chat template by @wwl2755 in https://github.com/vllm-project/vllm/pull/15408
* [Misc] improve example script output by @reidliu41 in https://github.com/vllm-project/vllm/pull/15528
* Separate base model from `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/15467
* Apply torchfix by @cyyever in https://github.com/vllm-project/vllm/pull/15532
* Improve validation of TP in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/15540
* [Model] Add Reasoning Parser for Granite Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/14202
* multi-node offline DP+EP example by @youkaichao in https://github.com/vllm-project/vllm/pull/15484
* Fix weight loading for some models in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/15544
* [Refactor] Remove passthrough `backend` when generate grammar by @aarnphm in https://github.com/vllm-project/vllm/pull/15317
* [V1][Sampler] Faster top-k only implementation by @njhill in https://github.com/vllm-project/vllm/pull/15478
* Support SHA256 as hash function in prefix caching by @dr75 in https://github.com/vllm-project/vllm/pull/15297
* Applying some fixes for K8s agents in CI by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/15493
* [V1] TPU - Revert to exponential padding by default by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15565
* [V1] TPU CI - Fix test_compilation.py by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15570
* Use Cache Hinting for fused_moe kernel by @wrmedford in https://github.com/vllm-project/vllm/pull/15511
* [TPU] support disabling xla compilation cache by @yaochengji in https://github.com/vllm-project/vllm/pull/15567
* Support FIPS enabled machines with MD5 hashing by @MattTheCuber in https://github.com/vllm-project/vllm/pull/15299
* [Kernel] CUTLASS grouped gemm fp8 MoE kernel by @ElizaWszola in https://github.com/vllm-project/vllm/pull/13972
* Add automatic tpu label to mergify.yml by @mgoin in https://github.com/vllm-project/vllm/pull/15560
* add platform check back by @Chenyaaang in https://github.com/vllm-project/vllm/pull/15578
* [misc] LoRA: Remove unused long context test data by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15558
* [Doc] Update V1 user guide for fp8 kv cache support by @wayzeng in https://github.com/vllm-project/vllm/pull/15585
* [moe][quant] add weight name case for offset by @MengqingCao in https://github.com/vllm-project/vllm/pull/15515
* [V1] Refactor num_computed_tokens logic by @comaniac in https://github.com/vllm-project/vllm/pull/15307
* Allow torchao quantization in SiglipMLP by @jerryzh168 in https://github.com/vllm-project/vllm/pull/15575
* [ROCm] Env variable to trigger custom PA by @gshtras in https://github.com/vllm-project/vllm/pull/15557
* [TPU] [V1] fix cases when max_num_reqs is set smaller than MIN_NUM_SEQS by @yaochengji in https://github.com/vllm-project/vllm/pull/15583
* [Misc] Restrict ray version dependency and update PP feature warning in V1 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15556
* [TPU] Avoid Triton Import by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15589
* [Misc] Consolidate LRUCache implementations by @Avabowler in https://github.com/vllm-project/vllm/pull/15481
* [Quantization] Fp8 Channelwise Dynamic Per Token GroupedGEMM by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15587
* [Misc] Clean up `scatter_patch_features` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15559
* [Misc] Use model_redirect to redirect the model name to a local folder. by @noooop in https://github.com/vllm-project/vllm/pull/14116
* Fix incorrect filenames in vllm_compile_cache.py by @zou3519 in https://github.com/vllm-project/vllm/pull/15494
* [Doc] update --system for transformers installation in docker doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/15616
* [Model] MiniCPM-V/O supports V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15487
* [Bugfix] Fix use_cascade_attention handling for Alibi-based models on vllm/v1 by @h-sugi in https://github.com/vllm-project/vllm/pull/15211
* [Doc] Link to onboarding tasks by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15629
* [Misc] Replace `is_encoder_decoder_inputs` with `split_enc_dec_inputs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15620
* [Feature] Add middleware to log API Server responses by @terrytangyuan in https://github.com/vllm-project/vllm/pull/15593
* [Misc] Avoid direct access of global `mm_registry` in `compute_encoder_budget` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15621
* [Doc] Use absolute placement for Ask AI button by @hmellor in https://github.com/vllm-project/vllm/pull/15628
* [Bugfix][TPU][V1] Fix recompilation by @NickLucche in https://github.com/vllm-project/vllm/pull/15553
* Correct PowerPC to modern IBM Power by @clnperez in https://github.com/vllm-project/vllm/pull/15635
* [CI] Update rules for applying `tpu` label. by @russellb in https://github.com/vllm-project/vllm/pull/15634
* [V1] AsyncLLM data parallel by @njhill in https://github.com/vllm-project/vllm/pull/13923
* [TPU] Lazy Import by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15656
* [Quantization][V1]  BitsAndBytes support V1 by @jeejeelee in https://github.com/vllm-project/vllm/pull/15611
* [Bugfix] Fix failure to launch in Tensor Parallel TP mode on macOS. by @kebe7jun in https://github.com/vllm-project/vllm/pull/14948
* [Doc] Fix dead links in Job Board by @wwl2755 in https://github.com/vllm-project/vllm/pull/15637
* [CI][TPU] Temporarily Disable Quant Test on TPU by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15649
* Revert "Use Cache Hinting for fused_moe kernel (#15511)" by @wrmedford in https://github.com/vllm-project/vllm/pull/15645
* [Misc]add coding benchmark for speculative decoding by @CXIAAAAA in https://github.com/vllm-project/vllm/pull/15303
* [Quantization][FP8] Adding support for fp8 gemm layer input in fp8 by @gshtras in https://github.com/vllm-project/vllm/pull/14578
* Refactor error handling for multiple exceptions in preprocessing by @JasonZhu1313 in https://github.com/vllm-project/vllm/pull/15650
* [Bugfix] Fix `mm_hashes` forgetting to be passed by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15668
* [V1] Remove legacy input registry by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15673
* [TPU][CI] Fix TPUModelRunner Test by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15667
* [Refactor][Frontend] Keep all logic about reasoning into one class by @gaocegege in https://github.com/vllm-project/vllm/pull/14428
* [CPU][CI] Improve CPU Dockerfile by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/15690
* [Bugfix] Fix 'InductorAdaptor object has no attribute 'cache_dir' by @jeejeelee in https://github.com/vllm-project/vllm/pull/15674
* [Misc] Fix test_sleep to use query parameters by @lizzzcai in https://github.com/vllm-project/vllm/pull/14373
* [Bugfix][Frontend] Eliminate regex based check in reasoning full generator by @gaocegege in https://github.com/vllm-project/vllm/pull/14821
* [Frontend] update priority for --api-key and VLLM_API_KEY by @reidliu41 in https://github.com/vllm-project/vllm/pull/15588
* [Docs] Add "Generation quality changed" section to troubleshooting by @hmellor in https://github.com/vllm-project/vllm/pull/15701
* [Model] Adding torch compile annotations to chatglm by @jeejeelee in https://github.com/vllm-project/vllm/pull/15624
* [Bugfix][v1] xgrammar structured output supports Enum. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15594
* [Bugfix] `embed_is_patch` for Idefics3 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15696
* [V1] Support disable_any_whtespace for guidance backend by @russellb in https://github.com/vllm-project/vllm/pull/15584
* [doc] add missing imports by @reidliu41 in https://github.com/vllm-project/vllm/pull/15699
* [Bugfix] Fix regex compile display format by @kebe7jun in https://github.com/vllm-project/vllm/pull/15368
* Fix cpu offload testing for gptq/awq/ct by @mgoin in https://github.com/vllm-project/vllm/pull/15648
* [Minor] Remove TGI launching script  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15646
* [Misc] Remove unused utils and clean up imports by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15708
* [Misc] Remove stale func in KVTransferConfig by @ShangmingCai in https://github.com/vllm-project/vllm/pull/14746
* [TPU] [Perf] Improve Memory Usage Estimation by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15671
* [Bugfix] [torch.compile] Add Dynamo metrics context during compilation by @ProExpertProg in https://github.com/vllm-project/vllm/pull/15639
* [V1] TPU - Fix the chunked prompt bug by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15713
* [Misc] cli auto show default value by @reidliu41 in https://github.com/vllm-project/vllm/pull/15582
* implement prometheus fast-api-instrumentor for http service metrics by @daniel-salib in https://github.com/vllm-project/vllm/pull/15657
* [Docs][V1] Optimize diagrams in prefix caching design by @simpx in https://github.com/vllm-project/vllm/pull/15716
* [ROCm][AMD][Build] Update AMD supported arch list by @gshtras in https://github.com/vllm-project/vllm/pull/15632
* [Model] Support Skywork-R1V by @pengyuange in https://github.com/vllm-project/vllm/pull/15397
* [Docs] Document v0 engine support in reasoning outputs by @gaocegege in https://github.com/vllm-project/vllm/pull/15739
* [Misc][V1] Misc code streamlining by @njhill in https://github.com/vllm-project/vllm/pull/15723
* [Bugfix] LoRA V1: add and fix entrypoints tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15715
* [CI] Speed up V1 structured output tests by @russellb in https://github.com/vllm-project/vllm/pull/15718
* Use numba 0.61 for python 3.10+ to support numpy>=2 by @cyyever in https://github.com/vllm-project/vllm/pull/15692
* [Bugfix] set VLLM_WORKER_MULTIPROC_METHOD=spawn for vllm.entrypoionts.openai.api_server by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/15700
* [TPU][V1][Bugfix] Fix w8a8 recompiilation with GSM8K by @NickLucche in https://github.com/vllm-project/vllm/pull/15714
* [Kernel][TPU][ragged-paged-attn] vLLM code change for PR#8896 by @yarongmu-google in https://github.com/vllm-project/vllm/pull/15659
* [doc] update doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/15740
* [FEAT] [ROCm] Add AITER int8 scaled gemm kernel by @tjtanaa in https://github.com/vllm-project/vllm/pull/15433
* [V1] [Feature] Collective RPC by @wwl2755 in https://github.com/vllm-project/vllm/pull/15444
* [Feature][Disaggregated] Support XpYd disaggregated prefill with MooncakeStore by @ShangmingCai in https://github.com/vllm-project/vllm/pull/12957
* [V1] Support interleaved modality items by @ywang96 in https://github.com/vllm-project/vllm/pull/15605
* [V1][Minor] Simplify rejection sampler's parse_output by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15741
* [Bugfix] Fix Mllama interleaved images input support by @Isotr0py in https://github.com/vllm-project/vllm/pull/15564
* [CI] xgrammar structured output supports Enum. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15757
* [Bugfix] Fix Mistral guided generation using xgrammar by @juliendenize in https://github.com/vllm-project/vllm/pull/15704
* [doc] update conda to usage link in installation by @reidliu41 in https://github.com/vllm-project/vllm/pull/15761
* fix test_phi3v by @pansicheng in https://github.com/vllm-project/vllm/pull/15321
* [V1] Override `mm_counts` for dummy data creation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15703
* fix: lint fix a ruff checkout syntax error by @yihong0618 in https://github.com/vllm-project/vllm/pull/15767
* [Bugfix] Added `embed_is_patch` mask for fuyu model by @kylehh in https://github.com/vllm-project/vllm/pull/15731
* fix: Comments to English for better dev experience by @yihong0618 in https://github.com/vllm-project/vllm/pull/15768
* [V1][Scheduler] Avoid calling `_try_schedule_encoder_inputs` for every request by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15778
* [Misc] update the comments by @lcy4869 in https://github.com/vllm-project/vllm/pull/15780
* [Benchmark] Update Vision Arena Dataset and HuggingFaceDataset Setup by @JenZhao in https://github.com/vllm-project/vllm/pull/15748
* [Feature][ROCm]Enable fusion pass for torch.compile on ROCm by @charlifu in https://github.com/vllm-project/vllm/pull/15050
* Recommend developing with Python 3.12 in developer guide by @hmellor in https://github.com/vllm-project/vllm/pull/15811
* fix: better install requirement for install in setup.py by @yihong0618 in https://github.com/vllm-project/vllm/pull/15796
* [V1] Fully Transparent Implementation of CPU Offloading by @youkaichao in https://github.com/vllm-project/vllm/pull/15354
* [Model] Update support for NemotronNAS models by @Naveassaf in https://github.com/vllm-project/vllm/pull/15008
* [Bugfix] Fix Crashing When Loading Modules With Batchnorm Stats by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/15813
* [Bugfix] Fix missing return value in load_weights method of adapters.py by @noc-turne in https://github.com/vllm-project/vllm/pull/15542
* Upgrade `transformers` to `v4.50.3` by @hmellor in https://github.com/vllm-project/vllm/pull/13905
* [Bugfix] Check dimensions of multimodal embeddings in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15816
* [V1][Spec Decode] Remove deprecated spec decode config params by @ShangmingCai in https://github.com/vllm-project/vllm/pull/15466
* fix: change GB to GiB in logging close #14979 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15807
* [V1] TPU CI - Add basic perf regression test by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15414
* Fix Transformers backend compatibility check by @hmellor in https://github.com/vllm-project/vllm/pull/15290
* [V1][Core] Remove unused speculative config from scheduler by @markmc in https://github.com/vllm-project/vllm/pull/15818
* Move dockerfiles into their own directory by @hmellor in https://github.com/vllm-project/vllm/pull/14549
* [Distributed] Add custom allreduce support for ROCM by @ilmarkov in https://github.com/vllm-project/vllm/pull/14125
* Rename fallback model and refactor supported models section by @hmellor in https://github.com/vllm-project/vllm/pull/15829
* [Frontend] Add Phi-4-mini function calling support by @kinfey in https://github.com/vllm-project/vllm/pull/14886
* [Bugfix][Model] fix mllama multi-image by @yma11 in https://github.com/vllm-project/vllm/pull/14883
* [Bugfix] Fix extra comma by @haochengxia in https://github.com/vllm-project/vllm/pull/15851
* [Bugfix]: Fix is_embedding_layer condition in VocabParallelEmbedding  by @alexwl in https://github.com/vllm-project/vllm/pull/15824
* [V1] TPU - Fix fused MOE by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15834
* [sleep mode] clear pytorch cache after sleep by @lionelvillard in https://github.com/vllm-project/vllm/pull/15248
* [ROCm] Use device name in the warning by @gshtras in https://github.com/vllm-project/vllm/pull/15838
* [V1] Implement sliding window attention in kv_cache_manager by @heheda12345 in https://github.com/vllm-project/vllm/pull/14097
* fix: can not use uv run collect_env close #13888 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15792
* [Feature] specify model in config.yaml by @wayzeng in https://github.com/vllm-project/vllm/pull/15798
* [Misc] Enable V1 LoRA by default by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15320
* [Misc] Fix speculative config repr string by @ShangmingCai in https://github.com/vllm-project/vllm/pull/15860
* [Docs] Fix small error in link text by @hmellor in https://github.com/vllm-project/vllm/pull/15868
* [Bugfix] Fix no video/image profiling edge case for `MultiModalDataParser` by @Isotr0py in https://github.com/vllm-project/vllm/pull/15828
* [Misc] Use envs.VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15831
* setup correct nvcc version with CUDA_HOME by @chenyang78 in https://github.com/vllm-project/vllm/pull/15725
* [Model] Support Mistral3 in the HF Transformers format by @mgoin in https://github.com/vllm-project/vllm/pull/15505
* [Misc] remove unused script by @reidliu41 in https://github.com/vllm-project/vllm/pull/15746
* Remove `format.sh` as it's been unsupported >70 days by @hmellor in https://github.com/vllm-project/vllm/pull/15884
* [New Model]: jinaai/jina-reranker-v2-base-multilingual  by @noooop in https://github.com/vllm-project/vllm/pull/15876
* [Doc] Quark quantization documentation by @cha557 in https://github.com/vllm-project/vllm/pull/15861
* Reinstate `format.sh` and make `pre-commit` installation simpler by @hmellor in https://github.com/vllm-project/vllm/pull/15890
* [Misc] Allow using OpenCV as video IO fallback by @Isotr0py in https://github.com/vllm-project/vllm/pull/15055
* [ROCm][Build][Bugfix] Bring the base dockerfile in sync with the ROCm fork by @gshtras in https://github.com/vllm-project/vllm/pull/15820
* Add option to use DeepGemm contiguous grouped gemm kernel for fused MoE operations. by @bnellnm in https://github.com/vllm-project/vllm/pull/13932
* [CI/Build] Clean up LoRA tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/15867
* [Model] Aya Vision by @JenZhao in https://github.com/vllm-project/vllm/pull/15441
* [Model] Add module name prefixes to gemma3 by @cloud11665 in https://github.com/vllm-project/vllm/pull/15889
* [CI] Disable flaky structure decoding test temporarily. by @ywang96 in https://github.com/vllm-project/vllm/pull/15892
* [V1][Metrics] Initial speculative decoding metrics by @markmc in https://github.com/vllm-project/vllm/pull/15151
* [V1][Spec Decode] Implement Eagle Proposer [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15729
* [Docs] update usage stats language by @simon-mo in https://github.com/vllm-project/vllm/pull/15898
* [BugFix] make sure socket close by @yihong0618 in https://github.com/vllm-project/vllm/pull/15875
* [Model][MiniMaxText01] Support MiniMaxText01 model inference by @ZZBoom in https://github.com/vllm-project/vllm/pull/13454
* [Docs] Add Ollama meetup slides by @simon-mo in https://github.com/vllm-project/vllm/pull/15905
* [Docs] Add Intel as Sponsor by @simon-mo in https://github.com/vllm-project/vllm/pull/15913
* Fix input triton kernel for eagle by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/15909
* [V1] Fix: make sure `k_index` is int64 for `apply_top_k_only` by @b8zhong in https://github.com/vllm-project/vllm/pull/15907
* [Bugfix] Fix imports for MoE on CPU by @gau-nernst in https://github.com/vllm-project/vllm/pull/15841
* [V1][Minor] Enhance SpecDecoding Metrics Log in V1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15902
* [Doc] Update rocm.inc.md by @chun37 in https://github.com/vllm-project/vllm/pull/15917
* [V1][Bugfix] Fix typo in MoE TPU checking by @ywang96 in https://github.com/vllm-project/vllm/pull/15927
* [Benchmark]Fix error message by @Potabk in https://github.com/vllm-project/vllm/pull/15866
* [Misc] Replace print with logger by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15923
* [CI/Build] Further clean up LoRA tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/15920
* [Bugfix] Fix cache block size calculation for CPU MLA by @gau-nernst in https://github.com/vllm-project/vllm/pull/15848
* [Build/CI] Update lm-eval to 0.4.8 by @cthi in https://github.com/vllm-project/vllm/pull/15912
* [Kernel] Add more dtype support for GGUF dequantization by @LukasBluebaum in https://github.com/vllm-project/vllm/pull/15879
* [core] Add tags parameter to wake_up() by @erictang000 in https://github.com/vllm-project/vllm/pull/15500
* [V1] Fix json_object support with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/15488
* Add minimum version for `huggingface_hub` to enable Xet downloads by @hmellor in https://github.com/vllm-project/vllm/pull/15873
* [Bugfix][Benchmarks] Ensure `async_request_deepspeed_mii` uses the OpenAI choices key by @b8zhong in https://github.com/vllm-project/vllm/pull/15926
* [CI] Remove duplicate entrypoints-test by @yankay in https://github.com/vllm-project/vllm/pull/15940
* [Bugfix] Fix the issue where the model name is empty string, causing no response with the model name. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15938
* [Metrics] Hide deprecated metrics by @markmc in https://github.com/vllm-project/vllm/pull/15458
* [Frontend] Implement Tool Calling with `tool_choice='required'` by @meffmadd in https://github.com/vllm-project/vllm/pull/13483
* [CPU][Bugfix] Using custom allreduce for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/15934
* [Model] use AutoWeightsLoader in model load_weights by @lengrongfu in https://github.com/vllm-project/vllm/pull/15770
* [Misc] V1 LoRA support CPU offload by @jeejeelee in https://github.com/vllm-project/vllm/pull/15843
* Restricted cmake to be less than version 4 as 4.x breaks the build ofâ€¦ by @npanpaliya in https://github.com/vllm-project/vllm/pull/15859
* [misc] instruct pytorch to use nvml-based cuda check by @youkaichao in https://github.com/vllm-project/vllm/pull/15951
* [V1] Support Mistral3 in V1 by @mgoin in https://github.com/vllm-project/vllm/pull/15950
* Fix `huggingface-cli[hf-xet]` -> `huggingface-cli[hf_xet]` by @hmellor in https://github.com/vllm-project/vllm/pull/15969
* [V1][TPU] TPU-optimized top-p implementation (avoids scattering). by @hyeygit in https://github.com/vllm-project/vllm/pull/15736
* [TPU] optimize the all-reduce performance by @yaochengji in https://github.com/vllm-project/vllm/pull/15903
* [V1][TPU] Do not compile sampling more than needed by @NickLucche in https://github.com/vllm-project/vllm/pull/15883
* [ROCM][KERNEL] Paged attention for V1 by @maleksan85 in https://github.com/vllm-project/vllm/pull/15720
* fix: better error message for get_config close #13889 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15943
* [bugfix] add seed in torchrun_example.py by @youkaichao in https://github.com/vllm-project/vllm/pull/15980
* [ROCM][V0] PA kennel selection when no sliding window provided by @maleksan85 in https://github.com/vllm-project/vllm/pull/15982
* [Benchmark] Add AIMO Dataset to Benchmark by @StevenShi-23 in https://github.com/vllm-project/vllm/pull/15955
* [misc] improve error message for "Failed to infer device type" by @youkaichao in https://github.com/vllm-project/vllm/pull/15994
* [Bugfix][V1] Fix bug from putting llm_engine.model_executor in a background process by @wwl2755 in https://github.com/vllm-project/vllm/pull/15367
* [doc] update contribution link by @reidliu41 in https://github.com/vllm-project/vllm/pull/15922
* fix: tiny fix make format.sh excutable by @yihong0618 in https://github.com/vllm-project/vllm/pull/16015
* [SupportsQuant] Bert, Blip, Blip2, Bloom by @kylesayrs in https://github.com/vllm-project/vllm/pull/15573
* [SupportsQuant] Chameleon, Chatglm, Commandr by @kylesayrs in https://github.com/vllm-project/vllm/pull/15952
* [Neuron][kernel] Fuse kv cache into a single tensor by @liangfu in https://github.com/vllm-project/vllm/pull/15911
* [Minor] Fused experts refactor by @bnellnm in https://github.com/vllm-project/vllm/pull/15914
* [Misc][Performance] Advance tpu.txt to the most recent nightly torch â€¦ by @yarongmu-google in https://github.com/vllm-project/vllm/pull/16024
* Re-enable the AMD Testing for the passing tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/15586
* [TPU] Support sliding window and logit soft capping in the paged attention kernel for TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/15732
* [TPU] Switch Test to Non-Sliding Window by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15981
* [Bugfix] Fix function names in test_block_fp8.py by @bnellnm in https://github.com/vllm-project/vllm/pull/16033
* [ROCm] Tweak the benchmark script to run on ROCm by @huydhn in https://github.com/vllm-project/vllm/pull/14252
* [Misc] improve gguf check by @reidliu41 in https://github.com/vllm-project/vllm/pull/15974
* [TPU][V1] Remove ragged attention kernel parameter hard coding by @yaochengji in https://github.com/vllm-project/vllm/pull/16041
* doc: add info for macos clang errors by @yihong0618 in https://github.com/vllm-project/vllm/pull/16049
* [V1][Spec Decode] Avoid logging useless nan metrics by @markmc in https://github.com/vllm-project/vllm/pull/16023
* [Model] use AutoWeightsLoader for baichuan, gpt-neox, mpt by @jonghyunchoe in https://github.com/vllm-project/vllm/pull/15939
* [Hardware][Gaudi][BugFix] fix arguments of hpu fused moe by @zhenwei-intel in https://github.com/vllm-project/vllm/pull/15945
* [Bugfix][kernels] Fix half2float conversion in gguf kernels by @Isotr0py in https://github.com/vllm-project/vllm/pull/15995
* [Benchmark][Doc] Update throughput benchmark and README by @StevenShi-23 in https://github.com/vllm-project/vllm/pull/15998
* [CPU] Change default block_size for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/16002
* [Distributed] [ROCM] Fix custom allreduce enable checks by @ilmarkov in https://github.com/vllm-project/vllm/pull/16010
* [ROCm][Bugfix] Use platform specific FP8 dtype by @gshtras in https://github.com/vllm-project/vllm/pull/15717
* [ROCm][Bugfix] Bring back fallback to eager mode removed in #14917, but for ROCm only by @gshtras in https://github.com/vllm-project/vllm/pull/15413
* [Bugfix] Fix default behavior/fallback for pp in v1 by @mgoin in https://github.com/vllm-project/vllm/pull/16057
* [CI] Reorganize .buildkite directory by @khluu in https://github.com/vllm-project/vllm/pull/16001
* [V1] DP scale-out (1/N): Use zmq ROUTER/DEALER sockets for input queue by @njhill in https://github.com/vllm-project/vllm/pull/15906
* [V1] Scatter and gather placeholders in the model runner by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15712
* Revert "[V1] Scatter and gather placeholders in the model runner" by @ywang96 in https://github.com/vllm-project/vllm/pull/16075
* [Kernel][Bugfix] Re-fuse triton moe weight application by @bnellnm in https://github.com/vllm-project/vllm/pull/16071
* [Bugfix][TPU] Fix V1 TPU worker for sliding window by @mgoin in https://github.com/vllm-project/vllm/pull/16059
* [V1][Spec Decode] Update N-gram Proposer Interface by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15750
* [Model] Support Llama4 in vLLM by @houseroad in https://github.com/vllm-project/vllm/pull/16104

## New Contributors
* @Shafi-Hussain made their first contribution in https://github.com/vllm-project/vllm/pull/15402
* @oteroantoniogom made their first contribution in https://github.com/vllm-project/vllm/pull/15471
* @cyyever made their first contribution in https://github.com/vllm-project/vllm/pull/15532
* @dr75 made their first contribution in https://github.com/vllm-project/vllm/pull/15297
* @wrmedford made their first contribution in https://github.com/vllm-project/vllm/pull/15511
* @MattTheCuber made their first contribution in https://github.com/vllm-project/vllm/pull/15299
* @jerryzh168 made their first contribution in https://github.com/vllm-project/vllm/pull/15575
* @Avabowler made their first contribution in https://github.com/vllm-project/vllm/pull/15481
* @zou3519 made their first contribution in https://github.com/vllm-project/vllm/pull/15494
* @h-sugi made their first contribution in https://github.com/vllm-project/vllm/pull/15211
* @clnperez made their first contribution in https://github.com/vllm-project/vllm/pull/15635
* @kebe7jun made their first contribution in https://github.com/vllm-project/vllm/pull/14948
* @CXIAAAAA made their first contribution in https://github.com/vllm-project/vllm/pull/15303
* @lizzzcai made their first contribution in https://github.com/vllm-project/vllm/pull/14373
* @simpx made their first contribution in https://github.com/vllm-project/vllm/pull/15716
* @pengyuange made their first contribution in https://github.com/vllm-project/vllm/pull/15397
* @pansicheng made their first contribution in https://github.com/vllm-project/vllm/pull/15321
* @lcy4869 made their first contribution in https://github.com/vllm-project/vllm/pull/15780
* @Naveassaf made their first contribution in https://github.com/vllm-project/vllm/pull/15008
* @noc-turne made their first contribution in https://github.com/vllm-project/vllm/pull/15542
* @ilmarkov made their first contribution in https://github.com/vllm-project/vllm/pull/14125
* @kinfey made their first contribution in https://github.com/vllm-project/vllm/pull/14886
* @haochengxia made their first contribution in https://github.com/vllm-project/vllm/pull/15851
* @alexwl made their first contribution in https://github.com/vllm-project/vllm/pull/15824
* @lionelvillard made their first contribution in https://github.com/vllm-project/vllm/pull/15248
* @cha557 made their first contribution in https://github.com/vllm-project/vllm/pull/15861
* @cloud11665 made their first contribution in https://github.com/vllm-project/vllm/pull/15889
* @ZZBoom made their first contribution in https://github.com/vllm-project/vllm/pull/13454
* @ekagra-ranjan made their first contribution in https://github.com/vllm-project/vllm/pull/15909
* @chun37 made their first contribution in https://github.com/vllm-project/vllm/pull/15917
* @cthi made their first contribution in https://github.com/vllm-project/vllm/pull/15912
* @LukasBluebaum made their first contribution in https://github.com/vllm-project/vllm/pull/15879
* @erictang000 made their first contribution in https://github.com/vllm-project/vllm/pull/15500
* @yankay made their first contribution in https://github.com/vllm-project/vllm/pull/15940
* @meffmadd made their first contribution in https://github.com/vllm-project/vllm/pull/13483
* @lengrongfu made their first contribution in https://github.com/vllm-project/vllm/pull/15770
* @StevenShi-23 made their first contribution in https://github.com/vllm-project/vllm/pull/15955

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.2...v0.8.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.3)

---

## v0.8.3rc1: v0.8.3rc1
**Published:** 2025-04-05
**Pre-release**

## What's Changed
* Fix CUDA kernel index data type in vllm/csrc/quantization/gptq_marlin/awq_marlin_repack.cu +10 by @houseroad in https://github.com/vllm-project/vllm/pull/15160
* [Hardware][TPU][Bugfix] Fix v1 mp profiler by @lsy323 in https://github.com/vllm-project/vllm/pull/15409
* [Kernel][CPU] CPU MLA by @gau-nernst in https://github.com/vllm-project/vllm/pull/14744
* Dockerfile.ppc64le changes to move to UBI by @Shafi-Hussain in https://github.com/vllm-project/vllm/pull/15402
* [Misc] Clean up MiniCPM-V/O code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15337
* [Misc] Remove redundant `num_embeds` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15443
* [Doc] Update V1 user guide for multi-modality by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15460
* [Kernel] Fix conflicting macro names for gguf kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/15456
* [bugfix] fix inductor cache on max_position_embeddings by @youkaichao in https://github.com/vllm-project/vllm/pull/15436
* [CI/Build] Add tests for the V1 tpu_model_runner. by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14843
* [Bugfix] Support triton==3.3.0+git95326d9f for RTX 5090 (Unsloth + vLLM compatibility) by @oteroantoniogom in https://github.com/vllm-project/vllm/pull/15471
* [bugfix] add supports_v1 platform interface by @joerunde in https://github.com/vllm-project/vllm/pull/15417
* Add workaround for shared field_names in pydantic model class by @maxdebayser in https://github.com/vllm-project/vllm/pull/13925
* [TPU][V1] Fix Sampler recompilation by @NickLucche in https://github.com/vllm-project/vllm/pull/15309
* [V1][Minor] Use `SchedulerInterface` type for engine scheduler field by @njhill in https://github.com/vllm-project/vllm/pull/15499
* [V1] Support long_prefill_token_threshold in v1 scheduler by @houseroad in https://github.com/vllm-project/vllm/pull/15419
* [core] add bucket padding to tpu_model_runner by @Chenyaaang in https://github.com/vllm-project/vllm/pull/14995
* [Core] LoRA: V1 Scheduler optimization by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15422
* [CI/Build] LoRA: Delete long context tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15503
* Transformers backend already supports V1 by @hmellor in https://github.com/vllm-project/vllm/pull/15463
* [Model] Support multi-image for Molmo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15438
* [Misc] Warn about v0 in benchmark_paged_attn.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/15495
* [BugFix] Fix nightly MLA failure (FA2 + MLA chunked prefill, i.e. V1, producing bad results) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/15492
* [misc] LoRA - Skip LoRA kernels when not required by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15152
* Fix raw_request extraction in load_aware_call decorator by @daniel-salib in https://github.com/vllm-project/vllm/pull/15382
* [Feature] Enhance EAGLE Architecture with Proper RMS Norms by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/14990
* [FEAT][ROCm] Integrate Fused MoE Kernels from AITER by @vllmellm in https://github.com/vllm-project/vllm/pull/14967
* [Misc] Enhance warning information to user-defined chat template by @wwl2755 in https://github.com/vllm-project/vllm/pull/15408
* [Misc] improve example script output by @reidliu41 in https://github.com/vllm-project/vllm/pull/15528
* Separate base model from `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/15467
* Apply torchfix by @cyyever in https://github.com/vllm-project/vllm/pull/15532
* Improve validation of TP in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/15540
* [Model] Add Reasoning Parser for Granite Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/14202
* multi-node offline DP+EP example by @youkaichao in https://github.com/vllm-project/vllm/pull/15484
* Fix weight loading for some models in Transformers backend by @hmellor in https://github.com/vllm-project/vllm/pull/15544
* [Refactor] Remove passthrough `backend` when generate grammar by @aarnphm in https://github.com/vllm-project/vllm/pull/15317
* [V1][Sampler] Faster top-k only implementation by @njhill in https://github.com/vllm-project/vllm/pull/15478
* Support SHA256 as hash function in prefix caching by @dr75 in https://github.com/vllm-project/vllm/pull/15297
* Applying some fixes for K8s agents in CI by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/15493
* [V1] TPU - Revert to exponential padding by default by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15565
* [V1] TPU CI - Fix test_compilation.py by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15570
* Use Cache Hinting for fused_moe kernel by @wrmedford in https://github.com/vllm-project/vllm/pull/15511
* [TPU] support disabling xla compilation cache by @yaochengji in https://github.com/vllm-project/vllm/pull/15567
* Support FIPS enabled machines with MD5 hashing by @MattTheCuber in https://github.com/vllm-project/vllm/pull/15299
* [Kernel] CUTLASS grouped gemm fp8 MoE kernel by @ElizaWszola in https://github.com/vllm-project/vllm/pull/13972
* Add automatic tpu label to mergify.yml by @mgoin in https://github.com/vllm-project/vllm/pull/15560
* add platform check back by @Chenyaaang in https://github.com/vllm-project/vllm/pull/15578
* [misc] LoRA: Remove unused long context test data by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15558
* [Doc] Update V1 user guide for fp8 kv cache support by @wayzeng in https://github.com/vllm-project/vllm/pull/15585
* [moe][quant] add weight name case for offset by @MengqingCao in https://github.com/vllm-project/vllm/pull/15515
* [V1] Refactor num_computed_tokens logic by @comaniac in https://github.com/vllm-project/vllm/pull/15307
* Allow torchao quantization in SiglipMLP by @jerryzh168 in https://github.com/vllm-project/vllm/pull/15575
* [ROCm] Env variable to trigger custom PA by @gshtras in https://github.com/vllm-project/vllm/pull/15557
* [TPU] [V1] fix cases when max_num_reqs is set smaller than MIN_NUM_SEQS by @yaochengji in https://github.com/vllm-project/vllm/pull/15583
* [Misc] Restrict ray version dependency and update PP feature warning in V1 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15556
* [TPU] Avoid Triton Import by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15589
* [Misc] Consolidate LRUCache implementations by @Avabowler in https://github.com/vllm-project/vllm/pull/15481
* [Quantization] Fp8 Channelwise Dynamic Per Token GroupedGEMM by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15587
* [Misc] Clean up `scatter_patch_features` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15559
* [Misc] Use model_redirect to redirect the model name to a local folder. by @noooop in https://github.com/vllm-project/vllm/pull/14116
* Fix incorrect filenames in vllm_compile_cache.py by @zou3519 in https://github.com/vllm-project/vllm/pull/15494
* [Doc] update --system for transformers installation in docker doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/15616
* [Model] MiniCPM-V/O supports V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15487
* [Bugfix] Fix use_cascade_attention handling for Alibi-based models on vllm/v1 by @h-sugi in https://github.com/vllm-project/vllm/pull/15211
* [Doc] Link to onboarding tasks by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15629
* [Misc] Replace `is_encoder_decoder_inputs` with `split_enc_dec_inputs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15620
* [Feature] Add middleware to log API Server responses by @terrytangyuan in https://github.com/vllm-project/vllm/pull/15593
* [Misc] Avoid direct access of global `mm_registry` in `compute_encoder_budget` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15621
* [Doc] Use absolute placement for Ask AI button by @hmellor in https://github.com/vllm-project/vllm/pull/15628
* [Bugfix][TPU][V1] Fix recompilation by @NickLucche in https://github.com/vllm-project/vllm/pull/15553
* Correct PowerPC to modern IBM Power by @clnperez in https://github.com/vllm-project/vllm/pull/15635
* [CI] Update rules for applying `tpu` label. by @russellb in https://github.com/vllm-project/vllm/pull/15634
* [V1] AsyncLLM data parallel by @njhill in https://github.com/vllm-project/vllm/pull/13923
* [TPU] Lazy Import by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15656
* [Quantization][V1]  BitsAndBytes support V1 by @jeejeelee in https://github.com/vllm-project/vllm/pull/15611
* [Bugfix] Fix failure to launch in Tensor Parallel TP mode on macOS. by @kebe7jun in https://github.com/vllm-project/vllm/pull/14948
* [Doc] Fix dead links in Job Board by @wwl2755 in https://github.com/vllm-project/vllm/pull/15637
* [CI][TPU] Temporarily Disable Quant Test on TPU by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15649
* Revert "Use Cache Hinting for fused_moe kernel (#15511)" by @wrmedford in https://github.com/vllm-project/vllm/pull/15645
* [Misc]add coding benchmark for speculative decoding by @CXIAAAAA in https://github.com/vllm-project/vllm/pull/15303
* [Quantization][FP8] Adding support for fp8 gemm layer input in fp8 by @gshtras in https://github.com/vllm-project/vllm/pull/14578
* Refactor error handling for multiple exceptions in preprocessing by @JasonZhu1313 in https://github.com/vllm-project/vllm/pull/15650
* [Bugfix] Fix `mm_hashes` forgetting to be passed by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15668
* [V1] Remove legacy input registry by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15673
* [TPU][CI] Fix TPUModelRunner Test by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15667
* [Refactor][Frontend] Keep all logic about reasoning into one class by @gaocegege in https://github.com/vllm-project/vllm/pull/14428
* [CPU][CI] Improve CPU Dockerfile by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/15690
* [Bugfix] Fix 'InductorAdaptor object has no attribute 'cache_dir' by @jeejeelee in https://github.com/vllm-project/vllm/pull/15674
* [Misc] Fix test_sleep to use query parameters by @lizzzcai in https://github.com/vllm-project/vllm/pull/14373
* [Bugfix][Frontend] Eliminate regex based check in reasoning full generator by @gaocegege in https://github.com/vllm-project/vllm/pull/14821
* [Frontend] update priority for --api-key and VLLM_API_KEY by @reidliu41 in https://github.com/vllm-project/vllm/pull/15588
* [Docs] Add "Generation quality changed" section to troubleshooting by @hmellor in https://github.com/vllm-project/vllm/pull/15701
* [Model] Adding torch compile annotations to chatglm by @jeejeelee in https://github.com/vllm-project/vllm/pull/15624
* [Bugfix][v1] xgrammar structured output supports Enum. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15594
* [Bugfix] `embed_is_patch` for Idefics3 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15696
* [V1] Support disable_any_whtespace for guidance backend by @russellb in https://github.com/vllm-project/vllm/pull/15584
* [doc] add missing imports by @reidliu41 in https://github.com/vllm-project/vllm/pull/15699
* [Bugfix] Fix regex compile display format by @kebe7jun in https://github.com/vllm-project/vllm/pull/15368
* Fix cpu offload testing for gptq/awq/ct by @mgoin in https://github.com/vllm-project/vllm/pull/15648
* [Minor] Remove TGI launching script  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15646
* [Misc] Remove unused utils and clean up imports by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15708
* [Misc] Remove stale func in KVTransferConfig by @ShangmingCai in https://github.com/vllm-project/vllm/pull/14746
* [TPU] [Perf] Improve Memory Usage Estimation by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15671
* [Bugfix] [torch.compile] Add Dynamo metrics context during compilation by @ProExpertProg in https://github.com/vllm-project/vllm/pull/15639
* [V1] TPU - Fix the chunked prompt bug by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15713
* [Misc] cli auto show default value by @reidliu41 in https://github.com/vllm-project/vllm/pull/15582
* implement prometheus fast-api-instrumentor for http service metrics by @daniel-salib in https://github.com/vllm-project/vllm/pull/15657
* [Docs][V1] Optimize diagrams in prefix caching design by @simpx in https://github.com/vllm-project/vllm/pull/15716
* [ROCm][AMD][Build] Update AMD supported arch list by @gshtras in https://github.com/vllm-project/vllm/pull/15632
* [Model] Support Skywork-R1V by @pengyuange in https://github.com/vllm-project/vllm/pull/15397
* [Docs] Document v0 engine support in reasoning outputs by @gaocegege in https://github.com/vllm-project/vllm/pull/15739
* [Misc][V1] Misc code streamlining by @njhill in https://github.com/vllm-project/vllm/pull/15723
* [Bugfix] LoRA V1: add and fix entrypoints tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15715
* [CI] Speed up V1 structured output tests by @russellb in https://github.com/vllm-project/vllm/pull/15718
* Use numba 0.61 for python 3.10+ to support numpy>=2 by @cyyever in https://github.com/vllm-project/vllm/pull/15692
* [Bugfix] set VLLM_WORKER_MULTIPROC_METHOD=spawn for vllm.entrypoionts.openai.api_server by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/15700
* [TPU][V1][Bugfix] Fix w8a8 recompiilation with GSM8K by @NickLucche in https://github.com/vllm-project/vllm/pull/15714
* [Kernel][TPU][ragged-paged-attn] vLLM code change for PR#8896 by @yarongmu-google in https://github.com/vllm-project/vllm/pull/15659
* [doc] update doc by @reidliu41 in https://github.com/vllm-project/vllm/pull/15740
* [FEAT] [ROCm] Add AITER int8 scaled gemm kernel by @tjtanaa in https://github.com/vllm-project/vllm/pull/15433
* [V1] [Feature] Collective RPC by @wwl2755 in https://github.com/vllm-project/vllm/pull/15444
* [Feature][Disaggregated] Support XpYd disaggregated prefill with MooncakeStore by @ShangmingCai in https://github.com/vllm-project/vllm/pull/12957
* [V1] Support interleaved modality items by @ywang96 in https://github.com/vllm-project/vllm/pull/15605
* [V1][Minor] Simplify rejection sampler's parse_output by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15741
* [Bugfix] Fix Mllama interleaved images input support by @Isotr0py in https://github.com/vllm-project/vllm/pull/15564
* [CI] xgrammar structured output supports Enum. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15757
* [Bugfix] Fix Mistral guided generation using xgrammar by @juliendenize in https://github.com/vllm-project/vllm/pull/15704
* [doc] update conda to usage link in installation by @reidliu41 in https://github.com/vllm-project/vllm/pull/15761
* fix test_phi3v by @pansicheng in https://github.com/vllm-project/vllm/pull/15321
* [V1] Override `mm_counts` for dummy data creation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15703
* fix: lint fix a ruff checkout syntax error by @yihong0618 in https://github.com/vllm-project/vllm/pull/15767
* [Bugfix] Added `embed_is_patch` mask for fuyu model by @kylehh in https://github.com/vllm-project/vllm/pull/15731
* fix: Comments to English for better dev experience by @yihong0618 in https://github.com/vllm-project/vllm/pull/15768
* [V1][Scheduler] Avoid calling `_try_schedule_encoder_inputs` for every request by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15778
* [Misc] update the comments by @lcy4869 in https://github.com/vllm-project/vllm/pull/15780
* [Benchmark] Update Vision Arena Dataset and HuggingFaceDataset Setup by @JenZhao in https://github.com/vllm-project/vllm/pull/15748
* [Feature][ROCm]Enable fusion pass for torch.compile on ROCm by @charlifu in https://github.com/vllm-project/vllm/pull/15050
* Recommend developing with Python 3.12 in developer guide by @hmellor in https://github.com/vllm-project/vllm/pull/15811
* fix: better install requirement for install in setup.py by @yihong0618 in https://github.com/vllm-project/vllm/pull/15796
* [V1] Fully Transparent Implementation of CPU Offloading by @youkaichao in https://github.com/vllm-project/vllm/pull/15354
* [Model] Update support for NemotronNAS models by @Naveassaf in https://github.com/vllm-project/vllm/pull/15008
* [Bugfix] Fix Crashing When Loading Modules With Batchnorm Stats by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/15813
* [Bugfix] Fix missing return value in load_weights method of adapters.py by @noc-turne in https://github.com/vllm-project/vllm/pull/15542
* Upgrade `transformers` to `v4.50.3` by @hmellor in https://github.com/vllm-project/vllm/pull/13905
* [Bugfix] Check dimensions of multimodal embeddings in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15816
* [V1][Spec Decode] Remove deprecated spec decode config params by @ShangmingCai in https://github.com/vllm-project/vllm/pull/15466
* fix: change GB to GiB in logging close #14979 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15807
* [V1] TPU CI - Add basic perf regression test by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15414
* Fix Transformers backend compatibility check by @hmellor in https://github.com/vllm-project/vllm/pull/15290
* [V1][Core] Remove unused speculative config from scheduler by @markmc in https://github.com/vllm-project/vllm/pull/15818
* Move dockerfiles into their own directory by @hmellor in https://github.com/vllm-project/vllm/pull/14549
* [Distributed] Add custom allreduce support for ROCM by @ilmarkov in https://github.com/vllm-project/vllm/pull/14125
* Rename fallback model and refactor supported models section by @hmellor in https://github.com/vllm-project/vllm/pull/15829
* [Frontend] Add Phi-4-mini function calling support by @kinfey in https://github.com/vllm-project/vllm/pull/14886
* [Bugfix][Model] fix mllama multi-image by @yma11 in https://github.com/vllm-project/vllm/pull/14883
* [Bugfix] Fix extra comma by @haochengxia in https://github.com/vllm-project/vllm/pull/15851
* [Bugfix]: Fix is_embedding_layer condition in VocabParallelEmbedding  by @alexwl in https://github.com/vllm-project/vllm/pull/15824
* [V1] TPU - Fix fused MOE by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15834
* [sleep mode] clear pytorch cache after sleep by @lionelvillard in https://github.com/vllm-project/vllm/pull/15248
* [ROCm] Use device name in the warning by @gshtras in https://github.com/vllm-project/vllm/pull/15838
* [V1] Implement sliding window attention in kv_cache_manager by @heheda12345 in https://github.com/vllm-project/vllm/pull/14097
* fix: can not use uv run collect_env close #13888 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15792
* [Feature] specify model in config.yaml by @wayzeng in https://github.com/vllm-project/vllm/pull/15798
* [Misc] Enable V1 LoRA by default by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15320
* [Misc] Fix speculative config repr string by @ShangmingCai in https://github.com/vllm-project/vllm/pull/15860
* [Docs] Fix small error in link text by @hmellor in https://github.com/vllm-project/vllm/pull/15868
* [Bugfix] Fix no video/image profiling edge case for `MultiModalDataParser` by @Isotr0py in https://github.com/vllm-project/vllm/pull/15828
* [Misc] Use envs.VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15831
* setup correct nvcc version with CUDA_HOME by @chenyang78 in https://github.com/vllm-project/vllm/pull/15725
* [Model] Support Mistral3 in the HF Transformers format by @mgoin in https://github.com/vllm-project/vllm/pull/15505
* [Misc] remove unused script by @reidliu41 in https://github.com/vllm-project/vllm/pull/15746
* Remove `format.sh` as it's been unsupported >70 days by @hmellor in https://github.com/vllm-project/vllm/pull/15884
* [New Model]: jinaai/jina-reranker-v2-base-multilingual  by @noooop in https://github.com/vllm-project/vllm/pull/15876
* [Doc] Quark quantization documentation by @cha557 in https://github.com/vllm-project/vllm/pull/15861
* Reinstate `format.sh` and make `pre-commit` installation simpler by @hmellor in https://github.com/vllm-project/vllm/pull/15890
* [Misc] Allow using OpenCV as video IO fallback by @Isotr0py in https://github.com/vllm-project/vllm/pull/15055
* [ROCm][Build][Bugfix] Bring the base dockerfile in sync with the ROCm fork by @gshtras in https://github.com/vllm-project/vllm/pull/15820
* Add option to use DeepGemm contiguous grouped gemm kernel for fused MoE operations. by @bnellnm in https://github.com/vllm-project/vllm/pull/13932
* [CI/Build] Clean up LoRA tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/15867
* [Model] Aya Vision by @JenZhao in https://github.com/vllm-project/vllm/pull/15441
* [Model] Add module name prefixes to gemma3 by @cloud11665 in https://github.com/vllm-project/vllm/pull/15889
* [CI] Disable flaky structure decoding test temporarily. by @ywang96 in https://github.com/vllm-project/vllm/pull/15892
* [V1][Metrics] Initial speculative decoding metrics by @markmc in https://github.com/vllm-project/vllm/pull/15151
* [V1][Spec Decode] Implement Eagle Proposer [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15729
* [Docs] update usage stats language by @simon-mo in https://github.com/vllm-project/vllm/pull/15898
* [BugFix] make sure socket close by @yihong0618 in https://github.com/vllm-project/vllm/pull/15875
* [Model][MiniMaxText01] Support MiniMaxText01 model inference by @ZZBoom in https://github.com/vllm-project/vllm/pull/13454
* [Docs] Add Ollama meetup slides by @simon-mo in https://github.com/vllm-project/vllm/pull/15905
* [Docs] Add Intel as Sponsor by @simon-mo in https://github.com/vllm-project/vllm/pull/15913
* Fix input triton kernel for eagle by @ekagra-ranjan in https://github.com/vllm-project/vllm/pull/15909
* [V1] Fix: make sure `k_index` is int64 for `apply_top_k_only` by @b8zhong in https://github.com/vllm-project/vllm/pull/15907
* [Bugfix] Fix imports for MoE on CPU by @gau-nernst in https://github.com/vllm-project/vllm/pull/15841
* [V1][Minor] Enhance SpecDecoding Metrics Log in V1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15902
* [Doc] Update rocm.inc.md by @chun37 in https://github.com/vllm-project/vllm/pull/15917
* [V1][Bugfix] Fix typo in MoE TPU checking by @ywang96 in https://github.com/vllm-project/vllm/pull/15927
* [Benchmark]Fix error message by @Potabk in https://github.com/vllm-project/vllm/pull/15866
* [Misc] Replace print with logger by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15923
* [CI/Build] Further clean up LoRA tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/15920
* [Bugfix] Fix cache block size calculation for CPU MLA by @gau-nernst in https://github.com/vllm-project/vllm/pull/15848
* [Build/CI] Update lm-eval to 0.4.8 by @cthi in https://github.com/vllm-project/vllm/pull/15912
* [Kernel] Add more dtype support for GGUF dequantization by @LukasBluebaum in https://github.com/vllm-project/vllm/pull/15879
* [core] Add tags parameter to wake_up() by @erictang000 in https://github.com/vllm-project/vllm/pull/15500
* [V1] Fix json_object support with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/15488
* Add minimum version for `huggingface_hub` to enable Xet downloads by @hmellor in https://github.com/vllm-project/vllm/pull/15873
* [Bugfix][Benchmarks] Ensure `async_request_deepspeed_mii` uses the OpenAI choices key by @b8zhong in https://github.com/vllm-project/vllm/pull/15926
* [CI] Remove duplicate entrypoints-test by @yankay in https://github.com/vllm-project/vllm/pull/15940
* [Bugfix] Fix the issue where the model name is empty string, causing no response with the model name. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15938
* [Metrics] Hide deprecated metrics by @markmc in https://github.com/vllm-project/vllm/pull/15458
* [Frontend] Implement Tool Calling with `tool_choice='required'` by @meffmadd in https://github.com/vllm-project/vllm/pull/13483
* [CPU][Bugfix] Using custom allreduce for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/15934
* [Model] use AutoWeightsLoader in model load_weights by @lengrongfu in https://github.com/vllm-project/vllm/pull/15770
* [Misc] V1 LoRA support CPU offload by @jeejeelee in https://github.com/vllm-project/vllm/pull/15843
* Restricted cmake to be less than version 4 as 4.x breaks the build ofâ€¦ by @npanpaliya in https://github.com/vllm-project/vllm/pull/15859
* [misc] instruct pytorch to use nvml-based cuda check by @youkaichao in https://github.com/vllm-project/vllm/pull/15951
* [V1] Support Mistral3 in V1 by @mgoin in https://github.com/vllm-project/vllm/pull/15950
* Fix `huggingface-cli[hf-xet]` -> `huggingface-cli[hf_xet]` by @hmellor in https://github.com/vllm-project/vllm/pull/15969
* [V1][TPU] TPU-optimized top-p implementation (avoids scattering). by @hyeygit in https://github.com/vllm-project/vllm/pull/15736
* [TPU] optimize the all-reduce performance by @yaochengji in https://github.com/vllm-project/vllm/pull/15903
* [V1][TPU] Do not compile sampling more than needed by @NickLucche in https://github.com/vllm-project/vllm/pull/15883
* [ROCM][KERNEL] Paged attention for V1 by @maleksan85 in https://github.com/vllm-project/vllm/pull/15720
* fix: better error message for get_config close #13889 by @yihong0618 in https://github.com/vllm-project/vllm/pull/15943
* [bugfix] add seed in torchrun_example.py by @youkaichao in https://github.com/vllm-project/vllm/pull/15980
* [ROCM][V0] PA kennel selection when no sliding window provided by @maleksan85 in https://github.com/vllm-project/vllm/pull/15982
* [Benchmark] Add AIMO Dataset to Benchmark by @StevenShi-23 in https://github.com/vllm-project/vllm/pull/15955
* [misc] improve error message for "Failed to infer device type" by @youkaichao in https://github.com/vllm-project/vllm/pull/15994
* [Bugfix][V1] Fix bug from putting llm_engine.model_executor in a background process by @wwl2755 in https://github.com/vllm-project/vllm/pull/15367
* [doc] update contribution link by @reidliu41 in https://github.com/vllm-project/vllm/pull/15922
* fix: tiny fix make format.sh excutable by @yihong0618 in https://github.com/vllm-project/vllm/pull/16015
* [SupportsQuant] Bert, Blip, Blip2, Bloom by @kylesayrs in https://github.com/vllm-project/vllm/pull/15573
* [SupportsQuant] Chameleon, Chatglm, Commandr by @kylesayrs in https://github.com/vllm-project/vllm/pull/15952
* [Neuron][kernel] Fuse kv cache into a single tensor by @liangfu in https://github.com/vllm-project/vllm/pull/15911
* [Minor] Fused experts refactor by @bnellnm in https://github.com/vllm-project/vllm/pull/15914
* [Misc][Performance] Advance tpu.txt to the most recent nightly torch â€¦ by @yarongmu-google in https://github.com/vllm-project/vllm/pull/16024
* Re-enable the AMD Testing for the passing tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/15586
* [TPU] Support sliding window and logit soft capping in the paged attention kernel for TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/15732
* [TPU] Switch Test to Non-Sliding Window by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/15981
* [Bugfix] Fix function names in test_block_fp8.py by @bnellnm in https://github.com/vllm-project/vllm/pull/16033
* [ROCm] Tweak the benchmark script to run on ROCm by @huydhn in https://github.com/vllm-project/vllm/pull/14252
* [Misc] improve gguf check by @reidliu41 in https://github.com/vllm-project/vllm/pull/15974
* [TPU][V1] Remove ragged attention kernel parameter hard coding by @yaochengji in https://github.com/vllm-project/vllm/pull/16041
* doc: add info for macos clang errors by @yihong0618 in https://github.com/vllm-project/vllm/pull/16049
* [V1][Spec Decode] Avoid logging useless nan metrics by @markmc in https://github.com/vllm-project/vllm/pull/16023
* [Model] use AutoWeightsLoader for baichuan, gpt-neox, mpt by @jonghyunchoe in https://github.com/vllm-project/vllm/pull/15939
* [Hardware][Gaudi][BugFix] fix arguments of hpu fused moe by @zhenwei-intel in https://github.com/vllm-project/vllm/pull/15945
* [Bugfix][kernels] Fix half2float conversion in gguf kernels by @Isotr0py in https://github.com/vllm-project/vllm/pull/15995
* [Benchmark][Doc] Update throughput benchmark and README by @StevenShi-23 in https://github.com/vllm-project/vllm/pull/15998
* [CPU] Change default block_size for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/16002
* [Distributed] [ROCM] Fix custom allreduce enable checks by @ilmarkov in https://github.com/vllm-project/vllm/pull/16010
* [ROCm][Bugfix] Use platform specific FP8 dtype by @gshtras in https://github.com/vllm-project/vllm/pull/15717
* [ROCm][Bugfix] Bring back fallback to eager mode removed in #14917, but for ROCm only by @gshtras in https://github.com/vllm-project/vllm/pull/15413
* [Bugfix] Fix default behavior/fallback for pp in v1 by @mgoin in https://github.com/vllm-project/vllm/pull/16057
* [CI] Reorganize .buildkite directory by @khluu in https://github.com/vllm-project/vllm/pull/16001
* [V1] DP scale-out (1/N): Use zmq ROUTER/DEALER sockets for input queue by @njhill in https://github.com/vllm-project/vllm/pull/15906
* [V1] Scatter and gather placeholders in the model runner by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15712
* Revert "[V1] Scatter and gather placeholders in the model runner" by @ywang96 in https://github.com/vllm-project/vllm/pull/16075
* [Kernel][Bugfix] Re-fuse triton moe weight application by @bnellnm in https://github.com/vllm-project/vllm/pull/16071
* [Bugfix][TPU] Fix V1 TPU worker for sliding window by @mgoin in https://github.com/vllm-project/vllm/pull/16059
* [V1][Spec Decode] Update N-gram Proposer Interface by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15750

## New Contributors
* @Shafi-Hussain made their first contribution in https://github.com/vllm-project/vllm/pull/15402
* @oteroantoniogom made their first contribution in https://github.com/vllm-project/vllm/pull/15471
* @cyyever made their first contribution in https://github.com/vllm-project/vllm/pull/15532
* @dr75 made their first contribution in https://github.com/vllm-project/vllm/pull/15297
* @wrmedford made their first contribution in https://github.com/vllm-project/vllm/pull/15511
* @MattTheCuber made their first contribution in https://github.com/vllm-project/vllm/pull/15299
* @jerryzh168 made their first contribution in https://github.com/vllm-project/vllm/pull/15575
* @Avabowler made their first contribution in https://github.com/vllm-project/vllm/pull/15481
* @zou3519 made their first contribution in https://github.com/vllm-project/vllm/pull/15494
* @h-sugi made their first contribution in https://github.com/vllm-project/vllm/pull/15211
* @clnperez made their first contribution in https://github.com/vllm-project/vllm/pull/15635
* @kebe7jun made their first contribution in https://github.com/vllm-project/vllm/pull/14948
* @CXIAAAAA made their first contribution in https://github.com/vllm-project/vllm/pull/15303
* @lizzzcai made their first contribution in https://github.com/vllm-project/vllm/pull/14373
* @simpx made their first contribution in https://github.com/vllm-project/vllm/pull/15716
* @pengyuange made their first contribution in https://github.com/vllm-project/vllm/pull/15397
* @pansicheng made their first contribution in https://github.com/vllm-project/vllm/pull/15321
* @lcy4869 made their first contribution in https://github.com/vllm-project/vllm/pull/15780
* @Naveassaf made their first contribution in https://github.com/vllm-project/vllm/pull/15008
* @noc-turne made their first contribution in https://github.com/vllm-project/vllm/pull/15542
* @ilmarkov made their first contribution in https://github.com/vllm-project/vllm/pull/14125
* @kinfey made their first contribution in https://github.com/vllm-project/vllm/pull/14886
* @haochengxia made their first contribution in https://github.com/vllm-project/vllm/pull/15851
* @alexwl made their first contribution in https://github.com/vllm-project/vllm/pull/15824
* @lionelvillard made their first contribution in https://github.com/vllm-project/vllm/pull/15248
* @cha557 made their first contribution in https://github.com/vllm-project/vllm/pull/15861
* @cloud11665 made their first contribution in https://github.com/vllm-project/vllm/pull/15889
* @ZZBoom made their first contribution in https://github.com/vllm-project/vllm/pull/13454
* @ekagra-ranjan made their first contribution in https://github.com/vllm-project/vllm/pull/15909
* @chun37 made their first contribution in https://github.com/vllm-project/vllm/pull/15917
* @cthi made their first contribution in https://github.com/vllm-project/vllm/pull/15912
* @LukasBluebaum made their first contribution in https://github.com/vllm-project/vllm/pull/15879
* @erictang000 made their first contribution in https://github.com/vllm-project/vllm/pull/15500
* @yankay made their first contribution in https://github.com/vllm-project/vllm/pull/15940
* @meffmadd made their first contribution in https://github.com/vllm-project/vllm/pull/13483
* @lengrongfu made their first contribution in https://github.com/vllm-project/vllm/pull/15770
* @StevenShi-23 made their first contribution in https://github.com/vllm-project/vllm/pull/15955
* @jonghyunchoe made their first contribution in https://github.com/vllm-project/vllm/pull/15939

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.2...v0.8.3rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.3rc1)

---

## v0.8.2: v0.8.2
**Published:** 2025-03-23

This release contains important bug fix for the V1 engine's memory usage. We highly recommend you upgrading! 

## Highlights
* Revert "Use uv python for docker rather than ppa:deadsnakess/ppa (#13569)" (#15377)
* Remove openvino support in favor of external plugin (#15339)

### V1 Engine
* Fix V1 Engine crash while handling requests with duplicate request id (#15043)
* Support FP8 KV Cache (#14570, #15191)
* Add flag to disable cascade attention (#15243)
* Scheduler Refactoring: Add Scheduler Interface (#15250)
* Structured Output
	* Add `disable-any-whitespace` option support for xgrammar (#15316)
	* guidance backend for structured output + `auto` fallback mode (#14779)
* Spec Decode
	* Enable spec decode for top-p & top-k sampling (#15063)
	* Use better defaults for N-gram (#15358)
	* Update target_logits in place for rejection sampling (#15427)
* AMD
	* Enable Triton(ROCm) Attention backend for Nvidia GPUs (#14071)
* TPU
	* Support V1 Sampler for ragged attention (#14227)
	* Tensor parallel MP support (#15059)
	* MHA Pallas backend (#15288)

### Features 
* Integrate `fastsafetensors` loader for loading model weights (#10647)
* Add guidance backend for structured output (#14589)

### Others
* Add Kubernetes deployment guide with CPUs (#14865)
* Support reset prefix cache by specified device (#15003)
* Support tool calling and reasoning parser (#14511)
* Support --disable-uvicorn-access-log parameters (#14754)
* Support Tele-FLM Model (#15023)
* Add pipeline parallel support to `TransformersModel` (#12832)
* Enable CUDA graph support for llama 3.2 vision (#14917)



## What's Changed
* [FEAT]Support reset prefix cache by specified device by @maobaolong in https://github.com/vllm-project/vllm/pull/15003
* [BugFix][V1] Update stats.py by @WrRan in https://github.com/vllm-project/vllm/pull/15139
* [V1][TPU] Change kv cache shape. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/15145
* [FrontEnd][Perf] `merge_async_iterators` fast-path for single-prompt requests by @njhill in https://github.com/vllm-project/vllm/pull/15150
* [Docs] Annouce Ollama and Singapore Meetups by @simon-mo in https://github.com/vllm-project/vllm/pull/15161
* [V1] TPU - Tensor parallel MP support by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15059
* [BugFix] Lazily import XgrammarBackend to avoid early cuda init by @njhill in https://github.com/vllm-project/vllm/pull/15171
* [Doc] Clarify run vllm only on one node in distributed inference by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15148
* Fix broken tests by @jovsa in https://github.com/vllm-project/vllm/pull/14713
* [Bugfix] Fix embedding assignment for InternVL-based models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15086
* fix "Total generated tokens:" is 0 if using --backend tgi and --endpoâ€¦ by @sywangyi in https://github.com/vllm-project/vllm/pull/14673
* [V1][TPU] Support V1 Sampler for ragged attention by @NickLucche in https://github.com/vllm-project/vllm/pull/14227
* [Benchmark] Allow oversample request in benchmark dataset by @JenZhao in https://github.com/vllm-project/vllm/pull/15170
* [Core][V0] Add guidance backend for structured output by @russellb in https://github.com/vllm-project/vllm/pull/14589
* [Doc] Update Mistral Small 3.1/Pixtral example by @ywang96 in https://github.com/vllm-project/vllm/pull/15184
* [Misc] support --disable-uvicorn-access-log parameters by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/14754
* [Attention] Flash Attention 3 - fp8 by @mickaelseznec in https://github.com/vllm-project/vllm/pull/14570
* [Doc] Update README.md by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15187
* Enable CUDA graph support for llama 3.2 vision by @mritterfigma in https://github.com/vllm-project/vllm/pull/14917
* typo: Update config.py by @WrRan in https://github.com/vllm-project/vllm/pull/15189
* [Frontend][Bugfix] support prefill decode disaggregation on deepseek by @billishyahao in https://github.com/vllm-project/vllm/pull/14824
* [release] Tag vllm-cpu with latest upon new version released by @khluu in https://github.com/vllm-project/vllm/pull/15193
* Fixing Imprecise Type Annotations by @WrRan in https://github.com/vllm-project/vllm/pull/15192
* [macOS] Ugrade pytorch to 2.6.0 by @linktohack in https://github.com/vllm-project/vllm/pull/15129
* [Bugfix] Multi-video inference on LLaVA-Onevision by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15082
* Add user forum to README by @hmellor in https://github.com/vllm-project/vllm/pull/15220
* Fix env vars for running Ray distributed backend on GKE by @richardsliu in https://github.com/vllm-project/vllm/pull/15166
* Replace `misc` issues with link to forum by @hmellor in https://github.com/vllm-project/vllm/pull/15226
* [ci] feat: make the test_torchrun_example run with tp=2, external_dp=2 by @vermouth1992 in https://github.com/vllm-project/vllm/pull/15172
* [Bugfix] fix V1 Engine crash while handling requests with duplicate request id by @JasonJ2021 in https://github.com/vllm-project/vllm/pull/15043
* [V1] Add flag to disable cascade attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15243
* Enforce that TP > 1 is not supported for Mamba2 if Quantization is Enabled. by @fabianlim in https://github.com/vllm-project/vllm/pull/14617
* [V1] Scheduler Refactoring [1/N] - Add Scheduler Interface by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15250
* [CI/Build] LoRA : make add_lora_test safer by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15181
* Fix CUDA kernel index data type in vllm/csrc/quantization/fused_kernels/layernorm_utils.cuh +10 by @houseroad in https://github.com/vllm-project/vllm/pull/15159
* [Misc] Clean up the BitsAndBytes arguments by @jeejeelee in https://github.com/vllm-project/vllm/pull/15140
* [ROCM] Upgrade torch to 2.6 by @SageMoore in https://github.com/vllm-project/vllm/pull/15244
* [Bugfix] Fix incorrect qwen2.5-vl attention mask pre-computation by @Isotr0py in https://github.com/vllm-project/vllm/pull/15200
* Mention `extra_body` as a way top pass vLLM only parameters using the OpenAI client by @hmellor in https://github.com/vllm-project/vllm/pull/15240
* [V1][TPU] Speed up top-k on TPU by using torch.topk by @hyeygit in https://github.com/vllm-project/vllm/pull/15242
* [Bugfix] detect alibi and revert to FA2 by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/15231
* [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  by @cyang49 in https://github.com/vllm-project/vllm/pull/14857
* [Docs] Trim the latest news in README by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15261
* [Misc] Better RayExecutor and multiprocessing compatibility by @comaniac in https://github.com/vllm-project/vllm/pull/14705
* Add an example for reproducibility by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15262
* [Hardware][TPU] Add check for no additional graph compilation during runtime by @lsy323 in https://github.com/vllm-project/vllm/pull/14710
* [V1] Enable Triton(ROCm) Attention backend for Nvidia GPUs by @Isotr0py in https://github.com/vllm-project/vllm/pull/14071
* [Doc] Update LWS docs by @Edwinhr716 in https://github.com/vllm-project/vllm/pull/15163
* [V1] Avoid redundant input processing in n>1 case by @njhill in https://github.com/vllm-project/vllm/pull/14985
* [Feature] specify model in config.yaml  by @wayzeng in https://github.com/vllm-project/vllm/pull/14855
* [Bugfix] Add int8 torch dtype for KVCache by @shen-shanshan in https://github.com/vllm-project/vllm/pull/15260
* [Misc] Add attention mask pre-computation optimization back to Qwen2.5-VL by @Isotr0py in https://github.com/vllm-project/vllm/pull/15273
* [Bugfix] Fix incorrect resolving order for transformers fallback by @Isotr0py in https://github.com/vllm-project/vllm/pull/15279
* [V1] Fix wrong import path of get_flash_attn_version by @lhtin in https://github.com/vllm-project/vllm/pull/15280
* [Bugfix] Fix broken kernel test due to missing rename for v1 Triton backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/15282
* [Misc] Add cProfile helpers by @russellb in https://github.com/vllm-project/vllm/pull/15074
* [v1] Refactor KVCacheConfig by @heheda12345 in https://github.com/vllm-project/vllm/pull/14079
* [Bugfix][VLM] fix llava processor by @MengqingCao in https://github.com/vllm-project/vllm/pull/15285
* Revert "[Feature] specify model in config.yaml  (#14855)" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15293
* [TPU][V1] MHA Pallas backend by @NickLucche in https://github.com/vllm-project/vllm/pull/15288
* [Build/CI] Fix env var typo by @russellb in https://github.com/vllm-project/vllm/pull/15305
* [Misc] Increase RayDistributedExecutor RAY_CGRAPH_get_timeout by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15301
* [Bugfix][V0] Multi-sequence logprobs streaming edge case by @andylolu2 in https://github.com/vllm-project/vllm/pull/15259
* [FEAT] [ROCm]:  Add AITER RMS Norm (Layer Norm) Feature by @tjtanaa in https://github.com/vllm-project/vllm/pull/14959
* [Doc] add load_format items in docs by @wwl2755 in https://github.com/vllm-project/vllm/pull/14804
* [Bugfix] Fix torch.compile raise FileNotFoundError by @jeejeelee in https://github.com/vllm-project/vllm/pull/15278
* [Bugfix] LoRA V0 - Fix case where `max_num_seqs` is between cudagraph capture sizes by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/15308
* [Model] Support Tele-FLM Model by @atone in https://github.com/vllm-project/vllm/pull/15023
* [V1] Add `disable-any-whitespace` option support for xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/15316
* [BugFix][Typing] Fix Imprecise Type Annotations by @WrRan in https://github.com/vllm-project/vllm/pull/15208
* Remove openvino support in favor of external plugin by @russellb in https://github.com/vllm-project/vllm/pull/15339
* [doc] Add back previous news by @heheda12345 in https://github.com/vllm-project/vllm/pull/15331
* Fix v1 supported oracle for worker-cls and worker-extension-cls by @hijkzzz in https://github.com/vllm-project/vllm/pull/15324
* [V1][Usage] Refactor speculative decoding configuration and tests by @ShangmingCai in https://github.com/vllm-project/vllm/pull/14434
* [ci/build] update torch nightly version for GH200 by @youkaichao in https://github.com/vllm-project/vllm/pull/15135
* [ci/build] fix broken tests in LLM.collective_rpc by @youkaichao in https://github.com/vllm-project/vllm/pull/15350
* [Misc] Add tuned R1 w8a8 and MoE configs for NVIDIA L20 by @DefTruth in https://github.com/vllm-project/vllm/pull/15322
* [Bugfix] fix torch.compiled cache hash error by @DefTruth in https://github.com/vllm-project/vllm/pull/14953
* [V1][Spec Decode] Respect prompt_lookup_max by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15348
* [V1][Spec Decode] Use better defaults for N-gram by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15358
* [Frontend] Support tool calling and reasoning parser by @WangErXiao in https://github.com/vllm-project/vllm/pull/14511
* [Misc][Doc] Add note regarding loading `generation_config` by default by @ywang96 in https://github.com/vllm-project/vllm/pull/15281
* [V1] Enable V1 Fp8 cache for FA3 in the oracle by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/15191
* [Fix] [torch.compile] Improve UUID system for custom passes by @ProExpertProg in https://github.com/vllm-project/vllm/pull/15249
* Fix non-contiguous input passed to Marlin kernel by @Qubitium in https://github.com/vllm-project/vllm/pull/15319
* [Misc] Upgrade BNB version by @jeejeelee in https://github.com/vllm-project/vllm/pull/15183
* [Misc] Remove ignore_reinit_error for ray.init() by @ruisearch42 in https://github.com/vllm-project/vllm/pull/15373
* [Bugfix][V1] Avoid importing PreTrainedModel by @HollowMan6 in https://github.com/vllm-project/vllm/pull/15366
* [Misc] Update guided decoding logs to debug by @sfbemerk in https://github.com/vllm-project/vllm/pull/15310
* Revert "[CI/Build] Use uv python for docker rather than ppa:deadsnakess/ppa (#13569)" by @simon-mo in https://github.com/vllm-project/vllm/pull/15377
* [Kernel] allow non-contiguous input for marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/14658
* Fix zmq IPv6 URL format error by @russellb in https://github.com/vllm-project/vllm/pull/15341
* [Bugfix] Fix chat template loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15143
* [distributed] fix dp group by @youkaichao in https://github.com/vllm-project/vllm/pull/15355
* [Core] Integrate `fastsafetensors` loader for loading model weights by @manish-sethi in https://github.com/vllm-project/vllm/pull/10647
* [Core] Don't force uppercase for VLLM_LOGGING_LEVEL by @russellb in https://github.com/vllm-project/vllm/pull/15306
* [V1][Minor]   fix comments by @Chen-0210 in https://github.com/vllm-project/vllm/pull/15392
* [MISC] Refine no available block debug msg by @yiliu30 in https://github.com/vllm-project/vllm/pull/15076
* [V1] Aggregate chunked prompt logprobs in model runner by @njhill in https://github.com/vllm-project/vllm/pull/14875
* [Hardware][Gaudi][Feature] Enable Dynamic MoE for Mixtral by @zhenwei-intel in https://github.com/vllm-project/vllm/pull/12303
* [DOC] Add Kubernetes deployment guide with CPUs by @terrytangyuan in https://github.com/vllm-project/vllm/pull/14865
* [Doc] Update docs on handling OOM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15357
* [V1][Perf] Simpler request output queues by @njhill in https://github.com/vllm-project/vllm/pull/15156
* [BugFix][V1] Quick fix for min_tokens with multiple EOS by @njhill in https://github.com/vllm-project/vllm/pull/15407
* [Hardware][TPU] Skip failed compilation test by @lsy323 in https://github.com/vllm-project/vllm/pull/15421
* [Build] Cython compilation support fix by @gshtras in https://github.com/vllm-project/vllm/pull/14296
* [ROCm][Kernel] MoE weights padding by @gshtras in https://github.com/vllm-project/vllm/pull/14454
* [V1][Spec Decode] Enable spec decode for top-p & top-k sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15063
* [Minor][Spec Decode] Remove compiled_softmax by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15416
* Add pipeline parallel support to `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/12832
* [Misc] Remove LoRA log by @jeejeelee in https://github.com/vllm-project/vllm/pull/15388
* Revert "Fix non-contiguous input passed to Marlin kernel (#15319)" by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/15398
* [Bugfix] Fixed the issue of not being able to input video and image simultaneously by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/15387
* [V1] guidance backend for structured output + `auto` fallback mode by @russellb in https://github.com/vllm-project/vllm/pull/14779
* [V1][Spec Decode] Update target_logits in place for rejection sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15427

## New Contributors
* @maobaolong made their first contribution in https://github.com/vllm-project/vllm/pull/15003
* @jovsa made their first contribution in https://github.com/vllm-project/vllm/pull/14713
* @mickaelseznec made their first contribution in https://github.com/vllm-project/vllm/pull/14570
* @mritterfigma made their first contribution in https://github.com/vllm-project/vllm/pull/14917
* @billishyahao made their first contribution in https://github.com/vllm-project/vllm/pull/14824
* @linktohack made their first contribution in https://github.com/vllm-project/vllm/pull/15129
* @vermouth1992 made their first contribution in https://github.com/vllm-project/vllm/pull/15172
* @JasonJ2021 made their first contribution in https://github.com/vllm-project/vllm/pull/15043
* @hyeygit made their first contribution in https://github.com/vllm-project/vllm/pull/15242
* @wayzeng made their first contribution in https://github.com/vllm-project/vllm/pull/14855
* @lhtin made their first contribution in https://github.com/vllm-project/vllm/pull/15280
* @wwl2755 made their first contribution in https://github.com/vllm-project/vllm/pull/14804
* @atone made their first contribution in https://github.com/vllm-project/vllm/pull/15023
* @hijkzzz made their first contribution in https://github.com/vllm-project/vllm/pull/15324
* @sfbemerk made their first contribution in https://github.com/vllm-project/vllm/pull/15310
* @manish-sethi made their first contribution in https://github.com/vllm-project/vllm/pull/10647
* @yiliu30 made their first contribution in https://github.com/vllm-project/vllm/pull/15076

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.1...v0.8.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.2)

---

## v0.8.1: v0.8.1
**Published:** 2025-03-19

This release contains important bug fixes for v0.8.0. We highly recommend upgrading! 

* V1 Fixes
	* Ensure using int64 for sampled token ids (#15065)
	* Fix long dtype in topk sampling (#15049)
	* Refactor Structured Output for multiple backends (#14694)
	* Fix size calculation of processing cache (#15114)
	* Optimize Rejection Sampler with Triton Kernels (#14930)
	* Fix oracle for device checking (#15104)

* TPU
	* Fix chunked prefill with padding (#15037)
	* Enhanced CI/CD (#15054, 14974)

* Model
	* Re-enable Gemma3 for V1 (#14980)
	* Embedding model support LoRA (#14935)
	* Pixtral: Remove layer instantiation duplication (#15053)


## What's Changed
* [Bugfix] Fix interface for Olmo2 on V1 by @ywang96 in https://github.com/vllm-project/vllm/pull/14976
* [CI/Build] Use `AutoModelForImageTextToText` to load image models in tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14945
* [V1] Guard Against Main Thread Usage by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14972
* [V1] TPU - Fix CI/CD runner for V1 and remove V0 tests by @alexm-redhat in https://github.com/vllm-project/vllm/pull/14974
* [Bugfix] Fix bnb quantization for models with both HF-format and Mistral-format weights by @tristanleclercq in https://github.com/vllm-project/vllm/pull/14950
* [Neuron] trim attention kernel tests to fit trn1.2x instance by @liangfu in https://github.com/vllm-project/vllm/pull/14988
* [Doc][V1] Fix V1 APC doc by @shen-shanshan in https://github.com/vllm-project/vllm/pull/14920
* [Kernels] LoRA - Retire SGMV and BGMV Kernels by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/14685
* [Mistral-Small 3.1] Update docs and tests by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/14977
* [Misc] Embedding model support LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/14935
* [Bugfix] torchrun compatibility by @hiyouga in https://github.com/vllm-project/vllm/pull/14899
* [Bugfix][Frontend] Fix validation of `logprobs` in `ChatCompletionRequest` by @schoennenbeck in https://github.com/vllm-project/vllm/pull/14352
* [Misc][Docs] fix the comments of KV_T and CACHE_T in CALL_RESHAPE_AND_CACHE_XX macros by @yangsijia-serena in https://github.com/vllm-project/vllm/pull/14347
* [Bugfix] Loosen type check to avoid errors in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15021
* [Bugfix] Register serializers for V0 MQ Engine by @simon-mo in https://github.com/vllm-project/vllm/pull/15009
* [TPU][V1][Bugfix] Fix chunked prefill with padding by @NickLucche in https://github.com/vllm-project/vllm/pull/15037
* MI325 configs, fused_moe_kernel bugfix by @ekuznetsov139 in https://github.com/vllm-project/vllm/pull/14987
* [MODEL] Add support for Zamba2 models by @yury-tokpanov in https://github.com/vllm-project/vllm/pull/13185
* [Bugfix] Fix broken CPU quantization due to triton import by @Isotr0py in https://github.com/vllm-project/vllm/pull/15038
* [Bugfix] Fix LoRA extra vocab size by @jeejeelee in https://github.com/vllm-project/vllm/pull/15047
* [V1] Refactor Structured Output for multiple backends by @russellb in https://github.com/vllm-project/vllm/pull/14694
* [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14930
* [V1] TPU - CI/CD use smaller model by @alexm-redhat in https://github.com/vllm-project/vllm/pull/15054
* fix long dtype in topk sampling by @chujiezheng in https://github.com/vllm-project/vllm/pull/15049
* [Doc] Minor v1_user_guide update by @JenZhao in https://github.com/vllm-project/vllm/pull/15064
* [Misc][V1] Skip device checking if not available by @comaniac in https://github.com/vllm-project/vllm/pull/15061
* [Model] Pixtral: Remove layer instantiation duplication by @juliendenize in https://github.com/vllm-project/vllm/pull/15053
* [Model] Remove duplicated message check in Mistral chat completion request by @b8zhong in https://github.com/vllm-project/vllm/pull/15069
* [Core] Update dtype detection and defaults by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14858
* [V1] Ensure using int64 for sampled token ids by @WoosukKwon in https://github.com/vllm-project/vllm/pull/15065
* [Bugfix] Re-enable Gemma3 for V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14980
* [CI][Intel GPU] update XPU dockerfile and CI script by @jikunshang in https://github.com/vllm-project/vllm/pull/15109
* [V1][Bugfix] Fix oracle for device checking by @ywang96 in https://github.com/vllm-project/vllm/pull/15104
* [Misc] Avoid unnecessary HF `do_rescale` warning when passing dummy data by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15107
* [Bugfix] Fix size calculation of processing cache by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/15114
* [Doc] Update tip info on using latest transformers when creating a custom Dockerfile  by @MarcCote in https://github.com/vllm-project/vllm/pull/15070
* [Misc][Benchmark] Add support for different `tokenizer_mode` by @aarnphm in https://github.com/vllm-project/vllm/pull/15040
* [Bugfix] Adjust mllama to regional compilation by @jkaniecki in https://github.com/vllm-project/vllm/pull/15112
* [Doc] Update the "the first vLLM China Meetup" slides link to point to the first page by @imkero in https://github.com/vllm-project/vllm/pull/15134
* [Frontend] Remove custom_cache_manager by @fulvius31 in https://github.com/vllm-project/vllm/pull/13791
* [V1] Minor V1 async engine test refactor by @andoorve in https://github.com/vllm-project/vllm/pull/15075

## New Contributors
* @tristanleclercq made their first contribution in https://github.com/vllm-project/vllm/pull/14950
* @hiyouga made their first contribution in https://github.com/vllm-project/vllm/pull/14899
* @ekuznetsov139 made their first contribution in https://github.com/vllm-project/vllm/pull/14987
* @yury-tokpanov made their first contribution in https://github.com/vllm-project/vllm/pull/13185
* @juliendenize made their first contribution in https://github.com/vllm-project/vllm/pull/15053
* @MarcCote made their first contribution in https://github.com/vllm-project/vllm/pull/15070
* @jkaniecki made their first contribution in https://github.com/vllm-project/vllm/pull/15112
* @fulvius31 made their first contribution in https://github.com/vllm-project/vllm/pull/13791

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.0...v0.8.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.1)

---

## v0.8.0: v0.8.0
**Published:** 2025-03-18

v0.8.0 featured 523 commits from 166 total contributors (68 new contributors)! 

## Highlights

### V1

We have now enabled V1 engine by default (#13726) for supported use cases. Please refer to [V1 user guide](https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html) for more detail. We expect better performance for supported scenarios. If you'd like to disable V1 mode, please specify the environment variable `VLLM_USE_V1=0`, and send us a GitHub issue sharing the reason!

* Support variety of sampling parameters (#13376, #10980, #13210, #13774)
* Compatability prompt logprobs + prefix caching (#13949), sliding window + prefix caching (#13069)
* Stability fixes (#14380, #14379, #13298)
* Pluggable scheduler (#14466)
* `SupportsV0Only` protocol for model definitions (#13959)
* Metrics enhancements (#13299, #13504, #14695, #14082)
* V1 user guide (#13991) and design doc (#12745)
* Support for Structured Outputs (#12388, #14590, #14625, #14630, #14851)
* Support for LoRA (#13705, #13096, #14626)
* Enhance Pipeline Parallelism (#14585, #14643)
* Ngram speculative decoding (#13729, #13933)

### DeepSeek Improvements
We observe state of the art performance with vLLM running DeepSeek model on latest version of vLLM:
* MLA Enhancements: 
	* FlashMLA integration (#13747, #13867, #14451)
	* MLA support for V1 (#13789, #14253, #14384, #14540, #14921)
	* MLA with chunked prefill (#12639)
	* Holistic memory and performance optimization (#14769, #14770,#14842)
	* Support MLA for CompressedTensorsWNA16 (#13725)
* Distributed Expert Parallelism (EP) and Data Parallelism (DP)
	* EP Support for DeepSeek Models (#12583)
	* Add enable_expert_parallel arg (#14305)
	* EP/TP MoE + DP Attention (#13931)
	* Set up data parallel communication (#13591)
* MTP: Expand DeepSeek MTP code to support k > n_predict (#13626)
* Pipeline Parallelism:
    * DeepSeek V2/V3/R1 only place `lm_head` on last pp rank (#13833)
	* Improve pipeline partitioning (#13839)
* GEMM
	* Add streamK for block-quantized CUTLASS kernels (#12978)
	* Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM (#13917)
	* Add more tuned configs for H20 and others (#14877)

### New Models
* Gemma 3 (#14660)
  * **Note**: You have to install transformers from main branch (`pip install git+https://github.com/huggingface/transformers.git`) to use this model. Also, there may be numerical instabilities for `float16`/`half` dtype. Please use `bfloat16` (preferred by HF) or `float32` dtype.
* Mistral Small 3.1 (#14957)
* Phi-4-multimodal-instruct (#14119)
* Grok1 (#13795)
* QwQ-32B and toll calling (#14479, #14478)
* Zamba2 (#13185)

### NVIDIA Blackwell
* Support nvfp4 cutlass gemm (#13571)
* Add cutlass support for blackwell fp8 gemm (#13798)
* Update the flash attn tag to support Blackwell (#14244)
* Add ModelOpt FP4 Checkpoint Support (#12520)

### Breaking Changes				
* The default value of `seed` is now `None` to align with PyTorch and Hugging Face. Please explicitly set seed for reproduciblity. (#14274)
* The `kv_cache` and `attn_metadata` arguments for model's forward method has been removed; as the attention backend has access to these value via `forward_context`. (#13887)
* vLLM will now default `generation_config` from model for chat template, sampling parameters such as temperature, etc. (#12622)
* Several request time metrics (`vllm:time_in_queue_requests`, `vllm:model_forward_time_milliseconds`, `vllm:model_execute_time_milliseconds`) has been deprecated and subject to removal (#14135)

### Updates				
* Update to PyTorch 2.6.0 (#12721, #13860)
* Update to Python 3.9 typing (#14492, #13971)
* Update to CUDA 12.4 as default for release and nightly wheels (#12098)
* Update to Ray 2.43 (#13994)
* Upgrade aiohttp to incldue CVE fix (#14840)
* Upgrade jinja2 to get 3 moderate CVE fixes (#14839)

## Features

### Frontend API

* API Server
	* Support `return_tokens_as_token_id` as a request param (#14066)
	* Support Image Emedding as input (#13955)
	* New /load endpoint for load statistics (#13950)
	* New API endpoint `/is_sleeping` (#14312)
	* Enables /score endpoint for embedding models (#12846)
	* Enable streaming for Transcription API (#13301)
	* Make model param optional in request (#13568)
	* Support SSL Key Rotation in HTTP Server (#13495)
* Reasoning
	* Support reasoning output (#12955)
	* Support outlines engine with reasoning outputs (#14114)
	* Update reasoning with stream example to use OpenAI library (#14077)
* CLI
	* Ensure out-of-tree quantization method recognize by cli args (#14328)
	* Add `vllm bench` CLI (#13993)
* Make LLM API compatible for torchrun launcher (#13642)

### Disaggregated Serving
* Support KV cache offloading and disagg prefill with LMCache connector (#12953)
* Support chunked prefill for LMCache connector (#14505)

### LoRA				
* Add LoRA support for TransformersModel (#13770)
* Make the deviceprofilerinclude LoRA memory. (#14469)
* Gemma3ForConditionalGeneration supports LoRA (#14797)
* Retire SGMV and BGMV Kernels (#14685) (#14685)

### VLM				
* Generalized prompt updates for multi-modal processor (#13964)
* Deprecate legacy input mapper for OOT multimodal models (#13979)
* Refer code examples for common cases in dev multimodal processor (#14278)

### Quantization				
* BaiChuan SupportsQuant (#13710)
* BartModel SupportsQuant (#14699)
* Bamba SupportsQuant (#14698)
* Deepseek GGUF support (#13167)
* GGUF MoE kernel (#14613)
* Add GPTQAllSpark Quantization (#12931)
* Better performance of gptq marlin kernel when n is small (#14138)

### Structured Output				
* xgrammar: Expand list of unsupported jsonschema keywords (#13783)


### Hardware Support

AMD
* Faster Custom Paged Attention kernels (#12348)
* Improved performance for V1 Triton (ROCm) backend (#14152)
* Chunked prefill/paged attention in MLA on ROCm (#14316)
* Perf improvement for DSv3 on AMD GPUs (#13718)
* MoE fp8 block quant tuning support (#14068)

TPU				
* Integrate the new ragged paged attention kernel with vLLM v1 on TPU (#13379)
* Support start_profile/stop_profile in TPU worker (#13988)
* Add TPU v1 test (#14834)
* TPU multimodal model support for ragged attention (#14158)
* Add tensor parallel support via Ray (#13618)
* Enable prefix caching by default (#14773)

Neuron				
* Add Neuron device communicator for vLLM v1 (#14085)
* Add custom_ops for neuron backend (#13246)
* Add reshape_and_cache (#14391)
* Vectorize KV cache load in FlashPagedAttention to maximize DMA bandwidth (#13245)

CPU
* Upgrade CPU backend to torch-2.6 (#13381)
* Support FP8 KV cache in CPU Backend(#14741)

s390x
* Adding cpu inference with VXE ISA for s390x architecture (#12613)
* Add documentation for s390x cpu implementation (#14198)

Plugins				
* Remove cuda hard code in models and layers (#13658)
* Move use allgather to platform (#14010)

### Bugfix and Enhancements
* Illegal memory access for MoE On H20 (#13693)
* Fix FP16 overflow for DeepSeek V2 (#13232)
* Illegal Memory Access in the blockwise cutlass fp8 GEMMs (#14396)
* Pass all driver env vars to ray workers unless excluded (#14099)
* Use xgrammar shared context to avoid copy overhead for offline engine (#13837)
* Capture and log the time of loading weights (#13666)


### Developer Tooling
Benchmarks
* Consolidate performance benchmark datasets (#14036)
* Update benchmarks README (#14646)

CI and Build
* Add RELEASE.md (#13926)
* Use env var to control whether to use S3 bucket in CI (#13634)

### Documentation
* Add RLHF document (#14482)
* Add nsight guide to profiling docs (#14298)
* Add K8s deployment guide (#14084)
* Add developer documentation for `torch.compile` integration (#14437)




## What's Changed
* Update `pre-commit`'s `isort` version to remove warnings by @hmellor in https://github.com/vllm-project/vllm/pull/13614
* [V1][Minor] Print KV cache size in token counts by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13596
* fix neuron performance issue by @ajayvohra2005 in https://github.com/vllm-project/vllm/pull/13589
* [Frontend] Add backend-specific options for guided decoding by @joerunde in https://github.com/vllm-project/vllm/pull/13505
* [Bugfix] Fix max_num_batched_tokens for MLA by @mgoin in https://github.com/vllm-project/vllm/pull/13620
* [Neuron][Kernel] Vectorize KV cache load in FlashPagedAttention to maximize DMA bandwidth by @lingfanyu in https://github.com/vllm-project/vllm/pull/13245
* Add llmaz as another integration by @kerthcet in https://github.com/vllm-project/vllm/pull/13643
* [Misc] Adding script to setup ray for multi-node vllm deployments  by @Edwinhr716 in https://github.com/vllm-project/vllm/pull/12913
* [NVIDIA] Fix an issue to use current stream for the nvfp4 quant by @kaixih in https://github.com/vllm-project/vllm/pull/13632
* Use pre-commit to update `requirements-test.txt` by @hmellor in https://github.com/vllm-project/vllm/pull/13617
* [Bugfix] Add `mm_processor_kwargs` to chat-related protocols by @ywang96 in https://github.com/vllm-project/vllm/pull/13644
* [V1][Sampler] Avoid an operation during temperature application by @njhill in https://github.com/vllm-project/vllm/pull/13587
* Missing comment explaining VDR variable in GGUF kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/13290
* [FEATURE] Enables /score endpoint for embedding models by @gmarinho2 in https://github.com/vllm-project/vllm/pull/12846
* [ci] Fix metrics test model path by @khluu in https://github.com/vllm-project/vllm/pull/13635
* [Kernel]Add streamK for block-quantized CUTLASS kernels by @Hongbosherlock in https://github.com/vllm-project/vllm/pull/12978
* [Bugfix][CPU] Fix cpu all-reduce using native pytorch implementation  by @Isotr0py in https://github.com/vllm-project/vllm/pull/13586
* fix typo of grafana dashboard, with correct datasource by @johnzheng1975 in https://github.com/vllm-project/vllm/pull/13668
* [Attention] MLA with chunked prefill by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12639
* [Misc] Fix yapf linting tools etc not running on pre-commit by @Isotr0py in https://github.com/vllm-project/vllm/pull/13695
* docs: Add a note on full CI run in contributing guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13646
* [HTTP Server] Make model param optional in request by @youngkent in https://github.com/vllm-project/vllm/pull/13568
* [Bugfix][API Server] Fix invalid usage of 'ge' and 'le' in port validâ€¦ by @WangErXiao in https://github.com/vllm-project/vllm/pull/13672
* [Misc] Capture and log the time of loading weights by @waltforme in https://github.com/vllm-project/vllm/pull/13666
* [ROCM] fix native attention function call by @gongdao123 in https://github.com/vllm-project/vllm/pull/13650
* [Bugfix][Model] OLMo 2: split qkv correctly for GQA and MQA by @2015aroras in https://github.com/vllm-project/vllm/pull/13687
* [Misc] Bump compressed-tensors by @dsikka in https://github.com/vllm-project/vllm/pull/13619
* [Bugfix] Fix benchmark script bug: inaccurate stats for vllm backend when max_model_len < input_len + output_len by @WangErXiao in https://github.com/vllm-project/vllm/pull/13691
* [v1] Support allowed_token_ids in v1 Sampler by @houseroad in https://github.com/vllm-project/vllm/pull/13210
* [Bugfix] V1 Memory Profiling: V0 Sampler Integration without Rejection Sampler by @JenZhao in https://github.com/vllm-project/vllm/pull/13594
* Correction to TP logic for Mamba Mixer 2 when Num Groups not divisible by TP Size by @fabianlim in https://github.com/vllm-project/vllm/pull/13660
* [V1][Metrics] Support `vllm:cache_config_info` by @markmc in https://github.com/vllm-project/vllm/pull/13299
* [Metrics] Add `--show-hidden-metrics-for-version` CLI arg by @markmc in https://github.com/vllm-project/vllm/pull/13295
* [Misc] Reduce LoRA-related static variable by @jeejeelee in https://github.com/vllm-project/vllm/pull/13166
* [CI/Build] Fix pre-commit errors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13696
* [core] set up data parallel communication by @youkaichao in https://github.com/vllm-project/vllm/pull/13591
* [ci] fix linter by @youkaichao in https://github.com/vllm-project/vllm/pull/13701
* Support SSL Key Rotation in HTTP Server by @youngkent in https://github.com/vllm-project/vllm/pull/13495
* [NVIDIA] Support nvfp4 cutlass gemm by @kaixih in https://github.com/vllm-project/vllm/pull/13571
* [V1][Kernel] Refactor the prefix_prefill kernel so that the caller no longer has to pass in the context lengths by @SageMoore in https://github.com/vllm-project/vllm/pull/13095
* [ROCm] Apply FP8 weights padding to values not divisible by 512 bytes on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/13231
* [Doc] Dockerfile instructions for optional dependencies and dev transformers by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13699
* [Bugfix] Fix boolean conversion for OpenVINO env variable by @helena-intel in https://github.com/vllm-project/vllm/pull/13615
* [XPU]fix setuptools version for xpu by @yma11 in https://github.com/vllm-project/vllm/pull/13548
* [CI/Build] fix uv caching in Dockerfile by @dtrifiro in https://github.com/vllm-project/vllm/pull/13611
* [CI/Build] Fix pre-commit errors from #13571 by @ywang96 in https://github.com/vllm-project/vllm/pull/13709
* [BugFix] Minor: logger import in attention backend by @andylolu2 in https://github.com/vllm-project/vllm/pull/13706
* [ci] Use env var to control whether to use S3 bucket in CI by @khluu in https://github.com/vllm-project/vllm/pull/13634
* [Quant] BaiChuan SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/13710
* [LMM] Implement merged multimodal processor for whisper by @Isotr0py in https://github.com/vllm-project/vllm/pull/13278
* [Core][Distributed] Use IPC (domain socket) ZMQ socket for local comms by @njhill in https://github.com/vllm-project/vllm/pull/13688
* [Misc] Deprecate `--dataset` from `benchmark_serving.py` by @ywang96 in https://github.com/vllm-project/vllm/pull/13708
* [v1] torchrun compatibility by @youkaichao in https://github.com/vllm-project/vllm/pull/13642
* [V1][BugFix] Fix engine core client shutdown hangs by @njhill in https://github.com/vllm-project/vllm/pull/13298
* Fix some issues with benchmark data output by @huydhn in https://github.com/vllm-project/vllm/pull/13641
* [ci] Add logic to change model to S3 path only when S3 CI env var is on by @khluu in https://github.com/vllm-project/vllm/pull/13727
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/13721
* [model][refactor] remove cuda hard code in models and layers by @MengqingCao in https://github.com/vllm-project/vllm/pull/13658
* [Bugfix] fix(logging): add missing opening square bracket by @bufferoverflow in https://github.com/vllm-project/vllm/pull/13011
* [CI/Build] add python-json-logger to requirements-common by @bufferoverflow in https://github.com/vllm-project/vllm/pull/12842
* Expert Parallelism (EP) Support for DeepSeek Models by @cakeng in https://github.com/vllm-project/vllm/pull/12583
* [BugFix]  Illegal memory access for MoE On H20 by @Abatom in https://github.com/vllm-project/vllm/pull/13693
* [Misc][Docs] Raise error when flashinfer is not installed and `VLLM_ATTENTION_BACKEND` is set by @NickLucche in https://github.com/vllm-project/vllm/pull/12513
* [V1] V1 engine implements parallel sampling (AsyncLLM and LLMEngine) by @afeldman-nm in https://github.com/vllm-project/vllm/pull/10980
* Revert "[V1][Core] Fix memory issue with logits & sampling" by @ywang96 in https://github.com/vllm-project/vllm/pull/13775
* Fix precommit fail in fused_moe intermediate_cache2 chunking by @mgoin in https://github.com/vllm-project/vllm/pull/13772
* [Misc] Clean Up `EngineArgs.create_engine_config` by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13734
* [Misc][Chore] Clean Up `AsyncOutputProcessing` Logs by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13780
* Remove unused kwargs from model definitions by @hmellor in https://github.com/vllm-project/vllm/pull/13555
* [Doc] arg_utils.py: fixed a typo by @eli-b in https://github.com/vllm-project/vllm/pull/13785
* [Misc] set single whitespace between log sentences by @cjackal in https://github.com/vllm-project/vllm/pull/13771
* [Bugfix][Quantization] Fix FP8 + EP by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13784
* [Misc][Attention][Quantization] init property earlier by @wangxiyuan in https://github.com/vllm-project/vllm/pull/13733
* [V1][Metrics] Implement vllm:lora_requests_info metric by @markmc in https://github.com/vllm-project/vllm/pull/13504
* [Bugfix] Fix deepseek-v2 error: "missing 1 required positional argument: 'residual'" by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13802
* [Bugfix] Support MLA for CompressedTensorsWNA16 by @mgoin in https://github.com/vllm-project/vllm/pull/13725
* Fix CompressedTensorsWNA16MoE with grouped scales by @mgoin in https://github.com/vllm-project/vllm/pull/13769
* [Core] LoRA V1 - Add add/pin/list/remove_lora functions   by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/13705
* [Misc] Check that the model can be inspected upon registration by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13743
* [Core] xgrammar: Expand list of unsupported jsonschema keywords by @russellb in https://github.com/vllm-project/vllm/pull/13783
* [Bugfix] Modify modelscope api usage in transformer_utils by @shen-shanshan in https://github.com/vllm-project/vllm/pull/13807
* [misc] Clean up ray compiled graph type hints by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13731
* [Feature] Support KV cache offloading and disagg prefill with LMCache connector. by @YaoJiayi in https://github.com/vllm-project/vllm/pull/12953
* [ROCm][Quantization][Kernel] Using HIP FP8 header by @gshtras in https://github.com/vllm-project/vllm/pull/12593
* [CI/Build]  Fix V1 LoRA failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/13767
* [Misc]Clarify Error Handling for Non-existent Model Paths and HF Repo IDs by @Chen-0210 in https://github.com/vllm-project/vllm/pull/13724
* [Bugfix] Initialize attention bias on the same device as Query/Key/Value by @edwardzjl in https://github.com/vllm-project/vllm/pull/13468
* [Bugfix] Flush TunableOp results before worker processes are destroyed. by @naromero77amd in https://github.com/vllm-project/vllm/pull/13623
* [Bugfix] Fix deepseek-vl2 inference with more than 2 images by @Isotr0py in https://github.com/vllm-project/vllm/pull/13818
* Fix `/v1/audio/transcriptions ` Bad Request Error by @HermitSun in https://github.com/vllm-project/vllm/pull/13811
* [Bugfix] Revert inspection code in #13743 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13832
* Fix string parsing error by @Chen-0210 in https://github.com/vllm-project/vllm/pull/13825
* [Neuron] Add custom_ops for neuron backend by @liangfu in https://github.com/vllm-project/vllm/pull/13246
* Fix failing `MyGemma2Embedding` test by @hmellor in https://github.com/vllm-project/vllm/pull/13820
* [Model] Support Grok1 by @mgoin in https://github.com/vllm-project/vllm/pull/13795
* DeepSeek V2/V3/R1 only place `lm_head` on last pp rank by @hmellor in https://github.com/vllm-project/vllm/pull/13833
* [misc] Show driver IP info when Ray fails to allocate driver worker by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13858
* [V1][Spec Decode] Change Spec Decode Rejection Sampling API by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/13729
* [Misc]Code Cleanup by @noemotiovon in https://github.com/vllm-project/vllm/pull/13859
* [Kernel][Build/CI] Bump CUTLASS to 3.8 and add initializers for cutlass epilogues by @henrylhtsang in https://github.com/vllm-project/vllm/pull/13797
* Improve pipeline partitioning by @hmellor in https://github.com/vllm-project/vllm/pull/13839
* [Doc] fix the incorrect module path of tensorize_vllm_model by @tianyuzhou95 in https://github.com/vllm-project/vllm/pull/13863
* [ROCm] Disable chunked prefill/prefix caching when running MLA on non-cuda platforms by @SageMoore in https://github.com/vllm-project/vllm/pull/13844
* [v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine by @sethkimmel3 in https://github.com/vllm-project/vllm/pull/13837
* [Misc] Improve LoRA spelling by @jeejeelee in https://github.com/vllm-project/vllm/pull/13831
* [Misc] Fix input processing for Ultravox by @ywang96 in https://github.com/vllm-project/vllm/pull/13871
* [Bugfix] Add test example for Ultravox v0.5 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13890
* Add comments on accessing `kv_cache` and `attn_metadata` by @hmellor in https://github.com/vllm-project/vllm/pull/13887
* [Bugfix] Handle None parameters in Mistral function calls. by @fgreinacher in https://github.com/vllm-project/vllm/pull/13786
* [Misc]: Add support for goodput on guided benchmarking + TPOT calculation refactor by @b8zhong in https://github.com/vllm-project/vllm/pull/13736
* [Bugfix] Do not crash V0 engine on input errors by @joerunde in https://github.com/vllm-project/vllm/pull/13101
* [Bugfix] Update expected token counts for Ultravox tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13895
* [TPU] use torch2.6 with whl package by @Chenyaaang in https://github.com/vllm-project/vllm/pull/13860
* [Misc] fixed qwen_vl_utils parameter error by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13906
* [Bugfix] Backend option to disable xgrammar any_whitespace by @wallashss in https://github.com/vllm-project/vllm/pull/12744
* [BugFix] Make FP8 Linear compatible with torch.compile by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13918
* [Kernel] FlashMLA integration by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13747
* [ROCm][Quantization][Kernel] Use FP8 FNUZ when OCP flag is 0 or undefined by @HollowMan6 in https://github.com/vllm-project/vllm/pull/13851
* Use CUDA 12.4 as default for release and nightly wheels by @mgoin in https://github.com/vllm-project/vllm/pull/12098
* [misc] Rename Ray ADAG to Compiled Graph by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13928
* [ROCm][V1] Update reshape_and_cache to properly work with CUDA graph padding by @SageMoore in https://github.com/vllm-project/vllm/pull/13922
* [V1][Metrics] Handle preemptions by @markmc in https://github.com/vllm-project/vllm/pull/13169
* [CI/Build] Add examples/ directory to be labelled by `mergify` by @b8zhong in https://github.com/vllm-project/vllm/pull/13944
* [Misc] fixed 'required' is an invalid argument for positionals by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13948
* [PP] Correct cache size check by @zhengy001 in https://github.com/vllm-project/vllm/pull/13873
* Fix test_block_fp8.py test for MoE by @mgoin in https://github.com/vllm-project/vllm/pull/13915
* [VLM] Support multimodal inputs for Florence-2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/13320
* [Model] Deepseek GGUF support  by @SzymonOzog in https://github.com/vllm-project/vllm/pull/13167
* Update quickstart.md by @observerw in https://github.com/vllm-project/vllm/pull/13958
* Deduplicate `.pre-commit-config.yaml`'s `exclude` by @hmellor in https://github.com/vllm-project/vllm/pull/13967
* [bugfix] Fix profiling for RayDistributedExecutor by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13945
* Update LMFE version to v0.10.11 to support new versions of transformeâ€¦ by @noamgat in https://github.com/vllm-project/vllm/pull/13930
* [Bugfix] Fix qwen2.5-vl overflow issue by @Isotr0py in https://github.com/vllm-project/vllm/pull/13968
* [VLM] Generalized prompt updates for multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13964
* [Attention] MLA support for V1 by @chenyang78 in https://github.com/vllm-project/vllm/pull/13789
* Bump azure/setup-helm from 4.2.0 to 4.3.0 by @dependabot in https://github.com/vllm-project/vllm/pull/13742
* [VLM] Deprecate legacy input mapper for OOT multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13979
* [ROCm] Fix the Kernels, Core, and Prefix Caching AMD CI groups by @SageMoore in https://github.com/vllm-project/vllm/pull/13970
* [V1][Minor] Minor cleanup for GPU Model Runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13983
* [core] Perf improvement for DSv3 on AMD GPUs by @qli88 in https://github.com/vllm-project/vllm/pull/13718
* [Attention] Flash MLA for V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13867
* [Model][Speculative Decoding] Expand DeepSeek MTP code to support k > n_predict by @benchislett in https://github.com/vllm-project/vllm/pull/13626
* [Misc] Print FusedMoE detail info by @jeejeelee in https://github.com/vllm-project/vllm/pull/13974
* [V1]`SupportsV0Only` protocol for model definitions by @ywang96 in https://github.com/vllm-project/vllm/pull/13959
* [Bugfix] Check that number of images matches number of <|image|> tokens with mllama by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/13911
* [Doc] Move multimodal Embedding API example to Online Serving page by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14017
* [Bugfix][Disaggregated] patch the inflight batching on the decode node in SimpleConnector to avoid hangs in SimpleBuffer (nccl based) by @hasB4K in https://github.com/vllm-project/vllm/pull/13987
* Use smaller embedding model when not testing model specifically by @hmellor in https://github.com/vllm-project/vllm/pull/13891
* [Hardware][Intel-Gaudi] Regional compilation support by @Kacper-Pietkun in https://github.com/vllm-project/vllm/pull/13213
* [V1][Minor] Restore V1 compatibility with LLMEngine class by @Ryp in https://github.com/vllm-project/vllm/pull/13090
* Update AutoAWQ docs by @hmellor in https://github.com/vllm-project/vllm/pull/14042
* [Bugfix] Fix MoeWNA16Method activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/14024
* [VLM][Bugfix] Enable specifying prompt target via index by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14038
* [Bugfix] Initialize attention bias on the same device as Query/Key/Value for QwenVL Series by @LouieYang in https://github.com/vllm-project/vllm/pull/14031
* [Doc] Fix ROCm documentation by @b8zhong in https://github.com/vllm-project/vllm/pull/14041
* Fix entrypoint tests for embedding models by @hmellor in https://github.com/vllm-project/vllm/pull/14052
* [V1][TPU] Integrate the new ragged paged attention kernel with vLLM v1 on TPU by @vanbasten23 in https://github.com/vllm-project/vllm/pull/13379
* [v1] Cleanup the BlockTable in InputBatch by @heheda12345 in https://github.com/vllm-project/vllm/pull/13977
* Add RELEASE.md by @atalman in https://github.com/vllm-project/vllm/pull/13926
* [v1] Move block pool operations to a separate class by @heheda12345 in https://github.com/vllm-project/vllm/pull/13973
* [core] Bump ray to 2.43 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13994
* [torch.compile] Fix RMSNorm + quant fusion in the non-cutlass-fp8 case, rename RedundantReshapesPass to NoopEliminationPass by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10902
* [Docs] Add `pipeline_parallel_size` to optimization docs by @b8zhong in https://github.com/vllm-project/vllm/pull/14059
* [Bugfix] Add file lock for ModelScope download by @jeejeelee in https://github.com/vllm-project/vllm/pull/14060
* [Misc][Kernel]: Add GPTQAllSpark Quantization by @wyajieha in https://github.com/vllm-project/vllm/pull/12931
* [Bugfix][V1][Minor] Fix shutting_down flag checking in V1 MultiprocExecutor by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/14053
* [Documentation] Add more deployment guide for Kubernetes deployment by @KuntaiDu in https://github.com/vllm-project/vllm/pull/13841
* [Doc] Consolidate `whisper` and `florence2` examples by @Isotr0py in https://github.com/vllm-project/vllm/pull/14050
* [V1][Minor] Do not print attn backend twice by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13985
* [ROCm][V1][Bugfix] Add get_builder_cls method to the ROCmAttentionBackend class by @SageMoore in https://github.com/vllm-project/vllm/pull/14065
* [v1][Bugfix] Only cache blocks that are not in the prefix cache by @heheda12345 in https://github.com/vllm-project/vllm/pull/14073
* [v1] Add `__repr__` to KVCacheBlock to avoid recursive print by @heheda12345 in https://github.com/vllm-project/vllm/pull/14081
* [Model] Add LoRA support for TransformersModel by @jeejeelee in https://github.com/vllm-project/vllm/pull/13770
* [Misc] Accurately capture the time of loading weights by @waltforme in https://github.com/vllm-project/vllm/pull/14063
* [Doc] Source building add clone step by @qux-bbb in https://github.com/vllm-project/vllm/pull/14086
* [v0][structured output] Support reasoning output by @gaocegege in https://github.com/vllm-project/vllm/pull/12955
* Update deprecated Python 3.8 typing by @hmellor in https://github.com/vllm-project/vllm/pull/13971
* [Bugfix] Explicitly include "omp.h" for MacOS to avoid installation failure by @realShengYao in https://github.com/vllm-project/vllm/pull/14051
* [Misc] typo find in deepseek_v2  by @noooop in https://github.com/vllm-project/vllm/pull/14106
* [Misc][Platform] Move use allgather to platform by @MengqingCao in https://github.com/vllm-project/vllm/pull/14010
* [Build] Make sure local main branch is synced when VLLM_USE_PRECOMPILED=1 by @comaniac in https://github.com/vllm-project/vllm/pull/13921
* [V1] Refactor parallel sampling support by @markmc in https://github.com/vllm-project/vllm/pull/13774
* Improve the docs for `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/14147
* [ROCm] Faster Custom Paged Attention kernels by @tjtanaa in https://github.com/vllm-project/vllm/pull/12348
* Fix `head_dim` not existing in all model configs (Transformers backend) by @hmellor in https://github.com/vllm-project/vllm/pull/14141
* [V0][Metrics] Remove unimplemented `vllm:tokens_total` by @markmc in https://github.com/vllm-project/vllm/pull/14134
* [V0][Metrics] Deprecate some KV/prefix cache metrics by @markmc in https://github.com/vllm-project/vllm/pull/14136
* [V1] Simplify stats logging by @njhill in https://github.com/vllm-project/vllm/pull/14082
* [WIP][[V1][Metrics] Implement max_num_generation_tokens,  request_params_n, and request_params_max_tokens metrics by @markmc in https://github.com/vllm-project/vllm/pull/14055
* [Bugfix] Allow shared_experts skip quantization for DeepSeekV2/V3 by @mgoin in https://github.com/vllm-project/vllm/pull/14100
* [Kernel] Optimize moe intermediate_cache usage by @mgoin in https://github.com/vllm-project/vllm/pull/13625
* [Docs] Add GPTQModel by @Qubitium in https://github.com/vllm-project/vllm/pull/14056
* [v1] Add comments to the new ragged paged attention Pallas kernel by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14155
* [Model] Add support for GraniteMoeShared models by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/13313
* [core] moe fp8 block quant tuning support by @divakar-amd in https://github.com/vllm-project/vllm/pull/14068
* [Misc] Remove lru_cache in NvmlCudaPlatform by @comaniac in https://github.com/vllm-project/vllm/pull/14156
* [core] Pass all driver env vars to ray workers unless excluded by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14099
* Use math.prod instead of np.prod for trivial ops by @zhanwenchen in https://github.com/vllm-project/vllm/pull/14142
* Fix benchmark_moe.py tuning for CUDA devices by @mgoin in https://github.com/vllm-project/vllm/pull/14164
* [platform] add debug logging during inferring the device type by @youkaichao in https://github.com/vllm-project/vllm/pull/14195
* [sleep mode] error out with expandable_segments by @youkaichao in https://github.com/vllm-project/vllm/pull/14189
* [doc] add "Failed to infer device type" to faq by @youkaichao in https://github.com/vllm-project/vllm/pull/14200
* [Bugfix] Restrict MacOS CPU detection by @mgoin in https://github.com/vllm-project/vllm/pull/14210
* [V1][BugFix] Fix remaining sync engine client shutdown errors/hangs by @njhill in https://github.com/vllm-project/vllm/pull/13869
* [V0][Metrics] Deprecate some questionable request time metrics by @markmc in https://github.com/vllm-project/vllm/pull/14135
* [V1][Molmo] Fix get_multimodal_embeddings() in molmo.py by @lk-chen in https://github.com/vllm-project/vllm/pull/14161
* add cutlass support for blackwell fp8 gemm by @kushanam in https://github.com/vllm-project/vllm/pull/13798
* [TPU][Profiler] Support start_profile/stop_profile in TPU worker by @lsy323 in https://github.com/vllm-project/vllm/pull/13988
* Fix performance when `--generation-config` is not `None` by @hmellor in https://github.com/vllm-project/vllm/pull/14223
* [Frontend] Do `prompt_logprobs` clamping for chat as well as completions by @hmellor in https://github.com/vllm-project/vllm/pull/14225
* [Docs] Update Dockerfile dependency image by @mgoin in https://github.com/vllm-project/vllm/pull/14215
* [v1][Metrics] Add design doc by @markmc in https://github.com/vllm-project/vllm/pull/12745
* Serialize using safetensors for KV caches by @KuntaiDu in https://github.com/vllm-project/vllm/pull/14228
* Clean up unused padding_idx variables across many model definitions by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13240
* [ROCm] Disable a few more kernel tests that are broken on ROCm by @SageMoore in https://github.com/vllm-project/vllm/pull/14145
* [V1][TPU] TPU multimodal model support for ragged attention by @mgoin in https://github.com/vllm-project/vllm/pull/14158
* [misc] announce china meetup by @youkaichao in https://github.com/vllm-project/vllm/pull/14248
* Moved numba from common requirements to cuda/rocm specific requirements by @npanpaliya in https://github.com/vllm-project/vllm/pull/14199
* Disable GPTQ AllSpark kernels for CUDA Compiler < 12.0 by @mgoin in https://github.com/vllm-project/vllm/pull/14157
* [Bugfix] Fix gptq_marlin for deepseek-v3 by @rainkert in https://github.com/vllm-project/vllm/pull/13750
* [V1][Bugfix] Do not reset prefix caching metrics by @comaniac in https://github.com/vllm-project/vllm/pull/14235
* [Model] New model support for Phi-4-multimodal-instruct by @congcongchen123 in https://github.com/vllm-project/vllm/pull/14119
* [V1] EP/TP MoE + DP Attention by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13931
* [platforms] improve rocm debugging info by @youkaichao in https://github.com/vllm-project/vllm/pull/14257
* Temporarily disable test_awq_gemm_opcheck by @mgoin in https://github.com/vllm-project/vllm/pull/14251
* [Frontend] Allow return_tokens_as_token_ids to be passed as a request param by @benchislett in https://github.com/vllm-project/vllm/pull/14066
* [Misc][V1] Avoid using `envs.VLLM_USE_V1` in mm processing by @ywang96 in https://github.com/vllm-project/vllm/pull/14256
* [Bugfix][V1] Fix allowed_token_ids for v1 Sampler by @houseroad in https://github.com/vllm-project/vllm/pull/14169
* [Doc] Update nginx guide: remove privileged from vllm container run and add target GPU ID by @iacolippo in https://github.com/vllm-project/vllm/pull/14217
* [Doc] [3/N] Refer code examples for common cases in dev multimodal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14278
* Small update for external_launcher backend docs by @zhe-thoughts in https://github.com/vllm-project/vllm/pull/14288
* [V1][Frontend] Add Testing For V1 Runtime Parameters by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14159
* [LoRA] Remove linear hack outside transformers backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/14177
* [Misc] Add Qwen2MoeForCausalLM moe tuning support  by @jeejeelee in https://github.com/vllm-project/vllm/pull/14276
* [Doc] Fixed typo in prefix_caching.md by @DaividFrank in https://github.com/vllm-project/vllm/pull/14293
* [Bugfix] Fix broken vision language example by @Isotr0py in https://github.com/vllm-project/vllm/pull/14292
* [Docs] Add Meta Slides by @simon-mo in https://github.com/vllm-project/vllm/pull/14297
* [V1][Minor] Remove obsolete FIXME comment by @njhill in https://github.com/vllm-project/vllm/pull/14304
* Deprecate `best_of` Sampling Parameter in anticipation for vLLM V1 by @vincent-4 in https://github.com/vllm-project/vllm/pull/13997
* [V1][BugFix] Fix for mixed top_k batch by @njhill in https://github.com/vllm-project/vllm/pull/14301
* [misc] Add FlashMLA as a new option of VLLM_ATTENTION_BACKEND env by @yangsijia-serena in https://github.com/vllm-project/vllm/pull/14267
* [V1][Easy] Add empty allowed_token_ids in the v1 sampler test by @houseroad in https://github.com/vllm-project/vllm/pull/14308
* [Bugfix] Fix DeepSeek MTP crash when using TP1ModelRunner with CUDA graph due to shape mismatch by @pyc96 in https://github.com/vllm-project/vllm/pull/14237
* [Bugfix] Remove num_tokens_across_dp by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14302
* [BugFix] Fix prefix caching V0 MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14255
* [CI/Build] Use spawn multiprocessing mode for V1 test pipeline by @russellb in https://github.com/vllm-project/vllm/pull/14243
* Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM by @mgoin in https://github.com/vllm-project/vllm/pull/13917
* [Build] Add UV_HTTP_TIMEOUT to avoid timeout during installation by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13850
* [BugFix] MLA + V1, illegal memory access and accuracy issues by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14253
* [misc] Mention `ray list nodes` command to troubleshoot ray issues by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14318
* [Bugfix][Structured Output] Support outlines engine with reasoning outputs for DeepSeek R1 by @gaocegege in https://github.com/vllm-project/vllm/pull/14114
* [V1] LoRA - Enable more V1 tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/14315
* [Bugfix][CI] ALiBi test case in xformers multi_query_kv_attention by @NickLucche in https://github.com/vllm-project/vllm/pull/11301
* [Hardware] Update the flash attn tag to support Blackwell by @pavanimajety in https://github.com/vllm-project/vllm/pull/14244
* [Model] Update Paligemma multimodal processing with PromptUpdate  by @kylehh in https://github.com/vllm-project/vllm/pull/14015
* [VLM] Support Pixtral-HF on V1 by @lk-chen in https://github.com/vllm-project/vllm/pull/14275
* [Core] Optimizing cross-attention `QKVParallelLinear` computation by @NickLucche in https://github.com/vllm-project/vllm/pull/12325
* [Frontend][Docs] Transcription API streaming by @NickLucche in https://github.com/vllm-project/vllm/pull/13301
* [Doc] Update reasoning with stream example to use OpenAI library by @liuyanyi in https://github.com/vllm-project/vllm/pull/14077
* [Doc] Correct beam_search using in generative_models.md by @upayuryeva in https://github.com/vllm-project/vllm/pull/14363
* [Kernel] [V1] Improved performance for V1 Triton (ROCm) backend  by @tdoublep in https://github.com/vllm-project/vllm/pull/14152
* [Bugfix][Core] fix abort_seq_group and memory leak when n>1 by @courage17340 in https://github.com/vllm-project/vllm/pull/14326
* [Core] Don't use cache during multi-modal profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14336
* [Doc] Fix date typo in README.md by @jitseklomp in https://github.com/vllm-project/vllm/pull/14366
* [RLHF] use worker_extension_cls for compatibility with V0 and V1 by @youkaichao in https://github.com/vllm-project/vllm/pull/14185
* Reinstate `best_of` for V0 by @hmellor in https://github.com/vllm-project/vllm/pull/14356
* Adding cpu inference with VXE ISA for s390x architecture by @dilipgb in https://github.com/vllm-project/vllm/pull/12613
* Add authors to license header. by @tdoublep in https://github.com/vllm-project/vllm/pull/14371
* Fix mla prefill context performance by @ZhongYingMatrix in https://github.com/vllm-project/vllm/pull/13897
* [V1] Do not detokenize if sampling param detokenize is False by @hj-mistral in https://github.com/vllm-project/vllm/pull/14224
* [Distributed] Add enable_expert_parallel arg by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14305
* [CI/Build] Use uv python for docker rather than ppa:deadsnakes/ppa by @mgoin in https://github.com/vllm-project/vllm/pull/13569
* [CI] Disable spawn when running V1 Test by @tdoublep in https://github.com/vllm-project/vllm/pull/14345
* [Kernel] Add needs_fixed_stride_order tag to most GEMMs by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14306
* [Bugfix] Fix use_direct_call condition in FusedMoE layer for  by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14382
* [Bug] Fix Attention when ignored in by quant_method by @mgoin in https://github.com/vllm-project/vllm/pull/14313
* [V1][Bugfix] Standardize quantized kv cache rejection for attention backends by @mgoin in https://github.com/vllm-project/vllm/pull/14221
* [Docs] Add nsight guide to profiling docs by @mgoin in https://github.com/vllm-project/vllm/pull/14298
* [Hardware][TPU]Enable ragged paged attention kernel and resolve recompilation issue by @yaochengji in https://github.com/vllm-project/vllm/pull/14310
* [Doc] Fix a typo by @dyli-google in https://github.com/vllm-project/vllm/pull/14385
* [Bugfix] Correctly call `cudaProfilerStop` in benchmarks script by @b8zhong in https://github.com/vllm-project/vllm/pull/14183
* [Perf] Reduce MLA CPU overheads in V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14384
* [FP8] Refactor apply_fp8_linear and apply_fp8_linear_generic into an object by @ProExpertProg in https://github.com/vllm-project/vllm/pull/14390
* [BugFix] Illegal Memory Access in the blockwise cutlass fp8 GEMMs by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14396
* [Bugfix] Fix JambaForCausalLM LoRA  by @jeejeelee in https://github.com/vllm-project/vllm/pull/14370
* [Build] Add nightly wheel fallback when latest commit wheel unavailable by @Isotr0py in https://github.com/vllm-project/vllm/pull/14358
* OpenVINO: added CPU-like conditions by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/14338
* [GH] Auto-apply multi-modality label to relevant PRs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14402
* correct wrong markdown syntax by @vincent-pli in https://github.com/vllm-project/vllm/pull/14414
* [Bugfix] Further clean up LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14422
* [Bugfix] Clean up multi-modal processors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14417
* [Misc] Set default value of seed to None by @SmartManoj in https://github.com/vllm-project/vllm/pull/14274
* [BUGFIX] Skip tokenization support for throughput benchmark by @maleksan85 in https://github.com/vllm-project/vllm/pull/12712
* Fix missing `kv_caches` and `attn_metadata` in `OpenVINOCausalLM` by @hmellor in https://github.com/vllm-project/vllm/pull/14271
* Use the optimized block sizes after tuning the kernel. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14329
* [V1][Core] Support for Structured Outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/12388
* [Doc] Update prefix_caching.md to match the example image by @York-RDWang in https://github.com/vllm-project/vllm/pull/14420
* [Benchmarks] Make detokenization optional in benchmark scripts by @JArnoldAMD in https://github.com/vllm-project/vllm/pull/11697
* [Kernel] optimize performance of gptq marlin kernel when n is small by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/14138
* [Misc] Add Phi4-MM example by @jeejeelee in https://github.com/vllm-project/vllm/pull/14343
* [v1] torch.compile integration explanation by @youkaichao in https://github.com/vllm-project/vllm/pull/14437
* [V1] Eagerly remove finished requests from the batch by @njhill in https://github.com/vllm-project/vllm/pull/14388
* [V1][Metrics] Fix traceback with preemptions+LoRA by @markmc in https://github.com/vllm-project/vllm/pull/14220
* [Bugfix] Fix torch_xla which can't handle None seed introduced in #14274 by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14459
* [V1] Prompt logprobs + APC compatibility; prompt logprobs reqs cannot fill APC by @afeldman-nm in https://github.com/vllm-project/vllm/pull/13949
* [Bugfix][V1] Handle MLA in kv_cache_interface by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14462
* Revert "[Perf] Reduce MLA CPU overheads in V1 (#14384)" by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14471
* [Bugfix][Disaggregated] Add a check in send_kv_caches_and_hidden_states and fix the reshape of the KVCache by @hasB4K in https://github.com/vllm-project/vllm/pull/14369
* [MISC][V1] Register process killing handler only in the main thread by @comaniac in https://github.com/vllm-project/vllm/pull/14380
* [core] add `extra_args` to `SamplingParams` by @akeshet in https://github.com/vllm-project/vllm/pull/13300
* [CI/Build] refactor: set timezone of container to UTC by @bufferoverflow in https://github.com/vllm-project/vllm/pull/12888
* Default to `generation_config` from model by @hmellor in https://github.com/vllm-project/vllm/pull/12622
* [Doc]add doc for Qwen models tool calling by @WangErXiao in https://github.com/vllm-project/vllm/pull/14478
* [Doc] Added QwQ-32B to the supported models list in the reasoning outâ€¦ by @WangErXiao in https://github.com/vllm-project/vllm/pull/14479
* [Bugfix] Make the deviceprofiler include LoRA memory. by @jeejeelee in https://github.com/vllm-project/vllm/pull/14469
* Add training doc signposting to TRL by @hmellor in https://github.com/vllm-project/vllm/pull/14439
* [Build/BugFix] Fix hopper 12.8 build by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14354
* Add RLHF document by @hmellor in https://github.com/vllm-project/vllm/pull/14482
* [CI/Build] Use a fixed seed to avoid flaky tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14480
* [V1] TPU - Add tensor parallel support via Ray by @alexm-redhat in https://github.com/vllm-project/vllm/pull/13618
* [VLM] Add TP support for Phi-4-MM by @Isotr0py in https://github.com/vllm-project/vllm/pull/14453
* [Misc] add `use_tqdm_on_load` to reduce logs by @aarnphm in https://github.com/vllm-project/vllm/pull/14407
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/13776
* [benchmarks] Add option to use unique jsonschema for each request by @russellb in https://github.com/vllm-project/vllm/pull/14457
* [Misc] Don't run ruff at all on 3rd party libs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14493
* Move requirements into their own directory by @hmellor in https://github.com/vllm-project/vllm/pull/12547
* [Bugfix] DeepSeek Accuracy by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14476
* [Bugfix] Fix profiling OOM and decouple encoder multimodal profiling by @Isotr0py in https://github.com/vllm-project/vllm/pull/14361
* Update CODEOWNERS for structured output by @russellb in https://github.com/vllm-project/vllm/pull/14496
* [Misc] Upgrade to Python 3.9 typing for additional directories by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14492
* [V1] Support bad_words in sampler by @22quinn in https://github.com/vllm-project/vllm/pull/13376
* Revert "[V1][Core] Fix memory issue with logits & sampling" by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14504
* [Attention] Default to FlashMLA backend for MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14451
* [V1][TPU] Remove unnecessary padding for running on TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14467
* [Feat] Support chunked prefill for LMCache connector by @YaoJiayi in https://github.com/vllm-project/vllm/pull/14505
* [Bugfix] Fix tqdm progress bar when SamplingParams.n > 1 by @yanyc428 in https://github.com/vllm-project/vllm/pull/12428
* [Bugfix] Revert QKVCrossParallelLinear usage in Mllama to keep BNB quantization work by @Isotr0py in https://github.com/vllm-project/vllm/pull/14498
* [Hardware][TPU] Fix the recompiling issue in logits processor after warmup by @yaochengji in https://github.com/vllm-project/vllm/pull/14510
* [Misc] Ensure out-of-tree quantization method recognize by cli args by @liuyanyi in https://github.com/vllm-project/vllm/pull/14328
* [Bugfix] Wrong requirements path - rocm by @martinhoyer in https://github.com/vllm-project/vllm/pull/14527
* [Feature] Consolidate performance benchmark datasets by @JenZhao in https://github.com/vllm-project/vllm/pull/14036
* [Misc] Add log information for handle_process_request. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/14130
* [Docs] Mention `model_impl` arg when explaining Transformers fallback by @hmellor in https://github.com/vllm-project/vllm/pull/14552
* [Frontend] support image embeds by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13955
* [Kernel] Add more dtype support for GGUF kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14043
* [Doc] Update PaliGemma note to a warning by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14565
* Correct capitalisation: `Github` -> `GitHub` by @hmellor in https://github.com/vllm-project/vllm/pull/14561
* [V1][Bugfix] Fix handing of `second_per_grid_ts` for Qwen2-VL & Qwen2.5-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/14548
* Correct capitalisation: `VLLM` -> `vLLM` by @hmellor in https://github.com/vllm-project/vllm/pull/14562
* [Docs] Make installation URLs nicer by @hmellor in https://github.com/vllm-project/vllm/pull/14556
* [Bugfix][v1] fixed llava-hf/llava-1.5-7b-hf is broken on V1 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/14554
* [Perf] Improve MLA on V1 by @simon-mo in https://github.com/vllm-project/vllm/pull/14540
* [Minor] Update the tqdm bar for parallel sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14571
* [V1] LoRA - Add triton kernels for V1 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/13096
* Fix typo in benchmark_serving_structured_output.py by @russellb in https://github.com/vllm-project/vllm/pull/14566
* [V1] Prevent xgrammar from breaking TPU support by @russellb in https://github.com/vllm-project/vllm/pull/14575
* [Kernel] moe wna16 cuda kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/13321
* [MISC][V1] Handle exception of current_platform.get_device_name() in arg_utils by @comaniac in https://github.com/vllm-project/vllm/pull/14379
* [Neuron] Add Neuron device communicator for vLLM v1 by @gnovack in https://github.com/vllm-project/vllm/pull/14085
* [neuron] add reshape_and_cache by @liangfu in https://github.com/vllm-project/vllm/pull/14391
* [V1][PP] Do not block engine core when no requests to schedule by @comaniac in https://github.com/vllm-project/vllm/pull/14585
* [Bugfix] Fix FP16 overflow for DeepSeek V2 by @Concurrensee in https://github.com/vllm-project/vllm/pull/13232
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/14508
* [Misc] Correct deepseek-vl2 chat template by @Isotr0py in https://github.com/vllm-project/vllm/pull/14558
* [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync by @cynthieye in https://github.com/vllm-project/vllm/pull/14377
* [VLM] Cleanup siglip legacy code and fix broken paligemma multimodal processor by @Isotr0py in https://github.com/vllm-project/vllm/pull/14602
* benchmarks: simplify test jsonschema by @russellb in https://github.com/vllm-project/vllm/pull/14567
* dynamic distpatch of fp8 kernels by @jeffdaily in https://github.com/vllm-project/vllm/pull/14245
* [Bugfix] Update `--hf-overrides` for `Alibaba-NLP/gte-Qwen2` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14609
* Uninstall dependencies before installing requirements/tpu.txt by @richardsliu in https://github.com/vllm-project/vllm/pull/14586
* [V1] Add regex structured output support with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/14590
* docs: Add documentation for s390x cpu implementation by @dilipgb in https://github.com/vllm-project/vllm/pull/14198
* [BugFix/Build] Fix sparse kernels not getting built on hopper by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14572
* [Hardware][Intel GPU] upgrade IPEX dependency to 2.6.10.  by @jikunshang in https://github.com/vllm-project/vllm/pull/14564
* [V1] Remove cache from StructuredOutputManager by @russellb in https://github.com/vllm-project/vllm/pull/14622
* fix some typos : supported_head_sizes by @hackty in https://github.com/vllm-project/vllm/pull/14627
* [V1] Delay all xgrammar usage until needed by @russellb in https://github.com/vllm-project/vllm/pull/14616
* Fix run_tpu_test by @richardsliu in https://github.com/vllm-project/vllm/pull/14641
* [V1][TPU] Pad the block_table.shape[1] so the ragged paged attention can handle correctly by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14597
* [Bugfix][V1][PP] Only warmup sampler at last PP rank by @comaniac in https://github.com/vllm-project/vllm/pull/14643
* [release] Add commands to clean up logs on TPU release node by @khluu in https://github.com/vllm-project/vllm/pull/14642
* [Feature] Add `vllm bench` CLI by @randyjhc in https://github.com/vllm-project/vllm/pull/13993
* [core][V1] pluggable scheduler by @joerunde in https://github.com/vllm-project/vllm/pull/14466
* [Doc] Update benchmarks README by @JenZhao in https://github.com/vllm-project/vllm/pull/14646
* [Model] Extend Ultravox to accept audio longer than 30s by @farzadab in https://github.com/vllm-project/vllm/pull/13631
* [V1][Core] Support MistralTokenizer for Structured Output by @aarnphm in https://github.com/vllm-project/vllm/pull/14625
* [Core] Refactor `QKVCrossParallelLinear` implementation to support BNB 4-bit quantization by @Isotr0py in https://github.com/vllm-project/vllm/pull/14545
* [Kernel] GGUF MoE kernel by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14613
* [V1][Bugfix][Spec Decode] Fix incorrect outputs in V1 speculative decoding due to batch indexing by @benchislett in https://github.com/vllm-project/vllm/pull/14645
* [Kernel] Add ModelOpt FP4 Checkpoint Support by @pavanimajety in https://github.com/vllm-project/vllm/pull/12520
* [CPU] Upgrade CPU backend to torch-2.6 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/13381
* [ROCm][Bugfix] Ensure that the moe_wna16_gemm kernel is not built on ROCm platforms. by @SageMoore in https://github.com/vllm-project/vllm/pull/14629
* [Model] Add support for Gemma 3 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14660
* [Bugfix] Missing thumbnail from NVLM-D processor by @ameyanjarlekar in https://github.com/vllm-project/vllm/pull/14633
* [ROCm] Enable chunked prefill/paged attention in MLA on ROCm by @SageMoore in https://github.com/vllm-project/vllm/pull/14316
* [FEAT] [ROCm] [Embedding] Add encoder-only model support into ROCm Flash Attention to enable embedding models. by @tjtanaa in https://github.com/vllm-project/vllm/pull/14664
* [BugFix][V1] Fix parallel sampling finishing/aborts by @njhill in https://github.com/vllm-project/vllm/pull/14512
* [V1] Allow sliding window + prefix caching by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13069
* [release] Add force remove for TPU logs by @khluu in https://github.com/vllm-project/vllm/pull/14697
* [bugfix] fixup warning message for plugged schedulers for v1 by @joerunde in https://github.com/vllm-project/vllm/pull/14700
* Add ray[data] as tpu dependency by @richardsliu in https://github.com/vllm-project/vllm/pull/14691
* [ROCm][FP8] Fix for adjustments needed only for fnuz by @gshtras in https://github.com/vllm-project/vllm/pull/14689
* [BugFix][TritonMLA] Process weights after model loading for GGUF by @tywuAMD in https://github.com/vllm-project/vllm/pull/14555
* [Config][Disaggregated] Add timeout configuration for the torch.store and add KVTransferConfig.kv_connector_extra_config by @hasB4K in https://github.com/vllm-project/vllm/pull/14367
* [V1][TPU] Add assertion on multi-step-scheduler by @lsy323 in https://github.com/vllm-project/vllm/pull/14707
* [Quant] BartModel SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/14699
* [Quant] Bamba SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/14698
* [Bugfix] Fix chunked prefill for GGUF by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14666
* [CI/Build]  Delete ultravox LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14730
* [Bugfix] fix benchmark moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/14653
* [VLM] Support pan-and-scan for Gemma3 multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14672
* [VLM] Support loading InternVideo2.5 models as original InternVLChatModel by @Isotr0py in https://github.com/vllm-project/vllm/pull/14738
* [Bugfix] Fix prompt format of GLM4V by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14539
* [V1][Minor] Minor enhancements on scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14732
* [Misc] Clean up processor tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14771
* [V1][Core] using cached vocab_size for Structured Outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/14630
* [V1] Detokenizer: Respect Stop Tokens + `not include_stop_str_in_output` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/14624
* [Attention] Remove slow setattr in MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14769
* [Doc] Fix typo in documentation by @yasu52 in https://github.com/vllm-project/vllm/pull/14783
* [Doc] Fix small typo in Transformers fallback by @heheda12345 in https://github.com/vllm-project/vllm/pull/14791
* [V1] TPU - Enable prefix caching by default by @alexm-redhat in https://github.com/vllm-project/vllm/pull/14773
* forward fix PR 14245, restore build on ROCm 6.2 by @jeffdaily in https://github.com/vllm-project/vllm/pull/14709
* [V1] Move OOM check into sampler run by @ywang96 in https://github.com/vllm-project/vllm/pull/14728
* [V1] Temporarily disable FlashInfer Rejection Sampler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14788
* [Kernel] LoRA - Enable CUDAGraphs for V1 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/14626
* [Kernel] [V1] Further optimizations to ROCm (Triton) Backend to better handle GQA. by @tdoublep in https://github.com/vllm-project/vllm/pull/14431
* [Bugfix][IPEX] Add `VLLM_CPU_MOE_PREPACK` to allow disabling MoE prepack when CPU does not support it by @gau-nernst in https://github.com/vllm-project/vllm/pull/14681
* [ci] Reduce number of tests in fastcheck by @khluu in https://github.com/vllm-project/vllm/pull/14782
* [Misc][Minor] Simplify `SamplingParams.__post_init__()` by @njhill in https://github.com/vllm-project/vllm/pull/14772
* [Neuron] flatten test parameterization for neuron attention kernels by @liangfu in https://github.com/vllm-project/vllm/pull/14712
* [Feature] Add visionarena offline support for benchmark_throughput by @JenZhao in https://github.com/vllm-project/vllm/pull/14654
* [CI] Fix missing example model id in processor test by @ywang96 in https://github.com/vllm-project/vllm/pull/14787
* [Attention] MLA get rid of materialization by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14770
* [Bugfix][Kernel][CPU] Fix num_tokens in CPU rotary embedding kernel by @gau-nernst in https://github.com/vllm-project/vllm/pull/14667
* [BugFix]Fix performance serving benchmark when enable profiling by @Potabk in https://github.com/vllm-project/vllm/pull/14737
* [Misc] Clean up type annotation for `SupportsMultiModal` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14794
* [Bugfix] Fix small typo in the example of Streaming delimiter by @bravo325806 in https://github.com/vllm-project/vllm/pull/14793
* [Misc] Gemma3ForConditionalGeneration supports LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/14797
* [V1][Minor] Minor code cleanup for scheduling metrics by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14800
* [Bugfix][W8A8] fixed cutlass block fp8 binding by @DefTruth in https://github.com/vllm-project/vllm/pull/14796
* [VLM] Various cleanup and fixes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14806
* [BugFix]: properly catch templating error when preprocess input by @gcalmettes in https://github.com/vllm-project/vllm/pull/13976
* [Bugfix] Fix Aria test loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14823
* [V1] Fix vocab size calculation for structured output by @russellb in https://github.com/vllm-project/vllm/pull/14826
* [Frontend] Fix log message to use http vs https by @russellb in https://github.com/vllm-project/vllm/pull/14774
* [V1][Metrics] Updated list of deprecated metrics in v0.8 by @markmc in https://github.com/vllm-project/vllm/pull/14695
* [Frontend] track server_load by @daniel-salib in https://github.com/vllm-project/vllm/pull/13950
* [Bugfix][Kernel]: Fix AllSpark kernel compilation errors and enable for CUDA < 12.0 by @wyajieha in https://github.com/vllm-project/vllm/pull/14430
* [release] Remove log cleanup commands from TPU job by @khluu in https://github.com/vllm-project/vllm/pull/14838
* Re-enable the AMD Entrypoints Test by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/14711
* [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  by @cyang49 in https://github.com/vllm-project/vllm/pull/14778
* [V1] Fix model parameterization for structured output tests by @russellb in https://github.com/vllm-project/vllm/pull/14833
* Update to torch==2.6.0 by @mgoin in https://github.com/vllm-project/vllm/pull/12721
* [CI] Add TPU v1 test by @richardsliu in https://github.com/vllm-project/vllm/pull/14834
* [Build/CI] Move ninja to common deps by @russellb in https://github.com/vllm-project/vllm/pull/14835
* [Build/CI] Upgrade aiohttp to incldue CVE fix by @russellb in https://github.com/vllm-project/vllm/pull/14840
* [Doc] More neutral K8s deployment guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/14084
* [Bugfix] Fix torch_xla in V0 which can't handle None seed introduced â€¦ by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14844
* [Neuron][CI] update docker run command by @liangfu in https://github.com/vllm-project/vllm/pull/14829
* [Bugfix][V1] Fix flashinfer sampling by @DefTruth in https://github.com/vllm-project/vllm/pull/14815
* Revert "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Uâ€¦ by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14848
* Disable outlines cache by default by @russellb in https://github.com/vllm-project/vllm/pull/14837
* [Misc] Remove misleading message in gemma2 and gemma3 by @Isotr0py in https://github.com/vllm-project/vllm/pull/14850
* [Misc][Easy] Annotate unused vars in the csrc files by @houseroad in https://github.com/vllm-project/vllm/pull/14798
* [V1] V1 Enablement Oracle  by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13726
* [Docs] Add new East Coast vLLM Meetup slides to README and meetups.md by @simon-mo in https://github.com/vllm-project/vllm/pull/14852
* [CPU] Support FP8 KV cache by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/14741
* [Attention] Get rid of mla cache alignment by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14842
* [CI/Build] Delete LoRA bias test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14849
* [V1][Structured Output] calculate vocab_size eagerly by @aarnphm in https://github.com/vllm-project/vllm/pull/14851
* [Doc] V1 user guide by @JenZhao in https://github.com/vllm-project/vllm/pull/13991
* [Build/CI] Upgrade jinja2 to get 3 moderate CVE fixes by @russellb in https://github.com/vllm-project/vllm/pull/14839
* [Bugfix] EAGLE output norm bug by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/14464
* [VLM] Limit multimodal input cache by memory by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14805
* [CI][Intel GPU] refine intel GPU ci docker build by @jikunshang in https://github.com/vllm-project/vllm/pull/14860
* [Core] Expose API endpoint `/is_sleeping` by @waltforme in https://github.com/vllm-project/vllm/pull/14312
* [VLM] Merged multi-modal processor for Pixtral by @Flechman in https://github.com/vllm-project/vllm/pull/12211
* [Misc][Doc] Minor benchmark README update by @ywang96 in https://github.com/vllm-project/vllm/pull/14874
* [VLM] Clean up Phi-4-MM ViT implementation by @Isotr0py in https://github.com/vllm-project/vllm/pull/14812
* [V1] Remove V0 fallback for mistral-tokenizer by @ywang96 in https://github.com/vllm-project/vllm/pull/14873
* [Kernel] Add more tuned configs by @simon-mo in https://github.com/vllm-project/vllm/pull/14877
* [BugFix] Fix torch distributed stateless PG backend init by @njhill in https://github.com/vllm-project/vllm/pull/14870
* [V1] [Spec Decode] Fix ngram tests by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/14878
* [Bugfix] Limit profiling run sequence length by max_model_len by @kylesayrs in https://github.com/vllm-project/vllm/pull/14785
* [Bugfix] Explicitly disable Phi-4-multimodal in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14889
* Revert "[Bugfix] Limit profiling run sequence length by max_model_len (#14785) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14892
* [BugFix][V1] Fix overhead related to bad_words sampling when not in use by @njhill in https://github.com/vllm-project/vllm/pull/14894
* [V1][BugFix] Detect interleaved sliding window attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14896
* [Misc] Catching Ray Compiled Graph PP test failures for V1 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14847
* [Doc] Add guidance for using `ccache` with `pip install -e .` in doc by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/14901
* [V1] Enable Entrypoints Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14903
* [CI] Fix Tool Calling Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14898
* [CI/Build] Update defaults for test reproducibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14893
* [V1] Optimize the overhead of rewinding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14905
* [V1][Minor] Add __repr__ to ConstantList by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14907
* [BugFix] Fix MLA + V1 + TP==1 causing reinitialization of cuda context by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14910
* [Misc] Replace os environ to monkeypatch in test suite by @t-sibiraj in https://github.com/vllm-project/vllm/pull/14516
* [Benchmark] Do not save detailed info to json by default by @simon-mo in https://github.com/vllm-project/vllm/pull/14879
* [V1] [Spec Decode] Support random sampling for spec decode by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/13933
* [V1] Remove input cache client by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14864
* [Misc][XPU] Use None as device capacity for XPU by @yma11 in https://github.com/vllm-project/vllm/pull/14932
* [Doc] Add vLLM Beijing meetup slide by @heheda12345 in https://github.com/vllm-project/vllm/pull/14938
* setup.py: drop assumption about local `main` branch by @russellb in https://github.com/vllm-project/vllm/pull/14692
* [MISC] More AMD unused var clean up by @houseroad in https://github.com/vllm-project/vllm/pull/14926
* fix minor miscalled method by @kushanam in https://github.com/vllm-project/vllm/pull/14327
* [V1][TPU] Apply the ragged paged attention kernel fix and remove the padding. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14846
* [Bugfix] Fix Ultravox on V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14929
* [Misc] Add `--seed` option to offline multi-modal examples by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14934
* [Bugfix][ROCm] running new process using spawn method for rocm in tests. by @vllmellm in https://github.com/vllm-project/vllm/pull/14810
* [Doc] Fix misleading log during multi-modal profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14955
* Add patch merger by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/14957
* [V1] Default MLA to V1 by @simon-mo in https://github.com/vllm-project/vllm/pull/14921
* [Bugfix] Fix precommit - line too long in pixtral.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14960
* [Bugfix][Model] Mixtral: use unused head_dim config argument by @qtrrb in https://github.com/vllm-project/vllm/pull/14961
* [Fix][Structured Output] using vocab_size to construct matcher by @aarnphm in https://github.com/vllm-project/vllm/pull/14868
* [Bugfix] Make Gemma3 MM V0 only for now by @ywang96 in https://github.com/vllm-project/vllm/pull/14971

## New Contributors
* @ajayvohra2005 made their first contribution in https://github.com/vllm-project/vllm/pull/13589
* @Edwinhr716 made their first contribution in https://github.com/vllm-project/vllm/pull/12913
* @Hongbosherlock made their first contribution in https://github.com/vllm-project/vllm/pull/12978
* @johnzheng1975 made their first contribution in https://github.com/vllm-project/vllm/pull/13668
* @JenZhao made their first contribution in https://github.com/vllm-project/vllm/pull/13594
* @bufferoverflow made their first contribution in https://github.com/vllm-project/vllm/pull/13011
* @cakeng made their first contribution in https://github.com/vllm-project/vllm/pull/12583
* @eli-b made their first contribution in https://github.com/vllm-project/vllm/pull/13785
* @YaoJiayi made their first contribution in https://github.com/vllm-project/vllm/pull/12953
* @edwardzjl made their first contribution in https://github.com/vllm-project/vllm/pull/13468
* @naromero77amd made their first contribution in https://github.com/vllm-project/vllm/pull/13623
* @henrylhtsang made their first contribution in https://github.com/vllm-project/vllm/pull/13797
* @tianyuzhou95 made their first contribution in https://github.com/vllm-project/vllm/pull/13863
* @b8zhong made their first contribution in https://github.com/vllm-project/vllm/pull/13736
* @Chenyaaang made their first contribution in https://github.com/vllm-project/vllm/pull/13860
* @observerw made their first contribution in https://github.com/vllm-project/vllm/pull/13958
* @qli88 made their first contribution in https://github.com/vllm-project/vllm/pull/13718
* @benchislett made their first contribution in https://github.com/vllm-project/vllm/pull/13626
* @hasB4K made their first contribution in https://github.com/vllm-project/vllm/pull/13987
* @Kacper-Pietkun made their first contribution in https://github.com/vllm-project/vllm/pull/13213
* @Ryp made their first contribution in https://github.com/vllm-project/vllm/pull/13090
* @LouieYang made their first contribution in https://github.com/vllm-project/vllm/pull/14031
* @vanbasten23 made their first contribution in https://github.com/vllm-project/vllm/pull/13379
* @atalman made their first contribution in https://github.com/vllm-project/vllm/pull/13926
* @wyajieha made their first contribution in https://github.com/vllm-project/vllm/pull/12931
* @qux-bbb made their first contribution in https://github.com/vllm-project/vllm/pull/14086
* @realShengYao made their first contribution in https://github.com/vllm-project/vllm/pull/14051
* @zhanwenchen made their first contribution in https://github.com/vllm-project/vllm/pull/14142
* @rainkert made their first contribution in https://github.com/vllm-project/vllm/pull/13750
* @congcongchen123 made their first contribution in https://github.com/vllm-project/vllm/pull/14119
* @iacolippo made their first contribution in https://github.com/vllm-project/vllm/pull/14217
* @zhe-thoughts made their first contribution in https://github.com/vllm-project/vllm/pull/14288
* @DaividFrank made their first contribution in https://github.com/vllm-project/vllm/pull/14293
* @vincent-4 made their first contribution in https://github.com/vllm-project/vllm/pull/13997
* @pyc96 made their first contribution in https://github.com/vllm-project/vllm/pull/14237
* @upayuryeva made their first contribution in https://github.com/vllm-project/vllm/pull/14363
* @courage17340 made their first contribution in https://github.com/vllm-project/vllm/pull/14326
* @dilipgb made their first contribution in https://github.com/vllm-project/vllm/pull/12613
* @ZhongYingMatrix made their first contribution in https://github.com/vllm-project/vllm/pull/13897
* @hj-mistral made their first contribution in https://github.com/vllm-project/vllm/pull/14224
* @yaochengji made their first contribution in https://github.com/vllm-project/vllm/pull/14310
* @dyli-google made their first contribution in https://github.com/vllm-project/vllm/pull/14385
* @vincent-pli made their first contribution in https://github.com/vllm-project/vllm/pull/14414
* @York-RDWang made their first contribution in https://github.com/vllm-project/vllm/pull/14420
* @yarongmu-google made their first contribution in https://github.com/vllm-project/vllm/pull/14459
* @22quinn made their first contribution in https://github.com/vllm-project/vllm/pull/13376
* @yanyc428 made their first contribution in https://github.com/vllm-project/vllm/pull/12428
* @martinhoyer made their first contribution in https://github.com/vllm-project/vllm/pull/14527
* @gnovack made their first contribution in https://github.com/vllm-project/vllm/pull/14085
* @cynthieye made their first contribution in https://github.com/vllm-project/vllm/pull/14377
* @jeffdaily made their first contribution in https://github.com/vllm-project/vllm/pull/14245
* @hackty made their first contribution in https://github.com/vllm-project/vllm/pull/14627
* @randyjhc made their first contribution in https://github.com/vllm-project/vllm/pull/13993
* @ameyanjarlekar made their first contribution in https://github.com/vllm-project/vllm/pull/14633
* @tywuAMD made their first contribution in https://github.com/vllm-project/vllm/pull/14555
* @yasu52 made their first contribution in https://github.com/vllm-project/vllm/pull/14783
* @gau-nernst made their first contribution in https://github.com/vllm-project/vllm/pull/14681
* @Potabk made their first contribution in https://github.com/vllm-project/vllm/pull/14737
* @bravo325806 made their first contribution in https://github.com/vllm-project/vllm/pull/14793
* @daniel-salib made their first contribution in https://github.com/vllm-project/vllm/pull/13950
* @cyang49 made their first contribution in https://github.com/vllm-project/vllm/pull/14778
* @luyuzhe111 made their first contribution in https://github.com/vllm-project/vllm/pull/14464
* @Flechman made their first contribution in https://github.com/vllm-project/vllm/pull/12211
* @vadiklyutiy made their first contribution in https://github.com/vllm-project/vllm/pull/14901
* @t-sibiraj made their first contribution in https://github.com/vllm-project/vllm/pull/14516
* @vllmellm made their first contribution in https://github.com/vllm-project/vllm/pull/14810
* @qtrrb made their first contribution in https://github.com/vllm-project/vllm/pull/14961

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.7.3...v0.8.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.0)

---

## v0.8.0rc2: v0.8.0rc2
**Published:** 2025-03-17
**Pre-release**

## What's Changed
* [V1] Remove input cache client by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14864
* [Misc][XPU] Use None as device capacity for XPU by @yma11 in https://github.com/vllm-project/vllm/pull/14932
* [Doc] Add vLLM Beijing meetup slide by @heheda12345 in https://github.com/vllm-project/vllm/pull/14938
* setup.py: drop assumption about local `main` branch by @russellb in https://github.com/vllm-project/vllm/pull/14692
* [MISC] More AMD unused var clean up by @houseroad in https://github.com/vllm-project/vllm/pull/14926
* fix minor miscalled method by @kushanam in https://github.com/vllm-project/vllm/pull/14327
* [V1][TPU] Apply the ragged paged attention kernel fix and remove the padding. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14846
* [Bugfix] Fix Ultravox on V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14929
* [Misc] Add `--seed` option to offline multi-modal examples by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14934
* [Bugfix][ROCm] running new process using spawn method for rocm in tests. by @vllmellm in https://github.com/vllm-project/vllm/pull/14810
* [Doc] Fix misleading log during multi-modal profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14955
* Add patch merger by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/14957
* [V1] Default MLA to V1 by @simon-mo in https://github.com/vllm-project/vllm/pull/14921
* [Bugfix] Fix precommit - line too long in pixtral.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14960
* [Bugfix][Model] Mixtral: use unused head_dim config argument by @qtrrb in https://github.com/vllm-project/vllm/pull/14961
* [Fix][Structured Output] using vocab_size to construct matcher by @aarnphm in https://github.com/vllm-project/vllm/pull/14868
* [Bugfix] Make Gemma3 MM V0 only for now by @ywang96 in https://github.com/vllm-project/vllm/pull/14971

## New Contributors
* @vllmellm made their first contribution in https://github.com/vllm-project/vllm/pull/14810
* @qtrrb made their first contribution in https://github.com/vllm-project/vllm/pull/14961

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.8.0rc1...v0.8.0rc2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.0rc2)

---

## v0.8.0rc1: v0.8.0rc1
**Published:** 2025-03-17
**Pre-release**

Note: vLLM no longer sets the global seed (#14274). Please set the `seed` parameter if you need to reproduce your results.

## What's Changed
* Update `pre-commit`'s `isort` version to remove warnings by @hmellor in https://github.com/vllm-project/vllm/pull/13614
* [V1][Minor] Print KV cache size in token counts by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13596
* fix neuron performance issue by @ajayvohra2005 in https://github.com/vllm-project/vllm/pull/13589
* [Frontend] Add backend-specific options for guided decoding by @joerunde in https://github.com/vllm-project/vllm/pull/13505
* [Bugfix] Fix max_num_batched_tokens for MLA by @mgoin in https://github.com/vllm-project/vllm/pull/13620
* [Neuron][Kernel] Vectorize KV cache load in FlashPagedAttention to maximize DMA bandwidth by @lingfanyu in https://github.com/vllm-project/vllm/pull/13245
* Add llmaz as another integration by @kerthcet in https://github.com/vllm-project/vllm/pull/13643
* [Misc] Adding script to setup ray for multi-node vllm deployments  by @Edwinhr716 in https://github.com/vllm-project/vllm/pull/12913
* [NVIDIA] Fix an issue to use current stream for the nvfp4 quant by @kaixih in https://github.com/vllm-project/vllm/pull/13632
* Use pre-commit to update `requirements-test.txt` by @hmellor in https://github.com/vllm-project/vllm/pull/13617
* [Bugfix] Add `mm_processor_kwargs` to chat-related protocols by @ywang96 in https://github.com/vllm-project/vllm/pull/13644
* [V1][Sampler] Avoid an operation during temperature application by @njhill in https://github.com/vllm-project/vllm/pull/13587
* Missing comment explaining VDR variable in GGUF kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/13290
* [FEATURE] Enables /score endpoint for embedding models by @gmarinho2 in https://github.com/vllm-project/vllm/pull/12846
* [ci] Fix metrics test model path by @khluu in https://github.com/vllm-project/vllm/pull/13635
* [Kernel]Add streamK for block-quantized CUTLASS kernels by @Hongbosherlock in https://github.com/vllm-project/vllm/pull/12978
* [Bugfix][CPU] Fix cpu all-reduce using native pytorch implementation  by @Isotr0py in https://github.com/vllm-project/vllm/pull/13586
* fix typo of grafana dashboard, with correct datasource by @johnzheng1975 in https://github.com/vllm-project/vllm/pull/13668
* [Attention] MLA with chunked prefill by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12639
* [Misc] Fix yapf linting tools etc not running on pre-commit by @Isotr0py in https://github.com/vllm-project/vllm/pull/13695
* docs: Add a note on full CI run in contributing guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13646
* [HTTP Server] Make model param optional in request by @youngkent in https://github.com/vllm-project/vllm/pull/13568
* [Bugfix][API Server] Fix invalid usage of 'ge' and 'le' in port validâ€¦ by @WangErXiao in https://github.com/vllm-project/vllm/pull/13672
* [Misc] Capture and log the time of loading weights by @waltforme in https://github.com/vllm-project/vllm/pull/13666
* [ROCM] fix native attention function call by @gongdao123 in https://github.com/vllm-project/vllm/pull/13650
* [Bugfix][Model] OLMo 2: split qkv correctly for GQA and MQA by @2015aroras in https://github.com/vllm-project/vllm/pull/13687
* [Misc] Bump compressed-tensors by @dsikka in https://github.com/vllm-project/vllm/pull/13619
* [Bugfix] Fix benchmark script bug: inaccurate stats for vllm backend when max_model_len < input_len + output_len by @WangErXiao in https://github.com/vllm-project/vllm/pull/13691
* [v1] Support allowed_token_ids in v1 Sampler by @houseroad in https://github.com/vllm-project/vllm/pull/13210
* [Bugfix] V1 Memory Profiling: V0 Sampler Integration without Rejection Sampler by @JenZhao in https://github.com/vllm-project/vllm/pull/13594
* Correction to TP logic for Mamba Mixer 2 when Num Groups not divisible by TP Size by @fabianlim in https://github.com/vllm-project/vllm/pull/13660
* [V1][Metrics] Support `vllm:cache_config_info` by @markmc in https://github.com/vllm-project/vllm/pull/13299
* [Metrics] Add `--show-hidden-metrics-for-version` CLI arg by @markmc in https://github.com/vllm-project/vllm/pull/13295
* [Misc] Reduce LoRA-related static variable by @jeejeelee in https://github.com/vllm-project/vllm/pull/13166
* [CI/Build] Fix pre-commit errors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13696
* [core] set up data parallel communication by @youkaichao in https://github.com/vllm-project/vllm/pull/13591
* [ci] fix linter by @youkaichao in https://github.com/vllm-project/vllm/pull/13701
* Support SSL Key Rotation in HTTP Server by @youngkent in https://github.com/vllm-project/vllm/pull/13495
* [NVIDIA] Support nvfp4 cutlass gemm by @kaixih in https://github.com/vllm-project/vllm/pull/13571
* [V1][Kernel] Refactor the prefix_prefill kernel so that the caller no longer has to pass in the context lengths by @SageMoore in https://github.com/vllm-project/vllm/pull/13095
* [ROCm] Apply FP8 weights padding to values not divisible by 512 bytes on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/13231
* [Doc] Dockerfile instructions for optional dependencies and dev transformers by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13699
* [Bugfix] Fix boolean conversion for OpenVINO env variable by @helena-intel in https://github.com/vllm-project/vllm/pull/13615
* [XPU]fix setuptools version for xpu by @yma11 in https://github.com/vllm-project/vllm/pull/13548
* [CI/Build] fix uv caching in Dockerfile by @dtrifiro in https://github.com/vllm-project/vllm/pull/13611
* [CI/Build] Fix pre-commit errors from #13571 by @ywang96 in https://github.com/vllm-project/vllm/pull/13709
* [BugFix] Minor: logger import in attention backend by @andylolu2 in https://github.com/vllm-project/vllm/pull/13706
* [ci] Use env var to control whether to use S3 bucket in CI by @khluu in https://github.com/vllm-project/vllm/pull/13634
* [Quant] BaiChuan SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/13710
* [LMM] Implement merged multimodal processor for whisper by @Isotr0py in https://github.com/vllm-project/vllm/pull/13278
* [Core][Distributed] Use IPC (domain socket) ZMQ socket for local comms by @njhill in https://github.com/vllm-project/vllm/pull/13688
* [Misc] Deprecate `--dataset` from `benchmark_serving.py` by @ywang96 in https://github.com/vllm-project/vllm/pull/13708
* [v1] torchrun compatibility by @youkaichao in https://github.com/vllm-project/vllm/pull/13642
* [V1][BugFix] Fix engine core client shutdown hangs by @njhill in https://github.com/vllm-project/vllm/pull/13298
* Fix some issues with benchmark data output by @huydhn in https://github.com/vllm-project/vllm/pull/13641
* [ci] Add logic to change model to S3 path only when S3 CI env var is on by @khluu in https://github.com/vllm-project/vllm/pull/13727
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/13721
* [model][refactor] remove cuda hard code in models and layers by @MengqingCao in https://github.com/vllm-project/vllm/pull/13658
* [Bugfix] fix(logging): add missing opening square bracket by @bufferoverflow in https://github.com/vllm-project/vllm/pull/13011
* [CI/Build] add python-json-logger to requirements-common by @bufferoverflow in https://github.com/vllm-project/vllm/pull/12842
* Expert Parallelism (EP) Support for DeepSeek Models by @cakeng in https://github.com/vllm-project/vllm/pull/12583
* [BugFix]  Illegal memory access for MoE On H20 by @Abatom in https://github.com/vllm-project/vllm/pull/13693
* [Misc][Docs] Raise error when flashinfer is not installed and `VLLM_ATTENTION_BACKEND` is set by @NickLucche in https://github.com/vllm-project/vllm/pull/12513
* [V1] V1 engine implements parallel sampling (AsyncLLM and LLMEngine) by @afeldman-nm in https://github.com/vllm-project/vllm/pull/10980
* Revert "[V1][Core] Fix memory issue with logits & sampling" by @ywang96 in https://github.com/vllm-project/vllm/pull/13775
* Fix precommit fail in fused_moe intermediate_cache2 chunking by @mgoin in https://github.com/vllm-project/vllm/pull/13772
* [Misc] Clean Up `EngineArgs.create_engine_config` by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13734
* [Misc][Chore] Clean Up `AsyncOutputProcessing` Logs by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13780
* Remove unused kwargs from model definitions by @hmellor in https://github.com/vllm-project/vllm/pull/13555
* [Doc] arg_utils.py: fixed a typo by @eli-b in https://github.com/vllm-project/vllm/pull/13785
* [Misc] set single whitespace between log sentences by @cjackal in https://github.com/vllm-project/vllm/pull/13771
* [Bugfix][Quantization] Fix FP8 + EP by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13784
* [Misc][Attention][Quantization] init property earlier by @wangxiyuan in https://github.com/vllm-project/vllm/pull/13733
* [V1][Metrics] Implement vllm:lora_requests_info metric by @markmc in https://github.com/vllm-project/vllm/pull/13504
* [Bugfix] Fix deepseek-v2 error: "missing 1 required positional argument: 'residual'" by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13802
* [Bugfix] Support MLA for CompressedTensorsWNA16 by @mgoin in https://github.com/vllm-project/vllm/pull/13725
* Fix CompressedTensorsWNA16MoE with grouped scales by @mgoin in https://github.com/vllm-project/vllm/pull/13769
* [Core] LoRA V1 - Add add/pin/list/remove_lora functions   by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/13705
* [Misc] Check that the model can be inspected upon registration by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13743
* [Core] xgrammar: Expand list of unsupported jsonschema keywords by @russellb in https://github.com/vllm-project/vllm/pull/13783
* [Bugfix] Modify modelscope api usage in transformer_utils by @shen-shanshan in https://github.com/vllm-project/vllm/pull/13807
* [misc] Clean up ray compiled graph type hints by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13731
* [Feature] Support KV cache offloading and disagg prefill with LMCache connector. by @YaoJiayi in https://github.com/vllm-project/vllm/pull/12953
* [ROCm][Quantization][Kernel] Using HIP FP8 header by @gshtras in https://github.com/vllm-project/vllm/pull/12593
* [CI/Build]  Fix V1 LoRA failure by @jeejeelee in https://github.com/vllm-project/vllm/pull/13767
* [Misc]Clarify Error Handling for Non-existent Model Paths and HF Repo IDs by @Chen-0210 in https://github.com/vllm-project/vllm/pull/13724
* [Bugfix] Initialize attention bias on the same device as Query/Key/Value by @edwardzjl in https://github.com/vllm-project/vllm/pull/13468
* [Bugfix] Flush TunableOp results before worker processes are destroyed. by @naromero77amd in https://github.com/vllm-project/vllm/pull/13623
* [Bugfix] Fix deepseek-vl2 inference with more than 2 images by @Isotr0py in https://github.com/vllm-project/vllm/pull/13818
* Fix `/v1/audio/transcriptions ` Bad Request Error by @HermitSun in https://github.com/vllm-project/vllm/pull/13811
* [Bugfix] Revert inspection code in #13743 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13832
* Fix string parsing error by @Chen-0210 in https://github.com/vllm-project/vllm/pull/13825
* [Neuron] Add custom_ops for neuron backend by @liangfu in https://github.com/vllm-project/vllm/pull/13246
* Fix failing `MyGemma2Embedding` test by @hmellor in https://github.com/vllm-project/vllm/pull/13820
* [Model] Support Grok1 by @mgoin in https://github.com/vllm-project/vllm/pull/13795
* DeepSeek V2/V3/R1 only place `lm_head` on last pp rank by @hmellor in https://github.com/vllm-project/vllm/pull/13833
* [misc] Show driver IP info when Ray fails to allocate driver worker by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13858
* [V1][Spec Decode] Change Spec Decode Rejection Sampling API by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/13729
* [Misc]Code Cleanup by @noemotiovon in https://github.com/vllm-project/vllm/pull/13859
* [Kernel][Build/CI] Bump CUTLASS to 3.8 and add initializers for cutlass epilogues by @henrylhtsang in https://github.com/vllm-project/vllm/pull/13797
* Improve pipeline partitioning by @hmellor in https://github.com/vllm-project/vllm/pull/13839
* [Doc] fix the incorrect module path of tensorize_vllm_model by @tianyuzhou95 in https://github.com/vllm-project/vllm/pull/13863
* [ROCm] Disable chunked prefill/prefix caching when running MLA on non-cuda platforms by @SageMoore in https://github.com/vllm-project/vllm/pull/13844
* [v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine by @sethkimmel3 in https://github.com/vllm-project/vllm/pull/13837
* [Misc] Improve LoRA spelling by @jeejeelee in https://github.com/vllm-project/vllm/pull/13831
* [Misc] Fix input processing for Ultravox by @ywang96 in https://github.com/vllm-project/vllm/pull/13871
* [Bugfix] Add test example for Ultravox v0.5 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13890
* Add comments on accessing `kv_cache` and `attn_metadata` by @hmellor in https://github.com/vllm-project/vllm/pull/13887
* [Bugfix] Handle None parameters in Mistral function calls. by @fgreinacher in https://github.com/vllm-project/vllm/pull/13786
* [Misc]: Add support for goodput on guided benchmarking + TPOT calculation refactor by @b8zhong in https://github.com/vllm-project/vllm/pull/13736
* [Bugfix] Do not crash V0 engine on input errors by @joerunde in https://github.com/vllm-project/vllm/pull/13101
* [Bugfix] Update expected token counts for Ultravox tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13895
* [TPU] use torch2.6 with whl package by @Chenyaaang in https://github.com/vllm-project/vllm/pull/13860
* [Misc] fixed qwen_vl_utils parameter error by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13906
* [Bugfix] Backend option to disable xgrammar any_whitespace by @wallashss in https://github.com/vllm-project/vllm/pull/12744
* [BugFix] Make FP8 Linear compatible with torch.compile by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13918
* [Kernel] FlashMLA integration by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13747
* [ROCm][Quantization][Kernel] Use FP8 FNUZ when OCP flag is 0 or undefined by @HollowMan6 in https://github.com/vllm-project/vllm/pull/13851
* Use CUDA 12.4 as default for release and nightly wheels by @mgoin in https://github.com/vllm-project/vllm/pull/12098
* [misc] Rename Ray ADAG to Compiled Graph by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13928
* [ROCm][V1] Update reshape_and_cache to properly work with CUDA graph padding by @SageMoore in https://github.com/vllm-project/vllm/pull/13922
* [V1][Metrics] Handle preemptions by @markmc in https://github.com/vllm-project/vllm/pull/13169
* [CI/Build] Add examples/ directory to be labelled by `mergify` by @b8zhong in https://github.com/vllm-project/vllm/pull/13944
* [Misc] fixed 'required' is an invalid argument for positionals by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13948
* [PP] Correct cache size check by @zhengy001 in https://github.com/vllm-project/vllm/pull/13873
* Fix test_block_fp8.py test for MoE by @mgoin in https://github.com/vllm-project/vllm/pull/13915
* [VLM] Support multimodal inputs for Florence-2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/13320
* [Model] Deepseek GGUF support  by @SzymonOzog in https://github.com/vllm-project/vllm/pull/13167
* Update quickstart.md by @observerw in https://github.com/vllm-project/vllm/pull/13958
* Deduplicate `.pre-commit-config.yaml`'s `exclude` by @hmellor in https://github.com/vllm-project/vllm/pull/13967
* [bugfix] Fix profiling for RayDistributedExecutor by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13945
* Update LMFE version to v0.10.11 to support new versions of transformeâ€¦ by @noamgat in https://github.com/vllm-project/vllm/pull/13930
* [Bugfix] Fix qwen2.5-vl overflow issue by @Isotr0py in https://github.com/vllm-project/vllm/pull/13968
* [VLM] Generalized prompt updates for multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13964
* [Attention] MLA support for V1 by @chenyang78 in https://github.com/vllm-project/vllm/pull/13789
* Bump azure/setup-helm from 4.2.0 to 4.3.0 by @dependabot in https://github.com/vllm-project/vllm/pull/13742
* [VLM] Deprecate legacy input mapper for OOT multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13979
* [ROCm] Fix the Kernels, Core, and Prefix Caching AMD CI groups by @SageMoore in https://github.com/vllm-project/vllm/pull/13970
* [V1][Minor] Minor cleanup for GPU Model Runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13983
* [core] Perf improvement for DSv3 on AMD GPUs by @qli88 in https://github.com/vllm-project/vllm/pull/13718
* [Attention] Flash MLA for V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/13867
* [Model][Speculative Decoding] Expand DeepSeek MTP code to support k > n_predict by @benchislett in https://github.com/vllm-project/vllm/pull/13626
* [Misc] Print FusedMoE detail info by @jeejeelee in https://github.com/vllm-project/vllm/pull/13974
* [V1]`SupportsV0Only` protocol for model definitions by @ywang96 in https://github.com/vllm-project/vllm/pull/13959
* [Bugfix] Check that number of images matches number of <|image|> tokens with mllama by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/13911
* [Doc] Move multimodal Embedding API example to Online Serving page by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14017
* [Bugfix][Disaggregated] patch the inflight batching on the decode node in SimpleConnector to avoid hangs in SimpleBuffer (nccl based) by @hasB4K in https://github.com/vllm-project/vllm/pull/13987
* Use smaller embedding model when not testing model specifically by @hmellor in https://github.com/vllm-project/vllm/pull/13891
* [Hardware][Intel-Gaudi] Regional compilation support by @Kacper-Pietkun in https://github.com/vllm-project/vllm/pull/13213
* [V1][Minor] Restore V1 compatibility with LLMEngine class by @Ryp in https://github.com/vllm-project/vllm/pull/13090
* Update AutoAWQ docs by @hmellor in https://github.com/vllm-project/vllm/pull/14042
* [Bugfix] Fix MoeWNA16Method activation by @jeejeelee in https://github.com/vllm-project/vllm/pull/14024
* [VLM][Bugfix] Enable specifying prompt target via index by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14038
* [Bugfix] Initialize attention bias on the same device as Query/Key/Value for QwenVL Series by @LouieYang in https://github.com/vllm-project/vllm/pull/14031
* [Doc] Fix ROCm documentation by @b8zhong in https://github.com/vllm-project/vllm/pull/14041
* Fix entrypoint tests for embedding models by @hmellor in https://github.com/vllm-project/vllm/pull/14052
* [V1][TPU] Integrate the new ragged paged attention kernel with vLLM v1 on TPU by @vanbasten23 in https://github.com/vllm-project/vllm/pull/13379
* [v1] Cleanup the BlockTable in InputBatch by @heheda12345 in https://github.com/vllm-project/vllm/pull/13977
* Add RELEASE.md by @atalman in https://github.com/vllm-project/vllm/pull/13926
* [v1] Move block pool operations to a separate class by @heheda12345 in https://github.com/vllm-project/vllm/pull/13973
* [core] Bump ray to 2.43 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/13994
* [torch.compile] Fix RMSNorm + quant fusion in the non-cutlass-fp8 case, rename RedundantReshapesPass to NoopEliminationPass by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10902
* [Docs] Add `pipeline_parallel_size` to optimization docs by @b8zhong in https://github.com/vllm-project/vllm/pull/14059
* [Bugfix] Add file lock for ModelScope download by @jeejeelee in https://github.com/vllm-project/vllm/pull/14060
* [Misc][Kernel]: Add GPTQAllSpark Quantization by @wyajieha in https://github.com/vllm-project/vllm/pull/12931
* [Bugfix][V1][Minor] Fix shutting_down flag checking in V1 MultiprocExecutor by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/14053
* [Documentation] Add more deployment guide for Kubernetes deployment by @KuntaiDu in https://github.com/vllm-project/vllm/pull/13841
* [Doc] Consolidate `whisper` and `florence2` examples by @Isotr0py in https://github.com/vllm-project/vllm/pull/14050
* [V1][Minor] Do not print attn backend twice by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13985
* [ROCm][V1][Bugfix] Add get_builder_cls method to the ROCmAttentionBackend class by @SageMoore in https://github.com/vllm-project/vllm/pull/14065
* [v1][Bugfix] Only cache blocks that are not in the prefix cache by @heheda12345 in https://github.com/vllm-project/vllm/pull/14073
* [v1] Add `__repr__` to KVCacheBlock to avoid recursive print by @heheda12345 in https://github.com/vllm-project/vllm/pull/14081
* [Model] Add LoRA support for TransformersModel by @jeejeelee in https://github.com/vllm-project/vllm/pull/13770
* [Misc] Accurately capture the time of loading weights by @waltforme in https://github.com/vllm-project/vllm/pull/14063
* [Doc] Source building add clone step by @qux-bbb in https://github.com/vllm-project/vllm/pull/14086
* [v0][structured output] Support reasoning output by @gaocegege in https://github.com/vllm-project/vllm/pull/12955
* Update deprecated Python 3.8 typing by @hmellor in https://github.com/vllm-project/vllm/pull/13971
* [Bugfix] Explicitly include "omp.h" for MacOS to avoid installation failure by @realShengYao in https://github.com/vllm-project/vllm/pull/14051
* [Misc] typo find in deepseek_v2  by @noooop in https://github.com/vllm-project/vllm/pull/14106
* [Misc][Platform] Move use allgather to platform by @MengqingCao in https://github.com/vllm-project/vllm/pull/14010
* [Build] Make sure local main branch is synced when VLLM_USE_PRECOMPILED=1 by @comaniac in https://github.com/vllm-project/vllm/pull/13921
* [V1] Refactor parallel sampling support by @markmc in https://github.com/vllm-project/vllm/pull/13774
* Improve the docs for `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/14147
* [ROCm] Faster Custom Paged Attention kernels by @tjtanaa in https://github.com/vllm-project/vllm/pull/12348
* Fix `head_dim` not existing in all model configs (Transformers backend) by @hmellor in https://github.com/vllm-project/vllm/pull/14141
* [V0][Metrics] Remove unimplemented `vllm:tokens_total` by @markmc in https://github.com/vllm-project/vllm/pull/14134
* [V0][Metrics] Deprecate some KV/prefix cache metrics by @markmc in https://github.com/vllm-project/vllm/pull/14136
* [V1] Simplify stats logging by @njhill in https://github.com/vllm-project/vllm/pull/14082
* [WIP][[V1][Metrics] Implement max_num_generation_tokens,  request_params_n, and request_params_max_tokens metrics by @markmc in https://github.com/vllm-project/vllm/pull/14055
* [Bugfix] Allow shared_experts skip quantization for DeepSeekV2/V3 by @mgoin in https://github.com/vllm-project/vllm/pull/14100
* [Kernel] Optimize moe intermediate_cache usage by @mgoin in https://github.com/vllm-project/vllm/pull/13625
* [Docs] Add GPTQModel by @Qubitium in https://github.com/vllm-project/vllm/pull/14056
* [v1] Add comments to the new ragged paged attention Pallas kernel by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14155
* [Model] Add support for GraniteMoeShared models by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/13313
* [core] moe fp8 block quant tuning support by @divakar-amd in https://github.com/vllm-project/vllm/pull/14068
* [Misc] Remove lru_cache in NvmlCudaPlatform by @comaniac in https://github.com/vllm-project/vllm/pull/14156
* [core] Pass all driver env vars to ray workers unless excluded by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14099
* Use math.prod instead of np.prod for trivial ops by @zhanwenchen in https://github.com/vllm-project/vllm/pull/14142
* Fix benchmark_moe.py tuning for CUDA devices by @mgoin in https://github.com/vllm-project/vllm/pull/14164
* [platform] add debug logging during inferring the device type by @youkaichao in https://github.com/vllm-project/vllm/pull/14195
* [sleep mode] error out with expandable_segments by @youkaichao in https://github.com/vllm-project/vllm/pull/14189
* [doc] add "Failed to infer device type" to faq by @youkaichao in https://github.com/vllm-project/vllm/pull/14200
* [Bugfix] Restrict MacOS CPU detection by @mgoin in https://github.com/vllm-project/vllm/pull/14210
* [V1][BugFix] Fix remaining sync engine client shutdown errors/hangs by @njhill in https://github.com/vllm-project/vllm/pull/13869
* [V0][Metrics] Deprecate some questionable request time metrics by @markmc in https://github.com/vllm-project/vllm/pull/14135
* [V1][Molmo] Fix get_multimodal_embeddings() in molmo.py by @lk-chen in https://github.com/vllm-project/vllm/pull/14161
* add cutlass support for blackwell fp8 gemm by @kushanam in https://github.com/vllm-project/vllm/pull/13798
* [TPU][Profiler] Support start_profile/stop_profile in TPU worker by @lsy323 in https://github.com/vllm-project/vllm/pull/13988
* Fix performance when `--generation-config` is not `None` by @hmellor in https://github.com/vllm-project/vllm/pull/14223
* [Frontend] Do `prompt_logprobs` clamping for chat as well as completions by @hmellor in https://github.com/vllm-project/vllm/pull/14225
* [Docs] Update Dockerfile dependency image by @mgoin in https://github.com/vllm-project/vllm/pull/14215
* [v1][Metrics] Add design doc by @markmc in https://github.com/vllm-project/vllm/pull/12745
* Serialize using safetensors for KV caches by @KuntaiDu in https://github.com/vllm-project/vllm/pull/14228
* Clean up unused padding_idx variables across many model definitions by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13240
* [ROCm] Disable a few more kernel tests that are broken on ROCm by @SageMoore in https://github.com/vllm-project/vllm/pull/14145
* [V1][TPU] TPU multimodal model support for ragged attention by @mgoin in https://github.com/vllm-project/vllm/pull/14158
* [misc] announce china meetup by @youkaichao in https://github.com/vllm-project/vllm/pull/14248
* Moved numba from common requirements to cuda/rocm specific requirements by @npanpaliya in https://github.com/vllm-project/vllm/pull/14199
* Disable GPTQ AllSpark kernels for CUDA Compiler < 12.0 by @mgoin in https://github.com/vllm-project/vllm/pull/14157
* [Bugfix] Fix gptq_marlin for deepseek-v3 by @rainkert in https://github.com/vllm-project/vllm/pull/13750
* [V1][Bugfix] Do not reset prefix caching metrics by @comaniac in https://github.com/vllm-project/vllm/pull/14235
* [Model] New model support for Phi-4-multimodal-instruct by @congcongchen123 in https://github.com/vllm-project/vllm/pull/14119
* [V1] EP/TP MoE + DP Attention by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13931
* [platforms] improve rocm debugging info by @youkaichao in https://github.com/vllm-project/vllm/pull/14257
* Temporarily disable test_awq_gemm_opcheck by @mgoin in https://github.com/vllm-project/vllm/pull/14251
* [Frontend] Allow return_tokens_as_token_ids to be passed as a request param by @benchislett in https://github.com/vllm-project/vllm/pull/14066
* [Misc][V1] Avoid using `envs.VLLM_USE_V1` in mm processing by @ywang96 in https://github.com/vllm-project/vllm/pull/14256
* [Bugfix][V1] Fix allowed_token_ids for v1 Sampler by @houseroad in https://github.com/vllm-project/vllm/pull/14169
* [Doc] Update nginx guide: remove privileged from vllm container run and add target GPU ID by @iacolippo in https://github.com/vllm-project/vllm/pull/14217
* [Doc] [3/N] Refer code examples for common cases in dev multimodal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14278
* Small update for external_launcher backend docs by @zhe-thoughts in https://github.com/vllm-project/vllm/pull/14288
* [V1][Frontend] Add Testing For V1 Runtime Parameters by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14159
* [LoRA] Remove linear hack outside transformers backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/14177
* [Misc] Add Qwen2MoeForCausalLM moe tuning support  by @jeejeelee in https://github.com/vllm-project/vllm/pull/14276
* [Doc] Fixed typo in prefix_caching.md by @DaividFrank in https://github.com/vllm-project/vllm/pull/14293
* [Bugfix] Fix broken vision language example by @Isotr0py in https://github.com/vllm-project/vllm/pull/14292
* [Docs] Add Meta Slides by @simon-mo in https://github.com/vllm-project/vllm/pull/14297
* [V1][Minor] Remove obsolete FIXME comment by @njhill in https://github.com/vllm-project/vllm/pull/14304
* Deprecate `best_of` Sampling Parameter in anticipation for vLLM V1 by @vincent-4 in https://github.com/vllm-project/vllm/pull/13997
* [V1][BugFix] Fix for mixed top_k batch by @njhill in https://github.com/vllm-project/vllm/pull/14301
* [misc] Add FlashMLA as a new option of VLLM_ATTENTION_BACKEND env by @yangsijia-serena in https://github.com/vllm-project/vllm/pull/14267
* [V1][Easy] Add empty allowed_token_ids in the v1 sampler test by @houseroad in https://github.com/vllm-project/vllm/pull/14308
* [Bugfix] Fix DeepSeek MTP crash when using TP1ModelRunner with CUDA graph due to shape mismatch by @pyc96 in https://github.com/vllm-project/vllm/pull/14237
* [Bugfix] Remove num_tokens_across_dp by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14302
* [BugFix] Fix prefix caching V0 MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14255
* [CI/Build] Use spawn multiprocessing mode for V1 test pipeline by @russellb in https://github.com/vllm-project/vllm/pull/14243
* Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM by @mgoin in https://github.com/vllm-project/vllm/pull/13917
* [Build] Add UV_HTTP_TIMEOUT to avoid timeout during installation by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13850
* [BugFix] MLA + V1, illegal memory access and accuracy issues by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14253
* [misc] Mention `ray list nodes` command to troubleshoot ray issues by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14318
* [Bugfix][Structured Output] Support outlines engine with reasoning outputs for DeepSeek R1 by @gaocegege in https://github.com/vllm-project/vllm/pull/14114
* [V1] LoRA - Enable more V1 tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/14315
* [Bugfix][CI] ALiBi test case in xformers multi_query_kv_attention by @NickLucche in https://github.com/vllm-project/vllm/pull/11301
* [Hardware] Update the flash attn tag to support Blackwell by @pavanimajety in https://github.com/vllm-project/vllm/pull/14244
* [Model] Update Paligemma multimodal processing with PromptUpdate  by @kylehh in https://github.com/vllm-project/vllm/pull/14015
* [VLM] Support Pixtral-HF on V1 by @lk-chen in https://github.com/vllm-project/vllm/pull/14275
* [Core] Optimizing cross-attention `QKVParallelLinear` computation by @NickLucche in https://github.com/vllm-project/vllm/pull/12325
* [Frontend][Docs] Transcription API streaming by @NickLucche in https://github.com/vllm-project/vllm/pull/13301
* [Doc] Update reasoning with stream example to use OpenAI library by @liuyanyi in https://github.com/vllm-project/vllm/pull/14077
* [Doc] Correct beam_search using in generative_models.md by @upayuryeva in https://github.com/vllm-project/vllm/pull/14363
* [Kernel] [V1] Improved performance for V1 Triton (ROCm) backend  by @tdoublep in https://github.com/vllm-project/vllm/pull/14152
* [Bugfix][Core] fix abort_seq_group and memory leak when n>1 by @courage17340 in https://github.com/vllm-project/vllm/pull/14326
* [Core] Don't use cache during multi-modal profiling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14336
* [Doc] Fix date typo in README.md by @jitseklomp in https://github.com/vllm-project/vllm/pull/14366
* [RLHF] use worker_extension_cls for compatibility with V0 and V1 by @youkaichao in https://github.com/vllm-project/vllm/pull/14185
* Reinstate `best_of` for V0 by @hmellor in https://github.com/vllm-project/vllm/pull/14356
* Adding cpu inference with VXE ISA for s390x architecture by @dilipgb in https://github.com/vllm-project/vllm/pull/12613
* Add authors to license header. by @tdoublep in https://github.com/vllm-project/vllm/pull/14371
* Fix mla prefill context performance by @ZhongYingMatrix in https://github.com/vllm-project/vllm/pull/13897
* [V1] Do not detokenize if sampling param detokenize is False by @hj-mistral in https://github.com/vllm-project/vllm/pull/14224
* [Distributed] Add enable_expert_parallel arg by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14305
* [CI/Build] Use uv python for docker rather than ppa:deadsnakes/ppa by @mgoin in https://github.com/vllm-project/vllm/pull/13569
* [CI] Disable spawn when running V1 Test by @tdoublep in https://github.com/vllm-project/vllm/pull/14345
* [Kernel] Add needs_fixed_stride_order tag to most GEMMs by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14306
* [Bugfix] Fix use_direct_call condition in FusedMoE layer for  by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14382
* [Bug] Fix Attention when ignored in by quant_method by @mgoin in https://github.com/vllm-project/vllm/pull/14313
* [V1][Bugfix] Standardize quantized kv cache rejection for attention backends by @mgoin in https://github.com/vllm-project/vllm/pull/14221
* [Docs] Add nsight guide to profiling docs by @mgoin in https://github.com/vllm-project/vllm/pull/14298
* [Hardware][TPU]Enable ragged paged attention kernel and resolve recompilation issue by @yaochengji in https://github.com/vllm-project/vllm/pull/14310
* [Doc] Fix a typo by @dyli-google in https://github.com/vllm-project/vllm/pull/14385
* [Bugfix] Correctly call `cudaProfilerStop` in benchmarks script by @b8zhong in https://github.com/vllm-project/vllm/pull/14183
* [Perf] Reduce MLA CPU overheads in V1 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14384
* [FP8] Refactor apply_fp8_linear and apply_fp8_linear_generic into an object by @ProExpertProg in https://github.com/vllm-project/vllm/pull/14390
* [BugFix] Illegal Memory Access in the blockwise cutlass fp8 GEMMs by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14396
* [Bugfix] Fix JambaForCausalLM LoRA  by @jeejeelee in https://github.com/vllm-project/vllm/pull/14370
* [Build] Add nightly wheel fallback when latest commit wheel unavailable by @Isotr0py in https://github.com/vllm-project/vllm/pull/14358
* OpenVINO: added CPU-like conditions by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/14338
* [GH] Auto-apply multi-modality label to relevant PRs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14402
* correct wrong markdown syntax by @vincent-pli in https://github.com/vllm-project/vllm/pull/14414
* [Bugfix] Further clean up LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14422
* [Bugfix] Clean up multi-modal processors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14417
* [Misc] Set default value of seed to None by @SmartManoj in https://github.com/vllm-project/vllm/pull/14274
* [BUGFIX] Skip tokenization support for throughput benchmark by @maleksan85 in https://github.com/vllm-project/vllm/pull/12712
* Fix missing `kv_caches` and `attn_metadata` in `OpenVINOCausalLM` by @hmellor in https://github.com/vllm-project/vllm/pull/14271
* Use the optimized block sizes after tuning the kernel. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14329
* [V1][Core] Support for Structured Outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/12388
* [Doc] Update prefix_caching.md to match the example image by @York-RDWang in https://github.com/vllm-project/vllm/pull/14420
* [Benchmarks] Make detokenization optional in benchmark scripts by @JArnoldAMD in https://github.com/vllm-project/vllm/pull/11697
* [Kernel] optimize performance of gptq marlin kernel when n is small by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/14138
* [Misc] Add Phi4-MM example by @jeejeelee in https://github.com/vllm-project/vllm/pull/14343
* [v1] torch.compile integration explanation by @youkaichao in https://github.com/vllm-project/vllm/pull/14437
* [V1] Eagerly remove finished requests from the batch by @njhill in https://github.com/vllm-project/vllm/pull/14388
* [V1][Metrics] Fix traceback with preemptions+LoRA by @markmc in https://github.com/vllm-project/vllm/pull/14220
* [Bugfix] Fix torch_xla which can't handle None seed introduced in #14274 by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14459
* [V1] Prompt logprobs + APC compatibility; prompt logprobs reqs cannot fill APC by @afeldman-nm in https://github.com/vllm-project/vllm/pull/13949
* [Bugfix][V1] Handle MLA in kv_cache_interface by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14462
* Revert "[Perf] Reduce MLA CPU overheads in V1 (#14384)" by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14471
* [Bugfix][Disaggregated] Add a check in send_kv_caches_and_hidden_states and fix the reshape of the KVCache by @hasB4K in https://github.com/vllm-project/vllm/pull/14369
* [MISC][V1] Register process killing handler only in the main thread by @comaniac in https://github.com/vllm-project/vllm/pull/14380
* [core] add `extra_args` to `SamplingParams` by @akeshet in https://github.com/vllm-project/vllm/pull/13300
* [CI/Build] refactor: set timezone of container to UTC by @bufferoverflow in https://github.com/vllm-project/vllm/pull/12888
* Default to `generation_config` from model by @hmellor in https://github.com/vllm-project/vllm/pull/12622
* [Doc]add doc for Qwen models tool calling by @WangErXiao in https://github.com/vllm-project/vllm/pull/14478
* [Doc] Added QwQ-32B to the supported models list in the reasoning outâ€¦ by @WangErXiao in https://github.com/vllm-project/vllm/pull/14479
* [Bugfix] Make the deviceprofiler include LoRA memory. by @jeejeelee in https://github.com/vllm-project/vllm/pull/14469
* Add training doc signposting to TRL by @hmellor in https://github.com/vllm-project/vllm/pull/14439
* [Build/BugFix] Fix hopper 12.8 build by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14354
* Add RLHF document by @hmellor in https://github.com/vllm-project/vllm/pull/14482
* [CI/Build] Use a fixed seed to avoid flaky tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14480
* [V1] TPU - Add tensor parallel support via Ray by @alexm-redhat in https://github.com/vllm-project/vllm/pull/13618
* [VLM] Add TP support for Phi-4-MM by @Isotr0py in https://github.com/vllm-project/vllm/pull/14453
* [Misc] add `use_tqdm_on_load` to reduce logs by @aarnphm in https://github.com/vllm-project/vllm/pull/14407
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/13776
* [benchmarks] Add option to use unique jsonschema for each request by @russellb in https://github.com/vllm-project/vllm/pull/14457
* [Misc] Don't run ruff at all on 3rd party libs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14493
* Move requirements into their own directory by @hmellor in https://github.com/vllm-project/vllm/pull/12547
* [Bugfix] DeepSeek Accuracy by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14476
* [Bugfix] Fix profiling OOM and decouple encoder multimodal profiling by @Isotr0py in https://github.com/vllm-project/vllm/pull/14361
* Update CODEOWNERS for structured output by @russellb in https://github.com/vllm-project/vllm/pull/14496
* [Misc] Upgrade to Python 3.9 typing for additional directories by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14492
* [V1] Support bad_words in sampler by @22quinn in https://github.com/vllm-project/vllm/pull/13376
* Revert "[V1][Core] Fix memory issue with logits & sampling" by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14504
* [Attention] Default to FlashMLA backend for MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14451
* [V1][TPU] Remove unnecessary padding for running on TPU. by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14467
* [Feat] Support chunked prefill for LMCache connector by @YaoJiayi in https://github.com/vllm-project/vllm/pull/14505
* [Bugfix] Fix tqdm progress bar when SamplingParams.n > 1 by @yanyc428 in https://github.com/vllm-project/vllm/pull/12428
* [Bugfix] Revert QKVCrossParallelLinear usage in Mllama to keep BNB quantization work by @Isotr0py in https://github.com/vllm-project/vllm/pull/14498
* [Hardware][TPU] Fix the recompiling issue in logits processor after warmup by @yaochengji in https://github.com/vllm-project/vllm/pull/14510
* [Misc] Ensure out-of-tree quantization method recognize by cli args by @liuyanyi in https://github.com/vllm-project/vllm/pull/14328
* [Bugfix] Wrong requirements path - rocm by @martinhoyer in https://github.com/vllm-project/vllm/pull/14527
* [Feature] Consolidate performance benchmark datasets by @JenZhao in https://github.com/vllm-project/vllm/pull/14036
* [Misc] Add log information for handle_process_request. by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/14130
* [Docs] Mention `model_impl` arg when explaining Transformers fallback by @hmellor in https://github.com/vllm-project/vllm/pull/14552
* [Frontend] support image embeds by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/13955
* [Kernel] Add more dtype support for GGUF kernels by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14043
* [Doc] Update PaliGemma note to a warning by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14565
* Correct capitalisation: `Github` -> `GitHub` by @hmellor in https://github.com/vllm-project/vllm/pull/14561
* [V1][Bugfix] Fix handing of `second_per_grid_ts` for Qwen2-VL & Qwen2.5-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/14548
* Correct capitalisation: `VLLM` -> `vLLM` by @hmellor in https://github.com/vllm-project/vllm/pull/14562
* [Docs] Make installation URLs nicer by @hmellor in https://github.com/vllm-project/vllm/pull/14556
* [Bugfix][v1] fixed llava-hf/llava-1.5-7b-hf is broken on V1 by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/14554
* [Perf] Improve MLA on V1 by @simon-mo in https://github.com/vllm-project/vllm/pull/14540
* [Minor] Update the tqdm bar for parallel sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14571
* [V1] LoRA - Add triton kernels for V1 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/13096
* Fix typo in benchmark_serving_structured_output.py by @russellb in https://github.com/vllm-project/vllm/pull/14566
* [V1] Prevent xgrammar from breaking TPU support by @russellb in https://github.com/vllm-project/vllm/pull/14575
* [Kernel] moe wna16 cuda kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/13321
* [MISC][V1] Handle exception of current_platform.get_device_name() in arg_utils by @comaniac in https://github.com/vllm-project/vllm/pull/14379
* [Neuron] Add Neuron device communicator for vLLM v1 by @gnovack in https://github.com/vllm-project/vllm/pull/14085
* [neuron] add reshape_and_cache by @liangfu in https://github.com/vllm-project/vllm/pull/14391
* [V1][PP] Do not block engine core when no requests to schedule by @comaniac in https://github.com/vllm-project/vllm/pull/14585
* [Bugfix] Fix FP16 overflow for DeepSeek V2 by @Concurrensee in https://github.com/vllm-project/vllm/pull/13232
* [V1][Core] Fix memory issue with logits & sampling by @ywang96 in https://github.com/vllm-project/vllm/pull/14508
* [Misc] Correct deepseek-vl2 chat template by @Isotr0py in https://github.com/vllm-project/vllm/pull/14558
* [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync by @cynthieye in https://github.com/vllm-project/vllm/pull/14377
* [VLM] Cleanup siglip legacy code and fix broken paligemma multimodal processor by @Isotr0py in https://github.com/vllm-project/vllm/pull/14602
* benchmarks: simplify test jsonschema by @russellb in https://github.com/vllm-project/vllm/pull/14567
* dynamic distpatch of fp8 kernels by @jeffdaily in https://github.com/vllm-project/vllm/pull/14245
* [Bugfix] Update `--hf-overrides` for `Alibaba-NLP/gte-Qwen2` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14609
* Uninstall dependencies before installing requirements/tpu.txt by @richardsliu in https://github.com/vllm-project/vllm/pull/14586
* [V1] Add regex structured output support with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/14590
* docs: Add documentation for s390x cpu implementation by @dilipgb in https://github.com/vllm-project/vllm/pull/14198
* [BugFix/Build] Fix sparse kernels not getting built on hopper by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14572
* [Hardware][Intel GPU] upgrade IPEX dependency to 2.6.10.  by @jikunshang in https://github.com/vllm-project/vllm/pull/14564
* [V1] Remove cache from StructuredOutputManager by @russellb in https://github.com/vllm-project/vllm/pull/14622
* fix some typos : supported_head_sizes by @hackty in https://github.com/vllm-project/vllm/pull/14627
* [V1] Delay all xgrammar usage until needed by @russellb in https://github.com/vllm-project/vllm/pull/14616
* Fix run_tpu_test by @richardsliu in https://github.com/vllm-project/vllm/pull/14641
* [V1][TPU] Pad the block_table.shape[1] so the ragged paged attention can handle correctly by @vanbasten23 in https://github.com/vllm-project/vllm/pull/14597
* [Bugfix][V1][PP] Only warmup sampler at last PP rank by @comaniac in https://github.com/vllm-project/vllm/pull/14643
* [release] Add commands to clean up logs on TPU release node by @khluu in https://github.com/vllm-project/vllm/pull/14642
* [Feature] Add `vllm bench` CLI by @randyjhc in https://github.com/vllm-project/vllm/pull/13993
* [core][V1] pluggable scheduler by @joerunde in https://github.com/vllm-project/vllm/pull/14466
* [Doc] Update benchmarks README by @JenZhao in https://github.com/vllm-project/vllm/pull/14646
* [Model] Extend Ultravox to accept audio longer than 30s by @farzadab in https://github.com/vllm-project/vllm/pull/13631
* [V1][Core] Support MistralTokenizer for Structured Output by @aarnphm in https://github.com/vllm-project/vllm/pull/14625
* [Core] Refactor `QKVCrossParallelLinear` implementation to support BNB 4-bit quantization by @Isotr0py in https://github.com/vllm-project/vllm/pull/14545
* [Kernel] GGUF MoE kernel by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14613
* [V1][Bugfix][Spec Decode] Fix incorrect outputs in V1 speculative decoding due to batch indexing by @benchislett in https://github.com/vllm-project/vllm/pull/14645
* [Kernel] Add ModelOpt FP4 Checkpoint Support by @pavanimajety in https://github.com/vllm-project/vllm/pull/12520
* [CPU] Upgrade CPU backend to torch-2.6 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/13381
* [ROCm][Bugfix] Ensure that the moe_wna16_gemm kernel is not built on ROCm platforms. by @SageMoore in https://github.com/vllm-project/vllm/pull/14629
* [Model] Add support for Gemma 3 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14660
* [Bugfix] Missing thumbnail from NVLM-D processor by @ameyanjarlekar in https://github.com/vllm-project/vllm/pull/14633
* [ROCm] Enable chunked prefill/paged attention in MLA on ROCm by @SageMoore in https://github.com/vllm-project/vllm/pull/14316
* [FEAT] [ROCm] [Embedding] Add encoder-only model support into ROCm Flash Attention to enable embedding models. by @tjtanaa in https://github.com/vllm-project/vllm/pull/14664
* [BugFix][V1] Fix parallel sampling finishing/aborts by @njhill in https://github.com/vllm-project/vllm/pull/14512
* [V1] Allow sliding window + prefix caching by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13069
* [release] Add force remove for TPU logs by @khluu in https://github.com/vllm-project/vllm/pull/14697
* [bugfix] fixup warning message for plugged schedulers for v1 by @joerunde in https://github.com/vllm-project/vllm/pull/14700
* Add ray[data] as tpu dependency by @richardsliu in https://github.com/vllm-project/vllm/pull/14691
* [ROCm][FP8] Fix for adjustments needed only for fnuz by @gshtras in https://github.com/vllm-project/vllm/pull/14689
* [BugFix][TritonMLA] Process weights after model loading for GGUF by @tywuAMD in https://github.com/vllm-project/vllm/pull/14555
* [Config][Disaggregated] Add timeout configuration for the torch.store and add KVTransferConfig.kv_connector_extra_config by @hasB4K in https://github.com/vllm-project/vllm/pull/14367
* [V1][TPU] Add assertion on multi-step-scheduler by @lsy323 in https://github.com/vllm-project/vllm/pull/14707
* [Quant] BartModel SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/14699
* [Quant] Bamba SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/14698
* [Bugfix] Fix chunked prefill for GGUF by @SzymonOzog in https://github.com/vllm-project/vllm/pull/14666
* [CI/Build]  Delete ultravox LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14730
* [Bugfix] fix benchmark moe by @jeejeelee in https://github.com/vllm-project/vllm/pull/14653
* [VLM] Support pan-and-scan for Gemma3 multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14672
* [VLM] Support loading InternVideo2.5 models as original InternVLChatModel by @Isotr0py in https://github.com/vllm-project/vllm/pull/14738
* [Bugfix] Fix prompt format of GLM4V by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14539
* [V1][Minor] Minor enhancements on scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14732
* [Misc] Clean up processor tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14771
* [V1][Core] using cached vocab_size for Structured Outputs by @aarnphm in https://github.com/vllm-project/vllm/pull/14630
* [V1] Detokenizer: Respect Stop Tokens + `not include_stop_str_in_output` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/14624
* [Attention] Remove slow setattr in MLA by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14769
* [Doc] Fix typo in documentation by @yasu52 in https://github.com/vllm-project/vllm/pull/14783
* [Doc] Fix small typo in Transformers fallback by @heheda12345 in https://github.com/vllm-project/vllm/pull/14791
* [V1] TPU - Enable prefix caching by default by @alexm-redhat in https://github.com/vllm-project/vllm/pull/14773
* forward fix PR 14245, restore build on ROCm 6.2 by @jeffdaily in https://github.com/vllm-project/vllm/pull/14709
* [V1] Move OOM check into sampler run by @ywang96 in https://github.com/vllm-project/vllm/pull/14728
* [V1] Temporarily disable FlashInfer Rejection Sampler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14788
* [Kernel] LoRA - Enable CUDAGraphs for V1 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/14626
* [Kernel] [V1] Further optimizations to ROCm (Triton) Backend to better handle GQA. by @tdoublep in https://github.com/vllm-project/vllm/pull/14431
* [Bugfix][IPEX] Add `VLLM_CPU_MOE_PREPACK` to allow disabling MoE prepack when CPU does not support it by @gau-nernst in https://github.com/vllm-project/vllm/pull/14681
* [ci] Reduce number of tests in fastcheck by @khluu in https://github.com/vllm-project/vllm/pull/14782
* [Misc][Minor] Simplify `SamplingParams.__post_init__()` by @njhill in https://github.com/vllm-project/vllm/pull/14772
* [Neuron] flatten test parameterization for neuron attention kernels by @liangfu in https://github.com/vllm-project/vllm/pull/14712
* [Feature] Add visionarena offline support for benchmark_throughput by @JenZhao in https://github.com/vllm-project/vllm/pull/14654
* [CI] Fix missing example model id in processor test by @ywang96 in https://github.com/vllm-project/vllm/pull/14787
* [Attention] MLA get rid of materialization by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14770
* [Bugfix][Kernel][CPU] Fix num_tokens in CPU rotary embedding kernel by @gau-nernst in https://github.com/vllm-project/vllm/pull/14667
* [BugFix]Fix performance serving benchmark when enable profiling by @Potabk in https://github.com/vllm-project/vllm/pull/14737
* [Misc] Clean up type annotation for `SupportsMultiModal` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14794
* [Bugfix] Fix small typo in the example of Streaming delimiter by @bravo325806 in https://github.com/vllm-project/vllm/pull/14793
* [Misc] Gemma3ForConditionalGeneration supports LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/14797
* [V1][Minor] Minor code cleanup for scheduling metrics by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14800
* [Bugfix][W8A8] fixed cutlass block fp8 binding by @DefTruth in https://github.com/vllm-project/vllm/pull/14796
* [VLM] Various cleanup and fixes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14806
* [BugFix]: properly catch templating error when preprocess input by @gcalmettes in https://github.com/vllm-project/vllm/pull/13976
* [Bugfix] Fix Aria test loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14823
* [V1] Fix vocab size calculation for structured output by @russellb in https://github.com/vllm-project/vllm/pull/14826
* [Frontend] Fix log message to use http vs https by @russellb in https://github.com/vllm-project/vllm/pull/14774
* [V1][Metrics] Updated list of deprecated metrics in v0.8 by @markmc in https://github.com/vllm-project/vllm/pull/14695
* [Frontend] track server_load by @daniel-salib in https://github.com/vllm-project/vllm/pull/13950
* [Bugfix][Kernel]: Fix AllSpark kernel compilation errors and enable for CUDA < 12.0 by @wyajieha in https://github.com/vllm-project/vllm/pull/14430
* [release] Remove log cleanup commands from TPU job by @khluu in https://github.com/vllm-project/vllm/pull/14838
* Re-enable the AMD Entrypoints Test by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/14711
* [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  by @cyang49 in https://github.com/vllm-project/vllm/pull/14778
* [V1] Fix model parameterization for structured output tests by @russellb in https://github.com/vllm-project/vllm/pull/14833
* Update to torch==2.6.0 by @mgoin in https://github.com/vllm-project/vllm/pull/12721
* [CI] Add TPU v1 test by @richardsliu in https://github.com/vllm-project/vllm/pull/14834
* [Build/CI] Move ninja to common deps by @russellb in https://github.com/vllm-project/vllm/pull/14835
* [Build/CI] Upgrade aiohttp to incldue CVE fix by @russellb in https://github.com/vllm-project/vllm/pull/14840
* [Doc] More neutral K8s deployment guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/14084
* [Bugfix] Fix torch_xla in V0 which can't handle None seed introduced â€¦ by @yarongmu-google in https://github.com/vllm-project/vllm/pull/14844
* [Neuron][CI] update docker run command by @liangfu in https://github.com/vllm-project/vllm/pull/14829
* [Bugfix][V1] Fix flashinfer sampling by @DefTruth in https://github.com/vllm-project/vllm/pull/14815
* Revert "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Uâ€¦ by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/14848
* Disable outlines cache by default by @russellb in https://github.com/vllm-project/vllm/pull/14837
* [Misc] Remove misleading message in gemma2 and gemma3 by @Isotr0py in https://github.com/vllm-project/vllm/pull/14850
* [Misc][Easy] Annotate unused vars in the csrc files by @houseroad in https://github.com/vllm-project/vllm/pull/14798
* [V1] V1 Enablement Oracle  by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/13726
* [Docs] Add new East Coast vLLM Meetup slides to README and meetups.md by @simon-mo in https://github.com/vllm-project/vllm/pull/14852
* [CPU] Support FP8 KV cache by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/14741
* [Attention] Get rid of mla cache alignment by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14842
* [CI/Build] Delete LoRA bias test by @jeejeelee in https://github.com/vllm-project/vllm/pull/14849
* [V1][Structured Output] calculate vocab_size eagerly by @aarnphm in https://github.com/vllm-project/vllm/pull/14851
* [Doc] V1 user guide by @JenZhao in https://github.com/vllm-project/vllm/pull/13991
* [Build/CI] Upgrade jinja2 to get 3 moderate CVE fixes by @russellb in https://github.com/vllm-project/vllm/pull/14839
* [Bugfix] EAGLE output norm bug by @luyuzhe111 in https://github.com/vllm-project/vllm/pull/14464
* [VLM] Limit multimodal input cache by memory by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14805
* [CI][Intel GPU] refine intel GPU ci docker build by @jikunshang in https://github.com/vllm-project/vllm/pull/14860
* [Core] Expose API endpoint `/is_sleeping` by @waltforme in https://github.com/vllm-project/vllm/pull/14312
* [VLM] Merged multi-modal processor for Pixtral by @Flechman in https://github.com/vllm-project/vllm/pull/12211
* [Misc][Doc] Minor benchmark README update by @ywang96 in https://github.com/vllm-project/vllm/pull/14874
* [VLM] Clean up Phi-4-MM ViT implementation by @Isotr0py in https://github.com/vllm-project/vllm/pull/14812
* [V1] Remove V0 fallback for mistral-tokenizer by @ywang96 in https://github.com/vllm-project/vllm/pull/14873
* [Kernel] Add more tuned configs by @simon-mo in https://github.com/vllm-project/vllm/pull/14877
* [BugFix] Fix torch distributed stateless PG backend init by @njhill in https://github.com/vllm-project/vllm/pull/14870
* [V1] [Spec Decode] Fix ngram tests by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/14878
* [Bugfix] Limit profiling run sequence length by max_model_len by @kylesayrs in https://github.com/vllm-project/vllm/pull/14785
* [Bugfix] Explicitly disable Phi-4-multimodal in V1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14889
* Revert "[Bugfix] Limit profiling run sequence length by max_model_len (#14785) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14892
* [BugFix][V1] Fix overhead related to bad_words sampling when not in use by @njhill in https://github.com/vllm-project/vllm/pull/14894
* [V1][BugFix] Detect interleaved sliding window attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14896
* [Misc] Catching Ray Compiled Graph PP test failures for V1 by @ruisearch42 in https://github.com/vllm-project/vllm/pull/14847
* [Doc] Add guidance for using `ccache` with `pip install -e .` in doc by @vadiklyutiy in https://github.com/vllm-project/vllm/pull/14901
* [V1] Enable Entrypoints Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14903
* [CI] Fix Tool Calling Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/14898
* [CI/Build] Update defaults for test reproducibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/14893
* [V1] Optimize the overhead of rewinding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14905
* [V1][Minor] Add __repr__ to ConstantList by @WoosukKwon in https://github.com/vllm-project/vllm/pull/14907
* [BugFix] Fix MLA + V1 + TP==1 causing reinitialization of cuda context by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/14910
* [Misc] Replace os environ to monkeypatch in test suite by @t-sibiraj in https://github.com/vllm-project/vllm/pull/14516
* [Benchmark] Do not save detailed info to json by default by @simon-mo in https://github.com/vllm-project/vllm/pull/14879
* [V1] [Spec Decode] Support random sampling for spec decode by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/13933

## New Contributors
* @ajayvohra2005 made their first contribution in https://github.com/vllm-project/vllm/pull/13589
* @Edwinhr716 made their first contribution in https://github.com/vllm-project/vllm/pull/12913
* @Hongbosherlock made their first contribution in https://github.com/vllm-project/vllm/pull/12978
* @johnzheng1975 made their first contribution in https://github.com/vllm-project/vllm/pull/13668
* @JenZhao made their first contribution in https://github.com/vllm-project/vllm/pull/13594
* @bufferoverflow made their first contribution in https://github.com/vllm-project/vllm/pull/13011
* @cakeng made their first contribution in https://github.com/vllm-project/vllm/pull/12583
* @eli-b made their first contribution in https://github.com/vllm-project/vllm/pull/13785
* @YaoJiayi made their first contribution in https://github.com/vllm-project/vllm/pull/12953
* @edwardzjl made their first contribution in https://github.com/vllm-project/vllm/pull/13468
* @naromero77amd made their first contribution in https://github.com/vllm-project/vllm/pull/13623
* @henrylhtsang made their first contribution in https://github.com/vllm-project/vllm/pull/13797
* @tianyuzhou95 made their first contribution in https://github.com/vllm-project/vllm/pull/13863
* @b8zhong made their first contribution in https://github.com/vllm-project/vllm/pull/13736
* @Chenyaaang made their first contribution in https://github.com/vllm-project/vllm/pull/13860
* @observerw made their first contribution in https://github.com/vllm-project/vllm/pull/13958
* @qli88 made their first contribution in https://github.com/vllm-project/vllm/pull/13718
* @benchislett made their first contribution in https://github.com/vllm-project/vllm/pull/13626
* @hasB4K made their first contribution in https://github.com/vllm-project/vllm/pull/13987
* @Kacper-Pietkun made their first contribution in https://github.com/vllm-project/vllm/pull/13213
* @Ryp made their first contribution in https://github.com/vllm-project/vllm/pull/13090
* @LouieYang made their first contribution in https://github.com/vllm-project/vllm/pull/14031
* @vanbasten23 made their first contribution in https://github.com/vllm-project/vllm/pull/13379
* @atalman made their first contribution in https://github.com/vllm-project/vllm/pull/13926
* @wyajieha made their first contribution in https://github.com/vllm-project/vllm/pull/12931
* @qux-bbb made their first contribution in https://github.com/vllm-project/vllm/pull/14086
* @realShengYao made their first contribution in https://github.com/vllm-project/vllm/pull/14051
* @zhanwenchen made their first contribution in https://github.com/vllm-project/vllm/pull/14142
* @rainkert made their first contribution in https://github.com/vllm-project/vllm/pull/13750
* @congcongchen123 made their first contribution in https://github.com/vllm-project/vllm/pull/14119
* @iacolippo made their first contribution in https://github.com/vllm-project/vllm/pull/14217
* @zhe-thoughts made their first contribution in https://github.com/vllm-project/vllm/pull/14288
* @DaividFrank made their first contribution in https://github.com/vllm-project/vllm/pull/14293
* @vincent-4 made their first contribution in https://github.com/vllm-project/vllm/pull/13997
* @yangsijia-serena made their first contribution in https://github.com/vllm-project/vllm/pull/14267
* @pyc96 made their first contribution in https://github.com/vllm-project/vllm/pull/14237
* @upayuryeva made their first contribution in https://github.com/vllm-project/vllm/pull/14363
* @courage17340 made their first contribution in https://github.com/vllm-project/vllm/pull/14326
* @dilipgb made their first contribution in https://github.com/vllm-project/vllm/pull/12613
* @ZhongYingMatrix made their first contribution in https://github.com/vllm-project/vllm/pull/13897
* @hj-mistral made their first contribution in https://github.com/vllm-project/vllm/pull/14224
* @yaochengji made their first contribution in https://github.com/vllm-project/vllm/pull/14310
* @dyli-google made their first contribution in https://github.com/vllm-project/vllm/pull/14385
* @vincent-pli made their first contribution in https://github.com/vllm-project/vllm/pull/14414
* @York-RDWang made their first contribution in https://github.com/vllm-project/vllm/pull/14420
* @yarongmu-google made their first contribution in https://github.com/vllm-project/vllm/pull/14459
* @22quinn made their first contribution in https://github.com/vllm-project/vllm/pull/13376
* @yanyc428 made their first contribution in https://github.com/vllm-project/vllm/pull/12428
* @martinhoyer made their first contribution in https://github.com/vllm-project/vllm/pull/14527
* @gnovack made their first contribution in https://github.com/vllm-project/vllm/pull/14085
* @cynthieye made their first contribution in https://github.com/vllm-project/vllm/pull/14377
* @jeffdaily made their first contribution in https://github.com/vllm-project/vllm/pull/14245
* @hackty made their first contribution in https://github.com/vllm-project/vllm/pull/14627
* @randyjhc made their first contribution in https://github.com/vllm-project/vllm/pull/13993
* @ameyanjarlekar made their first contribution in https://github.com/vllm-project/vllm/pull/14633
* @tywuAMD made their first contribution in https://github.com/vllm-project/vllm/pull/14555
* @yasu52 made their first contribution in https://github.com/vllm-project/vllm/pull/14783
* @gau-nernst made their first contribution in https://github.com/vllm-project/vllm/pull/14681
* @Potabk made their first contribution in https://github.com/vllm-project/vllm/pull/14737
* @bravo325806 made their first contribution in https://github.com/vllm-project/vllm/pull/14793
* @daniel-salib made their first contribution in https://github.com/vllm-project/vllm/pull/13950
* @cyang49 made their first contribution in https://github.com/vllm-project/vllm/pull/14778
* @luyuzhe111 made their first contribution in https://github.com/vllm-project/vllm/pull/14464
* @Flechman made their first contribution in https://github.com/vllm-project/vllm/pull/12211
* @vadiklyutiy made their first contribution in https://github.com/vllm-project/vllm/pull/14901
* @t-sibiraj made their first contribution in https://github.com/vllm-project/vllm/pull/14516

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.7.3...v0.8.0rc1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.8.0rc1)

---

## v0.7.3: v0.7.3
**Published:** 2025-02-20

## Highlights
ðŸŽ‰ 253 commits from 93 contributors, including 29 new contributors! 

* Deepseek enhancements:
	* Support for DeepSeek Multi-Token Prediction, 1.69x speedup in low QPS scenarios (#12755)
	* AMD support: DeepSeek tunings, yielding 17% latency reduction (#13199)
	* Using FlashAttention3 for MLA (#12807)
	* Align the expert selection code path with official implementation (#13474)
	* Optimize moe_align_block_size for deepseek_v3 (#12850)
	* Expand MLA to support most types of quantization (#13181)
* V1 Engine:
	* LoRA Support (#10957, #12883)
	* Logprobs and prompt logprobs support (#9880), min_p sampling support (#13191), logit_bias in v1 Sampler (#13079)
	* Use msgpack for core request serialization (#12918)
	* Pipeline parallelism support (#12996, #13353, #13472, #13417, #13315)
	* Metrics enhancements: GPU prefix cache hit rate % gauge (#12592), iteration_tokens_total histogram (#13288),  several request timing histograms (#12644)
	* Initial speculative decoding support with ngrams (#12193, #13365)

### Model Support
* Enhancement to Qwen2.5-VL: BNB support (#12944), LoRA (#13261), Optimizations (#13155)
* Support GPTQModel Dynamic [2,3,4,8]bit GPTQ quantization (#7086)
* Support Unsloth Dynamic 4bit BnB quantization (#12974)
* IBM/NASA Prithvi Geospatial model  (#12830)
* Support Mamba2 (Codestral Mamba) (#9292), Bamba Model (#10909)
* Ultravox Model: Support v0.5 Release (#12912)
* `transformers` backend
	* Enable quantization support for `transformers` backend (#12960)
	* Set `torch_dtype` in `TransformersModel` (#13088)
* VLM:
	* Implement merged multimodal processor for Mllama (#11427), GLM4V (#12449), Molmo (#12966)
	* Separate text-only and vision variants of the same model architecture (#13157)

### Hardware Support
* Pluggable platform-specific scheduler (#13161)
* NVIDIA: Support nvfp4 quantization (#12784)
* AMD:
	* Per-Token-Activation Per-Channel-Weight FP8 (#12501)
	* Tuning for Mixtral on MI325 and Qwen MoE on MI300 (#13503), Mixtral8x7B on MI300 (#13577)
	* Add intial ROCm support to V1 (#12790)
* TPU: V1 Support (#13049)
* Neuron: Support Longer Sequences in NKI-based Flash PagedAttention and Improve Efficiency (#12921)
* Gaudi: 
	* Support Contiguous Cache Fetch  (#12139)
	* Enable long-contexts + LoRA support (#12812)

### Engine Feature
* Add sleep and wake up endpoint and v1 support (#12987)
* Add `/v1/audio/transcriptions` OpenAI API endpoint (#12909)

### Performance
* Reduce TTFT with concurrent partial prefills (#10235)
* LoRA - Refactor sgmv kernels (#13110)


### Others
* Make vLLM compatible with veRL (#12824)
* Fixes for cases of FA2 illegal memory access error (#12848) 
* choice-based structured output with xgrammar (#12632)
* Run v1 benchmark and integrate with PyTorch OSS benchmark database (#13068)

## What's Changed
* [Misc] Update w2 scale loading for GPTQMarlinMoE by @dsikka in https://github.com/vllm-project/vllm/pull/12757
* [Docs] Add Google Cloud Slides by @simon-mo in https://github.com/vllm-project/vllm/pull/12814
* [Attention] Use FA3 for MLA on Hopper by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12807
* [misc] Reduce number of config file requests to HuggingFace by @khluu in https://github.com/vllm-project/vllm/pull/12797
* [Misc] Remove unnecessary decode call by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12833
* [Kernel] Make rotary_embedding ops more flexible with input shape by @Isotr0py in https://github.com/vllm-project/vllm/pull/12777
* [torch.compile] PyTorch 2.6 and nightly compatibility by @youkaichao in https://github.com/vllm-project/vllm/pull/12393
* [Doc] double quote cmake package in build.inc.md by @jitseklomp in https://github.com/vllm-project/vllm/pull/12840
* [Bugfix] Fix unsupported FA version check for Turing GPU by @Isotr0py in https://github.com/vllm-project/vllm/pull/12828
* [V1] LoRA Support by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/10957
* Add Bamba Model by @fabianlim in https://github.com/vllm-project/vllm/pull/10909
* [MISC] Check space in the file names in the pre commit checks by @houseroad in https://github.com/vllm-project/vllm/pull/12804
* [misc] Revert # 12833 by @khluu in https://github.com/vllm-project/vllm/pull/12857
* [Bugfix] FA2 illegal memory access by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12848
* Make vllm compatible with verl by @ZSL98 in https://github.com/vllm-project/vllm/pull/12824
* [Bugfix] Missing quant_config in deepseek embedding layer by @SzymonOzog in https://github.com/vllm-project/vllm/pull/12836
* Prevent unecessary requests to huggingface hub by @maxdebayser in https://github.com/vllm-project/vllm/pull/12837
* [MISC][EASY] Break check file names into entry and args in the pre-commit hooks by @houseroad in https://github.com/vllm-project/vllm/pull/12880
* [Misc] Remove unnecessary detokenization in multimodal processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12868
* [Model] Add support for partial rotary embeddings in Phi3 model by @garg-amit in https://github.com/vllm-project/vllm/pull/12718
* [V1] Logprobs and prompt logprobs support by @afeldman-nm in https://github.com/vllm-project/vllm/pull/9880
* [ROCm] [Feature] [Doc] [Dockerfile] [BugFix] Support Per-Token-Activation Per-Channel-Weight FP8 Quantization Inferencing by @tjtanaa in https://github.com/vllm-project/vllm/pull/12501
* [V1] LM Eval With Streaming Integration Tests by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11590
* [Bugfix] Fix disagg hang caused by the prefill and decode communication issues by @houseroad in https://github.com/vllm-project/vllm/pull/12723
* [V1][Minor] Remove outdated comment by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12928
* [V1] Move KV block hashes from Request to KVCacheManager by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12922
* [Bugfix] Fix Qwen2_5_VLForConditionalGeneration packed_modules_mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/12905
* [Misc] Fix typo in the example file by @DK-DARKmatter in https://github.com/vllm-project/vllm/pull/12896
* [Bugfix] Fix multi-round chat error when mistral tokenizer is used by @zifeitong in https://github.com/vllm-project/vllm/pull/12859
* [bugfix] respect distributed_executor_backend in world_size=1 by @youkaichao in https://github.com/vllm-project/vllm/pull/12934
* [Misc] Add offline test for disaggregated prefill by @Shaoting-Feng in https://github.com/vllm-project/vllm/pull/12418
* [V1][Minor] Move cascade attn logic outside _prepare_inputs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12943
* [Build] Make pypi install work on CPU platform by @wangxiyuan in https://github.com/vllm-project/vllm/pull/12874
* [Hardware][Intel-Gaudi] Enable long-contexts + LoRA support for Intel Gaudi by @SanjuCSudhakaran in https://github.com/vllm-project/vllm/pull/12812
* [misc]  Add LoRA to benchmark_serving by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/12898
* [Misc] Log time consumption on weight downloading by @waltforme in https://github.com/vllm-project/vllm/pull/12926
* [CI] Resolve transformers-neuronx version conflict by @liangfu in https://github.com/vllm-project/vllm/pull/12925
* [Doc] Correct HF repository for TeleChat2 models by @waltforme in https://github.com/vllm-project/vllm/pull/12949
* [Misc] Add qwen2.5-vl BNB support by @Isotr0py in https://github.com/vllm-project/vllm/pull/12944
* [CI/Build] Auto-fix Markdown files by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12941
* [Bugfix] Remove unused seq_group_metadata_list from ModelInputForGPU by @ShangmingCai in https://github.com/vllm-project/vllm/pull/12935
* [bugfix] fix early import of flash attention by @youkaichao in https://github.com/vllm-project/vllm/pull/12959
* [VLM] Merged multi-modal processor for GLM4V by @jeejeelee in https://github.com/vllm-project/vllm/pull/12449
* [V1][Minor] Remove outdated comment by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12968
* [RFC] [Mistral] FP8 format by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/10130
* [V1] Cache `uses_mrope` in GPUModelRunner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12969
* [core] port pynvml into vllm codebase by @youkaichao in https://github.com/vllm-project/vllm/pull/12963
* [MISC] Always import version library first in the vllm package by @houseroad in https://github.com/vllm-project/vllm/pull/12979
* [core] improve error handling when wake up from sleep mode by @youkaichao in https://github.com/vllm-project/vllm/pull/12981
* [core][rlhf] add colocate example for RLHF by @youkaichao in https://github.com/vllm-project/vllm/pull/12984
* [V1] Use msgpack for core request serialization by @njhill in https://github.com/vllm-project/vllm/pull/12918
* [Bugfix][Platform] Check whether selected backend is None in get_attn_backend_cls() by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12975
* [core] fix sleep mode and pytorch checkpoint compatibility by @youkaichao in https://github.com/vllm-project/vllm/pull/13001
* [Doc] Add link to tool_choice tracking issue in tool_calling.md by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13003
* [misc] Add retries with exponential backoff for HF file existence check by @khluu in https://github.com/vllm-project/vllm/pull/13008
* [Bugfix] Clean up and fix multi-modal processors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13012
* Fix seed parameter behavior in vLLM by @SmartManoj in https://github.com/vllm-project/vllm/pull/13007
* [Model] Ultravox Model: Support v0.5 Release by @farzadab in https://github.com/vllm-project/vllm/pull/12912
* [misc] Fix setup.py condition to avoid AMD from being mistaken with CPU by @khluu in https://github.com/vllm-project/vllm/pull/13022
* [V1][Minor] Move scheduler outputs to a separate file by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13062
* [Docs] Annouce Meta Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/13065
* [Bugfix] Support missing tool parameters in mistral tokenizer by @fgreinacher in https://github.com/vllm-project/vllm/pull/12884
* [Benchmark] Add BurstGPT to benchmark_serving by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13063
* [Core] Don't do platform detection at import time by @russellb in https://github.com/vllm-project/vllm/pull/12933
* [Misc] LoRA - Refactor Punica ops tests by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/12970
* [Bugfix]: Reasoning output bug according to the chat template change by @gaocegege in https://github.com/vllm-project/vllm/pull/13025
* [V1][Metrics] Add GPU prefix cache hit rate % gauge by @comaniac in https://github.com/vllm-project/vllm/pull/12592
* [executor] init `local_rank` as device index by @MengqingCao in https://github.com/vllm-project/vllm/pull/13027
* [ROCm] Using a more precise memory profiling by @gshtras in https://github.com/vllm-project/vllm/pull/12624
* [Build] Fix cuda link target of cumem_allocator in CPU env by @guoyuhong in https://github.com/vllm-project/vllm/pull/12863
* [Platform] add pre_register_and_update function by @wangxiyuan in https://github.com/vllm-project/vllm/pull/12432
* [Bugfix] fix flaky test by @SmartManoj in https://github.com/vllm-project/vllm/pull/13089
* [V1][Metrics] Add several request timing histograms by @markmc in https://github.com/vllm-project/vllm/pull/12644
* Set `torch_dtype` in `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/13088
* [Misc] Fix typo at comments at metrics.py by @je1lee in https://github.com/vllm-project/vllm/pull/13024
* [Bugfix] Do not use resource module on Windows (#12858) by @MoonRide303 in https://github.com/vllm-project/vllm/pull/13029
* [BugFix] Pop instead of del CUDA_VISIBLE_DEVICES by @HollowMan6 in https://github.com/vllm-project/vllm/pull/12962
* Fix initializing GGUF weights for ColumnParallelLinear when using tensor parallel > 1 by @SzymonOzog in https://github.com/vllm-project/vllm/pull/13023
* [CI/Build][Bugfix] Fix CPU backend default threads num by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/13077
* [Doc] Improve OpenVINO installation doc by @hmellor in https://github.com/vllm-project/vllm/pull/13102
* [Bugfix] Guided decoding falls back to outlines when fails to import xgrammar by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12976
* [Misc] Move pre-commit suggestion back to the end by @russellb in https://github.com/vllm-project/vllm/pull/13114
* [RFC][vllm-API] Support tokenizer registry for customized tokenizer in vLLM by @youngkent in https://github.com/vllm-project/vllm/pull/12518
* [Model] IBM/NASA Prithvi Geospatial model  by @christian-pinto in https://github.com/vllm-project/vllm/pull/12830
* [ci] Add more source file dependencies for some tests by @khluu in https://github.com/vllm-project/vllm/pull/13123
* [Neuron][Kernel] Support Longer Sequences in NKI-based Flash PagedAttention and Improve Efficiency by @lingfanyu in https://github.com/vllm-project/vllm/pull/12921
* Bump helm/kind-action from 1.10.0 to 1.12.0 by @dependabot in https://github.com/vllm-project/vllm/pull/11612
* Bump actions/stale from 9.0.0 to 9.1.0 by @dependabot in https://github.com/vllm-project/vllm/pull/12462
* Bump helm/chart-testing-action from 2.6.1 to 2.7.0 by @dependabot in https://github.com/vllm-project/vllm/pull/12463
* Bump actions/setup-python from 5.3.0 to 5.4.0 by @dependabot in https://github.com/vllm-project/vllm/pull/12672
* Further reduce the HTTP calls to huggingface.co by @maxdebayser in https://github.com/vllm-project/vllm/pull/13107
* [Misc] AMD Build Improvements by @842974287 in https://github.com/vllm-project/vllm/pull/12923
* [Bug] [V1] Try fetching stop_reason from EngineOutput before checking the request by @bnellnm in https://github.com/vllm-project/vllm/pull/13108
* [Bugfix] Fix num video tokens calculation for Qwen2-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13148
* [Frontend] Generate valid tool call IDs when using `tokenizer-mode=mistral` by @rafvasq in https://github.com/vllm-project/vllm/pull/12332
* [Misc] Delete unused LoRA modules by @jeejeelee in https://github.com/vllm-project/vllm/pull/13151
* Introduce VLLM_CUDART_SO_PATH to allow users specify the .so path by @houseroad in https://github.com/vllm-project/vllm/pull/12998
* [CI/Build] Use mypy matcher for pre-commit CI job by @russellb in https://github.com/vllm-project/vllm/pull/13162
* [CORE] [QUANT] Support for GPTQModel's `dynamic` quantization per module override/control by @Qubitium in https://github.com/vllm-project/vllm/pull/7086
* [Bugfix] Allow fallback to AWQ from AWQMarlin at per-layer granularity by @mgoin in https://github.com/vllm-project/vllm/pull/13119
* [CI] Fix failing FP8 cpu offload test by @mgoin in https://github.com/vllm-project/vllm/pull/13170
* [V1][Bugfix] Copy encoder input ids to fix set iteration issue during VLM abort by @andoorve in https://github.com/vllm-project/vllm/pull/13173
* [CI/Build] Ignore ruff warning up007 by @russellb in https://github.com/vllm-project/vllm/pull/13182
* [perf-benchmark] cleanup unused Docker images and volumes in H100 benchmark instance by @khluu in https://github.com/vllm-project/vllm/pull/12706
* [NVIDIA] Support nvfp4 quantization by @kaixih in https://github.com/vllm-project/vllm/pull/12784
* [Bugfix][Example] Fix GCed profiling server for TPU by @mgoin in https://github.com/vllm-project/vllm/pull/12792
* [VLM] Implement merged multimodal processor for Mllama by @Isotr0py in https://github.com/vllm-project/vllm/pull/11427
* Simplify logic of locating CUDART so file path by @houseroad in https://github.com/vllm-project/vllm/pull/13203
* [Build] Automatically use the wheel of the base commit with Python-only build by @comaniac in https://github.com/vllm-project/vllm/pull/13178
* [Bugfix] deepseek_r1_reasoning_parser put reason content in wrong field in certain edge case by @LikeSundayLikeRain in https://github.com/vllm-project/vllm/pull/13097
* [Frontend] Move CLI code into vllm.cmd package by @russellb in https://github.com/vllm-project/vllm/pull/12971
* Allow Unsloth Dynamic 4bit BnB quants to work by @danielhanchen in https://github.com/vllm-project/vllm/pull/12974
* [CI/Build] Allow ruff to auto-fix some issues by @russellb in https://github.com/vllm-project/vllm/pull/13180
* [V1][core] Implement pipeline parallel on Ray by @ruisearch42 in https://github.com/vllm-project/vllm/pull/12996
* [VLM] Remove input processor from clip and siglip by @Isotr0py in https://github.com/vllm-project/vllm/pull/13165
* [Frontend] Pass pre-created socket to uvicorn by @russellb in https://github.com/vllm-project/vllm/pull/13113
* [V1] Clarify input processing and multimodal feature caching logic by @ywang96 in https://github.com/vllm-project/vllm/pull/13211
* [VLM] Merged multi-modal processor for Molmo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12966
* [V1][Core] Add worker_base for v1 worker by @AoyuQC in https://github.com/vllm-project/vllm/pull/12816
* [Misc] Qwen2.5-VL Optimization by @wulipc in https://github.com/vllm-project/vllm/pull/13155
* [VLM] Separate text-only and vision variants of the same model architecture by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13157
* [Bugfix] Missing Content Type returns 500 Internal Server Error by @vaibhavjainwiz in https://github.com/vllm-project/vllm/pull/13193
* [Frontend] Add `/v1/audio/transcriptions` OpenAI API endpoint by @NickLucche in https://github.com/vllm-project/vllm/pull/12909
* Add label if pre-commit passes by @hmellor in https://github.com/vllm-project/vllm/pull/12527
* Optimize moe_align_block_size for deepseek_v3 by @mgoin in https://github.com/vllm-project/vllm/pull/12850
* [Kernel][Bugfix] Refactor and Fix CUTLASS 2:4 Sparse Kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13198
* Revert "Add label if pre-commit passes" by @hmellor in https://github.com/vllm-project/vllm/pull/13242
* [ROCm] Avoid using the default stream on ROCm as it is a performance killer by @gshtras in https://github.com/vllm-project/vllm/pull/13238
* [Kernel] Fix awq error when n is not divisable by 128 by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/13227
* [V1] Consolidate MM cache size to vllm.envs by @ywang96 in https://github.com/vllm-project/vllm/pull/13239
* [Bugfix/CI] Turn test_compressed_tensors_2of4_sparse back on by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13250
* [Bugfix][CI] Inherit codespell settings from pyproject.toml in the pre-commit-config by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13237
* [Bugfix] Offline example of disaggregated prefill by @XiaobingSuper in https://github.com/vllm-project/vllm/pull/13214
* [Misc] Remove redundant statements in scheduler.py by @WrRan in https://github.com/vllm-project/vllm/pull/13229
* Consolidate Llama model usage in tests by @hmellor in https://github.com/vllm-project/vllm/pull/13094
* Expand MLA to support most types of quantization by @mgoin in https://github.com/vllm-project/vllm/pull/13181
* [V1] LoRA - Enable Serving Usecase by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/12883
* [ROCm][V1] Add intial ROCm support to V1 by @SageMoore in https://github.com/vllm-project/vllm/pull/12790
* [Bugfix][V1] GPUModelRunner._update_states should return True when there is a finished request in batch by @imkero in https://github.com/vllm-project/vllm/pull/13126
* [WIP] TPU V1 Support Refactored by @alexm-redhat in https://github.com/vllm-project/vllm/pull/13049
* [Frontend] Optionally remove memory buffer used for uploading to URLs in run_batch by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/12927
* [Bugfix] Fix missing parentheses by @xu-song in https://github.com/vllm-project/vllm/pull/13263
* [Misc] Log time consumption of sleep and wake-up by @waltforme in https://github.com/vllm-project/vllm/pull/13115
* [VLM] Keep track of whether prompt replacements have been applied by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13215
* [V1] Simplify GPUModelRunner._update_states check by @njhill in https://github.com/vllm-project/vllm/pull/13265
* Support logit_bias in v1 Sampler by @houseroad in https://github.com/vllm-project/vllm/pull/13079
* [Core] choice-based structured output with xgrammar by @russellb in https://github.com/vllm-project/vllm/pull/12632
* [Hardware][Gaudi][Bugfix] Fix error for guided decoding by @zhouyu5 in https://github.com/vllm-project/vllm/pull/12317
* [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts by @mgoin in https://github.com/vllm-project/vllm/pull/13236
* [Core] Reduce TTFT with concurrent partial prefills by @joerunde in https://github.com/vllm-project/vllm/pull/10235
* [V1][Core] min_p sampling support by @AoyuQC in https://github.com/vllm-project/vllm/pull/13191
* [V1][CI] Fix failed v1-test because of min_p by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13316
* [V1][Sampler] Don't apply temp for greedy-only by @njhill in https://github.com/vllm-project/vllm/pull/13311
* [V1][PP] Fix memory profiling in PP by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13315
* [Bugfix][AMD] Update torch_bindings so that scaled_fp4_quant isn't build on ROCm by @SageMoore in https://github.com/vllm-project/vllm/pull/13235
* [Bugfix][Docs] Fix offline Whisper by @NickLucche in https://github.com/vllm-project/vllm/pull/13274
* [Bugfix] Massage MLA's usage of flash attn for RoCM by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13310
* [BugFix] Don't scan entire cache dir when loading model by @njhill in https://github.com/vllm-project/vllm/pull/13302
* [Bugfix]Fix search start_index of stop_checker by @xu-song in https://github.com/vllm-project/vllm/pull/13280
* [Bugfix] Fix qwen2.5-vl image processor by @Isotr0py in https://github.com/vllm-project/vllm/pull/13286
* [V1][Metrics] Add iteration_tokens_total histogram from V0 by @markmc in https://github.com/vllm-project/vllm/pull/13288
* [AMD] [Model] DeepSeek tunings by @rasmith in https://github.com/vllm-project/vllm/pull/13199
* [V1][PP] Run engine busy loop with batch queue by @comaniac in https://github.com/vllm-project/vllm/pull/13064
* [ci/build] update flashinfer by @youkaichao in https://github.com/vllm-project/vllm/pull/13323
* [Doc] [2/N] Add Fuyu E2E example for multimodal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13331
* [V1][Spec Decode] Ngram Spec Decode  by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/12193
* [Quant] Add `SupportsQuant` to phi3 and clip by @kylesayrs in https://github.com/vllm-project/vllm/pull/13104
* [Bugfix] Pin xgrammar to 0.1.11 by @mgoin in https://github.com/vllm-project/vllm/pull/13338
* [BugFix] Enhance test_pos_encoding to support execution on multi-devices by @wchen61 in https://github.com/vllm-project/vllm/pull/13187
* [V1] Update doc and examples for H2O-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/13349
* [ci] skip failed tests for flashinfer by @youkaichao in https://github.com/vllm-project/vllm/pull/13352
* [platform] add base class for communicators by @youkaichao in https://github.com/vllm-project/vllm/pull/13208
* [Bugfix] Fix 2 Node and Spec Decode tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13341
* [Docs] Change myenv to vllm. Update python_env_setup.inc.md by @arkylin in https://github.com/vllm-project/vllm/pull/13325
* [V1][BugFix] Add __init__.py to v1/spec_decode/ by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13359
* [V1][PP] Cache Intermediate Tensors by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13353
* [Bugfix][Platform][CPU] Fix cuda platform detection on CPU backend edge case by @Isotr0py in https://github.com/vllm-project/vllm/pull/13358
* [V1][BugFix] Clean up rejection sampler & Fix warning msg by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13362
* [V1][Misc] Avoid unnecessary log output by @jeejeelee in https://github.com/vllm-project/vllm/pull/13289
* [Feature][Spec Decode] Simplify the use of Eagle Spec Decode by @ShangmingCai in https://github.com/vllm-project/vllm/pull/12304
* Fix spelling error in index.md by @yankooo in https://github.com/vllm-project/vllm/pull/13369
* Run v1 benchmark and integrate with PyTorch OSS benchmark database by @huydhn in https://github.com/vllm-project/vllm/pull/13068
* [MISC] tiny fixes by @MengqingCao in https://github.com/vllm-project/vllm/pull/13378
* [VLM] Check required fields before initializing field config in `DictEmbeddingItems` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13380
* [Model] Support Mamba2 (Codestral Mamba) by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9292
* [Bugfix] fix xpu communicator by @yma11 in https://github.com/vllm-project/vllm/pull/13368
* [Bugfix] Fix VLLM_USE_MODELSCOPE issue by @r4ntix in https://github.com/vllm-project/vllm/pull/13384
* [V1] Get input tokens from scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13339
* [V1][PP] Fix intermediate tensor values by @comaniac in https://github.com/vllm-project/vllm/pull/13417
* [V1][Spec decode] Move drafter to model runner  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13363
* [Bugfix][CI][V1] Work around V1 + CUDA Graph + torch._scaled_mm fallback issue by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/13425
* [Misc] Remove dangling references to `SamplingType.BEAM` by @hmellor in https://github.com/vllm-project/vllm/pull/13402
* [Model] Enable quantization support for `transformers` backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/12960
* [ROCm] fix get_device_name for rocm by @divakar-amd in https://github.com/vllm-project/vllm/pull/13438
* [v1] fix parallel config rank by @youkaichao in https://github.com/vllm-project/vllm/pull/13445
* [Quant] Molmo SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/13336
* [Quant] Arctic SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/13366
* [Bugfix] Only print out chat template when supplied by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13444
* [core] fix sleep mode in pytorch 2.6 by @youkaichao in https://github.com/vllm-project/vllm/pull/13456
* [Quant] Aria SupportsQuant by @kylesayrs in https://github.com/vllm-project/vllm/pull/13416
* [V1][PP] Fix & Pin Ray version in requirements-cuda.txt by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13436
* Add outlines fallback when JSON schema has enum by @mgoin in https://github.com/vllm-project/vllm/pull/13449
* [Bugfix] Ensure LoRA path from the request can be included in err msg by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13450
* [Bugfix] Fix failing transformers dynamic module resolving with spawn multiproc method by @Isotr0py in https://github.com/vllm-project/vllm/pull/13403
* [Doc]: Improve feature tables  by @hmellor in https://github.com/vllm-project/vllm/pull/13224
* [Bugfix] Remove noisy error logging during local model loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/13458
* [ROCm] Make amdsmi import optional for other platforms by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13460
* [Bugfix] Handle content type with optional parameters by @zifeitong in https://github.com/vllm-project/vllm/pull/13383
* [Bugfix] Fix invalid rotary embedding unit test by @liangfu in https://github.com/vllm-project/vllm/pull/13431
* [CI/Build] migrate static project metadata from setup.py to pyproject.toml by @dtrifiro in https://github.com/vllm-project/vllm/pull/8772
* [V1][PP] Enable true PP with Ray executor  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13472
* [misc] fix debugging code by @youkaichao in https://github.com/vllm-project/vllm/pull/13487
* [V1][Tests] Adding additional testing for multimodal models to V1 by @andoorve in https://github.com/vllm-project/vllm/pull/13308
* [V1] Optimize handling of sampling metadata and req_ids list by @njhill in https://github.com/vllm-project/vllm/pull/13244
* Pin Ray version to 2.40.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13490
* [V1][Spec Decode] Optimize N-gram matching with Numba by @WoosukKwon in https://github.com/vllm-project/vllm/pull/13365
* [Misc] Remove dangling references to `--use-v2-block-manager` by @hmellor in https://github.com/vllm-project/vllm/pull/13492
* [Hardware][Gaudi][Feature] Support Contiguous Cache Fetch  by @zhouyu5 in https://github.com/vllm-project/vllm/pull/12139
* [perf-benchmark] Allow premerge ECR by @khluu in https://github.com/vllm-project/vllm/pull/13509
* [ROCm][MoE configs] mi325 mixtral & mi300 qwen_moe  by @divakar-amd in https://github.com/vllm-project/vllm/pull/13503
* [Doc] Add clarification note regarding paligemma by @ywang96 in https://github.com/vllm-project/vllm/pull/13511
* [1/n][CI] Load models in CI from S3 instead of HF by @khluu in https://github.com/vllm-project/vllm/pull/13205
* [perf-benchmark] Fix ECR path for premerge benchmark by @khluu in https://github.com/vllm-project/vllm/pull/13512
* Refactor GPUModelRunnerBase load_model method to include device param by @Zzhiter in https://github.com/vllm-project/vllm/pull/13037
* [Bugfix] Fix Positive Feature Layers in Llava Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/13514
* [Model][Speculative Decoding] DeepSeek MTP spec decode by @luccafong in https://github.com/vllm-project/vllm/pull/12755
* [V1][Core] Generic mechanism for handling engine utility methods by @njhill in https://github.com/vllm-project/vllm/pull/13060
* [Feature] Pluggable platform-specific scheduler by @yannicks1 in https://github.com/vllm-project/vllm/pull/13161
* [CI/Build] force writing version file by @dtrifiro in https://github.com/vllm-project/vllm/pull/13544
* [doc] clarify profiling is only for developers by @youkaichao in https://github.com/vllm-project/vllm/pull/13554
* [VLM][Bugfix] Pass processor kwargs properly on init by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/13516
* [Bugfix] Fix device ordinal when initializing spec_decode_sampler under multi-node setup by @ShangmingCai in https://github.com/vllm-project/vllm/pull/13269
* [doc] clarify multi-node serving doc by @youkaichao in https://github.com/vllm-project/vllm/pull/13558
* Fix copyright year to auto get current year by @wilsonwu in https://github.com/vllm-project/vllm/pull/13561
* [MISC] Logging the message about Ray teardown by @comaniac in https://github.com/vllm-project/vllm/pull/13502
* [Misc] Avoid calling unnecessary `hf_list_repo_files` for local model path by @Isotr0py in https://github.com/vllm-project/vllm/pull/13348
* [BugFix] Avoid error traceback in logs when V1 `LLM` terminates by @njhill in https://github.com/vllm-project/vllm/pull/13565
* [3/n][CI] Load Quantization test models with S3 by @khluu in https://github.com/vllm-project/vllm/pull/13570
* [Misc] Qwen2.5 VL support LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/13261
* [ci] Add AWS creds for AMD by @khluu in https://github.com/vllm-project/vllm/pull/13572
* [ROCm][MoE] mi300 mixtral8x7B perf for specific BS by @divakar-amd in https://github.com/vllm-project/vllm/pull/13577
* [core] add sleep and wake up endpoint and v1 support by @youkaichao in https://github.com/vllm-project/vllm/pull/12987
* [bugfix] spec decode worker get tp group only when initialized by @simon-mo in https://github.com/vllm-project/vllm/pull/13578
* [Misc] Warn if the vLLM version can't be retrieved by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/13501
* [Misc] add mm_processor_kwargs to extra_body for Qwen2.5-VL by @wulipc in https://github.com/vllm-project/vllm/pull/13533
* [ROCm] MI300A compile targets deprecation by @gshtras in https://github.com/vllm-project/vllm/pull/13560
* [API Server] Add port number range validation by @terrytangyuan in https://github.com/vllm-project/vllm/pull/13506
* [CI/Build] Use uv in the Dockerfile by @mgoin in https://github.com/vllm-project/vllm/pull/13566
* [ci] Fix spec decode test by @khluu in https://github.com/vllm-project/vllm/pull/13600
* [2/n][ci] S3: Use full model path by @khluu in https://github.com/vllm-project/vllm/pull/13564
* [Kernel] LoRA - Refactor sgmv kernels by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/13110
* Merge similar examples in `offline_inference` into single `basic` example by @hmellor in https://github.com/vllm-project/vllm/pull/12737
* [Bugfix] Fix deepseekv3 grouped topk error by @Chen-XiaoBing in https://github.com/vllm-project/vllm/pull/13474

## New Contributors
* @jitseklomp made their first contribution in https://github.com/vllm-project/vllm/pull/12840
* @fabianlim made their first contribution in https://github.com/vllm-project/vllm/pull/10909
* @ZSL98 made their first contribution in https://github.com/vllm-project/vllm/pull/12824
* @SzymonOzog made their first contribution in https://github.com/vllm-project/vllm/pull/12836
* @DK-DARKmatter made their first contribution in https://github.com/vllm-project/vllm/pull/12896
* @Shaoting-Feng made their first contribution in https://github.com/vllm-project/vllm/pull/12418
* @SmartManoj made their first contribution in https://github.com/vllm-project/vllm/pull/13007
* @farzadab made their first contribution in https://github.com/vllm-project/vllm/pull/12912
* @je1lee made their first contribution in https://github.com/vllm-project/vllm/pull/13024
* @MoonRide303 made their first contribution in https://github.com/vllm-project/vllm/pull/13029
* @christian-pinto made their first contribution in https://github.com/vllm-project/vllm/pull/12830
* @lingfanyu made their first contribution in https://github.com/vllm-project/vllm/pull/12921
* @842974287 made their first contribution in https://github.com/vllm-project/vllm/pull/12923
* @kaixih made their first contribution in https://github.com/vllm-project/vllm/pull/12784
* @LikeSundayLikeRain made their first contribution in https://github.com/vllm-project/vllm/pull/13097
* @danielhanchen made their first contribution in https://github.com/vllm-project/vllm/pull/12974
* @AoyuQC made their first contribution in https://github.com/vllm-project/vllm/pull/12816
* @wulipc made their first contribution in https://github.com/vllm-project/vllm/pull/13155
* @vaibhavjainwiz made their first contribution in https://github.com/vllm-project/vllm/pull/13193
* @xu-song made their first contribution in https://github.com/vllm-project/vllm/pull/13263
* @zhouyu5 made their first contribution in https://github.com/vllm-project/vllm/pull/12317
* @arkylin made their first contribution in https://github.com/vllm-project/vllm/pull/13325
* @yankooo made their first contribution in https://github.com/vllm-project/vllm/pull/13369
* @huydhn made their first contribution in https://github.com/vllm-project/vllm/pull/13068
* @r4ntix made their first contribution in https://github.com/vllm-project/vllm/pull/13384
* @Zzhiter made their first contribution in https://github.com/vllm-project/vllm/pull/13037
* @luccafong made their first contribution in https://github.com/vllm-project/vllm/pull/12755
* @wilsonwu made their first contribution in https://github.com/vllm-project/vllm/pull/13561
* @Chen-XiaoBing made their first contribution in https://github.com/vllm-project/vllm/pull/13474

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.7.2...v0.7.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.7.3)

---

## v0.7.2: v0.7.2
**Published:** 2025-02-06

## Highlights

* Qwen2.5-VL is now supported in vLLM. Please note that it requires a source installation from Hugging Face `transformers` library at the moment (#12604)
* Add `transformers` backend support via `--model-impl=transformers`. This allows vLLM to be ran with arbitrary Hugging Face text models (#11330, #12785, #12727). 
* Performance enhancement to DeepSeek models. 
	* Align KV caches entries to start 256 byte boundaries, yielding 43% throughput enhancement (#12676)
	* Apply `torch.compile` to fused_moe/grouped_topk, yielding 5% throughput enhancement (#12637)
	* Enable MLA for DeepSeek VL2 (#12729)
	* Enable DeepSeek model on ROCm (#12662)

### Core Engine
* Use `VLLM_LOGITS_PROCESSOR_THREADS` to speed up structured decoding in high batch size scenarios (#12368)

### Security Update
* Improve hash collision avoidance in prefix caching (#12621)
* Add SPDX-License-Identifier headers to python source files (#12628)

### Other
* Enable FusedSDPA support for Intel Gaudi (HPU) (#12359)

## What's Changed
* Apply torch.compile to fused_moe/grouped_topk by @mgoin in https://github.com/vllm-project/vllm/pull/12637
* doc: fixing minor typo in readme.md by @vicenteherrera in https://github.com/vllm-project/vllm/pull/12643
* [Bugfix] fix moe_wna16 get_quant_method by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/12648
* [Core] Silence unnecessary deprecation warnings by @russellb in https://github.com/vllm-project/vllm/pull/12620
* [V1][Minor] Avoid frequently creating ConstantList by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12653
* [Core][v1] Unify allocating slots in prefill and decode in KV cache manager by @ShawnD200 in https://github.com/vllm-project/vllm/pull/12608
* [Hardware][Intel GPU] add XPU bf16 support by @jikunshang in https://github.com/vllm-project/vllm/pull/12392
* [Misc] Add SPDX-License-Identifier headers to python source files by @russellb in https://github.com/vllm-project/vllm/pull/12628
* [doc][misc] clarify VLLM_HOST_IP for multi-node inference by @youkaichao in https://github.com/vllm-project/vllm/pull/12667
* [Doc] Deprecate Discord by @zhuohan123 in https://github.com/vllm-project/vllm/pull/12668
* [Kernel] port sgl moe_align_block_size kernels by @chenyang78 in https://github.com/vllm-project/vllm/pull/12574
* make sure mistral_common not imported for non-mistral models by @youkaichao in https://github.com/vllm-project/vllm/pull/12669
* Properly check if all fused layers are in the list of targets by @eldarkurtic in https://github.com/vllm-project/vllm/pull/12666
* Fix for attention layers to remain unquantized during moe_wn16 quant by @srikanthsrnvs in https://github.com/vllm-project/vllm/pull/12570
* [cuda] manually import the correct pynvml module by @youkaichao in https://github.com/vllm-project/vllm/pull/12679
* [ci/build] fix gh200 test by @youkaichao in https://github.com/vllm-project/vllm/pull/12681
* [Model]: Add `transformers` backend support by @ArthurZucker in https://github.com/vllm-project/vllm/pull/11330
* [Misc] Fix improper placement of SPDX header in scripts by @russellb in https://github.com/vllm-project/vllm/pull/12694
* [Bugfix][Kernel] Fix per-token/per-channel quantization for Hopper scaled mm by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12696
* Squelch MLA warning for Compressed-Tensors Models by @kylesayrs in https://github.com/vllm-project/vllm/pull/12704
* [Model] Add Deepseek V3 fp8_w8a8 configs for B200 by @kushanam in https://github.com/vllm-project/vllm/pull/12707
* [MISC] Remove model input dumping when exception by @comaniac in https://github.com/vllm-project/vllm/pull/12582
* [V1] Revert `uncache_blocks` and support recaching full blocks by @comaniac in https://github.com/vllm-project/vllm/pull/12415
* [Core] Improve hash collision avoidance in prefix caching by @russellb in https://github.com/vllm-project/vllm/pull/12621
* Support Pixtral-Large HF by using llava multimodal_projector_bias config by @mgoin in https://github.com/vllm-project/vllm/pull/12710
* [Doc] Replace ibm-fms with ibm-ai-platform by @tdoublep in https://github.com/vllm-project/vllm/pull/12709
* [Quant] Fix use_mla TypeError and support loading pure-sparsity Compressed Tensors configs by @kylesayrs in https://github.com/vllm-project/vllm/pull/12711
* [AMD][ROCm] Enable DeepSeek model on ROCm by @hongxiayang in https://github.com/vllm-project/vllm/pull/12662
* [Misc] Add BNB quantization for Whisper by @jeejeelee in https://github.com/vllm-project/vllm/pull/12381
* [VLM] Merged multi-modal processor for InternVL-based models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12553
* [V1] Remove constraints on partial requests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12674
* [VLM] Implement merged multimodal processor and V1 support for idefics3 by @Isotr0py in https://github.com/vllm-project/vllm/pull/12660
* [Model] [Bugfix] Fix loading of fine-tuned models based on Phi-3-Small  by @mgtk77 in https://github.com/vllm-project/vllm/pull/12689
* Avoid unnecessary multi-modal input data copy when len(batch) == 1 by @imkero in https://github.com/vllm-project/vllm/pull/12722
* [Build] update requirements of no-device for plugin usage by @sducouedic in https://github.com/vllm-project/vllm/pull/12630
* [Bugfix] Fix CI failures for InternVL and Mantis models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12728
* [V1][Metrics] Add request_success_total counter, labelled with finish reason by @markmc in https://github.com/vllm-project/vllm/pull/12579
* [Perf] Mem align KV caches for CUDA devices (MLA perf improvement) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12676
* [Core] add and implement `VLLM_LOGITS_PROCESSOR_THREADS` by @akeshet in https://github.com/vllm-project/vllm/pull/12368
* [ROCM][AMD][TRITON] Halving warps number for fw_prefill to reduce spilling by @maleksan85 in https://github.com/vllm-project/vllm/pull/12713
* Refactor `Linear` handling in `TransformersModel` by @hmellor in https://github.com/vllm-project/vllm/pull/12727
* [VLM] Add MLA with pure RoPE support for deepseek-vl2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/12729
* [Misc] Bump the compressed-tensors version by @dsikka in https://github.com/vllm-project/vllm/pull/12736
* [Model][Quant] Fix GLM, Fix fused module mappings for quantization by @kylesayrs in https://github.com/vllm-project/vllm/pull/12634
* [Doc] Update PR Reminder with link to Developer Slack by @mgoin in https://github.com/vllm-project/vllm/pull/12748
* [Bugfix] Fix OpenVINO model runner by @hmellor in https://github.com/vllm-project/vllm/pull/12750
* [V1][Misc] Shorten `FinishReason` enum and use constant strings by @njhill in https://github.com/vllm-project/vllm/pull/12760
* [Doc] Remove performance warning for auto_awq.md by @mgoin in https://github.com/vllm-project/vllm/pull/12743
* [Bugfix] Fix 'ModuleNotFoundError: No module named 'intel_extension_for_pytorch'' for --tensor-parallel-size more than 1  by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/12546
* [core][distributed] exact ray placement control by @youkaichao in https://github.com/vllm-project/vllm/pull/12732
* [Kernel] Use self.kv_cache and forward_context.attn_metadata in Attention.forward by @heheda12345 in https://github.com/vllm-project/vllm/pull/12536
* [Hardware][Intel-Gaudi] Enable FusedSDPA support for Intel Gaudi (HPU) by @SanjuCSudhakaran in https://github.com/vllm-project/vllm/pull/12359
* Add: Support for Sparse24Bitmask Compressed Models by @rahul-tuli in https://github.com/vllm-project/vllm/pull/12097
* [VLM] Use shared field to pass token ids to model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12767
* [Docs] Drop duplicate [source] links by @russellb in https://github.com/vllm-project/vllm/pull/12780
* [VLM] Qwen2.5-VL by @ywang96 in https://github.com/vllm-project/vllm/pull/12604
* [VLM] Update compatibility with transformers 4.49 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12781
* Quantization and MoE configs for GH200 machines  by @arvindsun in https://github.com/vllm-project/vllm/pull/12717
* [ROCm][Kernel] Using the correct warp_size value by @gshtras in https://github.com/vllm-project/vllm/pull/12789
* [Bugfix] Better FP8 supported defaults by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12796
* [Misc][Easy] Remove the space from the file name by @houseroad in https://github.com/vllm-project/vllm/pull/12799
* [Model] LoRA Support for Ultravox model by @thedebugger in https://github.com/vllm-project/vllm/pull/11253
* [Bugfix] Fix the test_ultravox.py's license by @houseroad in https://github.com/vllm-project/vllm/pull/12806
* Improve `TransformersModel` UX by @hmellor in https://github.com/vllm-project/vllm/pull/12785
* [Misc] Remove duplicated DeepSeek V2/V3 model definition by @mgoin in https://github.com/vllm-project/vllm/pull/12793
* [Misc] Improve error message for incorrect pynvml by @youkaichao in https://github.com/vllm-project/vllm/pull/12809

## New Contributors
* @vicenteherrera made their first contribution in https://github.com/vllm-project/vllm/pull/12643
* @chenyang78 made their first contribution in https://github.com/vllm-project/vllm/pull/12574
* @srikanthsrnvs made their first contribution in https://github.com/vllm-project/vllm/pull/12570
* @ArthurZucker made their first contribution in https://github.com/vllm-project/vllm/pull/11330
* @mgtk77 made their first contribution in https://github.com/vllm-project/vllm/pull/12689
* @sducouedic made their first contribution in https://github.com/vllm-project/vllm/pull/12630
* @akeshet made their first contribution in https://github.com/vllm-project/vllm/pull/12368
* @arvindsun made their first contribution in https://github.com/vllm-project/vllm/pull/12717
* @thedebugger made their first contribution in https://github.com/vllm-project/vllm/pull/11253

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.7.1...v0.7.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.7.2)

---

## v0.7.1: v0.7.1
**Published:** 2025-02-01

## Highlights

This release features MLA optimization for Deepseek family of models. Compared to v0.7.0 released this Monday, we offer ~3x the generation throughput, ~10x the memory capacity for tokens, and horizontal context scalability with pipeline parallelism 
* MLA Kernel (#12601, #12642,#12528).
* FP8 Kernels (#11589, #11868, #12587)

### V1
For the V1 architecture, we
* Added a new design document for zero overhead prefix caching [here](https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html) (#12598)
* Add metrics and enhance logging for V1 engine (#12569, #12561, #12416, #12516, #12530, #12478)

### Models
* New Model: MiniCPM-o (text outputs only) (#12069)

### Hardwares
* Neuron: NKI-based flash-attention kernel with paged KV cache (#11277)
* AMD: llama 3.2 support upstreaming (#12421)

### Others
* Support override generation config in engine arguments (#12409)
* Support reasoning content in API for deepseek R1 (#12473)

## What's Changed
* [Bugfix] Fix missing seq_start_loc in xformers prefill metadata by @Isotr0py in https://github.com/vllm-project/vllm/pull/12464
* [V1][Minor] Minor optimizations for update_from_output by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12454
* [Bugfix] Fix gpt2 GGUF inference by @Isotr0py in https://github.com/vllm-project/vllm/pull/12467
* [Build] Only build 9.0a for scaled_mm and sparse kernels by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12339
* [V1][Metrics] Add initial Prometheus logger by @markmc in https://github.com/vllm-project/vllm/pull/12416
* [V1][CI/Test] Do basic test for top-p & top-k sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12469
* [FlashInfer] Upgrade to 0.2.0 by @abmfy in https://github.com/vllm-project/vllm/pull/11194
* [Feature] [Spec decode]: Enable MLPSpeculator/Medusa and `prompt_logprobs` with ChunkedPrefill by @NickLucche in https://github.com/vllm-project/vllm/pull/10132
* Update `pre-commit` hooks by @hmellor in https://github.com/vllm-project/vllm/pull/12475
* [Neuron][Kernel] NKI-based flash-attention kernel with paged KV cache by @liangfu in https://github.com/vllm-project/vllm/pull/11277
* Fix bad path in prometheus example by @mgoin in https://github.com/vllm-project/vllm/pull/12481
* [CI/Build] Fixed the xla nightly issue report in #12451 by @hosseinsarshar in https://github.com/vllm-project/vllm/pull/12453
* [FEATURE] Enables offline /score for embedding models by @gmarinho2 in https://github.com/vllm-project/vllm/pull/12021
* [CI] fix pre-commit error by @MengqingCao in https://github.com/vllm-project/vllm/pull/12494
* Update README.md with V1 alpha release by @ywang96 in https://github.com/vllm-project/vllm/pull/12495
* [V1] Include Engine Version in Logs by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/12496
* [Core] Make raw_request optional in ServingCompletion by @schoennenbeck in https://github.com/vllm-project/vllm/pull/12503
* [VLM] Merged multi-modal processor and V1 support for Qwen-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12504
* [Doc] Fix typo for x86 CPU installation by @waltforme in https://github.com/vllm-project/vllm/pull/12514
* [V1][Metrics] Hook up IterationStats for Prometheus metrics by @markmc in https://github.com/vllm-project/vllm/pull/12478
* Replace missed warning_once for rerank API by @mgoin in https://github.com/vllm-project/vllm/pull/12472
* Do not run `suggestion` `pre-commit` hook multiple times by @hmellor in https://github.com/vllm-project/vllm/pull/12521
* [V1][Metrics] Add per-request prompt/generation_tokens histograms by @markmc in https://github.com/vllm-project/vllm/pull/12516
* [Kernel] Pipe attn_logits_soft_cap through paged attention TPU kernels by @fenghuizhang in https://github.com/vllm-project/vllm/pull/12482
* [TPU] Add example for profiling TPU inference by @mgoin in https://github.com/vllm-project/vllm/pull/12531
* [Frontend] Support reasoning content for deepseek r1 by @gaocegege in https://github.com/vllm-project/vllm/pull/12473
* [Doc] Convert docs to use colon fences by @hmellor in https://github.com/vllm-project/vllm/pull/12471
* [V1][Metrics] Add TTFT and TPOT histograms by @markmc in https://github.com/vllm-project/vllm/pull/12530
* Bugfix for whisper quantization due to fake k_proj bias by @mgoin in https://github.com/vllm-project/vllm/pull/12524
* [V1] Improve Error Message for Unsupported Config by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/12535
* Fix the pydantic logging validator by @maxdebayser in https://github.com/vllm-project/vllm/pull/12420
* [Bugfix] handle alignment of arguments in convert_sparse_cross_attention_mask_to_dense by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/12347
* [Model] Refactoring of MiniCPM-V and add MiniCPM-o-2.6 support for vLLM by @HwwwwwwwH in https://github.com/vllm-project/vllm/pull/12069
* [Frontend] Support override generation config in args by @liuyanyi in https://github.com/vllm-project/vllm/pull/12409
* [Hardware][NV] Fix Modelopt model loading for k-v-scales for Llama models. by @pavanimajety in https://github.com/vllm-project/vllm/pull/11787
* [Kernel] add triton fused moe kernel for gptq/awq by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/12185
* Revert "[Build/CI] Fix libcuda.so linkage" by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12552
* [V1][BugFix] Free encoder cache for aborted requests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12545
* [Misc][MoE] add Deepseek-V3 moe tuning support by @divakar-amd in https://github.com/vllm-project/vllm/pull/12558
* [V1][Metrics] Add GPU cache usage % gauge by @markmc in https://github.com/vllm-project/vllm/pull/12561
* Set `?device={device}` when changing tab in installation guides by @hmellor in https://github.com/vllm-project/vllm/pull/12560
* [Misc] fix typo: add missing space in lora adapter error message by @Beim in https://github.com/vllm-project/vllm/pull/12564
* [Kernel] Triton Configs for Fp8 Block Quantization by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11589
* [CPU][PPC] Updated torch, torchvision, torchaudio dependencies by @npanpaliya in https://github.com/vllm-project/vllm/pull/12555
* [V1][Log] Add max request concurrency log to V1 by @mgoin in https://github.com/vllm-project/vllm/pull/12569
* [Kernel] Update `cutlass_scaled_mm` to support 2d group (blockwise) scaling by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/11868
* [ROCm][AMD][Model] llama 3.2 support upstreaming by @maleksan85 in https://github.com/vllm-project/vllm/pull/12421
* [Attention] MLA decode optimizations by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12528
* [Bugfix] Gracefully handle huggingface hub http error by @ywang96 in https://github.com/vllm-project/vllm/pull/12571
* Add favicon to docs by @hmellor in https://github.com/vllm-project/vllm/pull/12611
* [BugFix] Fix Torch.Compile For DeepSeek by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/12594
* [Git] Automatically sign-off commits by @comaniac in https://github.com/vllm-project/vllm/pull/12595
* [Docs][V1] Prefix caching design by @comaniac in https://github.com/vllm-project/vllm/pull/12598
* [v1][Bugfix] Add extra_keys to block_hash for prefix caching by @heheda12345 in https://github.com/vllm-project/vllm/pull/12603
* [release] Add input step to ask for Release version by @khluu in https://github.com/vllm-project/vllm/pull/12631
* [Bugfix] Revert MoE Triton Config Default by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/12629
* [Kernel][Quantization] Integrate block-quantized CUTLASS kernels for DeepSeekV3 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12587
* [Feature] Fix guided decoding blocking bitmask memcpy by @xpbowler in https://github.com/vllm-project/vllm/pull/12563
* [Doc] Improve installation signposting by @hmellor in https://github.com/vllm-project/vllm/pull/12575
* [Doc] int4 w4a16 example by @brian-dellabetta in https://github.com/vllm-project/vllm/pull/12585
* [V1] Bugfix: Validate Model Input Length by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/12600
* [BugFix] fix wrong output when using lora and num_scheduler_steps=8 by @sleepwalker2017 in https://github.com/vllm-project/vllm/pull/11161
* Fix target matching for fused layers with compressed-tensors by @eldarkurtic in https://github.com/vllm-project/vllm/pull/12617
* [ci] Upgrade transformers to 4.48.2 in CI dependencies by @khluu in https://github.com/vllm-project/vllm/pull/12599
* [Bugfix/CI] Fixup benchmark_moe.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12562
* Fix: Respect `sparsity_config.ignore` in Cutlass Integration by @rahul-tuli in https://github.com/vllm-project/vllm/pull/12517
* [Attention] Deepseek v3 MLA support with FP8 compute by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12601
* [CI/Build] Add label automation for structured-output, speculative-decoding, v1 by @russellb in https://github.com/vllm-project/vllm/pull/12280
* Disable chunked prefill and/or prefix caching when MLA is enabled  by @simon-mo in https://github.com/vllm-project/vllm/pull/12642

## New Contributors
* @abmfy made their first contribution in https://github.com/vllm-project/vllm/pull/11194
* @hosseinsarshar made their first contribution in https://github.com/vllm-project/vllm/pull/12453
* @gmarinho2 made their first contribution in https://github.com/vllm-project/vllm/pull/12021
* @waltforme made their first contribution in https://github.com/vllm-project/vllm/pull/12514
* @fenghuizhang made their first contribution in https://github.com/vllm-project/vllm/pull/12482
* @gaocegege made their first contribution in https://github.com/vllm-project/vllm/pull/12473
* @Beim made their first contribution in https://github.com/vllm-project/vllm/pull/12564
* @xpbowler made their first contribution in https://github.com/vllm-project/vllm/pull/12563
* @brian-dellabetta made their first contribution in https://github.com/vllm-project/vllm/pull/12585
* @sleepwalker2017 made their first contribution in https://github.com/vllm-project/vllm/pull/11161
* @eldarkurtic made their first contribution in https://github.com/vllm-project/vllm/pull/12617

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.7.0...v0.7.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.7.1)

---

## v0.7.0: v0.7.0
**Published:** 2025-01-27

## Highlights
* vLLM's V1 engine is ready for testing! This is a rewritten engine designed for performance and architectural simplicity. You can turn it on by setting environment variable `VLLM_USE_V1=1`. See [our blog](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html) for more details. (44 commits).
* New methods (`LLM.sleep`, `LLM.wake_up`, `LLM.collective_rpc`, `LLM.reset_prefix_cache`) in vLLM for the post training frameworks! (#12361, #12084, #12284).
* `torch.compile` is now fully integrated in vLLM, and enabled by default in V1. You can turn it on via `-O3` engine parameter. (#11614, #12243, #12043, #12191, #11677, #12182, #12246).

This release features 
* 400 commits from 132 contributors, including 57 new contributors. 
	* 28 CI and build enhancements, including testing for nightly torch (#12270) and inclusion of genai-perf for benchmark (#10704).
	* 58 documentation enhancements, including reorganized documentation structure (#11645, #11755, #11766, #11843, #11896). 
	* more than 161 bug fixes and miscellaneous enhancements

### Features

*Models*
* New generative models: CogAgent (#11742), Deepseek-VL2 (#11578, #12068, #12169), fairseq2 Llama (#11442), InternLM3 (#12037), Whisper (#11280)
* New pooling models: Qwen2 PRM (#12202), InternLM2 reward models (#11571)
* VLM: Merged multi-modal processor is now ready for model developers! (#11620, #11900, #11682, #11717, #11669, #11396)
  * Any model that implements merged multi-modal processor and the `get_*_embeddings` methods according to [this guide](https://docs.vllm.ai/en/latest/contributing/model/multimodal.html) is automatically supported by V1 engine.

*Hardwares*
* Apple: Native support for macOS Apple Silicon (#11696)
* AMD: MI300 FP8 format for block_quant (#12134), Tuned MoE configurations for multiple models (#12408, #12049), block size heuristic for avg 2.8x speedup for int8 models (#11698)
* TPU: support for `W8A8` (#11785)
* x86: Multi-LoRA (#11100) and MoE Support (#11831)
* Progress in out-of-tree hardware support (#12009, #11981, #11948, #11609, #12264, #11516, #11503, #11369, #11602)

*Features*
* Distributed:
	* Support torchrun and SPMD-style offline inference (#12071) 
	* New `collective_rpc` abstraction (#12151, #11256)
* API Server: Jina- and Cohere-compatible Rerank API (#12376)
* Kernels:
	* Flash Attention 3 Support (#12093)
	* Punica prefill kernels fusion (#11234)
	* For Deepseek V3: optimize `moe_align_block_size` for cuda graph and large num_experts (#12222)

### Others
* Benchmark: new script for CPU offloading  (#11533)
* Security: Set `weights_only=True` when using `torch.load()` (#12366)

## What's Changed
* [Docs] Document Deepseek V3 support by @simon-mo in https://github.com/vllm-project/vllm/pull/11535
* Update openai_compatible_server.md by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11536
* [V1] Use FlashInfer Sampling Kernel for Top-P & Top-K Sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11394
* [V1] Fix yapf by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11538
* [CI] Fix broken CI by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11543
* [misc] fix typing by @youkaichao in https://github.com/vllm-project/vllm/pull/11540
* [V1][3/N] API Server: Reduce Task Switching + Handle Abort Properly by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11534
* [BugFix] Deepseekv3 broke quantization for all other methods by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11547
* [Platform] Move model arch check to platform by @MengqingCao in https://github.com/vllm-project/vllm/pull/11503
* Update deploying_with_k8s.md with AMD ROCm GPU example by @AlexHe99 in https://github.com/vllm-project/vllm/pull/11465
* [Bugfix] Fix TeleChat2ForCausalLM weights mapper by @jeejeelee in https://github.com/vllm-project/vllm/pull/11546
* [Misc] Abstract out the logic for reading and writing media content by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11527
* [Doc]  Add xgrammar in doc by @Chen-0210 in https://github.com/vllm-project/vllm/pull/11549
* [VLM] Support caching in merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11396
* [MODEL] Update LoRA modules supported by Jamba by @ErezSC42 in https://github.com/vllm-project/vllm/pull/11209
* [Misc]Add BNB quantization for MolmoForCausalLM  by @jeejeelee in https://github.com/vllm-project/vllm/pull/11551
* [Misc] Improve BNB loader to handle mixture of sharded and merged weights with same suffix by @Isotr0py in https://github.com/vllm-project/vllm/pull/11566
* [Bugfix] Fix for ROCM compressed tensor support by @selalipop in https://github.com/vllm-project/vllm/pull/11561
* [Doc] Update mllama example based on official doc by @heheda12345 in https://github.com/vllm-project/vllm/pull/11567
* [V1] [4/N] API Server: ZMQ/MP Utilities by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11541
* [Bugfix] Last token measurement fix by @rajveerb in https://github.com/vllm-project/vllm/pull/11376
* [Model] Support InternLM2 Reward models by @Isotr0py in https://github.com/vllm-project/vllm/pull/11571
* [Model] Remove hardcoded image tokens ids from Pixtral by @ywang96 in https://github.com/vllm-project/vllm/pull/11582
* [Hardware][AMD]: Replace HIPCC version with more precise ROCm version by @hj-wei in https://github.com/vllm-project/vllm/pull/11515
* [V1][Minor] Set pin_memory=False for token_ids_cpu tensor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11581
* [Doc] Minor documentation fixes by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11580
* [bugfix] interleaving sliding window for cohere2 model by @youkaichao in https://github.com/vllm-project/vllm/pull/11583
* [V1] [5/N] API Server: unify `Detokenizer` and  `EngineCore` input by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11545
* [Doc] Convert list tables to MyST by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11594
* [v1][bugfix] fix cudagraph with inplace buffer assignment by @youkaichao in https://github.com/vllm-project/vllm/pull/11596
* [Misc] Use registry-based initialization for KV cache transfer connector. by @KuntaiDu in https://github.com/vllm-project/vllm/pull/11481
* Remove print statement in DeepseekScalingRotaryEmbedding by @mgoin in https://github.com/vllm-project/vllm/pull/11604
* [v1] fix compilation cache by @youkaichao in https://github.com/vllm-project/vllm/pull/11598
* [Docker] bump up neuron sdk v2.21 by @liangfu in https://github.com/vllm-project/vllm/pull/11593
* [Build][Kernel] Update CUTLASS to v3.6.0 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11607
* [CI/Build][CPU] Fix CPU CI by lazy importing triton FP8 kernels by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/11618
* [platforms] enable platform plugins by @youkaichao in https://github.com/vllm-project/vllm/pull/11602
* [VLM] Abstract out multi-modal data parsing in merged processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11620
* [V1] [6/N] API Server: Better Shutdown by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11586
* [Bugfix] Validate and concatenate image embeddings in MiniCPMVBaseModel by @whyiug in https://github.com/vllm-project/vllm/pull/11631
* [benchmark] Remove dependency for H100 benchmark step by @khluu in https://github.com/vllm-project/vllm/pull/11572
* [Model][LoRA]LoRA support added for MolmoForCausalLM by @ayylemao in https://github.com/vllm-project/vllm/pull/11439
* [Bugfix] Fix OpenAI parallel sampling when using xgrammar by @mgoin in https://github.com/vllm-project/vllm/pull/11637
* [Misc][LoRA] Support Rank Stabilized LoRA (RSLoRA) by @JohnGiorgi in https://github.com/vllm-project/vllm/pull/6909
* [Bugfix] Move the _touch(computed_blocks) call in the allocate_slots method to after the check for allocating new blocks. by @sakunkun in https://github.com/vllm-project/vllm/pull/11565
* [V1] Simpify vision block hash for prefix caching by removing offset from hash by @heheda12345 in https://github.com/vllm-project/vllm/pull/11646
* [V1][VLM] V1 support for selected single-image models. by @ywang96 in https://github.com/vllm-project/vllm/pull/11632
* [Benchmark] Add benchmark script for CPU offloading  by @ApostaC in https://github.com/vllm-project/vllm/pull/11533
* [Bugfix][Refactor] Unify model management in frontend by @joerunde in https://github.com/vllm-project/vllm/pull/11660
* [VLM] Add max-count checking in data parser for single image models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11661
* [Misc] Optimize Qwen2-VL LoRA test by @jeejeelee in https://github.com/vllm-project/vllm/pull/11663
* [Misc] Replace space with - in the file names by @houseroad in https://github.com/vllm-project/vllm/pull/11667
* [Doc] Fix typo by @serihiro in https://github.com/vllm-project/vllm/pull/11666
* [V1] Implement Cascade Attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11635
* [VLM] Move supported limits and max tokens to merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11669
* [VLM][Bugfix] Multi-modal processor compatible with V1 multi-input by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11674
* [mypy] Pass type checking in vllm/inputs by @CloseChoice in https://github.com/vllm-project/vllm/pull/11680
* [VLM] Merged multi-modal processor for LLaVA-NeXT by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11682
* According to vllm.EngineArgs, the name should be distributed_executor_backend by @chunyang-wen in https://github.com/vllm-project/vllm/pull/11689
* [Bugfix] Free cross attention block table for preempted-for-recompute sequence group. by @kathyyu-google in https://github.com/vllm-project/vllm/pull/10013
* [V1][Minor] Optimize token_ids_cpu copy by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11692
* [Bugfix] Change kv scaling factor by param json on nvidia gpu by @bjmsong in https://github.com/vllm-project/vllm/pull/11688
* Resolve race conditions in Marlin kernel by @wchen61 in https://github.com/vllm-project/vllm/pull/11493
* [Misc] Minimum requirements for SageMaker compatibility by @nathan-az in https://github.com/vllm-project/vllm/pull/11576
* Update default max_num_batch_tokens for chunked prefill by @SachinVarghese in https://github.com/vllm-project/vllm/pull/11694
* [Bugfix] Check chain_speculative_sampling before calling it by @houseroad in https://github.com/vllm-project/vllm/pull/11673
* [perf-benchmark] Fix dependency for steps in benchmark pipeline by @khluu in https://github.com/vllm-project/vllm/pull/11710
* [Model] Whisper model implementation by @aurickq in https://github.com/vllm-project/vllm/pull/11280
* [V1] Simplify Shutdown by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11659
* [Bugfix] Fix ColumnParallelLinearWithLoRA slice by @zinccat in https://github.com/vllm-project/vllm/pull/11708
* [V1] Improve TP>1 Error Handling + Stack Trace by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11721
* [Misc]Add BNB quantization for Qwen2VL by @jeejeelee in https://github.com/vllm-project/vllm/pull/11719
* Update requirements-tpu.txt to support python 3.9 and 3.11 by @mgoin in https://github.com/vllm-project/vllm/pull/11695
* [V1] Chore: cruft removal by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11724
* log GPU blocks num for MultiprocExecutor by @WangErXiao in https://github.com/vllm-project/vllm/pull/11656
* Update tool_calling.md by @Bryce1010 in https://github.com/vllm-project/vllm/pull/11701
* Update bnb.md with example for OpenAI by @bet0x in https://github.com/vllm-project/vllm/pull/11718
* [V1] Add `RayExecutor` support for `AsyncLLM` (api server) by @jikunshang in https://github.com/vllm-project/vllm/pull/11712
* [V1] Add kv cache utils tests. by @xcnick in https://github.com/vllm-project/vllm/pull/11513
* [Core][Bugfix] Use correct device to initialize GPU data during CUDA-graph-capture by @yanburman in https://github.com/vllm-project/vllm/pull/11233
* [VLM] Merged multi-modal processors for LLaVA-NeXT-Video and LLaVA-OneVision by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11717
* [Bugfix] Fix precision error in LLaVA-NeXT feature size calculation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11735
* [Model] Remove unnecessary weight initialization logic by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11736
* [Bugfix][V1] Fix test_kv_cache_utils.py by @jeejeelee in https://github.com/vllm-project/vllm/pull/11738
* [MISC] Replace c10::optional with std::optional by @houseroad in https://github.com/vllm-project/vllm/pull/11730
* [distributed] remove pynccl's redundant stream by @cennn in https://github.com/vllm-project/vllm/pull/11744
* fix: [doc] fix typo by @RuixiangMa in https://github.com/vllm-project/vllm/pull/11751
* [Frontend] Improve `StreamingResponse` Exception Handling by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11752
* [distributed] remove pynccl's redundant change_state by @cennn in https://github.com/vllm-project/vllm/pull/11749
* [Doc] [1/N] Reorganize Getting Started section by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11645
* [Bugfix] Remove block size constraint by @comaniac in https://github.com/vllm-project/vllm/pull/11723
* [V1] Add BlockTable class by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11693
* [Misc] Fix typo for valid_tool_parses  by @ruisearch42 in https://github.com/vllm-project/vllm/pull/11753
* [V1] Refactor get_executor_cls by @ruisearch42 in https://github.com/vllm-project/vllm/pull/11754
* [mypy] Forward pass function type hints in lora by @lucas-tucker in https://github.com/vllm-project/vllm/pull/11740
* k8s-config: Update the secret to use stringData by @surajssd in https://github.com/vllm-project/vllm/pull/11679
* [VLM] Separate out profiling-related logic by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11746
* [Doc][2/N] Reorganize Models and Usage sections by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11755
* [Bugfix] Fix max image size for LLaVA-Onevision by @ywang96 in https://github.com/vllm-project/vllm/pull/11769
* [doc] explain how to add interleaving sliding window support by @youkaichao in https://github.com/vllm-project/vllm/pull/11771
* [Bugfix][V1] Fix molmo text-only inputs by @jeejeelee in https://github.com/vllm-project/vllm/pull/11676
* [Kernel] Move attn_type to Attention.__init__() by @heheda12345 in https://github.com/vllm-project/vllm/pull/11690
* [V1] Extend beyond image modality and support mixed-modality inference with Llava-OneVision by @ywang96 in https://github.com/vllm-project/vllm/pull/11685
* [Bugfix] Fix LLaVA-NeXT feature size precision error (for real) by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11772
* [Model] Future-proof Qwen2-Audio multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11776
* [XPU] Make pp group initilized for pipeline-parallelism by @ys950902 in https://github.com/vllm-project/vllm/pull/11648
* [Doc][3/N] Reorganize Serving section by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11766
* [Kernel][LoRA]Punica prefill  kernels fusion by @jeejeelee in https://github.com/vllm-project/vllm/pull/11234
* [Bugfix] Update attention interface in `Whisper` by @ywang96 in https://github.com/vllm-project/vllm/pull/11784
* [CI] Fix neuron CI and run offline tests by @liangfu in https://github.com/vllm-project/vllm/pull/11779
* fix init error for MessageQueue when n_local_reader is zero by @XiaobingSuper in https://github.com/vllm-project/vllm/pull/11768
* [Doc] Create a vulnerability management team by @russellb in https://github.com/vllm-project/vllm/pull/9925
* [CI][CPU] adding build number to docker image name by @zhouyuan in https://github.com/vllm-project/vllm/pull/11788
* [V1][Doc] Update V1 support for `LLaVa-NeXT-Video` by @ywang96 in https://github.com/vllm-project/vllm/pull/11798
* [Bugfix] Comprehensively test and fix LLaVA-NeXT feature size calculation by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11800
* [doc] add doc to explain how to use uv by @youkaichao in https://github.com/vllm-project/vllm/pull/11773
* [V1] Support audio language models on V1 by @ywang96 in https://github.com/vllm-project/vllm/pull/11733
* [doc] update how pip can install nightly wheels by @youkaichao in https://github.com/vllm-project/vllm/pull/11806
* [Doc] Add note to `gte-Qwen2` models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11808
* [optimization] remove python function call for custom op by @youkaichao in https://github.com/vllm-project/vllm/pull/11750
* [Bugfix] update the prefix for qwen2 by @jiangjiadi in https://github.com/vllm-project/vllm/pull/11795
* [Doc]Add documentation for using EAGLE in vLLM by @sroy745 in https://github.com/vllm-project/vllm/pull/11417
* [Bugfix] Significant performance drop on CPUs with --num-scheduler-steps > 1 by @DamonFool in https://github.com/vllm-project/vllm/pull/11794
* [Doc] Group examples into categories by @hmellor in https://github.com/vllm-project/vllm/pull/11782
* [Bugfix] Fix image input for Pixtral-HF by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11741
* [Misc] sort torch profiler table by kernel timing by @divakar-amd in https://github.com/vllm-project/vllm/pull/11813
* Remove the duplicate imports of MultiModalKwargs and PlaceholderRangeâ€¦ by @WangErXiao in https://github.com/vllm-project/vllm/pull/11824
* Fixed docker build for ppc64le by @npanpaliya in https://github.com/vllm-project/vllm/pull/11518
* [OpenVINO] Fixed Docker.openvino build by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/11732
* [Bugfix] Add checks for LoRA and CPU offload by @jeejeelee in https://github.com/vllm-project/vllm/pull/11810
* [Docs] reorganize sponsorship page by @simon-mo in https://github.com/vllm-project/vllm/pull/11639
* [Bug] Fix pickling of `ModelConfig` when RunAI Model Streamer is used by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11825
* [misc] improve memory profiling by @youkaichao in https://github.com/vllm-project/vllm/pull/11809
* [doc] update wheels url by @youkaichao in https://github.com/vllm-project/vllm/pull/11830
* [Docs] Update sponsor name: 'Novita' to 'Novita AI' by @simon-mo in https://github.com/vllm-project/vllm/pull/11833
* [Hardware][Apple] Native support for macOS Apple Silicon by @wallashss in https://github.com/vllm-project/vllm/pull/11696
* [torch.compile] consider relevant code in compilation cache by @youkaichao in https://github.com/vllm-project/vllm/pull/11614
* [VLM] Reorganize profiling/processing-related code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11812
* [Doc] Move examples into categories by @hmellor in https://github.com/vllm-project/vllm/pull/11840
* [Doc][4/N] Reorganize API Reference by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11843
* [CI/Build][Bugfix] Fix CPU CI image clean up by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/11836
* [Bugfix][XPU] fix silu_and_mul by @yma11 in https://github.com/vllm-project/vllm/pull/11823
* [Misc] Move some model utils into vision file by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11848
* [Doc] Expand Multimodal API Reference by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11852
* [Misc]add some explanations for BlockHashType by @WangErXiao in https://github.com/vllm-project/vllm/pull/11847
* [TPU][Quantization] TPU `W8A8` by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11785
* [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models by @rasmith in https://github.com/vllm-project/vllm/pull/11698
* [Docs] Add Google Cloud Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/11864
* [CI] Turn on basic correctness tests for V1 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/10864
* treat do_lower_case in the same way as the sentence-transformers library by @maxdebayser in https://github.com/vllm-project/vllm/pull/11815
* [Doc] Recommend uv and python 3.12 for quickstart guide by @mgoin in https://github.com/vllm-project/vllm/pull/11849
* [Misc] Move `print_*_once` from utils to logger by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11298
* [Doc] Intended links Python multiprocessing library by @guspan-tanadi in https://github.com/vllm-project/vllm/pull/11878
* [perf]fix current stream by @youkaichao in https://github.com/vllm-project/vllm/pull/11870
* [Bugfix] Override dunder methods of placeholder modules by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11882
* [Bugfix] fix beam search input errors and latency benchmark script by @yeqcharlotte in https://github.com/vllm-project/vllm/pull/11875
* [Doc] Add model development API Reference by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11884
* [platform] Allow platform specify attention backend by @wangxiyuan in https://github.com/vllm-project/vllm/pull/11609
* [ci]try to fix flaky multi-step tests by @youkaichao in https://github.com/vllm-project/vllm/pull/11894
* [Misc] Provide correct Pixtral-HF chat template by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11891
* [Docs] Add Modal to deployment frameworks by @charlesfrye in https://github.com/vllm-project/vllm/pull/11907
* [Doc][5/N] Move Community and API Reference to the bottom by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11896
* [VLM] Enable tokenized inputs for merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11900
* [Doc] Show default pooling method in a table by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11904
* [torch.compile] Hide KV cache behind torch.compile boundary by @heheda12345 in https://github.com/vllm-project/vllm/pull/11677
* [Bugfix] Validate lora adapters to avoid crashing server by @joerunde in https://github.com/vllm-project/vllm/pull/11727
* [BUGFIX] Fix `UnspecifiedPlatform` package name by @jikunshang in https://github.com/vllm-project/vllm/pull/11916
* [ci] fix gh200 tests by @youkaichao in https://github.com/vllm-project/vllm/pull/11919
* [optimization] remove python function call for custom activation op by @cennn in https://github.com/vllm-project/vllm/pull/11885
* [platform] support pytorch custom op pluggable by @wangxiyuan in https://github.com/vllm-project/vllm/pull/11328
* Replace "online inference" with "online serving" by @hmellor in https://github.com/vllm-project/vllm/pull/11923
* [ci] Fix sampler tests by @youkaichao in https://github.com/vllm-project/vllm/pull/11922
* [Doc] [1/N] Initial guide for merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11925
* [platform] support custom torch.compile backend key by @wangxiyuan in https://github.com/vllm-project/vllm/pull/11318
* [Doc] Rename offline inference examples by @hmellor in https://github.com/vllm-project/vllm/pull/11927
* [Docs] Fix docstring in `get_ip` function by @KuntaiDu in https://github.com/vllm-project/vllm/pull/11932
* [Doc] Docstring fix in `benchmark_long_document_qa_throughput.py` by @KuntaiDu in https://github.com/vllm-project/vllm/pull/11933
* [Hardware][CPU] Support MOE models on x86 CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/11831
* [Misc] Clean up debug code in Deepseek-V3 by @Isotr0py in https://github.com/vllm-project/vllm/pull/11930
* [Misc] Update benchmark_prefix_caching.py fixed example usage by @remimin in https://github.com/vllm-project/vllm/pull/11920
* [Bugfix] Check that number of images matches number of <|image|> tokens with mllama by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/11939
* [mypy] Fix mypy warnings in api_server.py by @frreiss in https://github.com/vllm-project/vllm/pull/11941
* [ci] fix broken distributed-tests-4-gpus by @youkaichao in https://github.com/vllm-project/vllm/pull/11937
* [Bugfix][SpecDecode] Adjust Eagle model architecture to align with intended design by @llsj14 in https://github.com/vllm-project/vllm/pull/11672
* [Bugfix] fused_experts_impl wrong compute type for float32 by @shaochangxu in https://github.com/vllm-project/vllm/pull/11921
* [CI/Build] Move model-specific multi-modal processing tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11934
* [Doc] Basic guide for writing unit tests for new models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11951
* [Bugfix] Fix RobertaModel loading by @NickLucche in https://github.com/vllm-project/vllm/pull/11940
* [Model] Add cogagent model support vLLM by @sixsixcoder in https://github.com/vllm-project/vllm/pull/11742
* [V1] Avoid sending text prompt to core engine by @ywang96 in https://github.com/vllm-project/vllm/pull/11963
* [CI/Build] Add markdown linter by @rafvasq in https://github.com/vllm-project/vllm/pull/11857
* [Model] Initialize support for Deepseek-VL2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/11578
* [Hardware][CPU] Multi-LoRA implementation for the CPU backend by @Akshat-Tripathi in https://github.com/vllm-project/vllm/pull/11100
* [Hardware][TPU] workaround fix for MoE on TPU by @avshalomman in https://github.com/vllm-project/vllm/pull/11764
* [V1][Core][1/n] Logging and Metrics by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11962
* [Model] Support GGUF models newly added in `transformers` 4.46.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/9685
* [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction by @robertgshaw2-redhat in https://github.com/vllm-project/vllm/pull/11973
* [MISC] fix typo in kv transfer send recv test by @yyccli in https://github.com/vllm-project/vllm/pull/11983
* [Bug] Fix usage of `.transpose()` and `.view()` consecutively. by @liaoyanqing666 in https://github.com/vllm-project/vllm/pull/11979
* [CI][Spec Decode] fix: broken test for EAGLE model by @llsj14 in https://github.com/vllm-project/vllm/pull/11972
* [Misc] Fix Deepseek V2 fp8 kv-scale remapping by @Concurrensee in https://github.com/vllm-project/vllm/pull/11947
* [Misc]Minor Changes about Worker by @noemotiovon in https://github.com/vllm-project/vllm/pull/11555
* [platform] add ray_device_key by @youkaichao in https://github.com/vllm-project/vllm/pull/11948
* Fix Max Token ID for Qwen-VL-Chat by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/11980
* [Kernel] Attention.forward with unified_attention when use_direct_call=True by @heheda12345 in https://github.com/vllm-project/vllm/pull/11967
* [Doc][V1] Update model implementation guide for V1 support by @ywang96 in https://github.com/vllm-project/vllm/pull/11998
* [Doc] Organise installation documentation into categories and tabs by @hmellor in https://github.com/vllm-project/vllm/pull/11935
* [platform] add device_control env var by @youkaichao in https://github.com/vllm-project/vllm/pull/12009
* [Platform] Move get_punica_wrapper() function to Platform by @shen-shanshan in https://github.com/vllm-project/vllm/pull/11516
* bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function by @e1ijah1 in https://github.com/vllm-project/vllm/pull/11982
* [Doc] Fix build from source and installation link in README.md by @Yikun in https://github.com/vllm-project/vllm/pull/12013
* [Bugfix] Fix deepseekv3 gate bias error by @SunflowerAries in https://github.com/vllm-project/vllm/pull/12002
* [Docs] Add Sky Computing Lab to project intro by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12019
* [Hardware][Gaudi][Bugfix] Fix set_forward_context arguments and CI test execution by @kzawora-intel in https://github.com/vllm-project/vllm/pull/12014
* [Doc] Update Quantization Hardware Support Documentation by @tjtanaa in https://github.com/vllm-project/vllm/pull/12025
* [HPU][misc] add comments for explanation by @youkaichao in https://github.com/vllm-project/vllm/pull/12034
* [Bugfix] Fix various bugs in multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12031
* [Kernel] Revert the API change of Attention.forward by @heheda12345 in https://github.com/vllm-project/vllm/pull/12038
* [Platform] Add output for Attention Backend by @wangxiyuan in https://github.com/vllm-project/vllm/pull/11981
* [Bugfix][Kernel] Give unique name to BlockSparseFlashAttention by @heheda12345 in https://github.com/vllm-project/vllm/pull/12040
* Explain where the engine args go when using Docker by @hmellor in https://github.com/vllm-project/vllm/pull/12041
* [Doc]: Update the Json Example of the `Engine Arguments` document by @maang-h in https://github.com/vllm-project/vllm/pull/12045
* [Misc]  Merge bitsandbytes_stacked_params_mapping and packed_modules_mapping by @jeejeelee in https://github.com/vllm-project/vllm/pull/11924
* [Kernel] Support MulAndSilu by @jeejeelee in https://github.com/vllm-project/vllm/pull/11624
* [HPU][Bugfix] Don't use /dev/accel/accel0 for HPU autodetection in setup.py by @kzawora-intel in https://github.com/vllm-project/vllm/pull/12046
* [Platform] Refactor current_memory_usage() function in DeviceMemoryProfiler to Platform by @shen-shanshan in https://github.com/vllm-project/vllm/pull/11369
* [V1][BugFix] Fix edge case in VLM scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12065
* [Misc] Add multipstep chunked-prefill support for FlashInfer by @elfiegg in https://github.com/vllm-project/vllm/pull/10467
* [core] Turn off GPU communication overlap for Ray executor by @ruisearch42 in https://github.com/vllm-project/vllm/pull/12051
* [core] platform agnostic executor via collective_rpc by @youkaichao in https://github.com/vllm-project/vllm/pull/11256
* [Doc] Update examples to remove SparseAutoModelForCausalLM by @kylesayrs in https://github.com/vllm-project/vllm/pull/12062
* [V1][Prefix Cache] Move the logic of num_computed_tokens into KVCacheManager by @heheda12345 in https://github.com/vllm-project/vllm/pull/12003
* Fix: cases with empty sparsity config by @rahul-tuli in https://github.com/vllm-project/vllm/pull/12057
* Type-fix: make execute_model output type optional by @youngkent in https://github.com/vllm-project/vllm/pull/12020
* [Platform] Do not raise error if _Backend is not found by @wangxiyuan in https://github.com/vllm-project/vllm/pull/12023
* [Model]: Support internlm3 by @RunningLeon in https://github.com/vllm-project/vllm/pull/12037
* Misc: allow to use proxy in `HTTPConnection` by @zhouyuan in https://github.com/vllm-project/vllm/pull/12042
* [Misc][Quark] Upstream Quark format to VLLM by @kewang-xlnx in https://github.com/vllm-project/vllm/pull/10765
* [Doc]: Update `OpenAI-Compatible Server` documents by @maang-h in https://github.com/vllm-project/vllm/pull/12082
* [Bugfix] use right truncation for non-generative tasks by @joerunde in https://github.com/vllm-project/vllm/pull/12050
* [V1][Core] Autotune encoder cache budget by @ywang96 in https://github.com/vllm-project/vllm/pull/11895
* [Bugfix] Fix _get_lora_device for HQQ marlin by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/12090
* Allow hip sources to be directly included when compiling for rocm. by @tvirolai-amd in https://github.com/vllm-project/vllm/pull/12087
* [Core] Default to using per_token quantization for fp8 when cutlass is supported. by @elfiegg in https://github.com/vllm-project/vllm/pull/8651
* [Doc] Add documentation for specifying model architecture by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12105
* Various cosmetic/comment fixes by @mgoin in https://github.com/vllm-project/vllm/pull/12089
* [Bugfix] Remove hardcoded `head_size=256` for Deepseek v2 and v3 by @Isotr0py in https://github.com/vllm-project/vllm/pull/12067
* Support torchrun and SPMD-style offline inference by @youkaichao in https://github.com/vllm-project/vllm/pull/12071
* [core] LLM.collective_rpc interface and RLHF example by @youkaichao in https://github.com/vllm-project/vllm/pull/12084
* [Bugfix] Fix max image feature size for Llava-one-vision by @ywang96 in https://github.com/vllm-project/vllm/pull/12104
* [misc] Add LoRA kernel micro benchmarks by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/11579
* [Model] Add support for deepseek-vl2-tiny model by @Isotr0py in https://github.com/vllm-project/vllm/pull/12068
* [Bugfix] Set enforce_eager automatically for mllama by @heheda12345 in https://github.com/vllm-project/vllm/pull/12127
* [Bugfix] Fix a path bug in disaggregated prefill example script. by @KuntaiDu in https://github.com/vllm-project/vllm/pull/12121
* [CI]add genai-perf benchmark in nightly benchmark by @jikunshang in https://github.com/vllm-project/vllm/pull/10704
* [Doc] Add instructions on using Podman when SELinux is active by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12136
* [Bugfix] Revert PR #11435: Fix issues in CPU build Dockerfile. Fixes #9182 by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12135
* [BugFix] add more `is not None` check in VllmConfig.__post_init__ by @heheda12345 in https://github.com/vllm-project/vllm/pull/12138
* [Misc] Add deepseek_vl2 chat template by @Isotr0py in https://github.com/vllm-project/vllm/pull/12143
* [ROCm][MoE] moe tuning support for rocm by @divakar-amd in https://github.com/vllm-project/vllm/pull/12049
* [V1] Move more control of kv cache initialization from model_executor to EngineCore by @heheda12345 in https://github.com/vllm-project/vllm/pull/11960
* [Misc][LoRA] Improve the readability of LoRA error messages during loading by @jeejeelee in https://github.com/vllm-project/vllm/pull/12102
* [CI/Build][CPU][Bugfix] Fix CPU CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/12150
* [core] allow callable in collective_rpc by @youkaichao in https://github.com/vllm-project/vllm/pull/12151
* [Bugfix] Fix score api for missing max_model_len validation by @wallashss in https://github.com/vllm-project/vllm/pull/12119
* [Bugfix] Mistral tokenizer encode accept list of str by @jikunshang in https://github.com/vllm-project/vllm/pull/12149
* [AMD][FP8] Using MI300 FP8 format on ROCm for block_quant by @gshtras in https://github.com/vllm-project/vllm/pull/12134
* [torch.compile] disable logging when cache is disabled by @youkaichao in https://github.com/vllm-project/vllm/pull/12043
* [misc] fix cross-node TP by @youkaichao in https://github.com/vllm-project/vllm/pull/12166
* [AMD][CI/Build][Bugfix] updated pytorch stale wheel path by using stable wheel by @hongxiayang in https://github.com/vllm-project/vllm/pull/12172
* [core] further polish memory profiling by @youkaichao in https://github.com/vllm-project/vllm/pull/12126
* [Docs] Fix broken link in SECURITY.md by @russellb in https://github.com/vllm-project/vllm/pull/12175
* [Model] Port deepseek-vl2 processor and remove `deepseek_vl2` dependency by @Isotr0py in https://github.com/vllm-project/vllm/pull/12169
* [core] clean up executor class hierarchy between v1 and v0 by @youkaichao in https://github.com/vllm-project/vllm/pull/12171
* [Misc] Support register quantization method out-of-tree by @ice-tong in https://github.com/vllm-project/vllm/pull/11969
* [V1] Collect env var for usage stats by @simon-mo in https://github.com/vllm-project/vllm/pull/12115
* [BUGFIX] Move scores to float32 in case of running xgrammar on cpu by @madamczykhabana in https://github.com/vllm-project/vllm/pull/12152
* [Bugfix] Fix multi-modal processors for transformers 4.48 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12187
* [torch.compile] store inductor compiled Python file by @youkaichao in https://github.com/vllm-project/vllm/pull/12182
* benchmark_serving support --served-model-name param by @gujingit in https://github.com/vllm-project/vllm/pull/12109
* [Misc] Add BNB support to GLM4-V model by @Isotr0py in https://github.com/vllm-project/vllm/pull/12184
* [V1] Add V1 support of Qwen2-VL  by @ywang96 in https://github.com/vllm-project/vllm/pull/12128
* [Model] Support for fairseq2 Llama by @MartinGleize in https://github.com/vllm-project/vllm/pull/11442
* [Bugfix] Fix num_heads value for simple connector when tp enabled by @ShangmingCai in https://github.com/vllm-project/vllm/pull/12074
* [torch.compile] fix sym_tensor_indices by @youkaichao in https://github.com/vllm-project/vllm/pull/12191
* Move linting to `pre-commit` by @hmellor in https://github.com/vllm-project/vllm/pull/11975
* [DOC] Fix typo in SingleStepOutputProcessor docstring and assert message by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12194
* [DOC] Add missing docstring for additional args in LLMEngine.add_request() by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12195
* [Bugfix] Fix incorrect types in LayerwiseProfileResults by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12196
* [Model] Add Qwen2 PRM model support by @Isotr0py in https://github.com/vllm-project/vllm/pull/12202
* [Core] Interface for accessing model from `VllmRunner` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10353
* [misc] add placeholder format.sh by @youkaichao in https://github.com/vllm-project/vllm/pull/12206
* [CI/Build] Remove dummy CI steps by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12208
* [CI/Build] Make pre-commit faster by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12212
* [Model] Upgrade Aria to transformers 4.48 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12203
* [misc] print a message to suggest how to bypass commit hooks by @youkaichao in https://github.com/vllm-project/vllm/pull/12217
* [core][bugfix] configure env var during import vllm by @youkaichao in https://github.com/vllm-project/vllm/pull/12209
* [V1] Remove `_get_cache_block_size` by @heheda12345 in https://github.com/vllm-project/vllm/pull/12214
* [Misc] Pass `attention` to impl backend by @wangxiyuan in https://github.com/vllm-project/vllm/pull/12218
* [Bugfix] Fix `HfExampleModels.find_hf_info` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12223
* [CI] Pass local python version explicitly to pre-commit mypy.sh by @heheda12345 in https://github.com/vllm-project/vllm/pull/12224
* [Misc] Update CODEOWNERS by @ywang96 in https://github.com/vllm-project/vllm/pull/12229
* fix: update platform detection for M-series arm based MacBook processors by @isikhi in https://github.com/vllm-project/vllm/pull/12227
* [misc] add cuda runtime version to usage data by @youkaichao in https://github.com/vllm-project/vllm/pull/12190
* [bugfix] catch xgrammar unsupported array constraints by @Jason-CKY in https://github.com/vllm-project/vllm/pull/12210
* [Kernel] optimize moe_align_block_size for cuda graph and large num_experts (e.g. DeepSeek-V3) by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/12222
* Add quantization and guided decoding CODEOWNERS by @mgoin in https://github.com/vllm-project/vllm/pull/12228
* [AMD][Build] Porting dockerfiles from the ROCm/vllm fork by @gshtras in https://github.com/vllm-project/vllm/pull/11777
* [BugFix] Fix GGUF tp>1 models when vocab_size is not divisible by 64 by @NickLucche in https://github.com/vllm-project/vllm/pull/12230
* [ci/build] disable failed and flaky tests by @youkaichao in https://github.com/vllm-project/vllm/pull/12240
* [Misc] Rename `MultiModalInputsV2 -> MultiModalInputs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12244
* [Misc]Add BNB quantization for PaliGemmaForConditionalGeneration  by @jeejeelee in https://github.com/vllm-project/vllm/pull/12237
* [Misc] Remove redundant TypeVar from base model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12248
* [Bugfix] Fix mm_limits access for merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12252
* [torch.compile] transparent compilation with more logging by @youkaichao in https://github.com/vllm-project/vllm/pull/12246
* [V1][Bugfix] Fix data item ordering in mixed-modality inference by @ywang96 in https://github.com/vllm-project/vllm/pull/12259
* [Bugfix] Remove comments re: pytorch for outlines + compressed-tensors dependencies by @tdoublep in https://github.com/vllm-project/vllm/pull/12260
* [Platform] improve platforms getattr by @MengqingCao in https://github.com/vllm-project/vllm/pull/12264
* [ci/build] add nightly torch for test by @youkaichao in https://github.com/vllm-project/vllm/pull/12270
* [Bugfix] fix race condition that leads to wrong order of token returned by @joennlae in https://github.com/vllm-project/vllm/pull/10802
* [Kernel] fix moe_align_block_size error condition by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/12239
* [v1][stats][1/n] Add RequestStatsUpdate and RequestStats types  by @rickyyx in https://github.com/vllm-project/vllm/pull/10907
* [Bugfix] Multi-sequence broken by @andylolu2 in https://github.com/vllm-project/vllm/pull/11898
* [Misc] Remove experimental dep from tracing.py by @codefromthecrypt in https://github.com/vllm-project/vllm/pull/12007
* [Misc] Set default backend to SDPA for get_vit_attn_backend by @wangxiyuan in https://github.com/vllm-project/vllm/pull/12235
* [Core] Free CPU pinned memory on environment cleanup by @janimo in https://github.com/vllm-project/vllm/pull/10477
* [bugfix] moe tuning. rm is_navi() by @divakar-amd in https://github.com/vllm-project/vllm/pull/12273
* [BUGFIX] When skip_tokenize_init and multistep are set, execution crashes by @maleksan85 in https://github.com/vllm-project/vllm/pull/12277
* [Documentation][AMD] Add information about prebuilt ROCm vLLM docker for perf validation purpose by @hongxiayang in https://github.com/vllm-project/vllm/pull/12281
* [VLM] Simplify post-processing of replacement info by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12269
* [ci/lint] Add back default arg for pre-commit by @khluu in https://github.com/vllm-project/vllm/pull/12279
* [CI] add docker volume prune to neuron CI by @liangfu in https://github.com/vllm-project/vllm/pull/12291
* [Ci/Build] Fix mypy errors on main by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12296
* [Benchmark] More accurate TPOT calc in `benchmark_serving.py` by @njhill in https://github.com/vllm-project/vllm/pull/12288
* [core] separate builder init and builder prepare for each batch by @youkaichao in https://github.com/vllm-project/vllm/pull/12253
* [Build] update requirements of no-device by @MengqingCao in https://github.com/vllm-project/vllm/pull/12299
* [Core] Support fully transparent sleep mode by @youkaichao in https://github.com/vllm-project/vllm/pull/11743
* [VLM] Avoid unnecessary tokenization by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12310
* [Model][Bugfix]: correct Aria model output by @xffxff in https://github.com/vllm-project/vllm/pull/12309
* [Bugfix][VLM] Fix mixed-modality inference backward compatibility for V0 by @ywang96 in https://github.com/vllm-project/vllm/pull/12313
* [Doc] Add docs for prompt replacement by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12318
* [Misc] Fix the error in the tip for the --lora-modules parameter by @WangErXiao in https://github.com/vllm-project/vllm/pull/12319
* [Misc]  Improve the readability of BNB error messages  by @jeejeelee in https://github.com/vllm-project/vllm/pull/12320
* [Hardware][Gaudi][Bugfix] Fix HPU tensor parallelism, enable multiprocessing executor by @kzawora-intel in https://github.com/vllm-project/vllm/pull/12167
* [Core] Support `reset_prefix_cache` by @comaniac in https://github.com/vllm-project/vllm/pull/12284
* [Frontend][V1] Online serving performance improvements by @njhill in https://github.com/vllm-project/vllm/pull/12287
* [AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD by @rasmith in https://github.com/vllm-project/vllm/pull/12282
* [Bugfix] Fixing  AMD LoRA CI test. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/12329
* [Docs] Update FP8 KV Cache documentation by @mgoin in https://github.com/vllm-project/vllm/pull/12238
* [Docs] Document vulnerability disclosure process by @russellb in https://github.com/vllm-project/vllm/pull/12326
* [V1] Add `uncache_blocks` by @comaniac in https://github.com/vllm-project/vllm/pull/12333
* [doc] explain common errors around torch.compile by @youkaichao in https://github.com/vllm-project/vllm/pull/12340
* [Hardware][Gaudi][BugFix] Fix dataclass error due to triton package update by @zhenwei-intel in https://github.com/vllm-project/vllm/pull/12338
* [Bugfix] Fix k_proj's bias for whisper self attention by @Isotr0py in https://github.com/vllm-project/vllm/pull/12342
* [Kernel] Flash Attention 3 Support by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12093
* [Doc] Troubleshooting errors during model inspection by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12351
* [V1] Simplify M-RoPE by @ywang96 in https://github.com/vllm-project/vllm/pull/12352
* [Bugfix] Fix broken internvl2 inference with v1 by @Isotr0py in https://github.com/vllm-project/vllm/pull/12360
* [core] add wake_up doc and some sanity check by @youkaichao in https://github.com/vllm-project/vllm/pull/12361
* [torch.compile] decouple compile sizes and cudagraph sizes by @youkaichao in https://github.com/vllm-project/vllm/pull/12243
* [FP8][Kernel] Dynamic kv cache scaling factors computation by @gshtras in https://github.com/vllm-project/vllm/pull/11906
* [TPU] Update TPU CI to use torchxla nightly on 20250122 by @lsy323 in https://github.com/vllm-project/vllm/pull/12334
* [Docs] Document Phi-4 support by @Isotr0py in https://github.com/vllm-project/vllm/pull/12362
* [BugFix] Fix parameter names and `process_after_weight_loading` for W4A16 MoE Group Act Order  by @dsikka in https://github.com/vllm-project/vllm/pull/11528
* [Misc] Fix OpenAI API Compatibility Issues in Benchmark Script by @jsato8094 in https://github.com/vllm-project/vllm/pull/12357
* [Docs] Add meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12345
* [Docs] Update spec decode + structured output in compat matrix by @russellb in https://github.com/vllm-project/vllm/pull/12373
* [V1][Frontend] Coalesce bunched `RequestOutput`s by @njhill in https://github.com/vllm-project/vllm/pull/12298
* Set weights_only=True when using torch.load() by @russellb in https://github.com/vllm-project/vllm/pull/12366
* [Bugfix] Path join when building local path for S3 clone by @omer-dayan in https://github.com/vllm-project/vllm/pull/12353
* Update compressed-tensors version by @dsikka in https://github.com/vllm-project/vllm/pull/12367
* [V1] Increase default batch size for H100/H200 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12369
* [perf] fix perf regression from #12253 by @youkaichao in https://github.com/vllm-project/vllm/pull/12380
* [Misc] Use VisionArena Dataset for VLM Benchmarking by @ywang96 in https://github.com/vllm-project/vllm/pull/12389
* [ci/build] fix wheel size check by @youkaichao in https://github.com/vllm-project/vllm/pull/12396
* [Hardware][Gaudi][Doc] Add missing step in setup instructions by @MohitIntel in https://github.com/vllm-project/vllm/pull/12382
* [ci/build] sync default value for wheel size by @youkaichao in https://github.com/vllm-project/vllm/pull/12398
* [Misc] Enable proxy support in benchmark script by @jsato8094 in https://github.com/vllm-project/vllm/pull/12356
* [Bugfix][Kernel] Fix CUDA 11.8 being broken by FA3 build by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12375
* [Misc] Remove deprecated code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12383
* [Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons). by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12405
* [Bugfix][Kernel] Fix moe align block issue for mixtral by @ElizaWszola in https://github.com/vllm-project/vllm/pull/12413
* [Bugfix] Fix BLIP-2 processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12412
* [ROCm][MoE] MI300 tuned configs Mixtral-8x(7B,22B) | fp16, fp8  by @divakar-amd in https://github.com/vllm-project/vllm/pull/12408
* [Misc] Add FA2 support to ViT MHA layer by @Isotr0py in https://github.com/vllm-project/vllm/pull/12355
* [TPU][CI] Update torchxla version in requirement-tpu.txt by @lsy323 in https://github.com/vllm-project/vllm/pull/12422
* [Misc][Bugfix] FA3 support to ViT MHA layer by @ywang96 in https://github.com/vllm-project/vllm/pull/12435
* [V1][Perf] Reduce scheduling overhead in model runner after cuda sync by @youngkent in https://github.com/vllm-project/vllm/pull/12094
* [V1][Bugfix] Fix assertion when mm hashing is turned off by @ywang96 in https://github.com/vllm-project/vllm/pull/12439
* [Misc] Revert FA on ViT #12355 and #12435 by @ywang96 in https://github.com/vllm-project/vllm/pull/12445
* [Frontend] Set server's maximum number of generated tokens using generation_config.json by @mhendrey in https://github.com/vllm-project/vllm/pull/12242
* [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12417
* [Bugfix/CI] Fix broken kernels/test_mha.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12450
* [Bugfix][Kernel] Fix perf regression caused by PR #12405 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/12434
* [Build/CI] Fix libcuda.so linkage by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/12424
* [Frontend] Rerank API (Jina- and Cohere-compatible API)  by @K-Mistele in https://github.com/vllm-project/vllm/pull/12376
* [DOC] Add link to vLLM blog by @terrytangyuan in https://github.com/vllm-project/vllm/pull/12460
* [V1] Avoid list creation in input preparation by @WoosukKwon in https://github.com/vllm-project/vllm/pull/12457
* [Frontend] Support scores endpoint in run_batch by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/12430
* [Bugfix] Fix Granite 3.0 MoE model loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/12446

## New Contributors
* @Chen-0210 made their first contribution in https://github.com/vllm-project/vllm/pull/11549
* @ErezSC42 made their first contribution in https://github.com/vllm-project/vllm/pull/11209
* @selalipop made their first contribution in https://github.com/vllm-project/vllm/pull/11561
* @rajveerb made their first contribution in https://github.com/vllm-project/vllm/pull/11376
* @hj-wei made their first contribution in https://github.com/vllm-project/vllm/pull/11515
* @ayylemao made their first contribution in https://github.com/vllm-project/vllm/pull/11439
* @JohnGiorgi made their first contribution in https://github.com/vllm-project/vllm/pull/6909
* @sakunkun made their first contribution in https://github.com/vllm-project/vllm/pull/11565
* @ApostaC made their first contribution in https://github.com/vllm-project/vllm/pull/11533
* @houseroad made their first contribution in https://github.com/vllm-project/vllm/pull/11667
* @serihiro made their first contribution in https://github.com/vllm-project/vllm/pull/11666
* @CloseChoice made their first contribution in https://github.com/vllm-project/vllm/pull/11680
* @chunyang-wen made their first contribution in https://github.com/vllm-project/vllm/pull/11689
* @kathyyu-google made their first contribution in https://github.com/vllm-project/vllm/pull/10013
* @bjmsong made their first contribution in https://github.com/vllm-project/vllm/pull/11688
* @nathan-az made their first contribution in https://github.com/vllm-project/vllm/pull/11576
* @SachinVarghese made their first contribution in https://github.com/vllm-project/vllm/pull/11694
* @zinccat made their first contribution in https://github.com/vllm-project/vllm/pull/11708
* @WangErXiao made their first contribution in https://github.com/vllm-project/vllm/pull/11656
* @Bryce1010 made their first contribution in https://github.com/vllm-project/vllm/pull/11701
* @bet0x made their first contribution in https://github.com/vllm-project/vllm/pull/11718
* @yanburman made their first contribution in https://github.com/vllm-project/vllm/pull/11233
* @RuixiangMa made their first contribution in https://github.com/vllm-project/vllm/pull/11751
* @surajssd made their first contribution in https://github.com/vllm-project/vllm/pull/11679
* @ys950902 made their first contribution in https://github.com/vllm-project/vllm/pull/11648
* @XiaobingSuper made their first contribution in https://github.com/vllm-project/vllm/pull/11768
* @jiangjiadi made their first contribution in https://github.com/vllm-project/vllm/pull/11795
* @guspan-tanadi made their first contribution in https://github.com/vllm-project/vllm/pull/11878
* @yeqcharlotte made their first contribution in https://github.com/vllm-project/vllm/pull/11875
* @charlesfrye made their first contribution in https://github.com/vllm-project/vllm/pull/11907
* @remimin made their first contribution in https://github.com/vllm-project/vllm/pull/11920
* @frreiss made their first contribution in https://github.com/vllm-project/vllm/pull/11941
* @shaochangxu made their first contribution in https://github.com/vllm-project/vllm/pull/11921
* @Akshat-Tripathi made their first contribution in https://github.com/vllm-project/vllm/pull/11100
* @liaoyanqing666 made their first contribution in https://github.com/vllm-project/vllm/pull/11979
* @Concurrensee made their first contribution in https://github.com/vllm-project/vllm/pull/11947
* @shen-shanshan made their first contribution in https://github.com/vllm-project/vllm/pull/11516
* @e1ijah1 made their first contribution in https://github.com/vllm-project/vllm/pull/11982
* @Yikun made their first contribution in https://github.com/vllm-project/vllm/pull/12013
* @SunflowerAries made their first contribution in https://github.com/vllm-project/vllm/pull/12002
* @maang-h made their first contribution in https://github.com/vllm-project/vllm/pull/12045
* @rahul-tuli made their first contribution in https://github.com/vllm-project/vllm/pull/12057
* @youngkent made their first contribution in https://github.com/vllm-project/vllm/pull/12020
* @RunningLeon made their first contribution in https://github.com/vllm-project/vllm/pull/12037
* @kewang-xlnx made their first contribution in https://github.com/vllm-project/vllm/pull/10765
* @tvirolai-amd made their first contribution in https://github.com/vllm-project/vllm/pull/12087
* @ice-tong made their first contribution in https://github.com/vllm-project/vllm/pull/11969
* @madamczykhabana made their first contribution in https://github.com/vllm-project/vllm/pull/12152
* @gujingit made their first contribution in https://github.com/vllm-project/vllm/pull/12109
* @MartinGleize made their first contribution in https://github.com/vllm-project/vllm/pull/11442
* @isikhi made their first contribution in https://github.com/vllm-project/vllm/pull/12227
* @Jason-CKY made their first contribution in https://github.com/vllm-project/vllm/pull/12210
* @andylolu2 made their first contribution in https://github.com/vllm-project/vllm/pull/11898
* @codefromthecrypt made their first contribution in https://github.com/vllm-project/vllm/pull/12007
* @zhenwei-intel made their first contribution in https://github.com/vllm-project/vllm/pull/12338
* @MohitIntel made their first contribution in https://github.com/vllm-project/vllm/pull/12382
* @mhendrey made their first contribution in https://github.com/vllm-project/vllm/pull/12242

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.6...v0.7.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.7.0)

---

## v0.6.6.post1: v0.6.6.post1
**Published:** 2024-12-27

This release restore functionalities for other quantized MoEs, which was introduced as part of initial DeepSeek V3 support ðŸ™‡ . 

## What's Changed
* [Docs] Document Deepseek V3 support by @simon-mo in https://github.com/vllm-project/vllm/pull/11535
* Update openai_compatible_server.md by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11536
* [V1] Use FlashInfer Sampling Kernel for Top-P & Top-K Sampling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11394
* [V1] Fix yapf by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11538
* [CI] Fix broken CI by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11543
* [misc] fix typing by @youkaichao in https://github.com/vllm-project/vllm/pull/11540
* [V1][3/N] API Server: Reduce Task Switching + Handle Abort Properly by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11534
* [BugFix] Deepseekv3 broke quantization for all other methods by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11547


**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.6...v0.6.6.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.6.post1)

---

## v0.6.6: v0.6.6
**Published:** 2024-12-27

## Highlights
* Support Deepseek V3 (#11523, #11502) model. 
	* On 8xH200s or MI300x: `vllm serve deepseek-ai/DeepSeek-V3 --tensor-parallel-size 8 --trust-remote-code --max-model-len 8192`. The context length can be increased to about 32K beyond running into memory issue.
	* For other devices, follow our [distributed inference](https://docs.vllm.ai/en/latest/serving/distributed_serving.html) guide to enable tensor parallel and/or pipeline parallel inference
	* We are just getting started for enhancing the support and unlock more performance. See #11539 for planned work. 
* Last mile stretch for V1 engine refactoring: API Server (#11529, #11530), penalties for sampler (#10681), prefix caching for vision language models (#11187, #11305), TP Ray executor (#11107,#11472)

* Breaking change: `X-Request-ID` echoing is now opt-in instead of on by default for performance reason. Set `--enable-request-id-headers` to enable it.

### Model Support
* IBM Granite 3.1 (#11307), JambaForSequenceClassification model  (#10860)
* Add `QVQ` and `QwQ` to the list of supported models (#11509)


### Performance
* Cutlass 2:4 Sparsity + FP8/INT8 Quant Support (#10995)


### Production Engine
* Support streaming model from S3 using RunAI Model Streamer as optional loader (#10192)
* Online Pooling API (#11457)
* Load video from base64 (#11492)

### Others
* Add pypi index for every commit and nightly build (#11404)

## What's Changed
* [Bugfix] Set temperature=0.7 in test_guided_choice_chat by @mgoin in https://github.com/vllm-project/vllm/pull/11264
* [V1] Prefix caching for vision language models by @comaniac in https://github.com/vllm-project/vllm/pull/11187
* [Bugfix] Restore support for larger block sizes by @kzawora-intel in https://github.com/vllm-project/vllm/pull/11259
* [Bugfix] Fix guided decoding with tokenizer mode mistral by @wallashss in https://github.com/vllm-project/vllm/pull/11046
* [MISC][XPU]update ipex link for CI fix by @yma11 in https://github.com/vllm-project/vllm/pull/11278
* [Kernel]: Cutlass 2:4 Sparsity + FP8/Int8 Quant Support by @dsikka in https://github.com/vllm-project/vllm/pull/10995
* [Bugfix] Fix broken phi3-v mm_processor_kwargs tests by @Isotr0py in https://github.com/vllm-project/vllm/pull/11263
* [CI][Misc] Remove Github Action Release Workflow by @simon-mo in https://github.com/vllm-project/vllm/pull/11274
* [FIX] update openai version by @jikunshang in https://github.com/vllm-project/vllm/pull/11287
* [Bugfix] fix minicpmv test by @joerunde in https://github.com/vllm-project/vllm/pull/11304
* [V1] VLM - enable processor cache by default by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/11305
* [Bugfix][Build/CI] Fix sparse CUTLASS compilation on CUDA [12.0, 12.2) by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11311
* [Model] IBM Granite 3.1 by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/11307
* [CI] Expand test_guided_generate to test all backends by @mgoin in https://github.com/vllm-project/vllm/pull/11313
* [V1] Simplify prefix caching logic by removing `num_evictable_computed_blocks` by @heheda12345 in https://github.com/vllm-project/vllm/pull/11310
* [VLM] Merged multimodal processor for Qwen2-Audio by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11303
* [Kernel] Refactor Cutlass c3x by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/10049
* [Misc] Optimize ray worker initialization time by @ruisearch42 in https://github.com/vllm-project/vllm/pull/11275
* [misc] benchmark_throughput : Add LoRA by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/11267
* [Feature] Add load generation config from model by @liuyanyi in https://github.com/vllm-project/vllm/pull/11164
* [Bugfix] Cleanup Pixtral HF code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11333
* [Model] Add JambaForSequenceClassification model  by @yecohn in https://github.com/vllm-project/vllm/pull/10860
* [V1] Fix multimodal profiling for `Molmo` by @ywang96 in https://github.com/vllm-project/vllm/pull/11325
* [Model] Refactor Qwen2-VL to use merged multimodal processor by @Isotr0py in https://github.com/vllm-project/vllm/pull/11258
* [Misc] Clean up and consolidate LRUCache by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11339
* [Bugfix] Fix broken CPU compressed-tensors test by @Isotr0py in https://github.com/vllm-project/vllm/pull/11338
* [Misc] Remove unused vllm/block.py by @Ghjk94522 in https://github.com/vllm-project/vllm/pull/11336
* [CI] Adding CPU docker pipeline by @zhouyuan in https://github.com/vllm-project/vllm/pull/11261
* [Bugfix][Hardware][POWERPC] Fix auto dtype failure in case of POWER10 by @Akashcodes732 in https://github.com/vllm-project/vllm/pull/11331
* [ci][gh200] dockerfile clean up by @youkaichao in https://github.com/vllm-project/vllm/pull/11351
* [Misc] Add tqdm progress bar during graph capture by @mgoin in https://github.com/vllm-project/vllm/pull/11349
* [Bugfix] Fix spec decoding when seed is none in a batch by @wallashss in https://github.com/vllm-project/vllm/pull/10863
* [misc] add early error message for custom ops by @youkaichao in https://github.com/vllm-project/vllm/pull/11355
* [doc] backward compatibility for 0.6.4 by @youkaichao in https://github.com/vllm-project/vllm/pull/11359
* [V1] Fix profiling for models with merged input processor by @ywang96 in https://github.com/vllm-project/vllm/pull/11370
* [CI/Build] fix pre-compiled wheel install for exact tag by @dtrifiro in https://github.com/vllm-project/vllm/pull/11373
* [Core] Loading model from S3 using RunAI Model Streamer as optional loader by @omer-dayan in https://github.com/vllm-project/vllm/pull/10192
* [Bugfix] Don't log OpenAI field aliases as ignored by @mgoin in https://github.com/vllm-project/vllm/pull/11378
* [doc] explain nccl requirements for rlhf by @youkaichao in https://github.com/vllm-project/vllm/pull/11381
* Add ray[default] to wget to run distributed inference out of box by @Jeffwan in https://github.com/vllm-project/vllm/pull/11265
* [V1][Bugfix] Skip hashing empty or None mm_data by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11386
* [Bugfix] update should_ignore_layer by @horheynm in https://github.com/vllm-project/vllm/pull/11354
* [V1] Make AsyncLLMEngine v1-v0 opaque by @rickyyx in https://github.com/vllm-project/vllm/pull/11383
* [Bugfix] Fix issues for `Pixtral-Large-Instruct-2411` by @ywang96 in https://github.com/vllm-project/vllm/pull/11393
* [CI] Fix flaky entrypoint tests by @ywang96 in https://github.com/vllm-project/vllm/pull/11403
* [cd][release] add pypi index for every commit and nightly build by @youkaichao in https://github.com/vllm-project/vllm/pull/11404
* [cd][release] fix race conditions by @youkaichao in https://github.com/vllm-project/vllm/pull/11407
* [Bugfix] Fix fully sharded LoRAs with Mixtral by @n1hility in https://github.com/vllm-project/vllm/pull/11390
* [CI] Unboock H100 Benchmark by @simon-mo in https://github.com/vllm-project/vllm/pull/11419
* [misc][perf] remove old code by @youkaichao in https://github.com/vllm-project/vllm/pull/11425
* mypy type checking for vllm/worker by @lucas-tucker in https://github.com/vllm-project/vllm/pull/11418
* [Bugfix] Fix CFGGuide and use outlines for grammars that can't convert to GBNF by @mgoin in https://github.com/vllm-project/vllm/pull/11389
* [Bugfix] torch nightly version in ROCm installation guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/11423
* [Misc] Add assertion and helpful message for marlin24 compressed models by @dsikka in https://github.com/vllm-project/vllm/pull/11388
* [Misc] add w8a8 asym models by @dsikka in https://github.com/vllm-project/vllm/pull/11075
* [CI] Expand OpenAI test_chat.py guided decoding tests by @mgoin in https://github.com/vllm-project/vllm/pull/11048
* [Bugfix] Add kv cache scales to gemma2.py by @mgoin in https://github.com/vllm-project/vllm/pull/11269
* [Doc] Fix typo in the help message of '--guided-decoding-backend' by @yansh97 in https://github.com/vllm-project/vllm/pull/11440
* [Docs] Convert rST to MyST (Markdown) by @rafvasq in https://github.com/vllm-project/vllm/pull/11145
* [V1] TP Ray executor by @ruisearch42 in https://github.com/vllm-project/vllm/pull/11107
* [Misc]Suppress irrelevant exception stack trace information when CUDAâ€¦ by @shiquan1988 in https://github.com/vllm-project/vllm/pull/11438
* [Frontend] Online Pooling API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11457
* [Bugfix] Fix Qwen2-VL LoRA weight loading  by @jeejeelee in https://github.com/vllm-project/vllm/pull/11430
* [Bugfix][Hardware][CPU] Fix CPU `input_positions` creation for text-only inputs with mrope by @Isotr0py in https://github.com/vllm-project/vllm/pull/11434
* [OpenVINO] Fixed installation conflicts by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/11458
* [attn][tiny fix] fix attn backend in MultiHeadAttention by @MengqingCao in https://github.com/vllm-project/vllm/pull/11463
* [Misc] Move weights mapper by @jeejeelee in https://github.com/vllm-project/vllm/pull/11443
* [Bugfix] Fix issues in CPU build Dockerfile. Fixes #9182 by @terrytangyuan in https://github.com/vllm-project/vllm/pull/11435
* [Model] Automatic conversion of classification and reward models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11469
* [V1] Unify VLLM_ENABLE_V1_MULTIPROCESSING handling in RayExecutor by @ruisearch42 in https://github.com/vllm-project/vllm/pull/11472
* [Misc] Update disaggregation benchmark scripts and test logs by @Jeffwan in https://github.com/vllm-project/vllm/pull/11456
* [Frontend] Enable decord to load video from base64 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11492
* [Doc] Improve GitHub links by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11491
* [Misc] Move some multimodal utils to modality-specific modules by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11494
* Mypy checking for vllm/compilation by @lucas-tucker in https://github.com/vllm-project/vllm/pull/11496
* [Misc][LoRA] Fix LoRA weight mapper by @jeejeelee in https://github.com/vllm-project/vllm/pull/11495
* [Doc] Add `QVQ` and `QwQ` to the list of supported models by @ywang96 in https://github.com/vllm-project/vllm/pull/11509
* [V1] Adding min tokens/repetition/presence/frequence penalties to V1 sampler by @sroy745 in https://github.com/vllm-project/vllm/pull/10681
* [Model]  Modify MolmoForCausalLM MLP  by @jeejeelee in https://github.com/vllm-project/vllm/pull/11510
* [Misc] Add placeholder module by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11501
* [Doc] Add video example to openai client for multimodal by @Isotr0py in https://github.com/vllm-project/vllm/pull/11521
* [V1] [1/N] [Breaking Change] API Server  (Remove Proxy)  by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11529
* [Model] [Quantization] Support deepseek_v3 w8a8 fp8 block-wise quantization by @mgoin in https://github.com/vllm-project/vllm/pull/11523
* [2/N] API Server: Avoid ulimit footgun by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/11530
* Deepseek v3 by @simon-mo in https://github.com/vllm-project/vllm/pull/11502

## New Contributors
* @Ghjk94522 made their first contribution in https://github.com/vllm-project/vllm/pull/11336
* @Akashcodes732 made their first contribution in https://github.com/vllm-project/vllm/pull/11331
* @omer-dayan made their first contribution in https://github.com/vllm-project/vllm/pull/10192
* @horheynm made their first contribution in https://github.com/vllm-project/vllm/pull/11354
* @n1hility made their first contribution in https://github.com/vllm-project/vllm/pull/11390
* @lucas-tucker made their first contribution in https://github.com/vllm-project/vllm/pull/11418
* @shiquan1988 made their first contribution in https://github.com/vllm-project/vllm/pull/11438

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.5...v0.6.6

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.6)

---

## v0.6.5: v0.6.5
**Published:** 2024-12-17

## Highlights
* Significant progress on the V1 engine refactor and multimodal support: New model executable interfaces for text-only and multimodal models, multiprocessing, improved configuration handling, and profiling enhancements (#10374, #10570, #10699, #11074, #11076, #10382, #10665, #10564, #11125, #11185, #11242).
* Major improvements in `torch.compile` integration: Support for all attention backends, encoder-based models, dynamic FP8 fusion, shape specialization fixes, and performance optimizations (#10558, #10613, #10121, #10383, #10399, #10406, #10437, #10460, #10552, #10622, #10722, #10620, #10906, #11108, #11059, #11005, #10838, #11081, #11110).
* Expanded model support, including Aria, Cross Encoders, GLM-4, OLMo November 2024, Telechat2, LoRA improvements and multimodal Granite models (#10514, #10400, #10561, #10503, #10311, #10291, #9057, #10418, #5064).
* Use xgrammar as the default guided decoding backend (#10785)
* Improved hardware enablement for AMD ROCm, ARM AARCH64, TPU prefix caching, XPU AWQ/GPTQ, and various CPU/Gaudi/HPU/NVIDIA enhancements (#10254, #9228, #10307, #10107, #10667, #10565, #10239, #11016, #9735, #10355, #10700).
* Note: Changed default temperature for ChatCompletionRequest from 0.7 to 1.0 to align with OpenAI (#11219)

## Model Support
* Added Aria (#10514), Cross Encoder (#10400), GLM-4 (#10561), OLMo (#10503), Telechat2 (#10311), Cohere R7B (#11203), GritLM embeddings (#10816)
* LoRA support for Internlm2, glm-4v, Pixtral-HF (#5064, #10418, #10795).
* Improved quantization (BNB, bitsandbytes) for multiple models (#10795, #10842, #10682, #10549)
* Expanded multimodal support (#10291, #11142).

## Hardware Support
* AMD ROCm GGUF quantization (#10254), ARM AARCH64 enablement (#9228), TPU prefix caching (#10307), XPU AWQ/GPTQ (#10107), CPU/Gaudi/HPU enhancements (#10355, #10667, #10565, #10239, #11016, #9735, #10541, #10394, #10700).

## Performance & Scheduling
* Prefix-cache aware scheduling (#10128), sliding window support (#10462), disaggregated prefill enhancements (#10502, #10884), evictor optimization (#7209).

## Benchmark & Frontend
* Benchmark structured outputs and vision datasets (#10804, #10557, #10880, #10547).
* Frontend: Automatic chat format detection (#9919), input_audio support (#11027), CLI --version (#10369), extra fields in requests (#10463).

## Documentation & Plugins
* Architecture overview (#10368), Helm chart (#9199), KubeAI integration (#10837), plugin system docs (#10372), disaggregated prefilling (#11197), structured outputs (#9943), usage section (#10827).

## Bugfixes & Misc
* Updated defaults for chunked prefill (#10544)
* Add GH200 support (#11212, #11244)

## What's Changed
* Add default value to avoid Falcon crash (#5363) by @wchen61 in https://github.com/vllm-project/vllm/pull/10347
* [Misc] Fix import error in tensorizer tests and cleanup some code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10349
* [Doc] Remove float32 choice from --lora-dtype by @xyang16 in https://github.com/vllm-project/vllm/pull/10348
* [Bugfix] Fix fully sharded LoRA bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/10352
* [Misc] Fix some help info of arg_utils to improve readability by @ShangmingCai in https://github.com/vllm-project/vllm/pull/10362
* [core][misc] keep compatibility for old-style classes by @youkaichao in https://github.com/vllm-project/vllm/pull/10356
* [Bugfix] Ensure special tokens are properly filtered out for guided structured output with MistralTokenizer by @gcalmettes in https://github.com/vllm-project/vllm/pull/10363
* [Misc] Bump up test_fused_moe tolerance by @ElizaWszola in https://github.com/vllm-project/vllm/pull/10364
* [Misc] bump mistral common version by @simon-mo in https://github.com/vllm-project/vllm/pull/10367
* [Docs] Add Nebius as sponsors by @simon-mo in https://github.com/vllm-project/vllm/pull/10371
* [Frontend] Add --version flag to CLI by @russellb in https://github.com/vllm-project/vllm/pull/10369
* [Doc] Move PR template content to docs by @russellb in https://github.com/vllm-project/vllm/pull/10159
* [Docs] Misc updates to TPU installation instructions by @mikegre-google in https://github.com/vllm-project/vllm/pull/10165
* [Frontend] Automatic detection of chat content format from AST by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9919
* [doc] add doc for the plugin system by @youkaichao in https://github.com/vllm-project/vllm/pull/10372
* [misc][plugin] improve log messages by @youkaichao in https://github.com/vllm-project/vllm/pull/10386
* [BugFix] [Kernel] Fix GPU SEGV occuring in fused_moe kernel by @rasmith in https://github.com/vllm-project/vllm/pull/10385
* [Misc] Update benchmark to support image_url file or http by @kakao-steve-ai in https://github.com/vllm-project/vllm/pull/10287
* [Misc] Medusa supports custom bias by @skylee-01 in https://github.com/vllm-project/vllm/pull/10361
* [Bugfix] Fix M-RoPE position calculation when chunked prefill is enabled by @imkero in https://github.com/vllm-project/vllm/pull/10388
* [V1] Add code owners for V1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10397
* [2/N][torch.compile] make compilation cfg part of vllm cfg by @youkaichao in https://github.com/vllm-project/vllm/pull/10383
* [V1] Refactor model executable interface for all text-only language models by @ywang96 in https://github.com/vllm-project/vllm/pull/10374
* [CI/Build] Fix IDC hpu [Device not found] issue by @xuechendi in https://github.com/vllm-project/vllm/pull/10384
* [Bugfix][Hardware][CPU] Fix CPU embedding runner with tensor parallel by @Isotr0py in https://github.com/vllm-project/vllm/pull/10394
* [platforms] refactor cpu code by @youkaichao in https://github.com/vllm-project/vllm/pull/10402
* [Hardware] [HPU]add `mark_step` for hpu by @jikunshang in https://github.com/vllm-project/vllm/pull/10239
* [Bugfix] Fix mrope_position_delta in non-last prefill chunk by @imkero in https://github.com/vllm-project/vllm/pull/10403
* [Misc] Enhance offline_inference to support user-configurable parametâ€¦ by @wchen61 in https://github.com/vllm-project/vllm/pull/10392
* [Misc] Add uninitialized params tracking for `AutoWeightsLoader` by @Isotr0py in https://github.com/vllm-project/vllm/pull/10327
* [Bugfix] Ignore ray reinit error when current platform is ROCm or XPU by @HollowMan6 in https://github.com/vllm-project/vllm/pull/10375
* [4/N][torch.compile] clean up set_torch_compile_backend by @youkaichao in https://github.com/vllm-project/vllm/pull/10401
* [VLM] Report multi_modal_placeholders in output by @lk-chen in https://github.com/vllm-project/vllm/pull/10407
* [Model] Remove redundant  softmax when using PoolingType.STEP by @Maybewuss in https://github.com/vllm-project/vllm/pull/10415
* [Model][LoRA]LoRA support added for glm-4v by @B-201 in https://github.com/vllm-project/vllm/pull/10418
* [Model] Remove transformers attention porting in VITs by @Isotr0py in https://github.com/vllm-project/vllm/pull/10414
* [Doc] Update doc for LoRA support in GLM-4V by @B-201 in https://github.com/vllm-project/vllm/pull/10425
* [5/N][torch.compile] torch.jit.script --> torch.compile by @youkaichao in https://github.com/vllm-project/vllm/pull/10406
* [Doc] Add documentation for Structured Outputs by @ismael-dm in https://github.com/vllm-project/vllm/pull/9943
* Fix open_collective value in FUNDING.yml by @andrew in https://github.com/vllm-project/vllm/pull/10426
* [Model][Bugfix] Support TP for PixtralHF ViT by @mgoin in https://github.com/vllm-project/vllm/pull/10405
* [Hardware][XPU] AWQ/GPTQ support for xpu backend by @yma11 in https://github.com/vllm-project/vllm/pull/10107
* [Kernel] Explicitly specify other value in tl.load calls by @angusYuhao in https://github.com/vllm-project/vllm/pull/9014
* [Kernel] Initial Machete W4A8 support + Refactors by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9855
* [3/N][torch.compile] consolidate custom op logging by @youkaichao in https://github.com/vllm-project/vllm/pull/10399
* [ci][bugfix] fix kernel tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10431
* [misc] Allow partial prefix benchmarking & random input generation for prefix benchmarking by @rickyyx in https://github.com/vllm-project/vllm/pull/9929
* [ci/build] Have dependabot ignore all patch update by @khluu in https://github.com/vllm-project/vllm/pull/10436
* [Bugfix]Fix Phi-3 BNB online quantization    by @jeejeelee in https://github.com/vllm-project/vllm/pull/10417
* [Platform][Refactor] Extract func `get_default_attn_backend` to `Platform` by @MengqingCao in https://github.com/vllm-project/vllm/pull/10358
* Add openai.beta.chat.completions.parse example to structured_outputs.rst by @mgoin in https://github.com/vllm-project/vllm/pull/10433
* [Bugfix] Guard for negative counter metrics to prevent crash by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/10430
* [Misc] Avoid misleading warning messages by @jeejeelee in https://github.com/vllm-project/vllm/pull/10438
* [Doc] Add the start of an arch overview page by @russellb in https://github.com/vllm-project/vllm/pull/10368
* [misc][plugin] improve plugin loading by @youkaichao in https://github.com/vllm-project/vllm/pull/10443
* [CI][CPU] adding numa node number as container name suffix by @zhouyuan in https://github.com/vllm-project/vllm/pull/10441
* [BugFix] Fix hermes tool parser output error stream arguments in some cases (#10395) by @xiyuan-lee in https://github.com/vllm-project/vllm/pull/10398
* [Pixtral-Large] Pixtral actually has no bias in vision-lang adapter by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/10449
* Fix: Build error seen on Power Architecture by @mikejuliet13 in https://github.com/vllm-project/vllm/pull/10421
* [Doc] fix link for page that was renamed by @russellb in https://github.com/vllm-project/vllm/pull/10455
* [6/N] torch.compile rollout to users by @youkaichao in https://github.com/vllm-project/vllm/pull/10437
* [Core] Avoid metrics log noise when idle by @russellb in https://github.com/vllm-project/vllm/pull/8868
* [Model][Quantization] HQQ support through Marlin kernel expansion by @ElizaWszola in https://github.com/vllm-project/vllm/pull/9766
* Change granite chat template to keep json list formatting for tool calls by @maxdebayser in https://github.com/vllm-project/vllm/pull/10452
* [CI/Build] Update Dockerfile.rocm by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/10434
* [Bugfix] Marlin 2:4 temp fix for large M dim (>256) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/10464
* [Misc] Add __setitem__ for LazyDict by @liuyanyi in https://github.com/vllm-project/vllm/pull/10469
* [Bugfix] Fix Mamba model initialization and MLP Speculator weights loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/10456
* [Bugfix] Enforce no chunked prefill for embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10470
* [CI/Build] Add sphinx/rst linter for docs by @rafvasq in https://github.com/vllm-project/vllm/pull/10366
* [CI/Build] Support compilation with local cutlass path (#10423) by @wchen61 in https://github.com/vllm-project/vllm/pull/10424
* [ci/build] Combine nightly and optional by @khluu in https://github.com/vllm-project/vllm/pull/10465
* [model] Reduce medusa weight by @skylee-01 in https://github.com/vllm-project/vllm/pull/10454
* [Bugfix] Handle conflicts between modern and legacy fields by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10471
* [Platforms] Refactor xpu code by @MengqingCao in https://github.com/vllm-project/vllm/pull/10468
* [Hardware][CPU] Support chunked-prefill and prefix-caching on CPU by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/10355
* [platforms] restore xpu check for parallel config by @youkaichao in https://github.com/vllm-project/vllm/pull/10479
* [perf bench] H200 development by @simon-mo in https://github.com/vllm-project/vllm/pull/9768
* [7/N] torch.compile, reduce compilation time by @youkaichao in https://github.com/vllm-project/vllm/pull/10460
* [Bugfix]: allow extra fields in requests to openai compatible server by @gcalmettes in https://github.com/vllm-project/vllm/pull/10463
* [TPU] Implement prefix caching for TPUs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10307
* [torch.compile] limit inductor threads and lazy import quant by @youkaichao in https://github.com/vllm-project/vllm/pull/10482
* [Core] Add Sliding Window Support with Flashinfer by @pavanimajety in https://github.com/vllm-project/vllm/pull/10462
* [Platforms] Add `device_type` in `Platform` by @MengqingCao in https://github.com/vllm-project/vllm/pull/10508
* [torch.compile] PostGradPassManager, Inductor code caching fix, fix_functionalization pass refactor + tests by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10273
* [Misc] Increase default video fetch timeout by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10495
* [platforms] improve error message for unspecified platforms by @youkaichao in https://github.com/vllm-project/vllm/pull/10520
* [Doc] fix a small typo in docstring of llama_tool_parser by @FerdinandZhong in https://github.com/vllm-project/vllm/pull/10513
* [Model] Add Support for Multimodal Granite Models by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/10291
* fix the issue that len(tokenizer(prompt)["input_ids"]) > prompt_len by @sywangyi in https://github.com/vllm-project/vllm/pull/10524
* [Model] Expose `dynamic_image_size` as mm_processor_kwargs for InternVL2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/10518
* [Bugfix] Embedding model pooling_type equals ALL and multi input's bug by @BBuf in https://github.com/vllm-project/vllm/pull/10494
* [Bug]: When apply continue_final_message for OpenAI server, the "echo":false is ignored by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/10180
* [Kernel] Register punica ops directly by @jeejeelee in https://github.com/vllm-project/vllm/pull/10522
* [Misc] Suppress duplicated logging regarding multimodal input pipeline by @ywang96 in https://github.com/vllm-project/vllm/pull/10530
* [Bugfix] Allow token ID-only inputs in Qwen2-Audio by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10536
* [8/N] enable cli flag without a space by @youkaichao in https://github.com/vllm-project/vllm/pull/10529
* [V1] Fix Compilation config & Enable CUDA graph by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10528
* [CI][Installation] Avoid uploading CUDA 11.8 wheel by @cermeng in https://github.com/vllm-project/vllm/pull/10535
* [misc] improve error message by @youkaichao in https://github.com/vllm-project/vllm/pull/10553
* [Minor] Revert change in offline inference example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10545
* Add small example to metrics.rst by @mgoin in https://github.com/vllm-project/vllm/pull/10550
* [Benchmark] Add new H100 machine  by @simon-mo in https://github.com/vllm-project/vllm/pull/10547
* [9/N] torch.compile LLM usage by @youkaichao in https://github.com/vllm-project/vllm/pull/10552
* [Minor] Fix line-too-long by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10563
* [platforms] absorb worker cls difference into platforms folder by @youkaichao in https://github.com/vllm-project/vllm/pull/10555
* [Bugfix] Fix Phi-3 BNB quantization with tensor parallel by @Isotr0py in https://github.com/vllm-project/vllm/pull/9948
* Remove token-adding chat embedding params by @noamgat in https://github.com/vllm-project/vllm/pull/10551
* [bugfix] fix full graph tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10581
* [torch.compile] support all attention backends by @youkaichao in https://github.com/vllm-project/vllm/pull/10558
* [v1] Refactor KVCacheManager for more hash input than token ids by @rickyyx in https://github.com/vllm-project/vllm/pull/10507
* support bitsandbytes quantization with qwen model by @zixuanzhang226 in https://github.com/vllm-project/vllm/pull/10549
* [Core] remove temporary local variables in LLMEngine.__init__ by @russellb in https://github.com/vllm-project/vllm/pull/10577
* [V1] EngineCore supports profiling by @Abatom in https://github.com/vllm-project/vllm/pull/10564
* [bugfix] fix cpu tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10585
* [Bugfix][Frontend] Update Llama Chat Templates to also support Non-Tool use by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/10164
* [Core] Fix broken log configuration by @russellb in https://github.com/vllm-project/vllm/pull/10458
* [Misc] Add pynccl wrappers for all_gather and reduce_scatter by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9432
* [core] gemma2 full context length support by @youkaichao in https://github.com/vllm-project/vllm/pull/10584
* [Bugfix] 500 Internal Server Error when tool_choice is incorrect. by @shenoyvvarun in https://github.com/vllm-project/vllm/pull/10567
* [Model] Fix Baichuan BNB online quantization by @CNTRYROA in https://github.com/vllm-project/vllm/pull/10572
* Update default max_num_batch_tokens for chunked prefill to 2048 by @mgoin in https://github.com/vllm-project/vllm/pull/10544
* [Kernel][Hardware][AMD] Add support for GGUF quantization on ROCm by @kliuae in https://github.com/vllm-project/vllm/pull/10254
* Prefix Cache Aware Scheduling [1/n]  by @rickyyx in https://github.com/vllm-project/vllm/pull/10128
* [2/N] Proper handling of placeholders in merged multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10485
* [Bugfix][Hardware][CPU] Fix `multi_modal_kwargs` broadcast for CPU tensor parallel by @Isotr0py in https://github.com/vllm-project/vllm/pull/10541
* [Platforms] Refactor openvino code by @statelesshz in https://github.com/vllm-project/vllm/pull/10573
* [CI/Build] For ppc64le, disabled tests for now and addressed space issues by @npanpaliya in https://github.com/vllm-project/vllm/pull/10538
* [Bugfix] Avoid import AttentionMetadata explicitly in Mllama and fix openvino import by @Isotr0py in https://github.com/vllm-project/vllm/pull/10593
* [bugfix] Fix example/tensorize_vllm_model tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/10595
* [Bugfix] Fix the LoRA weight sharding in ColumnParallelLinearWithLoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/10450
* [CI/Build] Print running script to enhance CI log readability by @jeejeelee in https://github.com/vllm-project/vllm/pull/10594
* Revert "[CI/Build] Print running script to enhance CI log readability" by @youkaichao in https://github.com/vllm-project/vllm/pull/10601
* [model][utils] add extract_layer_index utility function by @youkaichao in https://github.com/vllm-project/vllm/pull/10599
* [doc] update the code to add models by @youkaichao in https://github.com/vllm-project/vllm/pull/10603
* [Doc] Update README.md with Ray Summit talk links by @zhuohan123 in https://github.com/vllm-project/vllm/pull/10610
* Support Cross encoder models by @maxdebayser in https://github.com/vllm-project/vllm/pull/10400
* [Refactor][MISC] del redundant code in ParallelConfig.postinit by @MengqingCao in https://github.com/vllm-project/vllm/pull/10614
* [torch.compile] support encoder based models by @youkaichao in https://github.com/vllm-project/vllm/pull/10613
* [Doc] Add encoder-based models to Supported Models page by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10616
* [torch.compile] force inductor threads by @jeejeelee in https://github.com/vllm-project/vllm/pull/10620
* [torch.compile] add warning for unsupported models by @youkaichao in https://github.com/vllm-project/vllm/pull/10622
* [misc] add torch.compile compatibility check by @youkaichao in https://github.com/vllm-project/vllm/pull/10618
* [misc] move functions to config.py by @youkaichao in https://github.com/vllm-project/vllm/pull/10624
* [Model] Support `is_causal` HF config field for Qwen2 model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10621
* [Doc] Super tiny little typo fix by @fzyzcjy in https://github.com/vllm-project/vllm/pull/10633
* [Bug]: Authorization ignored when root_path is set by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/10606
* [Bugfix] Fix chunked prefill with model dtype float32 on Turing Devices by @wallashss in https://github.com/vllm-project/vllm/pull/9850
* [Docs] Add Snowflake Slides by @simon-mo in https://github.com/vllm-project/vllm/pull/10641
* [Model]: Add support for Aria model by @xffxff in https://github.com/vllm-project/vllm/pull/10514
* [Model] Enable optional prefix when loading embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10639
* [Doc] Fix typos in docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10636
* [Model] Add OLMo November 2024 model by @2015aroras in https://github.com/vllm-project/vllm/pull/10503
* [misc] do not read HOST_IP by @youkaichao in https://github.com/vllm-project/vllm/pull/10644
* [bugfix] fix aria model and add torch.compile by @youkaichao in https://github.com/vllm-project/vllm/pull/10645
* [Feature] vLLM ARM Enablement for AARCH64 CPUs by @sanketkaleoss in https://github.com/vllm-project/vllm/pull/9228
* [v1] EngineArgs for better config handling for v1 by @rickyyx in https://github.com/vllm-project/vllm/pull/10382
* custom allreduce + torch.compile by @SageMoore in https://github.com/vllm-project/vllm/pull/10121
* [Misc] Remove outdated init protocols by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10655
* [ci] add vllm_test_utils by @youkaichao in https://github.com/vllm-project/vllm/pull/10659
* [V1] Enable profile for LLMEngine by @jikunshang in https://github.com/vllm-project/vllm/pull/10665
* [Bugfix] Fix for Spec model TP + Chunked Prefill by @andoorve in https://github.com/vllm-project/vllm/pull/10232
* [Hardware][NVIDIA] Add non-NVML CUDA mode for Jetson by @conroy-cheers in https://github.com/vllm-project/vllm/pull/9735
* [Bugfix] Fix using `-O[0,3]` with LLM entrypoint by @mgoin in https://github.com/vllm-project/vllm/pull/10677
* [Bugfix] Check bnb_4bit_quant_storage for bitsandbytes by @mgoin in https://github.com/vllm-project/vllm/pull/10642
* [V1] Refactor model executable interface for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/10570
* [Kernel] Remove hard-dependencies of Speculative decode to CUDA workers by @xuechendi in https://github.com/vllm-project/vllm/pull/10587
* [V1] Update interface for idefics3 by @ywang96 in https://github.com/vllm-project/vllm/pull/10680
* [Bugfix][SpecDecode] apply sampling parameters to target probabilities for consistency in rejection sampling. by @jeongin601 in https://github.com/vllm-project/vllm/pull/10198
* [bugfix] fix the default value of llm_int8_threshold in BitsAndBytesConfig by @yansh97 in https://github.com/vllm-project/vllm/pull/10657
* [Hardware][Gaudi]add get_name method for HPUAttentionBackend by @jikunshang in https://github.com/vllm-project/vllm/pull/10667
* [Misc]Further  reduce BNB static variable by @jeejeelee in https://github.com/vllm-project/vllm/pull/10597
* [Cleanup][Kernel] Remove if-else with identical branches in marlin 2:4 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/10687
* [Model] Support telechat2 by @shunxing12345 in https://github.com/vllm-project/vllm/pull/10311
* [Bugfix][Hardware][CPU] Fix intel-omp version to avoid segfault by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/10700
* [V1] Update interface for mistral-format Pixtral by @ywang96 in https://github.com/vllm-project/vllm/pull/10703
* [ci] fix slow tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10698
* [torch.compile] fix shape specialization by @youkaichao in https://github.com/vllm-project/vllm/pull/10722
* [Bugfix] Fix GGUF inference with FP16 unquantized checkpoint by @Isotr0py in https://github.com/vllm-project/vllm/pull/10675
* [Bugfix][Mamba] Fix Multistep on Mamba-like models by @mzusman in https://github.com/vllm-project/vllm/pull/10705
* [Bugfix] Ignore `lm_head` when loading embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10719
* [Frontend] don't block event loop in tokenization (preprocess) in OpenAI compatible server by @tomeras91 in https://github.com/vllm-project/vllm/pull/10635
* [misc] upgrade filelock version by @youkaichao in https://github.com/vllm-project/vllm/pull/10731
* [Model] support bitsandbytes quantization with minicpm3 model by @zixuanzhang226 in https://github.com/vllm-project/vllm/pull/10682
* [Doc] Update model in arch_overview.rst to match comment by @spacewander in https://github.com/vllm-project/vllm/pull/10701
* [Bug][CLI] Allow users to disable prefix caching explicitly by @rickyyx in https://github.com/vllm-project/vllm/pull/10724
* [V1] Do not allocate beyond the max_model_len by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10730
* [Kernel] Update vllm-flash-attn version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10736
* Update requirements-tpu by @richardsliu in https://github.com/vllm-project/vllm/pull/10726
* [Model] Added GLM-4 series hf format model support vllm==0.6.4 by @sixsixcoder in https://github.com/vllm-project/vllm/pull/10561
* [Kernel] Update vllm-flash-attn version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10742
* [V1] Optimize the CPU overheads in FlashAttention custom op by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10733
* [Model] Add Internlm2 LoRA support by @Isotr0py in https://github.com/vllm-project/vllm/pull/5064
* [Model] Clean up MiniCPMV by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10751
* [Misc] typo find in sampling_metadata.py by @noooop in https://github.com/vllm-project/vllm/pull/10740
* [Bugfix] Fix Idefics3 bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/10778
* [platform] Add verify_quantization in platform. by @wangxiyuan in https://github.com/vllm-project/vllm/pull/10757
* [Bugfix] Fix OpenVino/Neuron `driver_worker` init by @NickLucche in https://github.com/vllm-project/vllm/pull/10779
* [Model] Refactor Molmo weights loading to use AutoWeightsLoader by @Isotr0py in https://github.com/vllm-project/vllm/pull/10771
* Interleaving sliding window for Ministral-8B-Instruct-2410 by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/10591
* [doc] format fix by @wangxiyuan in https://github.com/vllm-project/vllm/pull/10789
* [Model] Replace embedding models with pooling adapter by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10769
* [Misc] Improve type annotations for `support_torch_compile` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10763
* [Misc] Rename embedding classes to pooling by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10801
* [doc] add warning about comparing hf and vllm outputs by @youkaichao in https://github.com/vllm-project/vllm/pull/10805
* [Misc] Adding `MMMU-Pro` vision dataset to serving benchmark by @ywang96 in https://github.com/vllm-project/vllm/pull/10804
* [Core] Implement disagg prefill by StatelessProcessGroup by @KuntaiDu in https://github.com/vllm-project/vllm/pull/10502
* [Model] Add BNB support to Llava and Pixtral-HF by @Isotr0py in https://github.com/vllm-project/vllm/pull/10795
* [core] Avoid metrics log noise when idle - include speculative decodiâ€¦ by @cduk in https://github.com/vllm-project/vllm/pull/10809
* [Kernel] Use `out` in flash_attn_varlen_func by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10811
* Fill TorchSDPAAttentionMetadata seq_lens_field for prefill by @maxdebayser in https://github.com/vllm-project/vllm/pull/10799
* [misc] remove xverse modeling file by @youkaichao in https://github.com/vllm-project/vllm/pull/10814
* [doc]Update config docstring by @wangxiyuan in https://github.com/vllm-project/vllm/pull/10732
* [Model]: add some tests for aria model by @xffxff in https://github.com/vllm-project/vllm/pull/10770
* [CI/Build] Update `mistral_common` version for tests and docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10825
* [misc] use out argument for flash attention by @youkaichao in https://github.com/vllm-project/vllm/pull/10822
* [Misc][LoRA] Move the implementation of lora bias to punica.py by @jeejeelee in https://github.com/vllm-project/vllm/pull/10829
* [Misc][XPU] Avoid torch compile for XPU platform by @yma11 in https://github.com/vllm-project/vllm/pull/10747
* Fix openvino on GPU by @janimo in https://github.com/vllm-project/vllm/pull/10793
* [Model] Add TP and BNB quantization support to LlavaMultiModalProjector by @Isotr0py in https://github.com/vllm-project/vllm/pull/10834
* [Bugfix] Prevent benchmark_throughput.py from using duplicated random prompts by @mgoin in https://github.com/vllm-project/vllm/pull/10753
* [Model] support bitsandbytes quantization with minicpm model by @zixuanzhang226 in https://github.com/vllm-project/vllm/pull/10842
* [Bugfix] Fix QKVParallelLinearWithShardedLora bias bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/10844
* [core][distributed] add pynccl broadcast by @youkaichao in https://github.com/vllm-project/vllm/pull/10843
* [torch.compile] remove compilation_context and simplify code by @youkaichao in https://github.com/vllm-project/vllm/pull/10838
* [Doc] Add github links for source code references by @russellb in https://github.com/vllm-project/vllm/pull/10672
* [Misc] Remove deprecated names by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10817
* [Core][Performance] Add XGrammar support for guided decoding and set it as default by @aarnphm in https://github.com/vllm-project/vllm/pull/10785
* [Speculative Decoding] Move indices to device before filtering output by @zhengy001 in https://github.com/vllm-project/vllm/pull/10850
* [V1] VLM - Run the mm_mapper preprocessor in the frontend process by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/10640
* [MISC][XPU] quick fix for XPU CI by @yma11 in https://github.com/vllm-project/vllm/pull/10859
* [Bugfix] Only require XGrammar on x86 by @mgoin in https://github.com/vllm-project/vllm/pull/10865
* [Bugfix][Frontend] correctly record prefill and decode time metrics  by @tomeras91 in https://github.com/vllm-project/vllm/pull/10853
* [Build][Bugfix] Using the correct type hint by @gshtras in https://github.com/vllm-project/vllm/pull/10866
* [Benchmark] Benchmark structured output with datasets by @xuechendi in https://github.com/vllm-project/vllm/pull/10557
* [CI/Build] Replace mean with torch.all in test_pynccl.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/10876
* Drop ROCm load format check by @wangxiyuan in https://github.com/vllm-project/vllm/pull/10767
* [ci/build] Change queue name for Release jobs by @khluu in https://github.com/vllm-project/vllm/pull/10875
* [ci/build] Job to build and push release image by @khluu in https://github.com/vllm-project/vllm/pull/10877
* [bugfix] fixed parameter â€œnâ€ not work when set parameter â€œbestofâ€ > 1 by @o2363286 in https://github.com/vllm-project/vllm/pull/10854
* [ci/build] Update vLLM postmerge ECR repo by @khluu in https://github.com/vllm-project/vllm/pull/10887
* [LoRA] Change lora_tokenizers capacity by @xyang16 in https://github.com/vllm-project/vllm/pull/10796
* [Model] Consolidate ViTs attention implementation without mask by @Isotr0py in https://github.com/vllm-project/vllm/pull/10893
* Benchmark serving structured output by @xuechendi in https://github.com/vllm-project/vllm/pull/10880
* [CI/Build] improve python-only dev setup by @dtrifiro in https://github.com/vllm-project/vllm/pull/9621
* [V1] Fix when max_model_len is not divisible by block_size by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10903
* [benchmark] Make H100 benchmark optional by @khluu in https://github.com/vllm-project/vllm/pull/10908
* [Bugfix] Fallback to outlines for complex json schemas by @mgoin in https://github.com/vllm-project/vllm/pull/10899
* [Doc] Create a new "Usage" section by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10827
* [Bugfix] Fix BNB loader target_modules by @jeejeelee in https://github.com/vllm-project/vllm/pull/10720
* [Misc] Update llama 3.2 template to support system prompt with images by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/10901
* [Misc][LoRA] Clean up the function interface of Punica by @jeejeelee in https://github.com/vllm-project/vllm/pull/10917
* [CI/Build] Bump test transformers version by @Isotr0py in https://github.com/vllm-project/vllm/pull/10106
* [Misc][Gaudi] Avoid torch.compile and enable lazy collectives by default for HPU lazy backend by @kzawora-intel in https://github.com/vllm-project/vllm/pull/10897
* [ci][build] add tests for python only compilation by @youkaichao in https://github.com/vllm-project/vllm/pull/10915
* [torch.compile] use size tuning for specific sizes by @youkaichao in https://github.com/vllm-project/vllm/pull/10933
* [torch.compile] add logging for compilation time by @youkaichao in https://github.com/vllm-project/vllm/pull/10941
* [CI/Build] Fix broken multimodal test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10950
* [torch.compile] fix deprecated code by @youkaichao in https://github.com/vllm-project/vllm/pull/10948
* [Core] Support Lark grammars for XGrammar by @mgoin in https://github.com/vllm-project/vllm/pull/10870
* [Doc] add KubeAI to serving integrations by @samos123 in https://github.com/vllm-project/vllm/pull/10837
* [misc] fix typo by @youkaichao in https://github.com/vllm-project/vllm/pull/10960
* [ci] fix broken tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10956
* [Core] Cleanup startup logging a bit by @russellb in https://github.com/vllm-project/vllm/pull/10961
* [Bugfix] Fix test-pipeline.yaml by @jeejeelee in https://github.com/vllm-project/vllm/pull/10973
* [Model] Implement merged input processor for LLaVA model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10676
* [Build] Fix for the Wswitch-bool clang warning by @gshtras in https://github.com/vllm-project/vllm/pull/10060
* [Misc][LoRA] Refactor and clean MergedQKVParallelLinearWithLora implementation  by @Isotr0py in https://github.com/vllm-project/vllm/pull/10958
* [Model] Composite weight loading for multimodal Qwen2 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10944
* [Doc] Explicitly state that InternVL 2.5 is supported by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10978
* [Model] Update multi-modal processor to support Mantis(LLaVA) model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10711
* [Doc] Explicitly state that PP isn't compatible with speculative decoding yet by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10975
* [BugFix][Kernel]: fix illegal memory access in causal_conv1d when conv_states is None by @xffxff in https://github.com/vllm-project/vllm/pull/10928
* [core][executor] simplify instance id by @youkaichao in https://github.com/vllm-project/vllm/pull/10976
* [core][misc] remove use_dummy driver for _run_workers by @youkaichao in https://github.com/vllm-project/vllm/pull/10920
* [torch.compile] allow candidate compile sizes by @youkaichao in https://github.com/vllm-project/vllm/pull/10984
* [V1] Initial support of multimodal models for V1 re-arch by @ywang96 in https://github.com/vllm-project/vllm/pull/10699
* [torch.compile][misc] fix comments by @youkaichao in https://github.com/vllm-project/vllm/pull/10993
* [misc] clean up and unify logging by @youkaichao in https://github.com/vllm-project/vllm/pull/10999
* [Doc][V1] Add V1 support column for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/10998
* [torch.compile] add dynamo time tracking by @youkaichao in https://github.com/vllm-project/vllm/pull/11005
* [V1] Fix Detokenizer loading in `AsyncLLM` by @ywang96 in https://github.com/vllm-project/vllm/pull/10997
* [Core] Require xgrammar >= 0.1.6 by @russellb in https://github.com/vllm-project/vllm/pull/11021
* [Platform] Move `async output` check to platform by @wangxiyuan in https://github.com/vllm-project/vllm/pull/10768
* [V1] Input Batch Relocation by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/10962
* [ci/build] Recompile CI dependencies list with Python 3.12 by @khluu in https://github.com/vllm-project/vllm/pull/11013
* [V1] Further reduce CPU overheads in flash-attn by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10989
* [Misc][LoRA] Abstract PunicaWrapper by @jeejeelee in https://github.com/vllm-project/vllm/pull/10955
* [Model] Implement merged input processor for Phi-3-Vision models by @Isotr0py in https://github.com/vllm-project/vllm/pull/10977
* [Bugfix][Hardware][Gaudi] Bump vllm_hpu_extension version by @kzawora-intel in https://github.com/vllm-project/vllm/pull/11028
* [v1] fix use compile sizes by @youkaichao in https://github.com/vllm-project/vllm/pull/11000
* [Neuron] Upgrade neuron to 2.20.2 by @xendo in https://github.com/vllm-project/vllm/pull/11016
* [ROCm][bugfix] Setting the value for the scpecilative decoding worker class on rocm platform by @gshtras in https://github.com/vllm-project/vllm/pull/11035
* Build tpu image in release pipeline by @richardsliu in https://github.com/vllm-project/vllm/pull/10936
* [V1] Do not store `None` in self.generators by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11038
* [Docs] Add dedicated tool calling page to docs by @mgoin in https://github.com/vllm-project/vllm/pull/10554
* [Model] Add has_weight to RMSNorm and re-enable weights loading tracker for Mamba by @Isotr0py in https://github.com/vllm-project/vllm/pull/10739
* [Bugfix] Fix usage of `deprecated` decorator by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11025
* [Frontend] Use request id from header by @joerunde in https://github.com/vllm-project/vllm/pull/10968
* [Pixtral] Improve loading by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/11040
* [V1] Multiprocessing Tensor Parallel Support for v1 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9856
* monitor metrics of tokens per step using cudagraph batchsizes by @youkaichao in https://github.com/vllm-project/vllm/pull/11031
* [Bugfix] Fix xgrammar failing to read a vocab_size from LlavaConfig on PixtralHF. by @sjuxax in https://github.com/vllm-project/vllm/pull/11043
* Update README.md by @dmoliveira in https://github.com/vllm-project/vllm/pull/11034
* [Bugfix] cuda error running llama 3.2 by @GeneDer in https://github.com/vllm-project/vllm/pull/11047
* Add example of helm chart for vllm deployment on k8s by @mfournioux in https://github.com/vllm-project/vllm/pull/9199
* [Bugfix] Handle <|tool_call|> token in granite tool parser by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/11039
* [Misc][LoRA] Add PEFTHelper  for LoRA  by @jeejeelee in https://github.com/vllm-project/vllm/pull/11003
* [Bugfix] Backport request id validation to v0 by @joerunde in https://github.com/vllm-project/vllm/pull/11036
* [BUG] Remove token param #10921 by @flaviabeo in https://github.com/vllm-project/vllm/pull/11022
* [Core] Update to outlines >= 0.1.8 by @russellb in https://github.com/vllm-project/vllm/pull/10576
* [torch.compile] add a flag to track batchsize statistics by @youkaichao in https://github.com/vllm-project/vllm/pull/11059
* [V1][Bugfix] Always set enable_chunked_prefill = True for V1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11061
* [Bugfix] Fix Mamba multistep by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11071
* [Misc] LoRA + Chunked Prefill by @aurickq in https://github.com/vllm-project/vllm/pull/9057
* [Model] PP support for Mamba-like models by @mzusman in https://github.com/vllm-project/vllm/pull/10992
* Fix streaming for granite tool call when <|tool_call|> is present by @maxdebayser in https://github.com/vllm-project/vllm/pull/11069
* [CI/Build] Check transformers v4.47 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10991
* [ci/build] Fix AMD CI dependencies by @khluu in https://github.com/vllm-project/vllm/pull/11087
* [ci/build] Fix entrypoints test and pin outlines version by @khluu in https://github.com/vllm-project/vllm/pull/11088
* [Core] v1: Use atexit to handle engine core client shutdown by @russellb in https://github.com/vllm-project/vllm/pull/11076
* [Bugfix] Fix Idefics3 fails during multi-image inference by @B-201 in https://github.com/vllm-project/vllm/pull/11080
* [Bugfix]: Clamp `-inf` logprob values in prompt_logprobs by @rafvasq in https://github.com/vllm-project/vllm/pull/11073
* [Misc] Split up pooling tasks by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10820
* [Doc] Update docs to refer to pooling models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11093
* [CI/Build] Enable prefix caching test for AMD by @hissu-hyvarinen in https://github.com/vllm-project/vllm/pull/11098
* [Doc] Installed version of llmcompressor for int8/fp8 quantization by @bingps in https://github.com/vllm-project/vllm/pull/11103
* [torch.compile] use depyf to dump torch.compile internals by @youkaichao in https://github.com/vllm-project/vllm/pull/10972
* [V1] Use input_ids as input for text-only models  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11032
* [torch.compile] remove graph logging in ci by @youkaichao in https://github.com/vllm-project/vllm/pull/11110
* [core] Bump ray to use _overlap_gpu_communication in compiled graph tests by @ruisearch42 in https://github.com/vllm-project/vllm/pull/10410
* [CI/Build] Split up VLM tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11083
* [V1][Core] Remove should_shutdown to simplify core process termination by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11113
* [V1] VLM preprocessor hashing by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/11020
* [Bugfix] Multiple fixes to tool streaming with hermes and mistral by @cedonley in https://github.com/vllm-project/vllm/pull/10979
* [Docs] Add media kit by @simon-mo in https://github.com/vllm-project/vllm/pull/11121
* Update link to LlamaStack remote vLLM guide in serving_with_llamastack.rst by @terrytangyuan in https://github.com/vllm-project/vllm/pull/11112
* [Core] cleanup zmq ipc sockets on exit by @russellb in https://github.com/vllm-project/vllm/pull/11115
* [Model] Add support for embedding model GritLM by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/10816
* [V1] Use more persistent buffers to optimize input preparation overheads by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11111
* [Hardware][Intel-Gaudi] Enable LoRA support for Intel Gaudi (HPU) by @SanjuCSudhakaran in https://github.com/vllm-project/vllm/pull/10565
* [core][distributed] initialization from StatelessProcessGroup by @youkaichao in https://github.com/vllm-project/vllm/pull/10986
* [Misc][LoRA] Ensure Lora Adapter requests return adapter name by @Jeffwan in https://github.com/vllm-project/vllm/pull/11094
* [V1] Fix torch profiling for offline inference by @ywang96 in https://github.com/vllm-project/vllm/pull/11125
* fix(docs): typo in helm install instructions by @ramonziai in https://github.com/vllm-project/vllm/pull/11141
* [Bugfix] Quick fix to make Pixtral-HF load correctly again after 39e227c7ae. by @sjuxax in https://github.com/vllm-project/vllm/pull/11024
* [Misc] Validate grammar and fail early by @comaniac in https://github.com/vllm-project/vllm/pull/11119
* Fix logging of the vLLM Config by @JArnoldAMD in https://github.com/vllm-project/vllm/pull/11143
* [Bugfix] Fix value unpack error of simple connector for KVCache transfer. by @ShangmingCai in https://github.com/vllm-project/vllm/pull/11058
* [Misc][V1] Fix type in v1 prefix caching by @comaniac in https://github.com/vllm-project/vllm/pull/11151
* [torch.compile] Dynamic fp8 + rms_norm fusion by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10906
* [Bugfix] Use runner_type instead of task in GritLM by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/11144
* [Bugfix] Update starcoder2 to remap k/v scale names for kv_cache quantization by @dsikka in https://github.com/vllm-project/vllm/pull/11148
* [ROCm][AMD] Disable auto enabling chunked prefill on ROCm by @gshtras in https://github.com/vllm-project/vllm/pull/11146
* [Bugfix][V1] Fix 'NoneType' object has no attribute 'hash_value' by @comaniac in https://github.com/vllm-project/vllm/pull/11157
* [core] clean up cudagraph batchsize padding logic by @youkaichao in https://github.com/vllm-project/vllm/pull/10996
* PaliGemma 2 support by @janimo in https://github.com/vllm-project/vllm/pull/11142
* [Bugfix][CI][CPU] add missing datasets package to requirements-cpu.txt  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/11159
* [Frontend] Separate pooling APIs in offline inference by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11129
* [V1][VLM] Fix edge case bug for InternVL2 by @ywang96 in https://github.com/vllm-project/vllm/pull/11165
* [Refactor]A simple device-related refactor by @noemotiovon in https://github.com/vllm-project/vllm/pull/11163
* [Core] support LoRA and prompt adapter in content-based hashing for Block Manager v2 prefix caching by @llsj14 in https://github.com/vllm-project/vllm/pull/8240
* [Bugfix] using len(tokenizer) instead of tokenizer.vocab_size in AllowedTokenIdsLogitsProcessor by @zhangjf-nlp in https://github.com/vllm-project/vllm/pull/11156
* [Misc] Add tokenizer_mode param to benchmark_serving.py by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/11174
* [Doc] Reorganize online pooling APIs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11172
* [Bugfix][Hardware][CPU] Enable Gemma2 with SDPA on CPU backend by @janimo in https://github.com/vllm-project/vllm/pull/11169
* [Distributed] Allow the placement group more time to wait for resources to be ready by @Jeffwan in https://github.com/vllm-project/vllm/pull/11138
* [Core] V1: Use multiprocessing by default by @russellb in https://github.com/vllm-project/vllm/pull/11074
* [V1][Bugfix] Fix EngineCoreProc profile by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11185
* [Bugfix][V1] Re-compute an entire block when fully cache hit by @comaniac in https://github.com/vllm-project/vllm/pull/11186
* update compressed-tensors to latest version by @dhuangnm in https://github.com/vllm-project/vllm/pull/11183
* [Core] Update outlines and increase its threadpool size by @russellb in https://github.com/vllm-project/vllm/pull/11140
* [V1][Bugfix] Fix V1 TP trust-remote-code by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11182
* [Misc] Minor improvements to the readability of PunicaWrapperBase by @jeejeelee in https://github.com/vllm-project/vllm/pull/11200
* [Frontend] Add `logits_processors` as an extra completion argument by @bradhilton in https://github.com/vllm-project/vllm/pull/11150
* [VLM] Fully dynamic prompt replacement in merged input processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11199
* Enable mypy checking on V1 code by @markmc in https://github.com/vllm-project/vllm/pull/11105
* [Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion by @llsj14 in https://github.com/vllm-project/vllm/pull/7209
* [[Misc]Upgrade bitsandbytes to the latest version 0.45.0 by @jeejeelee in https://github.com/vllm-project/vllm/pull/11201
* [torch.compile] allow tracking forward time by @youkaichao in https://github.com/vllm-project/vllm/pull/11081
* [Misc] Clean up multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11207
* [Bugfix] Fix error handling of unsupported sliding window by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/11213
* [Doc] add documentation for disaggregated prefilling by @KuntaiDu in https://github.com/vllm-project/vllm/pull/11197
* [Core] Support disaggregated prefill with Mooncake Transfer Engine by @ShangmingCai in https://github.com/vllm-project/vllm/pull/10884
* [V1][Minor] Cache np arange to reduce input preparation overhead by @WoosukKwon in https://github.com/vllm-project/vllm/pull/11214
* Update deploying_with_k8s.rst by @AlexHe99 in https://github.com/vllm-project/vllm/pull/10922
* fix block-size description by @chenqianfzh in https://github.com/vllm-project/vllm/pull/10938
* [Bugfix] Fix the default value for temperature in ChatCompletionRequest by @yansh97 in https://github.com/vllm-project/vllm/pull/11219
* [CI/Build] simplify Dockerfile build for ARM64 / GH200 by @cennn in https://github.com/vllm-project/vllm/pull/11212
* [Model] Support Cohere2ForCausalLM (Cohere R7B) by @janimo in https://github.com/vllm-project/vllm/pull/11203
* [Model] Refactor Ultravox to use merged input processor by @Isotr0py in https://github.com/vllm-project/vllm/pull/11198
* [Doc] Reorder vision language examples in alphabet order by @Isotr0py in https://github.com/vllm-project/vllm/pull/11228
* [misc] Layerwise profile updates by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/10242
* [core] overhaul memory profiling and fix backward compatibility by @youkaichao in https://github.com/vllm-project/vllm/pull/10511
* [Docs] hint to enable use of GPU performance counters in profiling tools for multi-node distributed serving by @bk-TurbaAI in https://github.com/vllm-project/vllm/pull/11235
* [ci][tests] add gh200 tests by @youkaichao in https://github.com/vllm-project/vllm/pull/11244
* [torch.compile] fast inductor by @youkaichao in https://github.com/vllm-project/vllm/pull/11108
* fix gh200 tests on main by @youkaichao in https://github.com/vllm-project/vllm/pull/11246
* [CI] Add test case with JSON schema using references + use xgrammar by default with OpenAI parse by @mgoin in https://github.com/vllm-project/vllm/pull/10935
* [Frontend] Add OpenAI API support for input_audio by @kylehh in https://github.com/vllm-project/vllm/pull/11027
* [V1][VLM] Proper memory profiling for image language models by @ywang96 in https://github.com/vllm-project/vllm/pull/11210
* [Platform] platform agnostic for EngineArgs initialization by @wangxiyuan in https://github.com/vllm-project/vllm/pull/11225
* [V1][Core] Use weakref.finalize instead of atexit by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/11242
* [Misc] Kernel Benchmark for `RMSNorm` by @ywang96 in https://github.com/vllm-project/vllm/pull/11241
* [Misc] Allow passing logits_soft_cap for xformers backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/11252
* [Bugfix] Fix request cancellation without polling by @joerunde in https://github.com/vllm-project/vllm/pull/11190

## New Contributors
* @wchen61 made their first contribution in https://github.com/vllm-project/vllm/pull/10347
* @kakao-steve-ai made their first contribution in https://github.com/vllm-project/vllm/pull/10287
* @Maybewuss made their first contribution in https://github.com/vllm-project/vllm/pull/10415
* @ismael-dm made their first contribution in https://github.com/vllm-project/vllm/pull/9943
* @andrew made their first contribution in https://github.com/vllm-project/vllm/pull/10426
* @angusYuhao made their first contribution in https://github.com/vllm-project/vllm/pull/9014
* @xiyuan-lee made their first contribution in https://github.com/vllm-project/vllm/pull/10398
* @mikejuliet13 made their first contribution in https://github.com/vllm-project/vllm/pull/10421
* @BBuf made their first contribution in https://github.com/vllm-project/vllm/pull/10494
* @zixuanzhang226 made their first contribution in https://github.com/vllm-project/vllm/pull/10549
* @shenoyvvarun made their first contribution in https://github.com/vllm-project/vllm/pull/10567
* @CNTRYROA made their first contribution in https://github.com/vllm-project/vllm/pull/10572
* @npanpaliya made their first contribution in https://github.com/vllm-project/vllm/pull/10538
* @xffxff made their first contribution in https://github.com/vllm-project/vllm/pull/10514
* @2015aroras made their first contribution in https://github.com/vllm-project/vllm/pull/10503
* @sanketkaleoss made their first contribution in https://github.com/vllm-project/vllm/pull/9228
* @conroy-cheers made their first contribution in https://github.com/vllm-project/vllm/pull/9735
* @jeongin601 made their first contribution in https://github.com/vllm-project/vllm/pull/10198
* @shunxing12345 made their first contribution in https://github.com/vllm-project/vllm/pull/10311
* @spacewander made their first contribution in https://github.com/vllm-project/vllm/pull/10701
* @wangxiyuan made their first contribution in https://github.com/vllm-project/vllm/pull/10757
* @cduk made their first contribution in https://github.com/vllm-project/vllm/pull/10809
* @o2363286 made their first contribution in https://github.com/vllm-project/vllm/pull/10854
* @sjuxax made their first contribution in https://github.com/vllm-project/vllm/pull/11043
* @dmoliveira made their first contribution in https://github.com/vllm-project/vllm/pull/11034
* @mfournioux made their first contribution in https://github.com/vllm-project/vllm/pull/9199
* @bingps made their first contribution in https://github.com/vllm-project/vllm/pull/11103
* @cedonley made their first contribution in https://github.com/vllm-project/vllm/pull/10979
* @SanjuCSudhakaran made their first contribution in https://github.com/vllm-project/vllm/pull/10565
* @ramonziai made their first contribution in https://github.com/vllm-project/vllm/pull/11141
* @noemotiovon made their first contribution in https://github.com/vllm-project/vllm/pull/11163
* @zhangjf-nlp made their first contribution in https://github.com/vllm-project/vllm/pull/11156
* @dhuangnm made their first contribution in https://github.com/vllm-project/vllm/pull/11183
* @bradhilton made their first contribution in https://github.com/vllm-project/vllm/pull/11150
* @AlexHe99 made their first contribution in https://github.com/vllm-project/vllm/pull/10922
* @cennn made their first contribution in https://github.com/vllm-project/vllm/pull/11212
* @bk-TurbaAI made their first contribution in https://github.com/vllm-project/vllm/pull/11235
* @kylehh made their first contribution in https://github.com/vllm-project/vllm/pull/11027

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.4...v0.6.5

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.5)

---

## v0.6.4.post1: v0.6.4.post1
**Published:** 2024-11-15

This patch release covers bug fixes (#10347, #10349, #10348, #10352, #10363), keep compatibility for `vLLMConfig` usage in out of tree models (#10356)

## What's Changed
* Add default value to avoid Falcon crash (#5363) by @wchen61 in https://github.com/vllm-project/vllm/pull/10347
* [Misc] Fix import error in tensorizer tests and cleanup some code by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10349
* [Doc] Remove float32 choice from --lora-dtype by @xyang16 in https://github.com/vllm-project/vllm/pull/10348
* [Bugfix] Fix fully sharded LoRA bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/10352
* [Misc] Fix some help info of arg_utils to improve readability by @ShangmingCai in https://github.com/vllm-project/vllm/pull/10362
* [core][misc] keep compatibility for old-style classes by @youkaichao in https://github.com/vllm-project/vllm/pull/10356
* [Bugfix] Ensure special tokens are properly filtered out for guided structured output with MistralTokenizer by @gcalmettes in https://github.com/vllm-project/vllm/pull/10363
* [Misc] Bump up test_fused_moe tolerance by @ElizaWszola in https://github.com/vllm-project/vllm/pull/10364
* [Misc] bump mistral common version by @simon-mo in https://github.com/vllm-project/vllm/pull/10367

## New Contributors
* @wchen61 made their first contribution in https://github.com/vllm-project/vllm/pull/10347

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.4...v0.6.4.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.4.post1)

---

## v0.6.4: v0.6.4
**Published:** 2024-11-15

## Highlights
* Significant progress in V1 engine core refactor (#9826, #10135, #10288, #10211, #10225, #10228, #10268, #9954, #10272, #9971, #10224, #10166, #9289, #10058, #9888, #9972, #10059, #9945, #9679, #9871, #10227, #10245, #9629, #10097, #10203, #10148). You can checkout more details regarding the design and plan ahead in our recent [meetup slides](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit#slide=id.g31455c8bc1e_2_130)
* Signficant progress in `torch.compile` support. Many models now support torch compile with TorchInductor. You can checkout our [meetup slides](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit#slide=id.g31455c8bc1e_0_443) for more details. (#9775, #9614, #9639, #9641, #9876, #9946, #9589, #9896, #9637, #9300, #9947, #9138, #9715, #9866, #9632, #9858, #9889)

### Model Support
* New LLMs and VLMs: Idefics3 (#9767), H2OVL-Mississippi (#9747), Qwen2-Audio (#9248), Pixtral models in the HF Transformers format (#9036), FalconMamba (#9325), Florence-2 language backbone (#9555)
* New encoder-decoder embedding models: BERT (#9056), RoBERTa & XLM-RoBERTa (#9387)
* Expanded task support: Llama embeddings (#9806), Math-Shepherd (Mistral reward modeling) (#9697), Qwen2 classification (#9704), Qwen2 embeddings (#10184), VLM2Vec (Phi-3-Vision embeddings) (#9303), E5-V (LLaVA-NeXT embeddings) (#9576), Qwen2-VL embeddings (#9944)
    * Add user-configurable `--task` parameter for models that support both generation and embedding (#9424)
    * Chat-based Embeddings API (#9759)
* Tool calling parser for Granite 3.0 (#9027), Jamba (#9154), granite-20b-functioncalling (#8339)
* LoRA support for Granite 3.0 MoE (#9673), Idefics3 (#10281), Llama embeddings (#10071), Qwen (#9622), Qwen2-VL (#10022)
* BNB quantization support for Idefics3 (#10310), Mllama (#9720), Qwen2 (#9467, #9574), MiniCPMV (#9891)
* Unified multi-modal processor for VLM (#10040, #10044)
* Simplify model interface (#9933, #10237, #9938, #9958, #10007, #9978, #9983, #10205)

### Hardware Support
* Gaudi: Add Intel Gaudi (HPU) inference backend (#6143)
* CPU: Add embedding models support for CPU backend (#10193)
* TPU: Correctly profile peak memory usage & Upgrade PyTorch XLA (#9438)
* Triton: Add Triton implementation for scaled_mm_triton to support fp8 and int8 SmoothQuant, symmetric case (#9857)

### Performance
* Combine chunked prefill with speculative decoding (#9291)
* `fused_moe` Performance Improvement (#9384)

### Engine Core
* Override HF `config.json` via CLI (#5836)
* Add goodput metric support (#9338)
* Move parallel sampling out from vllm core, paving way for V1 engine (#9302)
* Add stateless process group for easier integration with RLHF and disaggregated prefill (#10216, #10072)

### Others
* Improvements to the pull request experience with DCO, mergify, stale bot, etc. (#9436, #9512, #9513, #9259, #10082, #10285, #9803)
* Dropped support for Python 3.8 (#10038, #8464)
* Basic Integration Test For TPU (#9968)
* Document the class hierarchy in vLLM (#10240), explain the integration with Hugging Face (#10173).
* Benchmark throughput now supports image input (#9851)



## What's Changed
* [TPU] Fix TPU SMEM OOM by Pallas paged attention kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9350
* [Frontend] merge beam search implementations by @LunrEclipse in https://github.com/vllm-project/vllm/pull/9296
* [Model] Make llama3.2 support multiple and interleaved images by @xiangxu-google in https://github.com/vllm-project/vllm/pull/9095
* [Bugfix] Clean up some cruft in mamba.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9343
* [Frontend] Clarify model_type error messages by @stevegrubb in https://github.com/vllm-project/vllm/pull/9345
* [Doc] Fix code formatting in spec_decode.rst by @mgoin in https://github.com/vllm-project/vllm/pull/9348
* [Bugfix] Update InternVL input mapper to support image embeds by @hhzhang16 in https://github.com/vllm-project/vllm/pull/9351
* [BugFix] Fix chat API continuous usage stats by @njhill in https://github.com/vllm-project/vllm/pull/9357
* pass ignore_eos parameter to all benchmark_serving calls by @gracehonv in https://github.com/vllm-project/vllm/pull/9349
* [Misc] Directly use compressed-tensors for checkpoint definitions by @mgoin in https://github.com/vllm-project/vllm/pull/8909
* [Bugfix] Fix vLLM UsageInfo and logprobs None AssertionError with empty token_ids by @CatherineSue in https://github.com/vllm-project/vllm/pull/9034
* [Bugfix][CI/Build] Fix CUDA 11.8 Build by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9386
* [Bugfix] Molmo text-only input bug fix by @mrsalehi in https://github.com/vllm-project/vllm/pull/9397
* [Misc] Standardize RoPE handling for Qwen2-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9250
* [Model] VLM2Vec, the first multimodal embedding model in vLLM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9303
* [CI/Build] Test VLM embeddings by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9406
* [Core] Rename input data types by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8688
* [Misc] Consolidate example usage of OpenAI client for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/9412
* [Model] Support SDPA attention for Molmo vision backbone by @Isotr0py in https://github.com/vllm-project/vllm/pull/9410
* Support mistral interleaved attn by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/9414
* [Kernel][Model] Improve continuous batching for Jamba and Mamba by @mzusman in https://github.com/vllm-project/vllm/pull/9189
* [Model][Bugfix] Add FATReLU activation and support for openbmb/MiniCPM-S-1B-sft by @streaver91 in https://github.com/vllm-project/vllm/pull/9396
* [Performance][Spec Decode] Optimize ngram lookup performance by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/9333
* [CI/Build] mypy: Resolve some errors from checking vllm/engine by @russellb in https://github.com/vllm-project/vllm/pull/9267
* [Bugfix][Kernel] Prevent integer overflow in fp8 dynamic per-token quantize kernel by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9425
* [BugFix] [Kernel] Fix GPU SEGV occurring in int8 kernels by @rasmith in https://github.com/vllm-project/vllm/pull/9391
* Add notes on the use of Slack by @terrytangyuan in https://github.com/vllm-project/vllm/pull/9442
* [Kernel] Add Exllama as a backend for compressed-tensors  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9395
* [Misc] Print stack trace using `logger.exception` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9461
* [misc] CUDA Time Layerwise Profiler by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/8337
* [Bugfix] Allow prefill of assistant response when using `mistral_common` by @sasha0552 in https://github.com/vllm-project/vllm/pull/9446
* [TPU] Call torch._sync(param) during weight loading by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9437
* [Hardware][CPU] compressed-tensor INT8 W8A8 AZP support  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/9344
* [Core] Deprecating block manager v1 and make block manager v2 default by @KuntaiDu in https://github.com/vllm-project/vllm/pull/8704
* [CI/Build] remove .github from .dockerignore, add dirty repo check by @dtrifiro in https://github.com/vllm-project/vllm/pull/9375
* [Misc] Remove commit id file by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9470
* [torch.compile] Fine-grained CustomOp enabling mechanism by @ProExpertProg in https://github.com/vllm-project/vllm/pull/9300
* [Bugfix] Fix support for dimension like integers and ScalarType by @bnellnm in https://github.com/vllm-project/vllm/pull/9299
* [Bugfix] Add random_seed to sample_hf_requests in benchmark_serving script by @wukaixingxp in https://github.com/vllm-project/vllm/pull/9013
* [Bugfix] Print warnings related to `mistral_common` tokenizer only once by @sasha0552 in https://github.com/vllm-project/vllm/pull/9468
* [Hardwware][Neuron] Simplify model load for transformers-neuronx library by @sssrijan-amazon in https://github.com/vllm-project/vllm/pull/9380
* Support `BERTModel` (first `encoder-only` embedding model) by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9056
* [BugFix] Stop silent failures on compressed-tensors parsing by @dsikka in https://github.com/vllm-project/vllm/pull/9381
* [Bugfix][Core] Use torch.cuda.memory_stats() to profile peak memory usage by @joerunde in https://github.com/vllm-project/vllm/pull/9352
* [Qwen2.5] Support bnb quant for Qwen2.5 by @blueyo0 in https://github.com/vllm-project/vllm/pull/9467
* [CI/Build] Use commit hash references for github actions by @russellb in https://github.com/vllm-project/vllm/pull/9430
* [BugFix] Typing fixes to RequestOutput.prompt and beam search by @njhill in https://github.com/vllm-project/vllm/pull/9473
* [Frontend][Feature] Add jamba tool parser by @tomeras91 in https://github.com/vllm-project/vllm/pull/9154
* [BugFix] Fix and simplify completion API usage streaming by @njhill in https://github.com/vllm-project/vllm/pull/9475
* [CI/Build] Fix lint errors in mistral tokenizer by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9504
* [Bugfix] Fix offline_inference_with_prefix.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9505
* [Misc] benchmark: Add option to set max concurrency by @russellb in https://github.com/vllm-project/vllm/pull/9390
* [Model] Add user-configurable task for models that support both generation and embedding by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9424
* [CI/Build] Add error matching config for mypy by @russellb in https://github.com/vllm-project/vllm/pull/9512
* [Model] Support Pixtral models in the HF Transformers format by @mgoin in https://github.com/vllm-project/vllm/pull/9036
* [MISC] Add lora requests to metrics by @coolkp in https://github.com/vllm-project/vllm/pull/9477
* [MISC] Consolidate cleanup() and refactor offline_inference_with_prefix.py by @comaniac in https://github.com/vllm-project/vllm/pull/9510
* [Kernel] Add env variable to force flashinfer backend to enable tensor cores by @tdoublep in https://github.com/vllm-project/vllm/pull/9497
* [Bugfix] Fix offline mode when using `mistral_common` by @sasha0552 in https://github.com/vllm-project/vllm/pull/9457
* :bug: fix torch memory profiling by @joerunde in https://github.com/vllm-project/vllm/pull/9516
* [Frontend] Avoid creating guided decoding LogitsProcessor unnecessarily by @njhill in https://github.com/vllm-project/vllm/pull/9521
* [Doc] update gpu-memory-utilization flag docs by @joerunde in https://github.com/vllm-project/vllm/pull/9507
* [CI/Build] Add error matching for ruff output by @russellb in https://github.com/vllm-project/vllm/pull/9513
* [CI/Build] Configure matcher for actionlint workflow by @russellb in https://github.com/vllm-project/vllm/pull/9511
* [Frontend] Support simpler image input format by @yue-anyscale in https://github.com/vllm-project/vllm/pull/9478
* [Bugfix] Fix missing task for speculative decoding by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9524
* [Model][Pixtral] Optimizations for input_processor_for_pixtral_hf by @mgoin in https://github.com/vllm-project/vllm/pull/9514
* [Bugfix] Pass json-schema to GuidedDecodingParams and make test stronger by @heheda12345 in https://github.com/vllm-project/vllm/pull/9530
* [Model][Pixtral] Use memory_efficient_attention for PixtralHFVision by @mgoin in https://github.com/vllm-project/vllm/pull/9520
* [Kernel] Support sliding window in flash attention backend by @heheda12345 in https://github.com/vllm-project/vllm/pull/9403
* [Frontend][Misc] Goodput metric support by @Imss27 in https://github.com/vllm-project/vllm/pull/9338
* [CI/Build] Split up decoder-only LM tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9488
* [Doc] Consistent naming of attention backends by @tdoublep in https://github.com/vllm-project/vllm/pull/9498
* [Model] FalconMamba Support by @dhiaEddineRhaiem in https://github.com/vllm-project/vllm/pull/9325
* [Bugfix][Misc]: fix graph capture for decoder by @yudian0504 in https://github.com/vllm-project/vllm/pull/9549
* [BugFix] Use correct python3 binary in Docker.ppc64le entrypoint by @varad-ahirwadkar in https://github.com/vllm-project/vllm/pull/9492
* [Model][Bugfix] Fix batching with multi-image in PixtralHF by @mgoin in https://github.com/vllm-project/vllm/pull/9518
* [Frontend] Reduce frequency of client cancellation checking by @njhill in https://github.com/vllm-project/vllm/pull/7959
* [doc] fix format by @youkaichao in https://github.com/vllm-project/vllm/pull/9562
* [BugFix] Update draft model TP size check to allow matching target TP size by @njhill in https://github.com/vllm-project/vllm/pull/9394
* [Frontend] Don't log duplicate error stacktrace for every request in the batch by @wallashss in https://github.com/vllm-project/vllm/pull/9023
* [CI] Make format checker error message more user-friendly by using emoji by @KuntaiDu in https://github.com/vllm-project/vllm/pull/9564
* :bug: Fixup more test failures from memory profiling by @joerunde in https://github.com/vllm-project/vllm/pull/9563
* [core] move parallel sampling out from vllm core by @youkaichao in https://github.com/vllm-project/vllm/pull/9302
* [Bugfix]: serialize config instances by value when using --trust-remote-code by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6751
* [CI/Build] Remove unnecessary `fork_new_process` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9484
* [Bugfix][OpenVINO] fix_dockerfile_openvino by @ngrozae in https://github.com/vllm-project/vllm/pull/9552
* [Bugfix]: phi.py get rope_theta from config file by @Falko1 in https://github.com/vllm-project/vllm/pull/9503
* [CI/Build] Replaced some models on tests for smaller ones by @wallashss in https://github.com/vllm-project/vllm/pull/9570
* [Core] Remove evictor_v1 by @KuntaiDu in https://github.com/vllm-project/vllm/pull/9572
* [Doc] Use shell code-blocks and fix section headers by @rafvasq in https://github.com/vllm-project/vllm/pull/9508
* support TP in qwen2 bnb by @chenqianfzh in https://github.com/vllm-project/vllm/pull/9574
* [Hardware][CPU] using current_platform.is_cpu by @wangshuai09 in https://github.com/vllm-project/vllm/pull/9536
* [V1] Implement vLLM V1 [1/N] by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9289
* [CI/Build][LoRA] Temporarily fix long context failure issue by @jeejeelee in https://github.com/vllm-project/vllm/pull/9579
* [Neuron] [Bugfix] Fix neuron startup by @xendo in https://github.com/vllm-project/vllm/pull/9374
* [Model][VLM] Initialize support for Mono-InternVL model by @Isotr0py in https://github.com/vllm-project/vllm/pull/9528
* [Bugfix] Eagle: change config name for fc bias by @gopalsarda in https://github.com/vllm-project/vllm/pull/9580
* [Hardware][Intel CPU][DOC] Update docs for CPU backend by @zhouyuan in https://github.com/vllm-project/vllm/pull/6212
* [Frontend] Support custom request_id from request by @guoyuhong in https://github.com/vllm-project/vllm/pull/9550
* [BugFix] Prevent exporting duplicate OpenTelemetry spans by @ronensc in https://github.com/vllm-project/vllm/pull/9017
* [torch.compile] auto infer dynamic_arg_dims from type annotation by @youkaichao in https://github.com/vllm-project/vllm/pull/9589
* [Bugfix] fix detokenizer shallow copy by @aurickq in https://github.com/vllm-project/vllm/pull/5919
* [Misc] Make benchmarks use EngineArgs by @JArnoldAMD in https://github.com/vllm-project/vllm/pull/9529
* [Bugfix] Fix spurious "No compiled cutlass_scaled_mm ..." for W8A8 on Turing by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9487
* [BugFix] Fix metrics error for --num-scheduler-steps > 1 by @yuleil in https://github.com/vllm-project/vllm/pull/8234
* [Doc]: Update tensorizer docs to include vllm[tensorizer] by @sethkimmel3 in https://github.com/vllm-project/vllm/pull/7889
* [Bugfix] Generate exactly input_len tokens in benchmark_throughput by @heheda12345 in https://github.com/vllm-project/vllm/pull/9592
* [Misc] Add an env var VLLM_LOGGING_PREFIX, if set, it will be prepend to all logging messages by @sfc-gh-zhwang in https://github.com/vllm-project/vllm/pull/9590
* [Model] Support E5-V by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9576
* [Build] Fix `FetchContent` multiple build issue by @ProExpertProg in https://github.com/vllm-project/vllm/pull/9596
* [Hardware][XPU] using current_platform.is_xpu by @MengqingCao in https://github.com/vllm-project/vllm/pull/9605
* [Model] Initialize Florence-2 language backbone support by @Isotr0py in https://github.com/vllm-project/vllm/pull/9555
* [VLM] Post-layernorm override and quant config in vision encoder by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9217
* [Model] Add min_pixels / max_pixels to Qwen2VL as mm_processor_kwargs by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9612
* [Bugfix] Fix `_init_vision_model` in NVLM_D model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9611
* [misc] comment to avoid future confusion about baichuan by @youkaichao in https://github.com/vllm-project/vllm/pull/9620
* [Bugfix] Fix divide by zero when serving Mamba models by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9617
* [Misc] Separate total and output tokens in benchmark_throughput.py by @mgoin in https://github.com/vllm-project/vllm/pull/8914
* [torch.compile] Adding torch compile annotations to some models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9614
* [Frontend] Enable Online Multi-image Support for MLlama by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9393
* [Model] Add Qwen2-Audio model support by @faychu in https://github.com/vllm-project/vllm/pull/9248
* [CI/Build] Add bot to close stale issues and PRs by @russellb in https://github.com/vllm-project/vllm/pull/9436
* [Bugfix][Model] Fix Mllama SDPA illegal memory access for batched multi-image by @mgoin in https://github.com/vllm-project/vllm/pull/9626
* [Bugfix] Use "vision_model" prefix for MllamaVisionModel by @mgoin in https://github.com/vllm-project/vllm/pull/9628
* [Bugfix]: Make chat content text allow type content by @vrdn-23 in https://github.com/vllm-project/vllm/pull/9358
* [XPU] avoid triton import for xpu by @yma11 in https://github.com/vllm-project/vllm/pull/9440
* [Bugfix] Fix PP for ChatGLM and Molmo, and weight loading for Qwen2.5-Math-RM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9422
* [V1][Bugfix] Clean up requests when aborted by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9629
* [core] simplify seq group code by @youkaichao in https://github.com/vllm-project/vllm/pull/9569
* [torch.compile] Adding torch compile annotations to some models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9639
* [Kernel] add kernel for FATReLU by @jeejeelee in https://github.com/vllm-project/vllm/pull/9610
* [torch.compile] expanding support and fix allgather compilation by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9637
* [Doc] Move additional tips/notes to the top by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9647
* [Bugfix]Disable the post_norm layer of the vision encoder for LLaVA models by @litianjian in https://github.com/vllm-project/vllm/pull/9653
* Increase operation per run limit for "Close inactive issues and PRs" workflow by @hmellor in https://github.com/vllm-project/vllm/pull/9661
* [torch.compile] Adding torch compile annotations to some models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9641
* [CI/Build] Fix VLM test failures when using transformers v4.46 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9666
* [Model] Compute Llava Next Max Tokens / Dummy Data From Gridpoints by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9650
* [Log][Bugfix] Fix default value check for `image_url.detail` by @mgoin in https://github.com/vllm-project/vllm/pull/9663
* [Performance][Kernel] Fused_moe Performance Improvement by @charlifu in https://github.com/vllm-project/vllm/pull/9384
* [Bugfix] Remove xformers requirement for Pixtral by @mgoin in https://github.com/vllm-project/vllm/pull/9597
* [ci/Build] Skip Chameleon for transformers 4.46.0 on broadcast test #9675 by @khluu in https://github.com/vllm-project/vllm/pull/9676
* [Model] add a lora module for granite 3.0 MoE models by @willmj in https://github.com/vllm-project/vllm/pull/9673
* [V1] Support sliding window attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9679
* [Bugfix] Fix compressed_tensors_moe bad config.strategy by @mgoin in https://github.com/vllm-project/vllm/pull/9677
* [Doc] Improve quickstart documentation by @rafvasq in https://github.com/vllm-project/vllm/pull/9256
* [Bugfix] Fix crash with llama 3.2 vision models and guided decoding by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/9631
* [Bugfix] Steaming continuous_usage_stats default to False by @samos123 in https://github.com/vllm-project/vllm/pull/9709
* [Hardware][openvino] is_openvino --> current_platform.is_openvino by @MengqingCao in https://github.com/vllm-project/vllm/pull/9716
* Fix: MI100 Support By Bypassing Custom Paged Attention by @MErkinSag in https://github.com/vllm-project/vllm/pull/9560
* [Frontend] Bad words sampling parameter by @Alvant in https://github.com/vllm-project/vllm/pull/9717
* [Model] Add classification Task with Qwen2ForSequenceClassification  by @kakao-kevin-us in https://github.com/vllm-project/vllm/pull/9704
* [Misc] SpecDecodeWorker supports profiling by @Abatom in https://github.com/vllm-project/vllm/pull/9719
* [core] cudagraph output with tensor weak reference by @youkaichao in https://github.com/vllm-project/vllm/pull/9724
* [Misc] Upgrade to pytorch 2.5 by @bnellnm in https://github.com/vllm-project/vllm/pull/9588
* Fix cache management in "Close inactive issues and PRs" actions workflow by @hmellor in https://github.com/vllm-project/vllm/pull/9734
* [Bugfix] Fix load config when using bools by @madt2709 in https://github.com/vllm-project/vllm/pull/9533
* [Hardware][ROCM] using current_platform.is_rocm by @wangshuai09 in https://github.com/vllm-project/vllm/pull/9642
* [torch.compile] support moe models by @youkaichao in https://github.com/vllm-project/vllm/pull/9632
* Fix beam search eos by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9627
* [Bugfix] Fix ray instance detect issue by @yma11 in https://github.com/vllm-project/vllm/pull/9439
* [CI/Build] Adopt Mergify for auto-labeling PRs by @russellb in https://github.com/vllm-project/vllm/pull/9259
* [Model][VLM] Add multi-video support for LLaVA-Onevision by @litianjian in https://github.com/vllm-project/vllm/pull/8905
* [torch.compile] Adding "torch compile" annotations to some models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9758
* [misc] avoid circular import by @youkaichao in https://github.com/vllm-project/vllm/pull/9765
* [torch.compile] add deepseek v2 compile by @youkaichao in https://github.com/vllm-project/vllm/pull/9775
* [Doc] fix third-party model example by @russellb in https://github.com/vllm-project/vllm/pull/9771
* [Model][LoRA]LoRA support added for Qwen by @jeejeelee in https://github.com/vllm-project/vllm/pull/9622
* [Doc] Specify async engine args in docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9726
* [Bugfix] Use temporary directory in registry by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9721
* [Frontend] re-enable multi-modality input in the new beam search implementation by @FerdinandZhong in https://github.com/vllm-project/vllm/pull/9427
* [Model] Add BNB quantization support for Mllama by @Isotr0py in https://github.com/vllm-project/vllm/pull/9720
* [Hardware] using current_platform.seed_everything by @wangshuai09 in https://github.com/vllm-project/vllm/pull/9785
* [Misc] Add metrics for request queue time, forward time, and execute time by @Abatom in https://github.com/vllm-project/vllm/pull/9659
* Fix the log to correct guide user to install modelscope by @tastelikefeet in https://github.com/vllm-project/vllm/pull/9793
* [Bugfix] Use host argument to bind to interface by @svenseeberg in https://github.com/vllm-project/vllm/pull/9798
* [Misc]: Typo fix: Renaming classes (casualLM -> causalLM) by @yannicks1 in https://github.com/vllm-project/vllm/pull/9801
* [Model]  Add LlamaEmbeddingModel as an embedding Implementation of LlamaModel by @jsato8094 in https://github.com/vllm-project/vllm/pull/9806
* [CI][Bugfix] Skip chameleon for transformers 4.46.1 by @mgoin in https://github.com/vllm-project/vllm/pull/9808
* [CI/Build] mergify: fix rules for ci/build label by @russellb in https://github.com/vllm-project/vllm/pull/9804
* [MISC] Set label value to timestamp over 0, to keep track of recent history  by @coolkp in https://github.com/vllm-project/vllm/pull/9777
* [Bugfix][Frontend] Guard against bad token ids by @joerunde in https://github.com/vllm-project/vllm/pull/9634
* [Model] tool calling support for ibm-granite/granite-20b-functioncalling by @wseaton in https://github.com/vllm-project/vllm/pull/8339
* [Docs] Add notes about Snowflake Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/9814
* [Bugfix] Fix prefix strings for quantized VLMs by @mgoin in https://github.com/vllm-project/vllm/pull/9772
* [core][distributed] fix custom allreduce in pytorch 2.5 by @youkaichao in https://github.com/vllm-project/vllm/pull/9815
* Update README.md by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/9819
* [Bugfix][VLM] Make apply_fp8_linear work with >2D input by @mgoin in https://github.com/vllm-project/vllm/pull/9812
* [ci/build] Pin CI dependencies version with pip-compile  by @khluu in https://github.com/vllm-project/vllm/pull/9810
* [Bugfix] Fix multi nodes TP+PP for XPU by @yma11 in https://github.com/vllm-project/vllm/pull/8884
* [Doc] Add the DCO to CONTRIBUTING.md by @russellb in https://github.com/vllm-project/vllm/pull/9803
* [torch.compile] rework compile control with piecewise cudagraph by @youkaichao in https://github.com/vllm-project/vllm/pull/9715
* [Misc] Specify minimum pynvml version by @jeejeelee in https://github.com/vllm-project/vllm/pull/9827
* [TPU] Correctly profile peak memory usage & Upgrade PyTorch XLA by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9438
* [CI/Build] VLM Test Consolidation by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9372
* [Model] Support math-shepherd-mistral-7b-prm model by @Went-Liang in https://github.com/vllm-project/vllm/pull/9697
* [Misc] Add chunked-prefill support on FlashInfer. by @elfiegg in https://github.com/vllm-project/vllm/pull/9781
* [Bugfix][core] replace heartbeat with pid check by @joerunde in https://github.com/vllm-project/vllm/pull/9818
* [Doc] link bug for multistep guided decoding by @joerunde in https://github.com/vllm-project/vllm/pull/9843
* [Neuron] Update Dockerfile.neuron to fix build failure by @hbikki in https://github.com/vllm-project/vllm/pull/9822
* [doc] update pp support by @youkaichao in https://github.com/vllm-project/vllm/pull/9853
* [CI/Build] Simplify exception trace in api server tests by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9787
* [torch.compile] upgrade tests by @youkaichao in https://github.com/vllm-project/vllm/pull/9858
* [Misc][OpenAI] deprecate max_tokens in favor of new max_completion_tokens field for chat completion endpoint by @gcalmettes in https://github.com/vllm-project/vllm/pull/9837
* Revert "[Bugfix] Use host argument to bind to interface (#9798)" by @khluu in https://github.com/vllm-project/vllm/pull/9852
* [Model] Support quantization of Qwen2VisionTransformer for Qwen2-VL by @mgoin in https://github.com/vllm-project/vllm/pull/9817
* [Misc] Remove deprecated arg for cuda graph capture by @ywang96 in https://github.com/vllm-project/vllm/pull/9864
* [Doc] Update Qwen documentation by @jeejeelee in https://github.com/vllm-project/vllm/pull/9869
* [CI/Build] Add Model Tests for Qwen2-VL by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9846
* [CI/Build] Adding a forced docker system prune to clean up space by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/9849
* [Bugfix] Fix `illegal memory access` error with chunked prefill, prefix caching, block manager v2 and xformers enabled together by @sasha0552 in https://github.com/vllm-project/vllm/pull/9532
* [BugFix][Kernel] Fix Illegal memory access in causal_conv1d in H100 by @mzusman in https://github.com/vllm-project/vllm/pull/9838
* [ci/build] Configure dependabot to update pip dependencies  by @khluu in https://github.com/vllm-project/vllm/pull/9811
* [Bugfix][Frontend] Reject guided decoding in multistep mode by @joerunde in https://github.com/vllm-project/vllm/pull/9892
* [torch.compile] directly register custom op by @youkaichao in https://github.com/vllm-project/vllm/pull/9896
* [Bugfix] Fix layer skip logic with bitsandbytes by @mgoin in https://github.com/vllm-project/vllm/pull/9887
* [torch.compile] rework test plans by @youkaichao in https://github.com/vllm-project/vllm/pull/9866
* [Model] Support bitsandbytes for MiniCPMV by @mgoin in https://github.com/vllm-project/vllm/pull/9891
* [torch.compile] Adding torch compile annotations to some models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9876
* [Doc] Update multi-input support by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9906
* [Frontend] Chat-based Embeddings API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9759
* [CI/Build] Add Model Tests for PixtralHF by @mgoin in https://github.com/vllm-project/vllm/pull/9813
* [Frontend] Use a proper chat template for VLM2Vec by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9912
* [Bugfix] Fix edge cases for MistralTokenizer by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/9625
* [Core] Refactor: Clean up unused argument preemption_mode in Scheduler._preempt by @andrejonasson in https://github.com/vllm-project/vllm/pull/9696
* [torch.compile] use interpreter with stable api from pytorch by @youkaichao in https://github.com/vllm-project/vllm/pull/9889
* [Bugfix/Core] Remove assertion for Flashinfer k_scale and v_scale by @pavanimajety in https://github.com/vllm-project/vllm/pull/9861
* [1/N] pass the complete config from engine to executor by @youkaichao in https://github.com/vllm-project/vllm/pull/9933
* [Bugfix] PicklingError on RayTaskError by @GeneDer in https://github.com/vllm-project/vllm/pull/9934
* Bump the patch-update group with 10 updates by @dependabot in https://github.com/vllm-project/vllm/pull/9897
* [Core][VLM] Add precise multi-modal placeholder tracking by @petersalas in https://github.com/vllm-project/vllm/pull/8346
* [ci/build] Have dependabot ignore pinned dependencies by @khluu in https://github.com/vllm-project/vllm/pull/9935
* [Encoder Decoder] Add flash_attn kernel support for encoder-decoder models by @sroy745 in https://github.com/vllm-project/vllm/pull/9559
* [torch.compile] fix cpu broken code by @youkaichao in https://github.com/vllm-project/vllm/pull/9947
* [Docs] Update Granite 3.0 models in supported models table by @njhill in https://github.com/vllm-project/vllm/pull/9930
* [Doc] Updated tpu-installation.rst with more details by @mikegre-google in https://github.com/vllm-project/vllm/pull/9926
* [2/N] executor pass the complete config to worker/modelrunner by @youkaichao in https://github.com/vllm-project/vllm/pull/9938
* [V1] Fix `EngineArgs` refactor on V1 by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9954
* [bugfix] fix chatglm dummy_data_for_glmv by @youkaichao in https://github.com/vllm-project/vllm/pull/9955
* [3/N] model runner pass the whole config to model by @youkaichao in https://github.com/vllm-project/vllm/pull/9958
* [CI/Build] Quoting around > by @nokados in https://github.com/vllm-project/vllm/pull/9956
* [torch.compile] Adding torch compile annotations to vision-language models by @CRZbulabula in https://github.com/vllm-project/vllm/pull/9946
* [bugfix] fix tsts by @youkaichao in https://github.com/vllm-project/vllm/pull/9959
* [V1] Support per-request seed by @njhill in https://github.com/vllm-project/vllm/pull/9945
* [Model] Add support for H2OVL-Mississippi models by @cooleel in https://github.com/vllm-project/vllm/pull/9747
* [V1] Fix Configs by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9971
* [Bugfix] Fix MiniCPMV and Mllama BNB  bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/9917
* [Bugfix]Using the correct type hints by @gshtras in https://github.com/vllm-project/vllm/pull/9885
* [Misc] Compute query_start_loc/seq_start_loc on CPU by @zhengy001 in https://github.com/vllm-project/vllm/pull/9447
* [Bugfix] Fix E2EL mean and median stats by @daitran2k1 in https://github.com/vllm-project/vllm/pull/9984
* [Bugfix][OpenVINO] Fix circular reference #9939 by @MengqingCao in https://github.com/vllm-project/vllm/pull/9974
* [Frontend] Multi-Modality Support for Loading Local Image Files by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/9915
* [4/N] make quant config first-class citizen by @youkaichao in https://github.com/vllm-project/vllm/pull/9978
* [Misc]Reduce BNB static variable by @jeejeelee in https://github.com/vllm-project/vllm/pull/9987
* [Model] factoring out MambaMixer out of Jamba by @mzusman in https://github.com/vllm-project/vllm/pull/8993
* [CI] Basic Integration Test For TPU by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9968
* [Bugfix][CI/Build][Hardware][AMD] Shard ID parameters in AMD tests running parallel jobs by @hissu-hyvarinen in https://github.com/vllm-project/vllm/pull/9279
* [Doc] Update VLM doc about loading from local files by @ywang96 in https://github.com/vllm-project/vllm/pull/9999
* [Bugfix] Fix `MQLLMEngine` hanging by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9973
* [Misc] Refactor benchmark_throughput.py by @lk-chen in https://github.com/vllm-project/vllm/pull/9779
* [Frontend] Add max_tokens prometheus metric by @tomeras91 in https://github.com/vllm-project/vllm/pull/9881
* [Bugfix] Upgrade to pytorch 2.5.1 by @bnellnm in https://github.com/vllm-project/vllm/pull/10001
* [4.5/N] bugfix for quant config in speculative decode by @youkaichao in https://github.com/vllm-project/vllm/pull/10007
* [Bugfix] Respect modules_to_not_convert within awq_marlin by @mgoin in https://github.com/vllm-project/vllm/pull/9895
* [Core] Use os.sched_yield in ShmRingBuffer instead of time.sleep by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9994
* [Core] Make encoder-decoder inputs a nested structure to be more composable by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9604
* [Bugfix] Fixup Mamba by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/10004
* [BugFix] Lazy import ray by @GeneDer in https://github.com/vllm-project/vllm/pull/10021
* [Misc] vllm CLI flags should be ordered for better user readability by @chaunceyjiang in https://github.com/vllm-project/vllm/pull/10017
* [Frontend] Fix tcp port reservation for api server by @russellb in https://github.com/vllm-project/vllm/pull/10012
* Refactor TPU requirements file and pin build dependencies by @richardsliu in https://github.com/vllm-project/vllm/pull/10010
* [Misc] Add logging for CUDA memory by @yangalan123 in https://github.com/vllm-project/vllm/pull/10027
* [CI/Build] Limit github CI jobs based on files changed by @russellb in https://github.com/vllm-project/vllm/pull/9928
* [Model] Support quantization of PixtralHFTransformer for PixtralHF by @mgoin in https://github.com/vllm-project/vllm/pull/9921
* [Feature] Update benchmark_throughput.py to support image input by @lk-chen in https://github.com/vllm-project/vllm/pull/9851
* [Misc] Modify BNB parameter name by @jeejeelee in https://github.com/vllm-project/vllm/pull/9997
* [CI] Prune tests/models/decoder_only/language/* tests by @mgoin in https://github.com/vllm-project/vllm/pull/9940
* [CI] Prune back the number of tests in tests/kernels/* by @mgoin in https://github.com/vllm-project/vllm/pull/9932
* [bugfix] fix weak ref in piecewise cudagraph and tractable test by @youkaichao in https://github.com/vllm-project/vllm/pull/10048
* [Bugfix] Properly propagate trust_remote_code settings by @zifeitong in https://github.com/vllm-project/vllm/pull/10047
* [Bugfix] Fix pickle of input when async output processing is on by @wallashss in https://github.com/vllm-project/vllm/pull/9931
* [Bugfix][SpecDecode] kv corruption with bonus tokens in spec decode by @llsj14 in https://github.com/vllm-project/vllm/pull/9730
* [v1] reduce graph capture time for piecewise cudagraph by @youkaichao in https://github.com/vllm-project/vllm/pull/10059
* [Misc] Sort the list of embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10037
* [Model][OpenVINO] Fix regressions from #8346 by @petersalas in https://github.com/vllm-project/vllm/pull/10045
* [Bugfix] Fix edge-case crash when using chat with the Mistral Tekken Tokenizer by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/10051
* [Bugfix] Gpt-j-6B patch kv_scale to k_scale path  by @arakowsk-amd in https://github.com/vllm-project/vllm/pull/10063
* [Bugfix] Remove CustomChatCompletionContentPartParam multimodal input type by @zifeitong in https://github.com/vllm-project/vllm/pull/10054
* [V1] Integrate Piecewise CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10058
* [distributed] add function to create ipc buffers directly by @youkaichao in https://github.com/vllm-project/vllm/pull/10064
* [CI/Build] drop support for  Python 3.8 EOL by @aarnphm in https://github.com/vllm-project/vllm/pull/8464
* [CI/Build] Fix large_gpu_mark reason by @Isotr0py in https://github.com/vllm-project/vllm/pull/10070
* [Hardware][Intel-Gaudi] Add Intel Gaudi (HPU) inference backend by @kzawora-intel in https://github.com/vllm-project/vllm/pull/6143
* [Hotfix] Fix ruff errors by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10073
* [Model][LoRA]LoRA support added for LlamaEmbeddingModel by @jeejeelee in https://github.com/vllm-project/vllm/pull/10071
* [Model] Add Idefics3 support by @jeejeelee in https://github.com/vllm-project/vllm/pull/9767
* [Model][LoRA]LoRA support added for Qwen2VLForConditionalGeneration by @ericperfect in https://github.com/vllm-project/vllm/pull/10022
* Remove ScaledActivation for AWQ by @mgoin in https://github.com/vllm-project/vllm/pull/10057
* [CI/Build] Drop Python 3.8 support by @russellb in https://github.com/vllm-project/vllm/pull/10038
* [CI/Build] change conflict PR comment from mergify by @russellb in https://github.com/vllm-project/vllm/pull/10080
* [V1] Make v1 more testable by @joerunde in https://github.com/vllm-project/vllm/pull/9888
* [CI/Build] Always run the ruff workflow by @russellb in https://github.com/vllm-project/vllm/pull/10092
* [core][distributed] add stateless_init_process_group by @youkaichao in https://github.com/vllm-project/vllm/pull/10072
* [Bugfix] Fix FP8 torch._scaled_mm fallback for torch>2.5 with CUDA<12.4 by @mgoin in https://github.com/vllm-project/vllm/pull/10095
* [Misc][XPU] Upgrade to Pytorch 2.5 for xpu backend by @yma11 in https://github.com/vllm-project/vllm/pull/9823
* [Frontend] Adjust try/except blocks in API impl by @njhill in https://github.com/vllm-project/vllm/pull/10056
* [Hardware][CPU] Update torch 2.5 by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/9911
* [doc] add back Python 3.8 ABI by @youkaichao in https://github.com/vllm-project/vllm/pull/10100
* [V1][BugFix] Fix Generator construction in greedy + seed case by @njhill in https://github.com/vllm-project/vllm/pull/10097
* [Misc] Consolidate ModelConfig code related to HF config by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10104
* [CI/Build] re-add codespell to CI by @russellb in https://github.com/vllm-project/vllm/pull/10083
* [Doc] Improve benchmark documentation by @rafvasq in https://github.com/vllm-project/vllm/pull/9927
* [Core][Distributed] Refactor ipc buffer init in CustomAllreduce by @hanzhi713 in https://github.com/vllm-project/vllm/pull/10030
* [CI/Build] Improve mypy + python version matrix by @russellb in https://github.com/vllm-project/vllm/pull/10041
* Adds method to read the pooling types from model's files by @flaviabeo in https://github.com/vllm-project/vllm/pull/9506
* [Frontend] Fix multiple values for keyword argument error (#10075) by @DIYer22 in https://github.com/vllm-project/vllm/pull/10076
* [Hardware][CPU][bugfix] Fix half dtype support on AVX2-only target by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/10108
* [Bugfix] Make image processor respect `mm_processor_kwargs` for Qwen2-VL by @li-plus in https://github.com/vllm-project/vllm/pull/10112
* [Misc] Add Gamma-Distribution Request Generation Support for Serving Benchmark. by @spliii in https://github.com/vllm-project/vllm/pull/10105
* [Frontend] Tool calling parser for Granite 3.0 models by @maxdebayser in https://github.com/vllm-project/vllm/pull/9027
* [Feature] [Spec decode]: Combine chunked prefill with speculative decoding by @NickLucche in https://github.com/vllm-project/vllm/pull/9291
* [CI/Build] Always run mypy by @russellb in https://github.com/vllm-project/vllm/pull/10122
* [CI/Build] Add shell script linting using shellcheck by @russellb in https://github.com/vllm-project/vllm/pull/7925
* [CI/Build] Automate PR body text cleanup by @russellb in https://github.com/vllm-project/vllm/pull/10082
* Bump actions/setup-python from 5.2.0 to 5.3.0 by @dependabot in https://github.com/vllm-project/vllm/pull/9745
* Online video support for VLMs by @litianjian in https://github.com/vllm-project/vllm/pull/10020
* Bump actions/checkout from 4.2.1 to 4.2.2 by @dependabot in https://github.com/vllm-project/vllm/pull/9746
* [Misc] Add environment variables collection in collect_env.py tool by @ycool in https://github.com/vllm-project/vllm/pull/9293
* [V1] Add all_token_ids attribute to Request by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10135
* [V1] Prefix caching (take 2) by @comaniac in https://github.com/vllm-project/vllm/pull/9972
* [CI/Build] Give PR cleanup job PR write access by @russellb in https://github.com/vllm-project/vllm/pull/10139
* [Doc] Update FAQ links in spec_decode.rst by @whyiug in https://github.com/vllm-project/vllm/pull/9662
* [Bugfix] Add error handling when server cannot respond any valid tokens by @DearPlanet in https://github.com/vllm-project/vllm/pull/5895
* [Misc] Fix ImportError causing by triton by @MengqingCao in https://github.com/vllm-project/vllm/pull/9493
* [Doc] Move CONTRIBUTING to docs site by @russellb in https://github.com/vllm-project/vllm/pull/9924
* Fixes a typo about 'max_decode_seq_len' which causes crashes with cuda graph. by @sighingnow in https://github.com/vllm-project/vllm/pull/9285
* Add hf_transfer to testing image by @mgoin in https://github.com/vllm-project/vllm/pull/10096
* [Misc] Fix typo in #5895 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10145
* [Bugfix][XPU] Fix xpu tp by introducing XpuCommunicator by @yma11 in https://github.com/vllm-project/vllm/pull/10144
* [Model] Expose size to Idefics3 as mm_processor_kwargs by @Isotr0py in https://github.com/vllm-project/vllm/pull/10146
* [V1]Enable APC by default only for text models by @ywang96 in https://github.com/vllm-project/vllm/pull/10148
* [CI/Build] Update CPU tests to include all "standard" tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5481
* Fix edge case Mistral tokenizer by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/10152
* Disable spec-decode + chunked-prefill for draft models with tensor parallelism > 1 by @sroy745 in https://github.com/vllm-project/vllm/pull/10136
* [Misc] Improve Web UI by @rafvasq in https://github.com/vllm-project/vllm/pull/10090
* [V1] Fix non-cudagraph op name by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10166
* [CI/Build] Ignore .gitignored files for shellcheck by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10162
* Rename vllm.logging to vllm.logging_utils by @flozi00 in https://github.com/vllm-project/vllm/pull/10134
* [torch.compile] Fuse RMSNorm with quant by @ProExpertProg in https://github.com/vllm-project/vllm/pull/9138
* [Bugfix] Fix SymIntArrayRef expected to contain only concrete integers by @bnellnm in https://github.com/vllm-project/vllm/pull/10170
* [Kernel][Triton] Add Triton implementation for scaled_mm_triton to support fp8 and int8 SmoothQuant, symmetric case by @rasmith in https://github.com/vllm-project/vllm/pull/9857
* [CI/Build] Adding timeout in CPU CI to avoid CPU test queue blocking by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/6892
* [0/N] Rename `MultiModalInputs` to `MultiModalKwargs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10040
* [Bugfix] Ignore GPTQ quantization of Qwen2-VL visual module by @mgoin in https://github.com/vllm-project/vllm/pull/10169
* [CI/Build] Fix VLM broadcast tests `tensor_parallel_size` passing by @Isotr0py in https://github.com/vllm-project/vllm/pull/10161
* [Doc] Adjust RunLLM location by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10176
* [5/N] pass the whole config to model by @youkaichao in https://github.com/vllm-project/vllm/pull/9983
* [CI/Build] Add run-hpu-test.sh script by @xuechendi in https://github.com/vllm-project/vllm/pull/10167
* [Bugfix] Enable some fp8 and quantized fullgraph tests by @bnellnm in https://github.com/vllm-project/vllm/pull/10171
* [bugfix] fix broken tests of mlp speculator by @youkaichao in https://github.com/vllm-project/vllm/pull/10177
* [doc] explaining the integration with huggingface by @youkaichao in https://github.com/vllm-project/vllm/pull/10173
* bugfix: fix the bug that stream generate not work by @caijizhuo in https://github.com/vllm-project/vllm/pull/2756
* [Frontend] add `add_request_id` middleware by @cjackal in https://github.com/vllm-project/vllm/pull/9594
* [Frontend][Core] Override HF `config.json` via CLI by @KrishnaM251 in https://github.com/vllm-project/vllm/pull/5836
* [CI/Build] Split up models tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10069
* [ci][build] limit cmake version by @youkaichao in https://github.com/vllm-project/vllm/pull/10188
* [Doc] Fix typo error in CONTRIBUTING.md by @FuryMartin in https://github.com/vllm-project/vllm/pull/10190
* [doc] Polish the integration with huggingface doc by @CRZbulabula in https://github.com/vllm-project/vllm/pull/10195
* [Misc] small fixes to function tracing file path by @ShawnD200 in https://github.com/vllm-project/vllm/pull/9543
* [misc] improve cloudpickle registration and tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10202
* [Doc] Fix typo error in vllm/entrypoints/openai/cli_args.py by @yansh97 in https://github.com/vllm-project/vllm/pull/10196
* [doc] improve debugging code by @youkaichao in https://github.com/vllm-project/vllm/pull/10206
* [6/N] pass whole config to inner model by @youkaichao in https://github.com/vllm-project/vllm/pull/10205
* Bump the patch-update group with 5 updates by @dependabot in https://github.com/vllm-project/vllm/pull/10210
* [Hardware][CPU] Add embedding models support for CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/10193
* [LoRA][Kernel] Remove the unused libentry module by @jeejeelee in https://github.com/vllm-project/vllm/pull/10214
* [V1] Allow `tokenizer_mode` and `trust_remote_code` for Detokenizer by @ywang96 in https://github.com/vllm-project/vllm/pull/10211
* [Bugfix][Hardware][CPU] Fix broken encoder-decoder CPU runner by @Isotr0py in https://github.com/vllm-project/vllm/pull/10218
* [Metrics] add more metrics by @HarryWu99 in https://github.com/vllm-project/vllm/pull/4464
* [Doc] fix doc string typo in block_manager `swap_out` function by @yyccli in https://github.com/vllm-project/vllm/pull/10212
* [core][distributed] add stateless process group by @youkaichao in https://github.com/vllm-project/vllm/pull/10216
* Bump actions/setup-python from 5.2.0 to 5.3.0 by @dependabot in https://github.com/vllm-project/vllm/pull/10209
* [V1] Fix detokenizer ports by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10224
* [V1] Do not use inductor for piecewise CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10225
* [v1][torch.compile] support managing cudagraph buffer by @youkaichao in https://github.com/vllm-project/vllm/pull/10203
* [V1] Use custom ops for piecewise CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10227
* Add docs on serving with Llama Stack by @terrytangyuan in https://github.com/vllm-project/vllm/pull/10183
* [misc][distributed] auto port selection and disable tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10226
* [V1] Enable custom ops with piecewise CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10228
* Make shutil rename in python_only_dev by @shcheglovnd in https://github.com/vllm-project/vllm/pull/10233
* [V1] `AsyncLLM` Implementation by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/9826
* [doc] update debugging guide by @youkaichao in https://github.com/vllm-project/vllm/pull/10236
* [Doc] Update help text for `--distributed-executor-backend` by @russellb in https://github.com/vllm-project/vllm/pull/10231
* [1/N] torch.compile user interface design by @youkaichao in https://github.com/vllm-project/vllm/pull/10237
* [Misc][LoRA] Replace hardcoded cuda device with configurable argument  by @jeejeelee in https://github.com/vllm-project/vllm/pull/10223
* Splitting attention kernel file by @maleksan85 in https://github.com/vllm-project/vllm/pull/10091
* [doc] explain the class hierarchy in vLLM by @youkaichao in https://github.com/vllm-project/vllm/pull/10240
* [CI][CPU]refactor CPU tests to allow to bind with different cores by @zhouyuan in https://github.com/vllm-project/vllm/pull/10222
* [BugFix] Do not raise a `ValueError` when `tool_choice` is set to the supported `none` option and `tools` are not defined. by @gcalmettes in https://github.com/vllm-project/vllm/pull/10000
* [Misc]Fix Idefics3Model argument by @jeejeelee in https://github.com/vllm-project/vllm/pull/10255
* [Bugfix] Fix QwenModel argument by @DamonFool in https://github.com/vllm-project/vllm/pull/10262
* [Frontend] Add per-request number of cached token stats by @zifeitong in https://github.com/vllm-project/vllm/pull/10174
* [V1] Use pickle for serializing EngineCoreRequest & Add multimodal inputs to EngineCoreRequest by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10245
* [Encoder Decoder] Update Mllama to run with both FlashAttention and XFormers by @sroy745 in https://github.com/vllm-project/vllm/pull/9982
* [LoRA] Adds support for bias in LoRA by @followumesh in https://github.com/vllm-project/vllm/pull/5733
* [V1] Enable Inductor when using piecewise CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10268
* [doc] fix location of runllm widget by @youkaichao in https://github.com/vllm-project/vllm/pull/10266
* [doc] improve debugging doc by @youkaichao in https://github.com/vllm-project/vllm/pull/10270
* Revert "[ci][build] limit cmake version" by @youkaichao in https://github.com/vllm-project/vllm/pull/10271
* [V1] Fix CI tests on V1 engine by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10272
* [core][distributed] use tcp store directly by @youkaichao in https://github.com/vllm-project/vllm/pull/10275
* [V1] Support VLMs with fine-grained scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9871
* Bump to compressed-tensors v0.8.0 by @dsikka in https://github.com/vllm-project/vllm/pull/10279
* [Doc] Fix typo in arg_utils.py by @xyang16 in https://github.com/vllm-project/vllm/pull/10264
* [Model] Add support for Qwen2-VL video embeddings input & multiple image embeddings input with varied resolutions by @imkero in https://github.com/vllm-project/vllm/pull/10221
* [Model] Adding Support for Qwen2VL as an Embedding Model. Using MrLight/dse-qwen2-2b-mrl-v1 by @FurtherAI in https://github.com/vllm-project/vllm/pull/9944
* [Core] Flashinfer - Remove advance step size restriction by @pavanimajety in https://github.com/vllm-project/vllm/pull/10282
* [Model][LoRA]LoRA support added for idefics3 by @B-201 in https://github.com/vllm-project/vllm/pull/10281
* [V1] Add missing tokenizer options for `Detokenizer` by @ywang96 in https://github.com/vllm-project/vllm/pull/10288
* [1/N] Initial prototype for multi-modal processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10044
* [Bugfix] bitsandbytes models fail to run pipeline parallel by @HoangCongDuc in https://github.com/vllm-project/vllm/pull/10200
* [Bugfix] Fix tensor parallel for qwen2 classification model by @Isotr0py in https://github.com/vllm-project/vllm/pull/10297
* [misc] error early for old-style class by @youkaichao in https://github.com/vllm-project/vllm/pull/10304
* [Misc] format.sh: Simplify tool_version_check by @russellb in https://github.com/vllm-project/vllm/pull/10305
* [Frontend] Pythonic tool parser by @mdepinet in https://github.com/vllm-project/vllm/pull/9859
* [BugFix]: properly deserialize `tool_calls` iterator before processing by mistral-common when MistralTokenizer is used by @gcalmettes in https://github.com/vllm-project/vllm/pull/9951
* [Model] Add BNB quantization support for Idefics3 by @B-201 in https://github.com/vllm-project/vllm/pull/10310
* [ci][distributed] disable hanging tests by @youkaichao in https://github.com/vllm-project/vllm/pull/10317
* [CI/Build] Fix CPU CI online inference timeout by @Isotr0py in https://github.com/vllm-project/vllm/pull/10314
* [CI/Build] Make shellcheck happy by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10285
* [Docs] Publish meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/10331
* Support Roberta embedding models by @maxdebayser in https://github.com/vllm-project/vllm/pull/9387
* [Perf] Reduce peak memory usage of llama by @andoorve in https://github.com/vllm-project/vllm/pull/10339
* [Bugfix] use AF_INET6 instead of AF_INET for OpenAI Compatible Server by @jxpxxzj in https://github.com/vllm-project/vllm/pull/9583
* [Tool parsing] Improve / correct mistral tool parsing by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/10333
* [Bugfix] Fix unable to load some models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10312
* [bugfix] Fix static asymmetric quantization case by @ProExpertProg in https://github.com/vllm-project/vllm/pull/10334
* [Misc] Change RedundantReshapesPass and FusionPass logging from info to debug by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/10308
* [Model] Support Qwen2 embeddings and use tags to select model tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10184
* [Bugfix]  Qwen-vl output is inconsistent in speculative decoding by @skylee-01 in https://github.com/vllm-project/vllm/pull/10350
* [Misc] Consolidate pooler config overrides by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/10351
* [Build] skip renaming files for release wheels pipeline by @simon-mo in https://github.com/vllm-project/vllm/pull/9671

## New Contributors
* @gracehonv made their first contribution in https://github.com/vllm-project/vllm/pull/9349
* @streaver91 made their first contribution in https://github.com/vllm-project/vllm/pull/9396
* @wukaixingxp made their first contribution in https://github.com/vllm-project/vllm/pull/9013
* @sssrijan-amazon made their first contribution in https://github.com/vllm-project/vllm/pull/9380
* @coolkp made their first contribution in https://github.com/vllm-project/vllm/pull/9477
* @yue-anyscale made their first contribution in https://github.com/vllm-project/vllm/pull/9478
* @dhiaEddineRhaiem made their first contribution in https://github.com/vllm-project/vllm/pull/9325
* @yudian0504 made their first contribution in https://github.com/vllm-project/vllm/pull/9549
* @ngrozae made their first contribution in https://github.com/vllm-project/vllm/pull/9552
* @Falko1 made their first contribution in https://github.com/vllm-project/vllm/pull/9503
* @wangshuai09 made their first contribution in https://github.com/vllm-project/vllm/pull/9536
* @gopalsarda made their first contribution in https://github.com/vllm-project/vllm/pull/9580
* @guoyuhong made their first contribution in https://github.com/vllm-project/vllm/pull/9550
* @JArnoldAMD made their first contribution in https://github.com/vllm-project/vllm/pull/9529
* @yuleil made their first contribution in https://github.com/vllm-project/vllm/pull/8234
* @sethkimmel3 made their first contribution in https://github.com/vllm-project/vllm/pull/7889
* @MengqingCao made their first contribution in https://github.com/vllm-project/vllm/pull/9605
* @CRZbulabula made their first contribution in https://github.com/vllm-project/vllm/pull/9614
* @faychu made their first contribution in https://github.com/vllm-project/vllm/pull/9248
* @vrdn-23 made their first contribution in https://github.com/vllm-project/vllm/pull/9358
* @willmj made their first contribution in https://github.com/vllm-project/vllm/pull/9673
* @samos123 made their first contribution in https://github.com/vllm-project/vllm/pull/9709
* @MErkinSag made their first contribution in https://github.com/vllm-project/vllm/pull/9560
* @Alvant made their first contribution in https://github.com/vllm-project/vllm/pull/9717
* @kakao-kevin-us made their first contribution in https://github.com/vllm-project/vllm/pull/9704
* @madt2709 made their first contribution in https://github.com/vllm-project/vllm/pull/9533
* @FerdinandZhong made their first contribution in https://github.com/vllm-project/vllm/pull/9427
* @svenseeberg made their first contribution in https://github.com/vllm-project/vllm/pull/9798
* @yannicks1 made their first contribution in https://github.com/vllm-project/vllm/pull/9801
* @wseaton made their first contribution in https://github.com/vllm-project/vllm/pull/8339
* @Went-Liang made their first contribution in https://github.com/vllm-project/vllm/pull/9697
* @andrejonasson made their first contribution in https://github.com/vllm-project/vllm/pull/9696
* @GeneDer made their first contribution in https://github.com/vllm-project/vllm/pull/9934
* @mikegre-google made their first contribution in https://github.com/vllm-project/vllm/pull/9926
* @nokados made their first contribution in https://github.com/vllm-project/vllm/pull/9956
* @cooleel made their first contribution in https://github.com/vllm-project/vllm/pull/9747
* @zhengy001 made their first contribution in https://github.com/vllm-project/vllm/pull/9447
* @daitran2k1 made their first contribution in https://github.com/vllm-project/vllm/pull/9984
* @chaunceyjiang made their first contribution in https://github.com/vllm-project/vllm/pull/9915
* @hissu-hyvarinen made their first contribution in https://github.com/vllm-project/vllm/pull/9279
* @lk-chen made their first contribution in https://github.com/vllm-project/vllm/pull/9779
* @yangalan123 made their first contribution in https://github.com/vllm-project/vllm/pull/10027
* @llsj14 made their first contribution in https://github.com/vllm-project/vllm/pull/9730
* @arakowsk-amd made their first contribution in https://github.com/vllm-project/vllm/pull/10063
* @kzawora-intel made their first contribution in https://github.com/vllm-project/vllm/pull/6143
* @DIYer22 made their first contribution in https://github.com/vllm-project/vllm/pull/10076
* @li-plus made their first contribution in https://github.com/vllm-project/vllm/pull/10112
* @spliii made their first contribution in https://github.com/vllm-project/vllm/pull/10105
* @flozi00 made their first contribution in https://github.com/vllm-project/vllm/pull/10134
* @xuechendi made their first contribution in https://github.com/vllm-project/vllm/pull/10167
* @caijizhuo made their first contribution in https://github.com/vllm-project/vllm/pull/2756
* @cjackal made their first contribution in https://github.com/vllm-project/vllm/pull/9594
* @KrishnaM251 made their first contribution in https://github.com/vllm-project/vllm/pull/5836
* @FuryMartin made their first contribution in https://github.com/vllm-project/vllm/pull/10190
* @ShawnD200 made their first contribution in https://github.com/vllm-project/vllm/pull/9543
* @yansh97 made their first contribution in https://github.com/vllm-project/vllm/pull/10196
* @yyccli made their first contribution in https://github.com/vllm-project/vllm/pull/10212
* @shcheglovnd made their first contribution in https://github.com/vllm-project/vllm/pull/10233
* @maleksan85 made their first contribution in https://github.com/vllm-project/vllm/pull/10091
* @followumesh made their first contribution in https://github.com/vllm-project/vllm/pull/5733
* @imkero made their first contribution in https://github.com/vllm-project/vllm/pull/10221
* @B-201 made their first contribution in https://github.com/vllm-project/vllm/pull/10281
* @HoangCongDuc made their first contribution in https://github.com/vllm-project/vllm/pull/10200
* @mdepinet made their first contribution in https://github.com/vllm-project/vllm/pull/9859
* @jxpxxzj made their first contribution in https://github.com/vllm-project/vllm/pull/9583
* @skylee-01 made their first contribution in https://github.com/vllm-project/vllm/pull/10350

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.3...v0.6.4

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.4)

---

## v0.6.3.post1: v0.6.3.post1
**Published:** 2024-10-17

## Highlights

### New Models
* Support Ministral 3B and Ministral 8B via interleaved attention (#9414)
* Support multiple and interleaved images for Llama3.2 (#9095)
* Support VLM2Vec, the first multimodal embedding model in vLLM (#9303)

### Important bug fix
* Fix chat API continuous usage stats (#9357)
* Fix vLLM UsageInfo and logprobs None AssertionError with empty token_ids (#9034)
* Fix Molmo text-only input bug (#9397)
* Fix CUDA 11.8 Build (#9386)
* Fix `_version.py` not found issue (#9375)

### Other Enhancements
* Remove block manager v1 and make block manager v2 default (#8704)
* Spec Decode Optimize ngram lookup performance (#9333)


## What's Changed
* [TPU] Fix TPU SMEM OOM by Pallas paged attention kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9350
* [Frontend] merge beam search implementations by @LunrEclipse in https://github.com/vllm-project/vllm/pull/9296
* [Model] Make llama3.2 support multiple and interleaved images by @xiangxu-google in https://github.com/vllm-project/vllm/pull/9095
* [Bugfix] Clean up some cruft in mamba.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9343
* [Frontend] Clarify model_type error messages by @stevegrubb in https://github.com/vllm-project/vllm/pull/9345
* [Doc] Fix code formatting in spec_decode.rst by @mgoin in https://github.com/vllm-project/vllm/pull/9348
* [Bugfix] Update InternVL input mapper to support image embeds by @hhzhang16 in https://github.com/vllm-project/vllm/pull/9351
* [BugFix] Fix chat API continuous usage stats by @njhill in https://github.com/vllm-project/vllm/pull/9357
* pass ignore_eos parameter to all benchmark_serving calls by @gracehonv in https://github.com/vllm-project/vllm/pull/9349
* [Misc] Directly use compressed-tensors for checkpoint definitions by @mgoin in https://github.com/vllm-project/vllm/pull/8909
* [Bugfix] Fix vLLM UsageInfo and logprobs None AssertionError with empty token_ids by @CatherineSue in https://github.com/vllm-project/vllm/pull/9034
* [Bugfix][CI/Build] Fix CUDA 11.8 Build by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9386
* [Bugfix] Molmo text-only input bug fix by @mrsalehi in https://github.com/vllm-project/vllm/pull/9397
* [Misc] Standardize RoPE handling for Qwen2-VL by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9250
* [Model] VLM2Vec, the first multimodal embedding model in vLLM by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9303
* [CI/Build] Test VLM embeddings by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9406
* [Core] Rename input data types by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8688
* [Misc] Consolidate example usage of OpenAI client for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/9412
* [Model] Support SDPA attention for Molmo vision backbone by @Isotr0py in https://github.com/vllm-project/vllm/pull/9410
* Support mistral interleaved attn by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/9414
* [Kernel][Model] Improve continuous batching for Jamba and Mamba by @mzusman in https://github.com/vllm-project/vllm/pull/9189
* [Model][Bugfix] Add FATReLU activation and support for openbmb/MiniCPM-S-1B-sft by @streaver91 in https://github.com/vllm-project/vllm/pull/9396
* [Performance][Spec Decode] Optimize ngram lookup performance by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/9333
* [CI/Build] mypy: Resolve some errors from checking vllm/engine by @russellb in https://github.com/vllm-project/vllm/pull/9267
* [Bugfix][Kernel] Prevent integer overflow in fp8 dynamic per-token quantize kernel by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9425
* [BugFix] [Kernel] Fix GPU SEGV occurring in int8 kernels by @rasmith in https://github.com/vllm-project/vllm/pull/9391
* Add notes on the use of Slack by @terrytangyuan in https://github.com/vllm-project/vllm/pull/9442
* [Kernel] Add Exllama as a backend for compressed-tensors  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9395
* [Misc] Print stack trace using `logger.exception` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9461
* [misc] CUDA Time Layerwise Profiler by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/8337
* [Bugfix] Allow prefill of assistant response when using `mistral_common` by @sasha0552 in https://github.com/vllm-project/vllm/pull/9446
* [TPU] Call torch._sync(param) during weight loading by @WoosukKwon in https://github.com/vllm-project/vllm/pull/9437
* [Hardware][CPU] compressed-tensor INT8 W8A8 AZP support  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/9344
* [Core] Deprecating block manager v1 and make block manager v2 default by @KuntaiDu in https://github.com/vllm-project/vllm/pull/8704
* [CI/Build] remove .github from .dockerignore, add dirty repo check by @dtrifiro in https://github.com/vllm-project/vllm/pull/9375

## New Contributors
* @gracehonv made their first contribution in https://github.com/vllm-project/vllm/pull/9349
* @streaver91 made their first contribution in https://github.com/vllm-project/vllm/pull/9396

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.3...v0.6.3.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.3.post1)

---

## v0.6.3: v0.6.3
**Published:** 2024-10-14

## Highlights

### Model Support 
* New Models:
	* Text: Granite MoE (#8206), Mamba (#6484, #8533)
	* Vision: GLM-4V (#9242), Molmo (#9016), NVLM-D (#9045)
	* Reward model support: Qwen2.5-Math-RM-72B (#8896)
* Expansion in functionality:
	* Add Gemma2 embedding model (#9004)
	* Support input embeddings for qwen2vl (#8856), minicpmv (#9237)
	* LoRA:
		* LoRA support for MiniCPMV2.5 (#7199), MiniCPMV2.6 (#8943)
		* Expand lora modules for mixtral (#9008)
	* Pipeline parallelism support to remaining text and embedding models (#7168, #9090)
	* Expanded bitsandbytes quantization support for Falcon, OPT, Gemma, Gemma2, and Phi (#9148)
	* Tool use:
		* Add support for Llama 3.1 and 3.2 tool use (#8343)
		* Support tool calling for InternLM2.5 (#8405)
* Out of tree support enhancements: Explicit interface for vLLM models and support OOT embedding models (#9108)

### Documentation
* New compatibility matrix for mutual exclusive features (#8512)
* Reorganized installation doc, note that we publish a per-commit docker image (#8931)

### Hardware Support:
* Cross-attention and Encoder-Decoder models support on x86 CPU backend (#9089)
* Support AWQ for CPU backend (#7515)
* Add async output processor for xpu (#8897)
* Add on-device sampling support for Neuron (#8746)

### Architectural Enhancements
* Progress in vLLM's refactoring to a core core:
	* Spec decode removing batch expansion (#8839, #9298).
	* We have made block manager V2 the default. This is an internal refactoring for cleaner and more tested code path (#8678).
	* Moving beam search from the core to the API level (#9105, #9087, #9117, #8928)
	* Move guided decoding params into sampling params (#8252)
* Torch Compile:
	* You can now set an env var `VLLM_TORCH_COMPILE_LEVEL` to control `torch.compile` various levels of compilation control and integration (#9058). Along with various improvements (#8982, #9258, #906, #8875), using `VLLM_TORCH_COMPILE_LEVEL=3` can turn on Inductor's full graph compilation without vLLM's custom ops. 

### Others
* Performance enhancements to turn on multi-step scheeduling by default (#8804, #8645, #8378)
* Enhancements towards priority scheduling (#8965, #8956, #8850)






## What's Changed
* [Misc] Update config loading for Qwen2-VL and remove Granite by @ywang96 in https://github.com/vllm-project/vllm/pull/8837
* [Build/CI] Upgrade to gcc 10 in the base build Docker image by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8814
* [Docs] Add README to the build docker image by @mgoin in https://github.com/vllm-project/vllm/pull/8825
* [CI/Build] Fix missing ci dependencies by @fyuan1316 in https://github.com/vllm-project/vllm/pull/8834
* [misc][installation] build from source without compilation by @youkaichao in https://github.com/vllm-project/vllm/pull/8818
* [ci] Soft fail Entrypoints, Samplers, LoRA, Decoder-only VLM by @khluu in https://github.com/vllm-project/vllm/pull/8872
* [Bugfix] Include encoder prompts len to non-stream api usage response by @Pernekhan in https://github.com/vllm-project/vllm/pull/8861
* [Misc] Change dummy profiling and BOS fallback warns to log once by @mgoin in https://github.com/vllm-project/vllm/pull/8820
* [Bugfix] Fix print_warning_once's line info by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8867
* fix validation: Only set tool_choice `auto` if at least one tool is provided by @chiragjn in https://github.com/vllm-project/vllm/pull/8568
* [Bugfix] Fixup advance_step.cu warning by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8815
* [BugFix] Fix test breakages from transformers 4.45 upgrade by @njhill in https://github.com/vllm-project/vllm/pull/8829
* [Installation] Allow lower versions of FastAPI to maintain Ray 2.9 compatibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8764
* [Feature] Add support for Llama 3.1 and 3.2 tool use by @maxdebayser in https://github.com/vllm-project/vllm/pull/8343
* [Core] Rename `PromptInputs` and `inputs` with backward compatibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8876
* [misc] fix collect env by @youkaichao in https://github.com/vllm-project/vllm/pull/8894
* [MISC] Fix invalid escape sequence '\' by @panpan0000 in https://github.com/vllm-project/vllm/pull/8830
* [Bugfix][VLM] Fix Fuyu batching inference with `max_num_seqs>1` by @Isotr0py in https://github.com/vllm-project/vllm/pull/8892
* [TPU] Update pallas.py to support trillium by @bvrockwell in https://github.com/vllm-project/vllm/pull/8871
* [torch.compile] use empty tensor instead of None for profiling by @youkaichao in https://github.com/vllm-project/vllm/pull/8875
* [Kernel] AQ AZP 4/4: Integrate asymmetric quantization to linear method by @ProExpertProg in https://github.com/vllm-project/vllm/pull/7271
* [Bugfix] fix for deepseek w4a16 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/8906
* [Core] Multi-Step + Single Step Prefills via Chunked Prefill code path by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/8378
* [misc][distributed] add VLLM_SKIP_P2P_CHECK flag by @youkaichao in https://github.com/vllm-project/vllm/pull/8911
* [Core] Priority-based scheduling in async engine by @schoennenbeck in https://github.com/vllm-project/vllm/pull/8850
* [misc] fix wheel name by @youkaichao in https://github.com/vllm-project/vllm/pull/8919
* [Bugfix][Intel] Fix XPU Dockerfile Build by @tylertitsworth in https://github.com/vllm-project/vllm/pull/7824
* [Misc] Remove vLLM patch of `BaichuanTokenizer` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8921
* [Bugfix] Fix code for downloading models from modelscope by @tastelikefeet in https://github.com/vllm-project/vllm/pull/8443
* [Bugfix] Fix PP for Multi-Step by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/8887
* [CI/Build] Update models tests & examples by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8874
* [Frontend] Make beam search emulator temperature modifiable by @nFunctor in https://github.com/vllm-project/vllm/pull/8928
* [Bugfix] Support testing prefill throughput with benchmark_serving.py --hf-output-len 1 by @heheda12345 in https://github.com/vllm-project/vllm/pull/8891
* [doc] organize installation doc and expose per-commit docker by @youkaichao in https://github.com/vllm-project/vllm/pull/8931
* [Core] Improve choice of Python multiprocessing method by @russellb in https://github.com/vllm-project/vllm/pull/8823
* [Bugfix] Block manager v2 with preemption and lookahead slots by @sroy745 in https://github.com/vllm-project/vllm/pull/8824
* [Bugfix] Fix Marlin MoE act order when is_k_full == False by @ElizaWszola in https://github.com/vllm-project/vllm/pull/8741
* [CI/Build] Add test decorator for minimum GPU memory by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8925
* [Build/CI] Set FETCHCONTENT_BASE_DIR to one location for better caching by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8930
* [Model] Support Qwen2.5-Math-RM-72B by @zhuzilin in https://github.com/vllm-project/vllm/pull/8896
* [Model][LoRA]LoRA support added for MiniCPMV2.5 by @jeejeelee in https://github.com/vllm-project/vllm/pull/7199
* [BugFix] Fix seeded random sampling with encoder-decoder models by @njhill in https://github.com/vllm-project/vllm/pull/8870
* [Misc] Fix typo in BlockSpaceManagerV1 by @juncheoll in https://github.com/vllm-project/vllm/pull/8944
* [Frontend] Added support for HF's new `continue_final_message` parameter by @danieljannai21 in https://github.com/vllm-project/vllm/pull/8942
* [Kernel][Model] Varlen prefill + Prefill chunking support for mamba kernels and Jamba model by @mzusman in https://github.com/vllm-project/vllm/pull/8533
* [Model] support input embeddings for qwen2vl by @whyiug in https://github.com/vllm-project/vllm/pull/8856
* [Misc][CI/Build] Include `cv2` via `mistral_common[opencv]`  by @ywang96 in https://github.com/vllm-project/vllm/pull/8951
* [Model][LoRA]LoRA support added for MiniCPMV2.6 by @jeejeelee in https://github.com/vllm-project/vllm/pull/8943
* [Model] Expose InternVL2 max_dynamic_patch as a mm_processor_kwarg by @Isotr0py in https://github.com/vllm-project/vllm/pull/8946
* [Core] Make scheduling policy settable via EngineArgs by @schoennenbeck in https://github.com/vllm-project/vllm/pull/8956
* [Misc] Adjust max_position_embeddings for LoRA compatibility by @jeejeelee in https://github.com/vllm-project/vllm/pull/8957
* [ci] Add CODEOWNERS for test directories  by @khluu in https://github.com/vllm-project/vllm/pull/8795
* [CI][SpecDecode] Fix spec decode tests, use flash attention backend for spec decode CI tests. by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/8975
* [Frontend][Core] Move guided decoding params into sampling params by @joerunde in https://github.com/vllm-project/vllm/pull/8252
* [CI/Build] Fix machete generated kernel files ordering by @khluu in https://github.com/vllm-project/vllm/pull/8976
* [torch.compile] fix tensor alias by @youkaichao in https://github.com/vllm-project/vllm/pull/8982
* [Misc] add process_weights_after_loading for DummyLoader by @divakar-amd in https://github.com/vllm-project/vllm/pull/8969
* [Bugfix] Fix Fuyu tensor parallel inference by @Isotr0py in https://github.com/vllm-project/vllm/pull/8986
* [Bugfix] Fix Token IDs Reference for MiniCPM-V When Images are Provided With No Placeholders by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8991
* [Core] [Frontend] Priority scheduling for embeddings and in the OpenAI-API by @schoennenbeck in https://github.com/vllm-project/vllm/pull/8965
* [Doc] Update list of supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8987
* Update benchmark_serving.py to read and write json-datasets, results in UTF8, for better compatibility with Windows by @vlsav in https://github.com/vllm-project/vllm/pull/8997
* [Spec Decode] (1/2) Remove batch expansion by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/8839
* [Core] Combined support for multi-step scheduling, chunked prefill & prefix caching by @afeldman-nm in https://github.com/vllm-project/vllm/pull/8804
* [Misc] Update Default Image Mapper Error Log by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8977
* [Core] CUDA Graphs for Multi-Step + Chunked-Prefill by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/8645
* [OpenVINO] Enable GPU support for OpenVINO vLLM backend by @sshlyapn in https://github.com/vllm-project/vllm/pull/8192
* [Model]  Adding Granite MoE. by @shawntan in https://github.com/vllm-project/vllm/pull/8206
* [Doc] Update Granite model docs by @njhill in https://github.com/vllm-project/vllm/pull/9025
* [Bugfix] example template should not add parallel_tool_prompt if tools is none by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/9007
* [Misc] log when using default MoE config by @divakar-amd in https://github.com/vllm-project/vllm/pull/8971
* [BugFix] Enforce Mistral ToolCall id constraint when using the Mistral tool call parser by @gcalmettes in https://github.com/vllm-project/vllm/pull/9020
* [Core] Make BlockSpaceManagerV2 the default BlockManager to use. by @sroy745 in https://github.com/vllm-project/vllm/pull/8678
* [Frontend] [Neuron] Parse literals out of override-neuron-config by @xendo in https://github.com/vllm-project/vllm/pull/8959
* [misc] add forward context for attention by @youkaichao in https://github.com/vllm-project/vllm/pull/9029
* Fix failing spec decode test by @sroy745 in https://github.com/vllm-project/vllm/pull/9054
* [Bugfix] Weight loading fix for OPT model by @domenVres in https://github.com/vllm-project/vllm/pull/9042
* [Frontend][Feature] support tool calling for internlm/internlm2_5-7b-chat model by @sydnash in https://github.com/vllm-project/vllm/pull/8405
* [CI/Build] Per file CUDA Archs (improve wheel size and dev build times) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/8845
* [Misc] Enable multi-step output streaming by default by @mgoin in https://github.com/vllm-project/vllm/pull/9047
* [Models] Add remaining model PP support by @andoorve in https://github.com/vllm-project/vllm/pull/7168
* [Misc] Move registry to its own file by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9064
* [Bugfix] Reshape the dimensions of the input image embeddings in Qwen2VL by @whyiug in https://github.com/vllm-project/vllm/pull/9071
* [Bugfix] Flash attention arches not getting set properly by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9062
* [Model] add a bunch of supported lora modules for mixtral by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/9008
* Remove AMD Ray Summit Banner by @simon-mo in https://github.com/vllm-project/vllm/pull/9075
* [Hardware][PowerPC] Make oneDNN dependency optional for Power by @varad-ahirwadkar in https://github.com/vllm-project/vllm/pull/9039
* [Core][VLM] Test registration for OOT multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/8717
* Adds truncate_prompt_tokens param for embeddings creation by @flaviabeo in https://github.com/vllm-project/vllm/pull/8999
* [Kernel] Zero point support in fused MarlinMoE kernel + AWQ Fused MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/8973
* [CI] Update performance benchmark: upgrade trt-llm to r24.07, and add SGLang by @KuntaiDu in https://github.com/vllm-project/vllm/pull/7412
* [Misc] Improved prefix cache example by @Imss27 in https://github.com/vllm-project/vllm/pull/9077
* [Misc] Add random seed for prefix cache benchmark by @Imss27 in https://github.com/vllm-project/vllm/pull/9081
* [Misc] Fix CI lint by @comaniac in https://github.com/vllm-project/vllm/pull/9085
* [Hardware][Neuron] Add on-device sampling support for Neuron by @chongmni-aws in https://github.com/vllm-project/vllm/pull/8746
* [torch.compile] improve allreduce registration by @youkaichao in https://github.com/vllm-project/vllm/pull/9061
* [Doc] Update README.md with Ray summit slides by @zhuohan123 in https://github.com/vllm-project/vllm/pull/9088
* [Bugfix] use blockmanagerv1 for encoder-decoder by @heheda12345 in https://github.com/vllm-project/vllm/pull/9084
* [Bugfix] Fixes for Phi3v and Ultravox Multimodal EmbeddingInputs Support by @hhzhang16 in https://github.com/vllm-project/vllm/pull/8979
* [Model] Support Gemma2 embedding model by @xyang16 in https://github.com/vllm-project/vllm/pull/9004
* [Bugfix] Deprecate registration of custom configs to huggingface by @heheda12345 in https://github.com/vllm-project/vllm/pull/9083
* [Bugfix] Fix order of arguments matters in config.yaml by @Imss27 in https://github.com/vllm-project/vllm/pull/8960
* [core] use forward context for flash infer by @youkaichao in https://github.com/vllm-project/vllm/pull/9097
* [Bugfix] Fix try-catch conditions to import correct Flash Attention Backend in Draft Model by @tjtanaa in https://github.com/vllm-project/vllm/pull/9101
* [Frontend] API support for beam search by @LunrEclipse in https://github.com/vllm-project/vllm/pull/9087
* [Misc] Remove user-facing error for removed VLM args by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9104
* [Model] PP support for embedding models and update docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9090
* [Bugfix] fix tool_parser error handling when serve a model not support it by @liuyanyi in https://github.com/vllm-project/vllm/pull/8709
* [Bugfix] Fix incorrect updates to num_computed_tokens in multi-step scheduling by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/9038
* [Bugfix][Hardware][CPU] Fix CPU model input for decode by @Isotr0py in https://github.com/vllm-project/vllm/pull/9044
* [BugFix][Core] Fix BlockManagerV2 when Encoder Input is None by @sroy745 in https://github.com/vllm-project/vllm/pull/9103
* [core] remove beam search from the core by @youkaichao in https://github.com/vllm-project/vllm/pull/9105
* [Model] Explicit interface for vLLM models and support OOT embedding models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9108
* [Hardware][CPU] Cross-attention and Encoder-Decoder models support on CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/9089
* [Core] Refactor GGUF parameters packing and forwarding by @Isotr0py in https://github.com/vllm-project/vllm/pull/8859
* [Model] Support NVLM-D and fix QK Norm in InternViT by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9045
* [Doc]: Add deploying_with_k8s guide by @haitwang-cloud in https://github.com/vllm-project/vllm/pull/8451
* [CI/Build] Add linting for github actions workflows by @russellb in https://github.com/vllm-project/vllm/pull/7876
* [Doc] Include performance benchmark in README by @KuntaiDu in https://github.com/vllm-project/vllm/pull/9135
* [misc] fix comment and variable name by @youkaichao in https://github.com/vllm-project/vllm/pull/9139
* Add Slack to README by @simon-mo in https://github.com/vllm-project/vllm/pull/9137
* [misc] update utils to support comparing multiple settings by @youkaichao in https://github.com/vllm-project/vllm/pull/9140
* [Intel GPU] Fix xpu decode input  by @jikunshang in https://github.com/vllm-project/vllm/pull/9145
* [misc] improve ux on readme by @youkaichao in https://github.com/vllm-project/vllm/pull/9147
* [Frontend] API support for beam search for MQLLMEngine by @LunrEclipse in https://github.com/vllm-project/vllm/pull/9117
* [Core][Frontend] Add Support for Inference Time mm_processor_kwargs by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9131
* [Frontend] Add Early Validation For Chat Template / Tool Call Parser by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/9151
* [CI/Build] Add examples folder into Docker image so that we can leverage the templates*.jinja when serving models by @panpan0000 in https://github.com/vllm-project/vllm/pull/8758
* [Bugfix] fix OpenAI API server startup with --disable-frontend-multiprocessing by @dtrifiro in https://github.com/vllm-project/vllm/pull/8537
* [Doc] Update vlm.rst to include an example on videos by @sayakpaul in https://github.com/vllm-project/vllm/pull/9155
* [Doc] Improve contributing and installation documentation by @rafvasq in https://github.com/vllm-project/vllm/pull/9132
* [Bugfix] Try to handle older versions of pytorch by @bnellnm in https://github.com/vllm-project/vllm/pull/9086
* mypy: check additional directories by @russellb in https://github.com/vllm-project/vllm/pull/9162
* Add `lm-eval` directly to requirements-test.txt by @mgoin in https://github.com/vllm-project/vllm/pull/9161
* support bitsandbytes quantization with more models by @chenqianfzh in https://github.com/vllm-project/vllm/pull/9148
* Add classifiers in setup.py by @terrytangyuan in https://github.com/vllm-project/vllm/pull/9171
* Update link to KServe deployment guide by @terrytangyuan in https://github.com/vllm-project/vllm/pull/9173
* [Misc] Improve validation errors around best_of and n by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/9167
* [Bugfix][Doc] Report neuron error in output by @joerowell in https://github.com/vllm-project/vllm/pull/9159
* [Model] Remap FP8 kv_scale in CommandR and DBRX by @hliuca in https://github.com/vllm-project/vllm/pull/9174
* [Frontend] Log the maximum supported concurrency by @AlpinDale in https://github.com/vllm-project/vllm/pull/8831
* [Bugfix] Optimize composite weight loading and fix EAGLE weight loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9160
* [ci][test] use load dummy for testing by @youkaichao in https://github.com/vllm-project/vllm/pull/9165
* [Doc] Fix VLM prompt placeholder sample bug by @ycool in https://github.com/vllm-project/vllm/pull/9170
* [Bugfix] Fix lora loading for Compressed Tensors in #9120 by @fahadh4ilyas in https://github.com/vllm-project/vllm/pull/9179
* [Bugfix] Access `get_vocab` instead of `vocab` in tool parsers by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9188
* Add Dependabot configuration for GitHub Actions updates by @EwoutH in https://github.com/vllm-project/vllm/pull/1217
* [Hardware][CPU] Support AWQ for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/7515
* [CI/Build] mypy: check vllm/entrypoints by @russellb in https://github.com/vllm-project/vllm/pull/9194
* [CI/Build] Update Dockerfile install+deploy image to ubuntu 22.04 by @mgoin in https://github.com/vllm-project/vllm/pull/9130
* [Core] Fix invalid args to _process_request by @russellb in https://github.com/vllm-project/vllm/pull/9201
* [misc] improve model support check in another process by @youkaichao in https://github.com/vllm-project/vllm/pull/9208
* [Bugfix] Fix Weight Loading Multiple GPU Test - Large Models by @mgoin in https://github.com/vllm-project/vllm/pull/9213
* [Bugfix] Machete garbage results for some models (large K dim) by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9212
* [Core] Add an environment variable which needs to be set explicitly to allow BlockSpaceManagerV1 by @sroy745 in https://github.com/vllm-project/vllm/pull/9149
* [Bugfix] Fix lm_head weights tying with lora for llama by @Isotr0py in https://github.com/vllm-project/vllm/pull/9227
* [Model] support input image embedding for minicpmv by @whyiug in https://github.com/vllm-project/vllm/pull/9237
* [OpenVINO] Use torch 2.4.0 and newer optimim version by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/9121
* [Bugfix] Fix Machete unittests failing with `NotImplementedError` by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9218
* [Doc] Improve debugging documentation by @rafvasq in https://github.com/vllm-project/vllm/pull/9204
* [CI/Build] Make the `Dockerfile.cpu` file's  `PIP_EXTRA_INDEX_URL` Configurable as a Build Argument by @jyono in https://github.com/vllm-project/vllm/pull/9252
* Suggest codeowners for the core componenets by @simon-mo in https://github.com/vllm-project/vllm/pull/9210
* [torch.compile] integration with compilation control by @youkaichao in https://github.com/vllm-project/vllm/pull/9058
* Bump actions/github-script from 6 to 7 by @dependabot in https://github.com/vllm-project/vllm/pull/9197
* Bump actions/checkout from 3 to 4 by @dependabot in https://github.com/vllm-project/vllm/pull/9196
* Bump actions/setup-python from 3 to 5 by @dependabot in https://github.com/vllm-project/vllm/pull/9195
* [ci/build] Add placeholder command for custom models test and add comments by @khluu in https://github.com/vllm-project/vllm/pull/9262
* [torch.compile] generic decorators by @youkaichao in https://github.com/vllm-project/vllm/pull/9258
* [Doc][Neuron] add note to neuron documentation about resolving triton issue by @omrishiv in https://github.com/vllm-project/vllm/pull/9257
* [Misc] Fix sampling from sonnet for long context case by @Imss27 in https://github.com/vllm-project/vllm/pull/9235
* [misc] hide best_of from engine by @youkaichao in https://github.com/vllm-project/vllm/pull/9261
* [Misc] Collect model support info in a single process per model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/9233
* [Misc][LoRA] Support loading LoRA weights for target_modules in reg format by @jeejeelee in https://github.com/vllm-project/vllm/pull/9275
* [Bugfix] Fix priority in multiprocessing engine by @schoennenbeck in https://github.com/vllm-project/vllm/pull/9277
* [Model] Support Mamba by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6484
* [Kernel] adding fused moe kernel config for L40S TP4 by @bringlein in https://github.com/vllm-project/vllm/pull/9245
* [Model] Add GLM-4v support and meet vllm==0.6.2  by @sixsixcoder in https://github.com/vllm-project/vllm/pull/9242
* [Doc] Remove outdated comment to avoid misunderstanding by @homeffjy in https://github.com/vllm-project/vllm/pull/9287
* [Doc] Compatibility matrix for mutual exclusive features by @wallashss in https://github.com/vllm-project/vllm/pull/8512
* [Bugfix][CI/Build] Fix docker build where CUDA archs < 7.0 are being detected by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/9254
* [Bugfix] Sets `is_first_step_output` for TPUModelRunner by @allenwang28 in https://github.com/vllm-project/vllm/pull/9202
* [bugfix] fix f-string for error by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/9295
* [BugFix] Fix tool call finish reason in streaming case by @maxdebayser in https://github.com/vllm-project/vllm/pull/9209
* [SpecDec] Remove Batch Expansion (2/3) by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/9298
* [Bugfix] Fix bug of xformer prefill for encoder-decoder by @xiangxu-google in https://github.com/vllm-project/vllm/pull/9026
* [Misc][Installation] Improve source installation script and related documentation by @cermeng in https://github.com/vllm-project/vllm/pull/9309
* [Bugfix]Fix MiniCPM's LoRA bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/9286
* [CI] Fix merge conflict by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/9317
* [Bugfix] Bandaid fix for speculative decoding tests by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/9327
* [Model] Molmo vLLM Integration by @mrsalehi in https://github.com/vllm-project/vllm/pull/9016
* [Hardware][intel GPU] add async output process for xpu by @jikunshang in https://github.com/vllm-project/vllm/pull/8897
* [CI/Build] setuptools-scm fixes by @dtrifiro in https://github.com/vllm-project/vllm/pull/8900
* [Docs] Remove PDF build from Readtehdocs by @simon-mo in https://github.com/vllm-project/vllm/pull/9347

## New Contributors
* @fyuan1316 made their first contribution in https://github.com/vllm-project/vllm/pull/8834
* @panpan0000 made their first contribution in https://github.com/vllm-project/vllm/pull/8830
* @bvrockwell made their first contribution in https://github.com/vllm-project/vllm/pull/8871
* @tylertitsworth made their first contribution in https://github.com/vllm-project/vllm/pull/7824
* @tastelikefeet made their first contribution in https://github.com/vllm-project/vllm/pull/8443
* @nFunctor made their first contribution in https://github.com/vllm-project/vllm/pull/8928
* @zhuzilin made their first contribution in https://github.com/vllm-project/vllm/pull/8896
* @juncheoll made their first contribution in https://github.com/vllm-project/vllm/pull/8944
* @vlsav made their first contribution in https://github.com/vllm-project/vllm/pull/8997
* @sshlyapn made their first contribution in https://github.com/vllm-project/vllm/pull/8192
* @gcalmettes made their first contribution in https://github.com/vllm-project/vllm/pull/9020
* @xendo made their first contribution in https://github.com/vllm-project/vllm/pull/8959
* @domenVres made their first contribution in https://github.com/vllm-project/vllm/pull/9042
* @sydnash made their first contribution in https://github.com/vllm-project/vllm/pull/8405
* @varad-ahirwadkar made their first contribution in https://github.com/vllm-project/vllm/pull/9039
* @flaviabeo made their first contribution in https://github.com/vllm-project/vllm/pull/8999
* @chongmni-aws made their first contribution in https://github.com/vllm-project/vllm/pull/8746
* @hhzhang16 made their first contribution in https://github.com/vllm-project/vllm/pull/8979
* @xyang16 made their first contribution in https://github.com/vllm-project/vllm/pull/9004
* @LunrEclipse made their first contribution in https://github.com/vllm-project/vllm/pull/9087
* @sayakpaul made their first contribution in https://github.com/vllm-project/vllm/pull/9155
* @joerowell made their first contribution in https://github.com/vllm-project/vllm/pull/9159
* @AlpinDale made their first contribution in https://github.com/vllm-project/vllm/pull/8831
* @ycool made their first contribution in https://github.com/vllm-project/vllm/pull/9170
* @fahadh4ilyas made their first contribution in https://github.com/vllm-project/vllm/pull/9179
* @EwoutH made their first contribution in https://github.com/vllm-project/vllm/pull/1217
* @jyono made their first contribution in https://github.com/vllm-project/vllm/pull/9252
* @dependabot made their first contribution in https://github.com/vllm-project/vllm/pull/9197
* @bringlein made their first contribution in https://github.com/vllm-project/vllm/pull/9245
* @sixsixcoder made their first contribution in https://github.com/vllm-project/vllm/pull/9242
* @homeffjy made their first contribution in https://github.com/vllm-project/vllm/pull/9287
* @allenwang28 made their first contribution in https://github.com/vllm-project/vllm/pull/9202
* @cermeng made their first contribution in https://github.com/vllm-project/vllm/pull/9309
* @mrsalehi made their first contribution in https://github.com/vllm-project/vllm/pull/9016

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.2...v0.6.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.3)

---

## v0.6.2: v0.6.2
**Published:** 2024-09-25

## Highlights

### Model Support
* Support Llama 3.2 models (#8811, #8822)
	```
	vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --enforce-eager --max-num-seqs 16
	```
* Beam search have been soft deprecated. We are moving towards a version of beam search that's more performant and also simplifying vLLM's core. (#8684, #8763, #8713)
  * âš ï¸ You will see the following error now, this is breaking change!

    > Using beam search as a sampling parameter is deprecated, and will be removed in the future release. Please use the `vllm.LLM.use_beam_search` method for dedicated beam search instead, or set the environment variable `VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1` to suppress this error. For more details, see https://github.com/vllm-project/vllm/issues/8306

* Support for Solar Model (#8386), minicpm3 (#8297), LLaVA-Onevision model support (#8486)
* Enhancements: pp for qwen2-vl (#8696), multiple images for qwen-vl (#8247), mistral function calling (#8515), bitsandbytes support for Gemma2 (#8338), tensor parallelism with bitsandbytes quantization (#8434)

### Hardware Support
* TPU: implement multi-step scheduling (#8489), use Ray for default distributed backend (#8389)
* CPU: Enable mrope and support Qwen2-VL on CPU backend (#8770)
* AMD: custom paged attention kernel for rocm (#8310), and fp8 kv cache support (#8577)

### Production Engine
* Initial support for priority sheduling (#5958)
* Support Lora lineage and base model metadata management (#6315)
* Batch inference for llm.chat() API  (#8648)

### Performance
* Introduce `MQLLMEngine` for API Server, boost throughput 30% in single step and 7% in multistep (#8157, #8761, #8584)
* Multi-step scheduling enhancements
	* Prompt logprobs support in Multi-step (#8199)
	* Add output streaming support to multi-step + async (#8335)
	* Add flashinfer backend (#7928)
* Add cuda graph support during decoding for encoder-decoder models (#7631)

### Others
* Support sample from HF datasets and image input for benchmark_serving (#8495)
* Progress in torch.compile integration (#8488, #8480, #8384, #8526, #8445)


## What's Changed
* [MISC] Dump model runner inputs when crashing by @comaniac in https://github.com/vllm-project/vllm/pull/8305
* [misc] remove engine_use_ray by @youkaichao in https://github.com/vllm-project/vllm/pull/8126
* [TPU] Use Ray for default distributed backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8389
* Fix the AMD weight loading tests by @mgoin in https://github.com/vllm-project/vllm/pull/8390
* [Bugfix]: Fix the logic for deciding if tool parsing is used by @tomeras91 in https://github.com/vllm-project/vllm/pull/8366
* [Gemma2] add bitsandbytes support for Gemma2 by @blueyo0 in https://github.com/vllm-project/vllm/pull/8338
* [Misc] Raise error when using encoder/decoder model with cpu backend by @kevin314 in https://github.com/vllm-project/vllm/pull/8355
* [Misc] Use RoPE cache for MRoPE by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8396
* [torch.compile] hide slicing under custom op for inductor by @youkaichao in https://github.com/vllm-project/vllm/pull/8384
* [Hotfix][VLM] Fixing max position embeddings for Pixtral by @ywang96 in https://github.com/vllm-project/vllm/pull/8399
* [Bugfix] Fix InternVL2 inference with various num_patches by @Isotr0py in https://github.com/vllm-project/vllm/pull/8375
* [Model] Support multiple images for qwen-vl by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8247
* [BugFix] lazy init _copy_stream to avoid torch init wrong gpu instance by @lnykww in https://github.com/vllm-project/vllm/pull/8403
* [BugFix] Fix Duplicate Assignment of Class Variable in Hermes2ProToolParser by @vegaluisjose in https://github.com/vllm-project/vllm/pull/8423
* [Bugfix] Offline mode fix by @joerunde in https://github.com/vllm-project/vllm/pull/8376
* [multi-step] add flashinfer backend by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7928
* [Core] Add engine option to return only deltas or final output by @njhill in https://github.com/vllm-project/vllm/pull/7381
* [Bugfix] multi-step + flashinfer: ensure cuda graph compatible  by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8427
* [Hotfix][Core][VLM] Disable chunked prefill by default and prefix caching for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/8425
* [CI/Build] Disable multi-node test for InternVL2 by @ywang96 in https://github.com/vllm-project/vllm/pull/8428
* [Hotfix][Pixtral] Fix multiple images bugs by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8415
* [Bugfix] Fix weight loading issue by rename variable. by @wenxcs in https://github.com/vllm-project/vllm/pull/8293
* [Misc] Update Pixtral example by @ywang96 in https://github.com/vllm-project/vllm/pull/8431
* [BugFix] fix group_topk by @dsikka in https://github.com/vllm-project/vllm/pull/8430
* [Core] Factor out input preprocessing to a separate class by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7329
* [Bugfix] Mapping physical device indices for e2e test utils by @ShangmingCai in https://github.com/vllm-project/vllm/pull/8290
* [Bugfix] Bump fastapi and pydantic version by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8435
* [CI/Build] Update pixtral tests to use JSON by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8436
* [Bugfix] Fix async log stats by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8417
* [bugfix] torch profiler bug for single gpu with GPUExecutor by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/8354
* bump version to v0.6.1.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/8440
* [CI/Build] Enable InternVL2 PP test only on single node by @Isotr0py in https://github.com/vllm-project/vllm/pull/8437
* [doc] recommend pip instead of conda by @youkaichao in https://github.com/vllm-project/vllm/pull/8446
* [Misc] Skip loading extra bias for Qwen2-VL GPTQ-Int8 by @jeejeelee in https://github.com/vllm-project/vllm/pull/8442
* [misc][ci] fix quant test by @youkaichao in https://github.com/vllm-project/vllm/pull/8449
* [Installation] Gate FastAPI version for Python 3.8 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8456
* [plugin][torch.compile] allow to add custom compile backend by @youkaichao in https://github.com/vllm-project/vllm/pull/8445
* [CI/Build] Reorganize models tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7820
* [Doc] Add oneDNN installation to CPU backend documentation by @Isotr0py in https://github.com/vllm-project/vllm/pull/8467
* [HotFix] Fix final output truncation with stop string + streaming by @njhill in https://github.com/vllm-project/vllm/pull/8468
* bump version to v0.6.1.post2 by @simon-mo in https://github.com/vllm-project/vllm/pull/8473
* [Hardware][intel GPU] bump up ipex version to 2.3 by @jikunshang in https://github.com/vllm-project/vllm/pull/8365
* [Kernel][Hardware][Amd]Custom paged attention kernel for rocm by @charlifu in https://github.com/vllm-project/vllm/pull/8310
* [Model] support minicpm3 by @SUDA-HLT-ywfang in https://github.com/vllm-project/vllm/pull/8297
* [torch.compile] fix functionalization by @youkaichao in https://github.com/vllm-project/vllm/pull/8480
* [torch.compile] add a flag to disable custom op by @youkaichao in https://github.com/vllm-project/vllm/pull/8488
* [TPU] Implement multi-step scheduling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8489
* [Bugfix][Model] Fix Python 3.8 compatibility in Pixtral model by updating type annotations by @chrisociepa in https://github.com/vllm-project/vllm/pull/8490
* [Bugfix][Kernel] Add `IQ1_M` quantization implementation to GGUF kernel by @Isotr0py in https://github.com/vllm-project/vllm/pull/8357
* [Kernel] Enable 8-bit weights in Fused Marlin MoE by @ElizaWszola in https://github.com/vllm-project/vllm/pull/8032
* [Frontend] Expose revision arg in OpenAI server by @lewtun in https://github.com/vllm-project/vllm/pull/8501
* [BugFix] Fix clean shutdown issues by @njhill in https://github.com/vllm-project/vllm/pull/8492
* [Bugfix][Kernel] Fix build for sm_60 in GGUF kernel by @sasha0552 in https://github.com/vllm-project/vllm/pull/8506
* [Kernel] AQ AZP 3/4: Asymmetric quantization kernels by @ProExpertProg in https://github.com/vllm-project/vllm/pull/7270
* [doc] update doc on testing and debugging by @youkaichao in https://github.com/vllm-project/vllm/pull/8514
* [Bugfix] Bind api server port before starting engine by @kevin314 in https://github.com/vllm-project/vllm/pull/8491
* [perf bench] set timeout to debug hanging by @simon-mo in https://github.com/vllm-project/vllm/pull/8516
* [misc] small qol fixes for release process by @simon-mo in https://github.com/vllm-project/vllm/pull/8517
* [Bugfix] Fix 3.12 builds on main by @joerunde in https://github.com/vllm-project/vllm/pull/8510
* [refactor] remove triton based sampler by @simon-mo in https://github.com/vllm-project/vllm/pull/8524
* [Frontend] Improve Nullable kv Arg Parsing by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8525
* [Misc][Bugfix] Disable guided decoding for mistral tokenizer by @ywang96 in https://github.com/vllm-project/vllm/pull/8521
* [torch.compile] register allreduce operations as custom ops by @youkaichao in https://github.com/vllm-project/vllm/pull/8526
* [Misc] Limit to ray[adag] 2.35 to avoid backward incompatible change by @ruisearch42 in https://github.com/vllm-project/vllm/pull/8509
* [Benchmark] Support sample from HF datasets and image input for benchmark_serving by @Isotr0py in https://github.com/vllm-project/vllm/pull/8495
* [Encoder decoder] Add cuda graph support during decoding for encoder-decoder models by @sroy745 in https://github.com/vllm-project/vllm/pull/7631
* [Feature][kernel] tensor parallelism with bitsandbytes quantization by @chenqianfzh in https://github.com/vllm-project/vllm/pull/8434
* [Model] Add mistral function calling format to all models loaded with "mistral" format by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8515
* [Misc] Don't dump contents of kvcache tensors on errors by @njhill in https://github.com/vllm-project/vllm/pull/8527
* [Bugfix] Fix TP > 1 for new granite by @joerunde in https://github.com/vllm-project/vllm/pull/8544
* [doc] improve installation doc by @youkaichao in https://github.com/vllm-project/vllm/pull/8550
* [CI/Build] Excluding kernels/test_gguf.py from ROCm by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8520
* [Kernel] Change interface to Mamba causal_conv1d_update for continuous batching by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8012
* [CI/Build] fix Dockerfile.cpu on podman by @dtrifiro in https://github.com/vllm-project/vllm/pull/8540
* [Misc] Add argument to disable FastAPI docs by @Jeffwan in https://github.com/vllm-project/vllm/pull/8554
* [CI/Build] Avoid CUDA initialization by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8534
* [CI/Build] Update Ruff version by @aarnphm in https://github.com/vllm-project/vllm/pull/8469
* [Core][Bugfix][Perf] Introduce `MQLLMEngine` to avoid `asyncio` OH by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8157
* [Core] *Prompt* logprobs support in Multi-step by @afeldman-nm in https://github.com/vllm-project/vllm/pull/8199
* [Core] zmq: bind only to 127.0.0.1 for local-only usage by @russellb in https://github.com/vllm-project/vllm/pull/8543
* [Model] Support Solar Model by @shing100 in https://github.com/vllm-project/vllm/pull/8386
* [AMD][ROCm]Quantization methods on ROCm; Fix _scaled_mm call by @gshtras in https://github.com/vllm-project/vllm/pull/8380
* [Kernel] Change interface to Mamba selective_state_update for continuous batching by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8039
* [BugFix] Nonzero exit code if MQLLMEngine startup fails by @njhill in https://github.com/vllm-project/vllm/pull/8572
* [Bugfix] add `dead_error` property to engine client by @joerunde in https://github.com/vllm-project/vllm/pull/8574
* [Kernel] Remove marlin moe templating on thread_m_blocks by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8573
* [Bugfix] [Encoder-Decoder] Bugfix for encoder specific metadata construction during decode of encoder-decoder models.  by @sroy745 in https://github.com/vllm-project/vllm/pull/8545
* Revert "[Misc][Bugfix] Disable guided decoding for mistral tokenizer" by @ywang96 in https://github.com/vllm-project/vllm/pull/8593
* [Bugfix] fixing sonnet benchmark bug in benchmark_serving.py by @KuntaiDu in https://github.com/vllm-project/vllm/pull/8616
* [MISC] remove engine_use_ray in benchmark_throughput.py by @jikunshang in https://github.com/vllm-project/vllm/pull/8615
* [Frontend] Use MQLLMEngine for embeddings models too by @njhill in https://github.com/vllm-project/vllm/pull/8584
* [Kernel][Amd] Add fp8 kv cache support for rocm custom paged attention by @charlifu in https://github.com/vllm-project/vllm/pull/8577
* [Core] simplify logits resort in _apply_top_k_top_p by @hidva in https://github.com/vllm-project/vllm/pull/8619
* [Doc] Add documentation for GGUF quantization by @Isotr0py in https://github.com/vllm-project/vllm/pull/8618
* Create SECURITY.md by @simon-mo in https://github.com/vllm-project/vllm/pull/8642
* [CI/Build] Re-enabling Entrypoints tests on ROCm, excluding ones that fail by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8551
* [Misc] guard against change in cuda library name by @bnellnm in https://github.com/vllm-project/vllm/pull/8609
* [Bugfix] Fix Phi3.5 mini and MoE LoRA inference by @garg-amit in https://github.com/vllm-project/vllm/pull/8571
* [bugfix] [AMD] add multi-step advance_step to ROCmFlashAttentionMetadata by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/8474
* [Core] Support Lora lineage and base model metadata management by @Jeffwan in https://github.com/vllm-project/vllm/pull/6315
* [Model] Add OLMoE by @Muennighoff in https://github.com/vllm-project/vllm/pull/7922
* [CI/Build] Removing entrypoints/openai/test_embedding.py test from ROCm build by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8670
* [Bugfix] Validate SamplingParam n is an int by @saumya-saran in https://github.com/vllm-project/vllm/pull/8548
* [Misc] Show AMD GPU topology in `collect_env.py` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8649
* [Bugfix] Config.__init__() got an unexpected keyword argument 'engine' api_server args by @Juelianqvq in https://github.com/vllm-project/vllm/pull/8556
* [Bugfix][Core] Fix tekken edge case for mistral tokenizer by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8640
* [Doc] neuron documentation update by @omrishiv in https://github.com/vllm-project/vllm/pull/8671
* [Hardware][AWS] update neuron to 2.20 by @omrishiv in https://github.com/vllm-project/vllm/pull/8676
* [Bugfix] Fix incorrect llava next feature size calculation by @zyddnys in https://github.com/vllm-project/vllm/pull/8496
* [Core] Rename `PromptInputs` to `PromptType`, and `inputs` to `prompt` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8673
* [MISC] add support custom_op check by @jikunshang in https://github.com/vllm-project/vllm/pull/8557
* [Core] Factor out common code in `SequenceData` and `Sequence` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8675
* [beam search] add output for manually checking the correctness by @youkaichao in https://github.com/vllm-project/vllm/pull/8684
* [Kernel] Build flash-attn from source by @ProExpertProg in https://github.com/vllm-project/vllm/pull/8245
* [VLM] Use `SequenceData.from_token_counts` to create dummy data by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8687
* [Doc] Fix typo in AMD installation guide by @Imss27 in https://github.com/vllm-project/vllm/pull/8689
* [Kernel][Triton][AMD] Remove tl.atomic_add from awq_gemm_kernel, 2-5x speedup MI300, minor improvement for MI250 by @rasmith in https://github.com/vllm-project/vllm/pull/8646
* [dbrx] refactor dbrx experts to extend FusedMoe class by @divakar-amd in https://github.com/vllm-project/vllm/pull/8518
* [Kernel][Bugfix] Delete some more useless code in marlin_moe_ops.cu by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8643
* [Bugfix] Refactor composite weight loading logic by @Isotr0py in https://github.com/vllm-project/vllm/pull/8656
* [ci][build] fix vllm-flash-attn by @youkaichao in https://github.com/vllm-project/vllm/pull/8699
* [Model] Refactor BLIP/BLIP-2 to support composite model loading by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8407
* [Misc] Use NamedTuple in Multi-image example by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8705
* [MISC] rename CudaMemoryProfiler to DeviceMemoryProfiler by @statelesshz in https://github.com/vllm-project/vllm/pull/8703
* [Model][VLM] Add LLaVA-Onevision model support by @litianjian in https://github.com/vllm-project/vllm/pull/8486
* [SpecDec][Misc] Cleanup, remove bonus token logic. by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/8701
* [build] enable existing pytorch (for GH200, aarch64, nightly) by @youkaichao in https://github.com/vllm-project/vllm/pull/8713
* [misc] upgrade mistral-common by @youkaichao in https://github.com/vllm-project/vllm/pull/8715
* [Bugfix] Avoid some bogus messages RE CUTLASS's revision when building by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8702
* [Bugfix] Fix CPU CMake build by @ProExpertProg in https://github.com/vllm-project/vllm/pull/8723
* [Bugfix] fix docker build for xpu by @yma11 in https://github.com/vllm-project/vllm/pull/8652
* [Core][Frontend] Support Passing Multimodal Processor Kwargs by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8657
* [Hardware][CPU] Refactor CPU model runner by @Isotr0py in https://github.com/vllm-project/vllm/pull/8729
* [Bugfix][CPU] fix missing input intermediate_tensors in the cpu_model_runner by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/8733
* [Model] Support pp for qwen2-vl by @liuyanyi in https://github.com/vllm-project/vllm/pull/8696
* [VLM] Fix paligemma, fuyu and persimmon with transformers 4.45 : use config.text_config.vocab_size by @janimo in https://github.com/vllm-project/vllm/pull/8707
* [CI/Build] use setuptools-scm to set __version__ by @dtrifiro in https://github.com/vllm-project/vllm/pull/4738
* [Kernel] (2/N) Machete - Integrate into CompressedTensorsWNA16 and GPTQMarlin by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7701
* [Kernel][LoRA]  Add assertion for punica sgmv kernels by @jeejeelee in https://github.com/vllm-project/vllm/pull/7585
* [Core] Allow IPv6 in VLLM_HOST_IP with zmq by @russellb in https://github.com/vllm-project/vllm/pull/8575
* Fix typical acceptance sampler with correct recovered token ids by @jiqing-feng in https://github.com/vllm-project/vllm/pull/8562
* Add output streaming support to multi-step + async while ensuring RequestOutput obj reuse by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8335
* [Hardware][AMD] ROCm6.2 upgrade by @hongxiayang in https://github.com/vllm-project/vllm/pull/8674
* Fix tests in test_scheduler.py that fail with BlockManager V2 by @sroy745 in https://github.com/vllm-project/vllm/pull/8728
* re-implement beam search on top of vllm core by @youkaichao in https://github.com/vllm-project/vllm/pull/8726
* Revert "[Core] Rename `PromptInputs` to `PromptType`, and `inputs` to `prompt`" by @simon-mo in https://github.com/vllm-project/vllm/pull/8750
* [MISC] Skip dumping inputs when unpicklable by @comaniac in https://github.com/vllm-project/vllm/pull/8744
* [Core][Model] Support loading weights by ID within models by @petersalas in https://github.com/vllm-project/vllm/pull/7931
* [Model] Expose Phi3v num_crops as a mm_processor_kwarg by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8658
* [Bugfix] Fix potentially unsafe custom allreduce synchronization by @hanzhi713 in https://github.com/vllm-project/vllm/pull/8558
* [Kernel] Split Marlin MoE kernels into multiple files by @ElizaWszola in https://github.com/vllm-project/vllm/pull/8661
* [Frontend] Batch inference for llm.chat() API  by @aandyw in https://github.com/vllm-project/vllm/pull/8648
* [Bugfix] Fix torch dynamo fixes caused by `replace_parameters` by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/8748
* [CI/Build] fix setuptools-scm usage by @dtrifiro in https://github.com/vllm-project/vllm/pull/8771
* [misc] soft drop beam search by @youkaichao in https://github.com/vllm-project/vllm/pull/8763
* [[Misc]Upgrade bitsandbytes to the latest version 0.44.0 by @jeejeelee in https://github.com/vllm-project/vllm/pull/8768
* [Core][Bugfix] Support prompt_logprobs returned with speculative decoding by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/8047
* [Core] Adding Priority Scheduling by @apatke in https://github.com/vllm-project/vllm/pull/5958
* [Bugfix] Use heartbeats instead of health checks by @joerunde in https://github.com/vllm-project/vllm/pull/8583
* Fix test_schedule_swapped_simple in test_scheduler.py by @sroy745 in https://github.com/vllm-project/vllm/pull/8780
* [Bugfix][Kernel] Implement acquire/release polyfill for Pascal by @sasha0552 in https://github.com/vllm-project/vllm/pull/8776
* Fix tests in test_chunked_prefill_scheduler which fail with BlockManager V2 by @sroy745 in https://github.com/vllm-project/vllm/pull/8752
* [BugFix] Propagate 'trust_remote_code' setting in internvl and minicpmv by @zifeitong in https://github.com/vllm-project/vllm/pull/8250
* [Hardware][CPU] Enable mrope and support Qwen2-VL on CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/8770
* [Bugfix] load fc bias from config for eagle by @sohamparikh in https://github.com/vllm-project/vllm/pull/8790
* [Frontend] OpenAI server: propagate usage accounting to FastAPI middleware layer by @agt in https://github.com/vllm-project/vllm/pull/8672
* [Bugfix] Ray 2.9.x doesn't expose available_resources_per_node by @darthhexx in https://github.com/vllm-project/vllm/pull/8767
* [Misc] Fix minor typo in scheduler by @wooyeonlee0 in https://github.com/vllm-project/vllm/pull/8765
* [CI/Build][Bugfix][Doc][ROCm] CI fix and doc update after ROCm 6.2 upgrade by @hongxiayang in https://github.com/vllm-project/vllm/pull/8777
* [Kernel] Fullgraph and opcheck tests by @bnellnm in https://github.com/vllm-project/vllm/pull/8479
* [[Misc]] Add extra deps for openai server image by @jeejeelee in https://github.com/vllm-project/vllm/pull/8792
* [VLM][Bugfix] enable internvl running with num_scheduler_steps > 1 by @DefTruth in https://github.com/vllm-project/vllm/pull/8614
* [Core] Rename `PromptInputs` and `inputs`, with backwards compatibility by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8760
* [Frontend] MQLLMEngine supports profiling. by @abatom in https://github.com/vllm-project/vllm/pull/8761
* [Misc] Support FP8 MoE for compressed-tensors by @mgoin in https://github.com/vllm-project/vllm/pull/8588
* Revert "rename PromptInputs and inputs with backward compatibility (#8760) by @simon-mo in https://github.com/vllm-project/vllm/pull/8810
* [Model] Add support for the multi-modal Llama 3.2 model by @heheda12345 in https://github.com/vllm-project/vllm/pull/8811
* [Doc] Update doc for Transformers 4.45 by @ywang96 in https://github.com/vllm-project/vllm/pull/8817
* [Misc] Support quantization of MllamaForCausalLM by @mgoin in https://github.com/vllm-project/vllm/pull/8822

## New Contributors
* @blueyo0 made their first contribution in https://github.com/vllm-project/vllm/pull/8338
* @lnykww made their first contribution in https://github.com/vllm-project/vllm/pull/8403
* @vegaluisjose made their first contribution in https://github.com/vllm-project/vllm/pull/8423
* @chrisociepa made their first contribution in https://github.com/vllm-project/vllm/pull/8490
* @lewtun made their first contribution in https://github.com/vllm-project/vllm/pull/8501
* @russellb made their first contribution in https://github.com/vllm-project/vllm/pull/8543
* @shing100 made their first contribution in https://github.com/vllm-project/vllm/pull/8386
* @hidva made their first contribution in https://github.com/vllm-project/vllm/pull/8619
* @Muennighoff made their first contribution in https://github.com/vllm-project/vllm/pull/7922
* @saumya-saran made their first contribution in https://github.com/vllm-project/vllm/pull/8548
* @zyddnys made their first contribution in https://github.com/vllm-project/vllm/pull/8496
* @Imss27 made their first contribution in https://github.com/vllm-project/vllm/pull/8689
* @statelesshz made their first contribution in https://github.com/vllm-project/vllm/pull/8703
* @litianjian made their first contribution in https://github.com/vllm-project/vllm/pull/8486
* @yma11 made their first contribution in https://github.com/vllm-project/vllm/pull/8652
* @liuyanyi made their first contribution in https://github.com/vllm-project/vllm/pull/8696
* @janimo made their first contribution in https://github.com/vllm-project/vllm/pull/8707
* @jiqing-feng made their first contribution in https://github.com/vllm-project/vllm/pull/8562
* @aandyw made their first contribution in https://github.com/vllm-project/vllm/pull/8648
* @apatke made their first contribution in https://github.com/vllm-project/vllm/pull/5958
* @sohamparikh made their first contribution in https://github.com/vllm-project/vllm/pull/8790
* @darthhexx made their first contribution in https://github.com/vllm-project/vllm/pull/8767
* @abatom made their first contribution in https://github.com/vllm-project/vllm/pull/8761
* @heheda12345 made their first contribution in https://github.com/vllm-project/vllm/pull/8811

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.1...v0.6.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.2)

---

## v0.6.1.post2: v0.6.1.post2
**Published:** 2024-09-13

## Highlights
* This release contains an important bugfix related to token streaming combined with stop string (#8468) 

## What's Changed
* [CI/Build] Enable InternVL2 PP test only on single node by @Isotr0py in https://github.com/vllm-project/vllm/pull/8437
* [doc] recommend pip instead of conda by @youkaichao in https://github.com/vllm-project/vllm/pull/8446
* [Misc] Skip loading extra bias for Qwen2-VL GPTQ-Int8 by @jeejeelee in https://github.com/vllm-project/vllm/pull/8442
* [misc][ci] fix quant test by @youkaichao in https://github.com/vllm-project/vllm/pull/8449
* [Installation] Gate FastAPI version for Python 3.8 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8456
* [plugin][torch.compile] allow to add custom compile backend by @youkaichao in https://github.com/vllm-project/vllm/pull/8445
* [CI/Build] Reorganize models tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7820
* [Doc] Add oneDNN installation to CPU backend documentation by @Isotr0py in https://github.com/vllm-project/vllm/pull/8467
* [HotFix] Fix final output truncation with stop string + streaming by @njhill in https://github.com/vllm-project/vllm/pull/8468
* bump version to v0.6.1.post2 by @simon-mo in https://github.com/vllm-project/vllm/pull/8473

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.1.post1...v0.6.1.post2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.1.post2)

---

## v0.6.1.post1: v0.6.1.post1
**Published:** 2024-09-13

## Highlights
This release features important bug fixes and enhancements for 
- Pixtral models. (#8415, #8425, #8399, #8431)
	- Chunked scheduling has been turned off for vision models. Please replace `--max_num_batched_tokens 16384` with `--max-model-len 16384`
- Multistep scheduling. (#8417, #7928, #8427)
- Tool use. (#8423, #8366)

Also
* support multiple images for qwen-vl (#8247)
* removes `engine_use_ray` (#8126)
* add engine option to return only deltas or final output (#7381)
* add bitsandbytes support for Gemma2 (#8338)


## What's Changed
* [MISC] Dump model runner inputs when crashing by @comaniac in https://github.com/vllm-project/vllm/pull/8305
* [misc] remove engine_use_ray by @youkaichao in https://github.com/vllm-project/vllm/pull/8126
* [TPU] Use Ray for default distributed backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8389
* Fix the AMD weight loading tests by @mgoin in https://github.com/vllm-project/vllm/pull/8390
* [Bugfix]: Fix the logic for deciding if tool parsing is used by @tomeras91 in https://github.com/vllm-project/vllm/pull/8366
* [Gemma2] add bitsandbytes support for Gemma2 by @blueyo0 in https://github.com/vllm-project/vllm/pull/8338
* [Misc] Raise error when using encoder/decoder model with cpu backend by @kevin314 in https://github.com/vllm-project/vllm/pull/8355
* [Misc] Use RoPE cache for MRoPE by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8396
* [torch.compile] hide slicing under custom op for inductor by @youkaichao in https://github.com/vllm-project/vllm/pull/8384
* [Hotfix][VLM] Fixing max position embeddings for Pixtral by @ywang96 in https://github.com/vllm-project/vllm/pull/8399
* [Bugfix] Fix InternVL2 inference with various num_patches by @Isotr0py in https://github.com/vllm-project/vllm/pull/8375
* [Model] Support multiple images for qwen-vl by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8247
* [BugFix] lazy init _copy_stream to avoid torch init wrong gpu instance by @lnykww in https://github.com/vllm-project/vllm/pull/8403
* [BugFix] Fix Duplicate Assignment of Class Variable in Hermes2ProToolParser by @vegaluisjose in https://github.com/vllm-project/vllm/pull/8423
* [Bugfix] Offline mode fix by @joerunde in https://github.com/vllm-project/vllm/pull/8376
* [multi-step] add flashinfer backend by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7928
* [Core] Add engine option to return only deltas or final output by @njhill in https://github.com/vllm-project/vllm/pull/7381
* [Bugfix] multi-step + flashinfer: ensure cuda graph compatible  by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8427
* [Hotfix][Core][VLM] Disable chunked prefill by default and prefix caching for multimodal models by @ywang96 in https://github.com/vllm-project/vllm/pull/8425
* [CI/Build] Disable multi-node test for InternVL2 by @ywang96 in https://github.com/vllm-project/vllm/pull/8428
* [Hotfix][Pixtral] Fix multiple images bugs by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8415
* [Bugfix] Fix weight loading issue by rename variable. by @wenxcs in https://github.com/vllm-project/vllm/pull/8293
* [Misc] Update Pixtral example by @ywang96 in https://github.com/vllm-project/vllm/pull/8431
* [BugFix] fix group_topk by @dsikka in https://github.com/vllm-project/vllm/pull/8430
* [Core] Factor out input preprocessing to a separate class by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7329
* [Bugfix] Mapping physical device indices for e2e test utils by @ShangmingCai in https://github.com/vllm-project/vllm/pull/8290
* [Bugfix] Bump fastapi and pydantic version by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8435
* [CI/Build] Update pixtral tests to use JSON by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8436
* [Bugfix] Fix async log stats by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8417
* [bugfix] torch profiler bug for single gpu with GPUExecutor by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/8354
* bump version to v0.6.1.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/8440

## New Contributors
* @blueyo0 made their first contribution in https://github.com/vllm-project/vllm/pull/8338
* @lnykww made their first contribution in https://github.com/vllm-project/vllm/pull/8403
* @vegaluisjose made their first contribution in https://github.com/vllm-project/vllm/pull/8423

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.1...v0.6.1.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.1.post1)

---

## v0.6.1: v0.6.1
**Published:** 2024-09-11

## Highlights

### Model Support
* Added support for Pixtral (`mistralai/Pixtral-12B-2409`). (#8377, #8168)
* Added support for Llava-Next-Video (#7559), Qwen-VL (#8029), Qwen2-VL (#7905)
* Multi-input support for LLaVA (#8238), InternVL2 models (#8201)

### Performance Enhancements
* Memory optimization for awq_gemm and awq_dequantize, 2x throughput (#8248)

### Production Engine
* Support load and unload LoRA in api server (#6566)
* Add progress reporting to batch runner (#8060)
* Add support for NVIDIA ModelOpt static scaling checkpoints. (#6112)

### Others
* Update the docker image to use Python 3.12 for small performance bump. (#8133)
* Added CODE_OF_CONDUCT.md (#8161)



## What's Changed
* [Doc] [Misc] Create CODE_OF_CONDUCT.md by @mmcelaney in https://github.com/vllm-project/vllm/pull/8161
* [bugfix] Upgrade minimum OpenAI version by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/8169
* [Misc] Clean up RoPE forward_native by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8076
* [ci] Mark LoRA test as soft-fail by @khluu in https://github.com/vllm-project/vllm/pull/8160
* [Core/Bugfix] Add query dtype as per FlashInfer API requirements. by @elfiegg in https://github.com/vllm-project/vllm/pull/8173
* [Doc] Add multi-image input example and update supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8181
* Inclusion of InternVLChatModel In PP_SUPPORTED_MODELS(Pipeline Parallelism) by @Manikandan-Thangaraj-ZS0321 in https://github.com/vllm-project/vllm/pull/7860
* [MODEL] Qwen Multimodal Support (Qwen-VL / Qwen-VL-Chat) by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/8029
* Move verify_marlin_supported to GPTQMarlinLinearMethod by @mgoin in https://github.com/vllm-project/vllm/pull/8165
* [Documentation][Spec Decode] Add documentation about lossless guarantees in Speculative Decoding in vLLM by @sroy745 in https://github.com/vllm-project/vllm/pull/7962
* [Core] Support load and unload LoRA in api server by @Jeffwan in https://github.com/vllm-project/vllm/pull/6566
* [BugFix] Fix Granite model configuration by @njhill in https://github.com/vllm-project/vllm/pull/8216
* [Frontend] Add --logprobs argument to `benchmark_serving.py` by @afeldman-nm in https://github.com/vllm-project/vllm/pull/8191
* [Misc] Use ray[adag] dependency instead of cuda by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7938
* [CI/Build] Increasing timeout for multiproc worker tests by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8203
* [Kernel] [Triton] Memory optimization for awq_gemm and awq_dequantize, 2x throughput by @rasmith in https://github.com/vllm-project/vllm/pull/8248
* [Misc] Remove `SqueezeLLM` by @dsikka in https://github.com/vllm-project/vllm/pull/8220
* [Model] Allow loading from original Mistral format by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8168
* [misc] [doc] [frontend] LLM torch profiler support by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7943
* [Bugfix] Fix Hermes tool call chat template bug by @K-Mistele in https://github.com/vllm-project/vllm/pull/8256
* [Model] Multi-input support for LLaVA and fix embedding inputs for multi-image models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8238
* Enable Random Prefix Caching in Serving Profiling Tool (benchmark_serving.py) by @wschin in https://github.com/vllm-project/vllm/pull/8241
* [tpu][misc] fix typo by @youkaichao in https://github.com/vllm-project/vllm/pull/8260
* [Bugfix] Fix broken OpenAI tensorizer test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8258
* [Model][VLM] Support multi-images inputs for InternVL2 models by @Isotr0py in https://github.com/vllm-project/vllm/pull/8201
* [Model][VLM] Decouple weight loading logic for `Paligemma` by @Isotr0py in https://github.com/vllm-project/vllm/pull/8269
* ppc64le: Dockerfile fixed, and a script for buildkite by @sumitd2 in https://github.com/vllm-project/vllm/pull/8026
* [CI/Build] Use python 3.12 in cuda image by @joerunde in https://github.com/vllm-project/vllm/pull/8133
* [Bugfix] Fix async postprocessor in case of preemption by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8267
* [Bugfix] Streamed tool calls now more strictly follow OpenAI's format; ensures Vercel AI SDK compatibility by @K-Mistele in https://github.com/vllm-project/vllm/pull/8272
* [Frontend] Add progress reporting to run_batch.py by @alugowski in https://github.com/vllm-project/vllm/pull/8060
* [Bugfix] Correct adapter usage for cohere and jamba by @vladislavkruglikov in https://github.com/vllm-project/vllm/pull/8292
* [Misc] GPTQ Activation Ordering by @kylesayrs in https://github.com/vllm-project/vllm/pull/8135
* [Misc] Fused MoE Marlin support for GPTQ by @dsikka in https://github.com/vllm-project/vllm/pull/8217
* Add NVIDIA Meetup slides, announce AMD meetup, and add contact info by @simon-mo in https://github.com/vllm-project/vllm/pull/8319
* [Bugfix] Fix missing `post_layernorm` in CLIP by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8155
* [CI/Build] enable ccache/scccache for HIP builds by @dtrifiro in https://github.com/vllm-project/vllm/pull/8327
* [Frontend] Clean up type annotations for mistral tokenizer by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8314
* [CI/Build] Enabling kernels tests for AMD, ignoring some of then that fail by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8130
* Fix ppc64le buildkite job by @sumitd2 in https://github.com/vllm-project/vllm/pull/8309
* [Spec Decode] Move ops.advance_step to flash attn advance_step by @kevin314 in https://github.com/vllm-project/vllm/pull/8224
* [Misc] remove peft as dependency for prompt models by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/8162
* [MISC] Keep chunked prefill enabled by default with long context when prefix caching is enabled by @comaniac in https://github.com/vllm-project/vllm/pull/8342
* [Bugfix] Ensure multistep lookahead allocation is compatible with cuda graph max capture by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8340
* [Core/Bugfix] pass VLLM_ATTENTION_BACKEND to ray workers by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/8172
* [CI/Build][Kernel] Update CUTLASS to 3.5.1 tag by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/8043
* [Misc] Skip loading extra bias for Qwen2-MOE GPTQ models by @jeejeelee in https://github.com/vllm-project/vllm/pull/8329
* [Bugfix] Fix InternVL2 vision embeddings process with pipeline parallel by @Isotr0py in https://github.com/vllm-project/vllm/pull/8299
* [Hardware][NV] Add support for ModelOpt static scaling checkpoints. by @pavanimajety in https://github.com/vllm-project/vllm/pull/6112
* [model] Support for Llava-Next-Video model by @TKONIY in https://github.com/vllm-project/vllm/pull/7559
* [Frontend] Create ErrorResponse instead of raising exceptions in run_batch by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/8347
* [Model][VLM] Add Qwen2-VL model support by @fyabc in https://github.com/vllm-project/vllm/pull/7905
* [Hardware][Intel] Support compressed-tensor W8A8 for CPU backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/7257
* [CI/Build] Excluding test_moe.py from AMD Kernels tests for investigation by @alexeykondrat in https://github.com/vllm-project/vllm/pull/8373
* [Bugfix] Add missing attributes in mistral tokenizer by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8364
* [Kernel][Misc] Add meta functions for ops to prevent graph breaks by @bnellnm in https://github.com/vllm-project/vllm/pull/6917
* [Misc] Move device options to a single place by @akx in https://github.com/vllm-project/vllm/pull/8322
* [Speculative Decoding] Test refactor by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/8317
* Pixtral by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/8377
* Bump version to v0.6.1 by @simon-mo in https://github.com/vllm-project/vllm/pull/8379

## New Contributors
* @mmcelaney made their first contribution in https://github.com/vllm-project/vllm/pull/8161
* @elfiegg made their first contribution in https://github.com/vllm-project/vllm/pull/8173
* @Manikandan-Thangaraj-ZS0321 made their first contribution in https://github.com/vllm-project/vllm/pull/7860
* @sumitd2 made their first contribution in https://github.com/vllm-project/vllm/pull/8026
* @alugowski made their first contribution in https://github.com/vllm-project/vllm/pull/8060
* @vladislavkruglikov made their first contribution in https://github.com/vllm-project/vllm/pull/8292
* @kevin314 made their first contribution in https://github.com/vllm-project/vllm/pull/8224
* @TKONIY made their first contribution in https://github.com/vllm-project/vllm/pull/7559
* @akx made their first contribution in https://github.com/vllm-project/vllm/pull/8322

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.6.0...v0.6.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.1)

---

## v0.6.0: v0.6.0
**Published:** 2024-09-04

## Highlights

### Performance Update
* We are excited to announce a faster vLLM delivering 2x more throughput compared to v0.5.3. The default parameters should achieve great speed up, but we recommend also try out turning on multi step scheduling. You can do so by setting `--num-scheduler-steps 8` in the engine arguments. Please note that it still have some limitations and being actively hardened, see #7528 for known issues. 
	* Multi-step scheduler now supports LLMEngine and log_probs (#7789, #7652)
	* Asynchronous output processor overlaps the output data structures construction with GPU works, delivering 12% throughput increase. (#7049, #7911, #7921, #8050)
	* Using FlashInfer backend for FP8 KV Cache (#7798, #7985), rejection sampling in Speculative Decoding (#7244)

### Model Support
* Support bitsandbytes 8-bit and FP4 quantized models (#7445)
* New LLMs: Exaone (#7819), Granite (#7436), Phi-3.5-MoE (#7729)
* A new tokenizer mode for mistral models to use the native mistral-commons package (#7739)
* Multi-modality: 
	* multi-image input support for LLaVA-Next (#7230), Phi-3-vision models (#7783)
	* Ultravox support for multiple audio chunks (#7963)
	* TP support for ViTs (#7186)

### Hardware Support
* NVIDIA GPU: extend cuda graph size for H200 (#7894)
* AMD: Triton implementations awq_dequantize and awq_gemm to support AWQ (#7386)
* Intel GPU: pipeline parallel support (#7810)
* Neuron: context lengths and token generation buckets (#7885, #8062)
* TPU: single and multi-host TPUs on GKE (#7613), Async output processing (#8011)

### Production Features
* OpenAI-Compatible Tools API + Streaming for Hermes & Mistral models! (#5649)
* Add json_schema support from OpenAI protocol (#7654)
* Enable chunked prefill and prefix caching together (#7753, #8120)
* Multimodal support in offline chat (#8098), and multiple multi-modal items in the OpenAI frontend (#8049)

### Misc
* Support benchmarking async engine in benchmark_throughput.py (#7964)
* Progress in integration with `torch.compile`: avoid Dynamo guard evaluation overhead (#7898), skip compile for profiling (#7796)

## What's Changed
* [Core] Add multi-step support to LLMEngine by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7789
* [Bugfix] Fix run_batch logger by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/7640
* [Frontend] Publish  Prometheus metrics in run_batch API by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/7641
* [Frontend] add json_schema support from OpenAI protocol by @rockwotj in https://github.com/vllm-project/vllm/pull/7654
* [misc][core] lazy import outlines by @youkaichao in https://github.com/vllm-project/vllm/pull/7831
* [ci][test] exclude model download time in server start time by @youkaichao in https://github.com/vllm-project/vllm/pull/7834
* [ci][test] fix RemoteOpenAIServer by @youkaichao in https://github.com/vllm-project/vllm/pull/7838
* [Bugfix] Fix Phi-3v crash when input images are of certain sizes by @zifeitong in https://github.com/vllm-project/vllm/pull/7840
* [Model][VLM] Support multi-images inputs for Phi-3-vision models  by @Isotr0py in https://github.com/vllm-project/vllm/pull/7783
* [Misc] Remove snapshot_download usage in InternVL2 test by @Isotr0py in https://github.com/vllm-project/vllm/pull/7835
* [misc][cuda] improve pynvml warning by @youkaichao in https://github.com/vllm-project/vllm/pull/7852
* [Spec Decoding] Streamline batch expansion tensor manipulation by @njhill in https://github.com/vllm-project/vllm/pull/7851
* [Bugfix]: Use float32 for base64 embedding by @HollowMan6 in https://github.com/vllm-project/vllm/pull/7855
* [CI/Build] Avoid downloading all HF files in `RemoteOpenAIServer` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7836
* [Performance][BlockManagerV2] Mark prefix cache block as computed after schedule by @comaniac in https://github.com/vllm-project/vllm/pull/7822
* [Misc] Update `qqq` to use vLLMParameters by @dsikka in https://github.com/vllm-project/vllm/pull/7805
* [Misc] Update `gptq_marlin_24` to use vLLMParameters by @dsikka in https://github.com/vllm-project/vllm/pull/7762
* [misc] fix custom allreduce p2p cache file generation by @youkaichao in https://github.com/vllm-project/vllm/pull/7853
* [Bugfix] neuron: enable tensor parallelism by @omrishiv in https://github.com/vllm-project/vllm/pull/7562
* [Misc] Update compressed tensors lifecycle to remove `prefix` from `create_weights` by @dsikka in https://github.com/vllm-project/vllm/pull/7825
* [Core] Asynchronous Output Processor by @megha95 in https://github.com/vllm-project/vllm/pull/7049
* [Tests] Disable retries and use context manager for openai client by @njhill in https://github.com/vllm-project/vllm/pull/7565
* [core][torch.compile] not compile for profiling by @youkaichao in https://github.com/vllm-project/vllm/pull/7796
* Revert #7509 by @comaniac in https://github.com/vllm-project/vllm/pull/7887
* [Model] Add Mistral Tokenization to improve robustness and chat encoding by @patrickvonplaten in https://github.com/vllm-project/vllm/pull/7739
* [CI/Build][VLM] Cleanup multiple images inputs model test by @Isotr0py in https://github.com/vllm-project/vllm/pull/7897
* [Hardware][Intel GPU] Add intel GPU pipeline parallel support. by @jikunshang in https://github.com/vllm-project/vllm/pull/7810
* [CI/Build][ROCm] Enabling tensorizer tests for ROCm by @alexeykondrat in https://github.com/vllm-project/vllm/pull/7237
* [Bugfix] Fix phi3v incorrect image_idx when using async engine by @Isotr0py in https://github.com/vllm-project/vllm/pull/7916
* [cuda][misc] error on empty CUDA_VISIBLE_DEVICES by @youkaichao in https://github.com/vllm-project/vllm/pull/7924
* [Kernel] Expand MoE weight loading + Add Fused Marlin MoE Kernel by @dsikka in https://github.com/vllm-project/vllm/pull/7766
* [benchmark] Update TGI version by @philschmid in https://github.com/vllm-project/vllm/pull/7917
* [Model] Add multi-image input support for LLaVA-Next offline inference by @zifeitong in https://github.com/vllm-project/vllm/pull/7230
* [mypy] Enable mypy type checking for `vllm/core` by @jberkhahn in https://github.com/vllm-project/vllm/pull/7229
* [Core][VLM] Stack multimodal tensors to represent multiple images within each prompt by @petersalas in https://github.com/vllm-project/vllm/pull/7902
* [hardware][rocm] allow rocm to override default env var by @youkaichao in https://github.com/vllm-project/vllm/pull/7926
* [Bugfix] Allow ScalarType to be compiled with pytorch 2.3 and add checks for registering FakeScalarType and dynamo support. by @bnellnm in https://github.com/vllm-project/vllm/pull/7886
* [mypy][CI/Build] Fix mypy errors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7929
* [Core] Async_output_proc: Add virtual engine support (towards pipeline parallel) by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7911
* [Performance] Enable chunked prefill and prefix caching together by @comaniac in https://github.com/vllm-project/vllm/pull/7753
* [ci][test] fix pp test failure by @youkaichao in https://github.com/vllm-project/vllm/pull/7945
* [Doc] fix the autoAWQ example by @stas00 in https://github.com/vllm-project/vllm/pull/7937
* [Bugfix][VLM] Fix incompatibility between #7902 and #7230 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7948
* [Core][Kernels] Use FlashInfer backend for FP8 KV Cache when available. by @pavanimajety in https://github.com/vllm-project/vllm/pull/7798
* [Kernel] [Triton] [AMD] Adding Triton implementations awq_dequantize and awq_gemm to support AWQ by @rasmith in https://github.com/vllm-project/vllm/pull/7386
* [TPU] Upgrade PyTorch XLA nightly by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7967
* [Doc] fix 404 link by @stas00 in https://github.com/vllm-project/vllm/pull/7966
* [Kernel/Model] Migrate mamba_ssm and causal_conv1d kernels to vLLM by @mzusman in https://github.com/vllm-project/vllm/pull/7651
* [Bugfix] Make torch registration of punica ops optional by @bnellnm in https://github.com/vllm-project/vllm/pull/7970
* [torch.compile] avoid Dynamo guard evaluation overhead by @youkaichao in https://github.com/vllm-project/vllm/pull/7898
* Remove faulty Meta-Llama-3-8B-Instruct-FP8.yaml lm-eval test by @mgoin in https://github.com/vllm-project/vllm/pull/7961
* [Frontend] Minor optimizations to zmq decoupled front-end by @njhill in https://github.com/vllm-project/vllm/pull/7957
* [torch.compile] remove reset by @youkaichao in https://github.com/vllm-project/vllm/pull/7975
* [VLM][Core] Fix exceptions on ragged NestedTensors by @petersalas in https://github.com/vllm-project/vllm/pull/7974
* Revert "[Core][Kernels] Use FlashInfer backend for FP8 KV Cache when available." by @youkaichao in https://github.com/vllm-project/vllm/pull/7982
* [Bugfix] Unify rank computation across regular decoding and speculative decoding by @jmkuebler in https://github.com/vllm-project/vllm/pull/7899
* [Core] Combine async postprocessor and multi-step by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7921
* [Core][Kernels] Enable FP8 KV Cache with Flashinfer backend.  + BugFix for kv_cache_dtype=auto by @pavanimajety in https://github.com/vllm-project/vllm/pull/7985
* extend cuda graph size for H200 by @kushanam in https://github.com/vllm-project/vllm/pull/7894
* [Bugfix] Fix incorrect vocal embedding shards for GGUF model in tensor parallelism by @Isotr0py in https://github.com/vllm-project/vllm/pull/7954
* [misc] update tpu int8 to use new vLLM Parameters by @dsikka in https://github.com/vllm-project/vllm/pull/7973
* [Neuron] Adding support for context-lenght, token-gen buckets. by @hbikki in https://github.com/vllm-project/vllm/pull/7885
* support bitsandbytes 8-bit and FP4 quantized models by @chenqianfzh in https://github.com/vllm-project/vllm/pull/7445
* Add more percentiles and latencies by @wschin in https://github.com/vllm-project/vllm/pull/7759
* [VLM] Disallow overflowing `max_model_len` for multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7998
* [Core] Logprobs support in Multi-step by @afeldman-nm in https://github.com/vllm-project/vllm/pull/7652
* [TPU] Async output processing for TPU by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8011
* [Kernel] changing fused moe kernel chunk size default to 32k by @avshalomman in https://github.com/vllm-project/vllm/pull/7995
* [MODEL] add Exaone model support by @nayohan in https://github.com/vllm-project/vllm/pull/7819
* Support vLLM single and multi-host TPUs on GKE by @richardsliu in https://github.com/vllm-project/vllm/pull/7613
* [Bugfix] Fix import error in Exaone model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8034
* [VLM][Model] TP support for ViTs by @ChristopherCho in https://github.com/vllm-project/vllm/pull/7186
* [Core] Increase default `max_num_batched_tokens` for multimodal models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8028
* [Frontend]-config-cli-args by @KaunilD in https://github.com/vllm-project/vllm/pull/7737
* [TPU][Bugfix] Fix tpu type api by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8035
* [Model] Adding support for MSFT Phi-3.5-MoE by @wenxcs in https://github.com/vllm-project/vllm/pull/7729
* [Bugfix] Address #8009 and add model test for flashinfer fp8 kv cache. by @pavanimajety in https://github.com/vllm-project/vllm/pull/8013
* [Bugfix] Fix import error in Phi-3.5-MoE by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8052
* [Bugfix] Fix ModelScope models in v0.5.5 by @NickLucche in https://github.com/vllm-project/vllm/pull/8037
* [BugFix][Core] Multistep Fix Crash on Request Cancellation by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/8059
* [Frontend][VLM] Add support for multiple multi-modal items in the OpenAI frontend by @ywang96 in https://github.com/vllm-project/vllm/pull/8049
* [Misc] Optional installation of audio related packages by @ywang96 in https://github.com/vllm-project/vllm/pull/8063
* [Model] Adding Granite model. by @shawntan in https://github.com/vllm-project/vllm/pull/7436
* [SpecDecode][Kernel] Use Flashinfer for Rejection Sampling in Speculative Decoding by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/7244
* [TPU] Align worker index with node boundary by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7932
* [Core][Bugfix] Accept GGUF model without .gguf extension by @Isotr0py in https://github.com/vllm-project/vllm/pull/8056
* [Bugfix] Fix internlm2 tensor parallel inference by @Isotr0py in https://github.com/vllm-project/vllm/pull/8055
* [Bugfix] Fix #7592 vllm 0.5.4 enable_chunked_prefill throughput is slightly lower than 0.5.3~0.5.0. by @noooop in https://github.com/vllm-project/vllm/pull/7874
* [Bugfix] Fix single output condition in output processor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7881
* [Bugfix][VLM] Add fallback to SDPA for ViT model running on CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/8061
* [Performance] Enable chunked prefill and prefix caching together by @comaniac in https://github.com/vllm-project/vllm/pull/8120
* [CI] Only PR reviewers/committers can trigger CI on PR by @khluu in https://github.com/vllm-project/vllm/pull/8124
* [Core] Optimize Async + Multi-step by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/8050
* [Misc] Raise a more informative exception in add/remove_logger by @Yard1 in https://github.com/vllm-project/vllm/pull/7750
* [CI/Build] fix: Add the +empty tag to the version only when the VLLM_TARGET_DEVICE envvar was explicitly set to "empty" by @tomeras91 in https://github.com/vllm-project/vllm/pull/8118
* [ci] Fix GHA workflow  by @khluu in https://github.com/vllm-project/vllm/pull/8129
* [TPU][Bugfix] Fix next_token_ids shape by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8128
* [CI] Change PR remainder to avoid at-mentions by @simon-mo in https://github.com/vllm-project/vllm/pull/8134
* [Misc] Update `GPTQ` to use `vLLMParameters` by @dsikka in https://github.com/vllm-project/vllm/pull/7976
* [Benchmark] Add `--async-engine` option to benchmark_throughput.py by @njhill in https://github.com/vllm-project/vllm/pull/7964
* [TPU][Bugfix] Use XLA rank for persistent cache path by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8137
* [Misc] Update fbgemmfp8 to use `vLLMParameters` by @dsikka in https://github.com/vllm-project/vllm/pull/7972
* [Model] Add Ultravox support for multiple audio chunks by @petersalas in https://github.com/vllm-project/vllm/pull/7963
* [Frontend] Multimodal support in offline chat by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/8098
* chore: Update check-wheel-size.py to read VLLM_MAX_SIZE_MB from env by @haitwang-cloud in https://github.com/vllm-project/vllm/pull/8103
* [Bugfix] remove post_layernorm in siglip by @wnma3mz in https://github.com/vllm-project/vllm/pull/8106
* [MISC] Consolidate FP8 kv-cache tests by @comaniac in https://github.com/vllm-project/vllm/pull/8131
* [CI/Build][ROCm] Enabling LoRA tests on ROCm by @alexeykondrat in https://github.com/vllm-project/vllm/pull/7369
* [CI] Change test input in Gemma LoRA test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/8163
* [Feature] OpenAI-Compatible Tools API + Streaming for Hermes & Mistral models by @K-Mistele in https://github.com/vllm-project/vllm/pull/5649
* [MISC] Replace input token throughput with total token throughput by @comaniac in https://github.com/vllm-project/vllm/pull/8164
* [Neuron] Adding support for adding/ overriding neuron configuration aâ€¦ by @hbikki in https://github.com/vllm-project/vllm/pull/8062
* Bump version to v0.6.0 by @simon-mo in https://github.com/vllm-project/vllm/pull/8166

## New Contributors
* @rockwotj made their first contribution in https://github.com/vllm-project/vllm/pull/7654
* @HollowMan6 made their first contribution in https://github.com/vllm-project/vllm/pull/7855
* @patrickvonplaten made their first contribution in https://github.com/vllm-project/vllm/pull/7739
* @philschmid made their first contribution in https://github.com/vllm-project/vllm/pull/7917
* @jberkhahn made their first contribution in https://github.com/vllm-project/vllm/pull/7229
* @pavanimajety made their first contribution in https://github.com/vllm-project/vllm/pull/7798
* @rasmith made their first contribution in https://github.com/vllm-project/vllm/pull/7386
* @jmkuebler made their first contribution in https://github.com/vllm-project/vllm/pull/7899
* @kushanam made their first contribution in https://github.com/vllm-project/vllm/pull/7894
* @hbikki made their first contribution in https://github.com/vllm-project/vllm/pull/7885
* @wschin made their first contribution in https://github.com/vllm-project/vllm/pull/7759
* @nayohan made their first contribution in https://github.com/vllm-project/vllm/pull/7819
* @richardsliu made their first contribution in https://github.com/vllm-project/vllm/pull/7613
* @KaunilD made their first contribution in https://github.com/vllm-project/vllm/pull/7737
* @wenxcs made their first contribution in https://github.com/vllm-project/vllm/pull/7729
* @NickLucche made their first contribution in https://github.com/vllm-project/vllm/pull/8037
* @shawntan made their first contribution in https://github.com/vllm-project/vllm/pull/7436
* @noooop made their first contribution in https://github.com/vllm-project/vllm/pull/7874
* @haitwang-cloud made their first contribution in https://github.com/vllm-project/vllm/pull/8103
* @wnma3mz made their first contribution in https://github.com/vllm-project/vllm/pull/8106
* @K-Mistele made their first contribution in https://github.com/vllm-project/vllm/pull/5649

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.5...v0.6.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.6.0)

---

## v0.5.5: v0.5.5
**Published:** 2024-08-23

## Highlights

### Performance Update
* We introduced a new mode that schedule multiple GPU steps in advance, reducing CPU overhead (#7000, #7387, #7452, #7703). Initial result shows 20% improvements in QPS for a single GPU running 8B and 30B models. You can set `--num-scheduler-steps 8` as a parameter to the API server (via `vllm serve`) or `AsyncLLMEngine`. We are working on expanding the coverage to `LLM` class and aiming to turning it on by default
* Various enhancements:
  * Use flashinfer sampling kernel when avaiable, leading to 7% decoding throughput speedup (#7137)
  * Reduce Python allocations, leading to 24% throughput speedup (#7162, 7364)
  * Improvements to the zeromq based decoupled frontend (#7570, #7716, #7484)

### Model Support
* Support Jamba 1.5 (#7415, #7601, #6739)
* Support for the first audio model `UltravoxModel` (#7615, #7446)
* Improvements to vision models:
    * Support image embeddings as input (#6613)
    * Support SigLIP encoder and alternative decoders for LLaVA models (#7153)
* Support loading GGUF model (#5191) with tensor parallelism (#7520)
* Progress in encoder decoder models: support for serving encoder/decoder models (#7258), and architecture for cross-attention (#4942)

### Hardware Support
* AMD: Add fp8 Linear Layer for rocm (#7210)
* Enhancements to TPU support: load time W8A16 quantization (#7005), optimized rope (#7635), and support multi-host inference (#7457).
* Intel: various refactoring for worker, executor, and model runner (#7686, #7712)

### Others
* Optimize prefix caching performance (#7193)
* Speculative decoding
    * Use target model max length as default for draft model (#7706)
    * EAGLE Implementation with Top-1 proposer (#6830)
* Entrypoints
    * A new `chat` method in the `LLM` class (#5049)
    * Support embeddings in the run_batch API (#7132)
    * Support `prompt_logprobs` in Chat Completion (#7453)
* Quantizations
    * Expand MoE weight loading + Add Fused Marlin MoE Kernel (#7527)
    * Machete - Hopper Optimized Mixed Precision Linear Kernel  (#7174)
* `torch.compile`: register custom ops for kernels (#7591, #7594, #7536)

## What's Changed
* [ci][frontend] deduplicate tests by @youkaichao in https://github.com/vllm-project/vllm/pull/7101
* [Doc] [SpecDecode] Update MLPSpeculator documentation by @tdoublep in https://github.com/vllm-project/vllm/pull/7100
* [Bugfix] Specify device when loading LoRA and embedding tensors by @jischein in https://github.com/vllm-project/vllm/pull/7129
* [MISC] Use non-blocking transfer in prepare_input by @comaniac in https://github.com/vllm-project/vllm/pull/7172
* [Core] Support loading GGUF model by @Isotr0py in https://github.com/vllm-project/vllm/pull/5191
* [Build] Add initial conditional testing spec by @simon-mo in https://github.com/vllm-project/vllm/pull/6841
* [LoRA] Relax LoRA condition by @jeejeelee in https://github.com/vllm-project/vllm/pull/7146
* [Model] Support SigLIP encoder and alternative decoders for LLaVA models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7153
* [BugFix] Fix DeepSeek remote code by @dsikka in https://github.com/vllm-project/vllm/pull/7178
* [ BugFix ] Fix ZMQ when `VLLM_PORT` is set by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/7205
* [Bugfix] add gguf dependency by @kpapis in https://github.com/vllm-project/vllm/pull/7198
* [SpecDecode] [Minor] Fix spec decode sampler tests by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/7183
* [Kernel] Add per-tensor and per-token AZP epilogues by @ProExpertProg in https://github.com/vllm-project/vllm/pull/5941
* [Core] Optimize evictor-v2 performance by @xiaobochen123 in https://github.com/vllm-project/vllm/pull/7193
* [Core] Subclass ModelRunner to support cross-attention & encoder sequences (towards eventual encoder/decoder model support) by @afeldman-nm in https://github.com/vllm-project/vllm/pull/4942
* [Bugfix] Fix GPTQ and GPTQ Marlin CPU Offloading by @mgoin in https://github.com/vllm-project/vllm/pull/7225
* [BugFix] Overhaul async request cancellation by @njhill in https://github.com/vllm-project/vllm/pull/7111
* [Doc] Mock new dependencies for documentation by @ywang96 in https://github.com/vllm-project/vllm/pull/7245
* [BUGFIX]: top_k is expected to be an integer. by @Atllkks10 in https://github.com/vllm-project/vllm/pull/7227
* [Frontend] Gracefully handle missing chat template and fix CI failure by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7238
* [distributed][misc] add specialized method for cuda platform by @youkaichao in https://github.com/vllm-project/vllm/pull/7249
* [Misc] Refactor linear layer weight loading; introduce `BasevLLMParameter` and `weight_loader_v2` by @dsikka in https://github.com/vllm-project/vllm/pull/5874
* [ BugFix ] Move `zmq` frontend to IPC instead of TCP by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/7222
* Fixes typo in function name by @rafvasq in https://github.com/vllm-project/vllm/pull/7275
* [Bugfix] Fix input processor for InternVL2 model by @Isotr0py in https://github.com/vllm-project/vllm/pull/7164
* [OpenVINO] migrate to latest dependencies versions by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/7251
* [Doc] add online speculative decoding example by @stas00 in https://github.com/vllm-project/vllm/pull/7243
* [BugFix] Fix frontend multiprocessing hang by @maxdebayser in https://github.com/vllm-project/vllm/pull/7217
* [Bugfix][FP8] Fix dynamic FP8 Marlin quantization by @mgoin in https://github.com/vllm-project/vllm/pull/7219
* [ci] Make building wheels per commit optional by @khluu in https://github.com/vllm-project/vllm/pull/7278
* [Bugfix] Fix gptq failure on T4s by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7264
* [FrontEnd] Make `merge_async_iterators` `is_cancelled` arg optional by @njhill in https://github.com/vllm-project/vllm/pull/7282
* [Doc] Update supported_hardware.rst by @mgoin in https://github.com/vllm-project/vllm/pull/7276
* [Kernel] Fix Flashinfer Correctness by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/7284
* [Misc] Fix typos in scheduler.py by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7285
* [Frontend] remove max_num_batched_tokens limit for lora by @NiuBlibing in https://github.com/vllm-project/vllm/pull/7288
* [Bugfix] Fix LoRA with PP by @andoorve in https://github.com/vllm-project/vllm/pull/7292
* [Model] Rename MiniCPMVQwen2 to MiniCPMV2.6 by @jeejeelee in https://github.com/vllm-project/vllm/pull/7273
* [Bugfix][Kernel] Increased atol to fix failing tests by @ProExpertProg in https://github.com/vllm-project/vllm/pull/7305
* [Frontend] Kill the server on engine death by @joerunde in https://github.com/vllm-project/vllm/pull/6594
* [Bugfix][fast] Fix the get_num_blocks_touched logic by @zachzzc in https://github.com/vllm-project/vllm/pull/6849
* [Doc] Put collect_env issue output in a <detail> block by @mgoin in https://github.com/vllm-project/vllm/pull/7310
* [CI/Build] Dockerfile.cpu improvements by @dtrifiro in https://github.com/vllm-project/vllm/pull/7298
* [Bugfix] Fix new Llama3.1 GGUF model loading by @Isotr0py in https://github.com/vllm-project/vllm/pull/7269
* [Misc] Temporarily resolve the error of BitAndBytes by @jeejeelee in https://github.com/vllm-project/vllm/pull/7308
* Add Skywork AI as Sponsor by @simon-mo in https://github.com/vllm-project/vllm/pull/7314
* [TPU] Add Load-time W8A16 quantization for TPU Backend by @lsy323 in https://github.com/vllm-project/vllm/pull/7005
* [Core] Support serving encoder/decoder models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7258
* [TPU] Fix dockerfile.tpu by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7331
* [Performance] Optimize e2e overheads: Reduce python allocations by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7162
* [Bugfix] Fix speculative decoding with MLPSpeculator with padded vocabulary by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/7218
* [Speculative decoding] [Multi-Step] decouple should_modify_greedy_probs_inplace by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/6971
* [Core] Streamline stream termination in `AsyncLLMEngine` by @njhill in https://github.com/vllm-project/vllm/pull/7336
* [Model][Jamba] Mamba cache single buffer  by @mzusman in https://github.com/vllm-project/vllm/pull/6739
* [VLM][Doc] Add `stop_token_ids` to InternVL example by @Isotr0py in https://github.com/vllm-project/vllm/pull/7354
* [Performance] e2e overheads reduction: Small followup diff by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7364
* [Bugfix] Fix reinit procedure in ModelInputForGPUBuilder by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/7360
* [Frontend] Support embeddings in the run_batch API by @pooyadavoodi in https://github.com/vllm-project/vllm/pull/7132
* [Bugfix] Fix ITL recording in serving benchmark by @ywang96 in https://github.com/vllm-project/vllm/pull/7372
* [Core] Add span metrics for model_forward, scheduler and sampler time by @sfc-gh-mkeralapura in https://github.com/vllm-project/vllm/pull/7089
* [Bugfix] Fix `PerTensorScaleParameter` weight loading for fused models by @dsikka in https://github.com/vllm-project/vllm/pull/7376
* [Misc] Add numpy implementation of `compute_slot_mapping` by @Yard1 in https://github.com/vllm-project/vllm/pull/7377
* [Core] Fix edge case in chunked prefill + block manager v2 by @cadedaniel in https://github.com/vllm-project/vllm/pull/7380
* [Bugfix] Fix phi3v batch inference when images have different aspect ratio by @Isotr0py in https://github.com/vllm-project/vllm/pull/7392
* [TPU] Use mark_dynamic to reduce compilation time by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7340
* Updating LM Format Enforcer version to v0.10.6 by @noamgat in https://github.com/vllm-project/vllm/pull/7189
* [core] [2/N] refactor worker_base input preparation for multi-step by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7387
* [CI/Build] build on empty device for better dev experience by @tomeras91 in https://github.com/vllm-project/vllm/pull/4773
* [Doc] add instructions about building vLLM with VLLM_TARGET_DEVICE=empty by @tomeras91 in https://github.com/vllm-project/vllm/pull/7403
* [misc] add commit id in collect env by @youkaichao in https://github.com/vllm-project/vllm/pull/7405
* [Docs] Update readme by @simon-mo in https://github.com/vllm-project/vllm/pull/7316
* [CI/Build] Minor refactoring for vLLM assets by @ywang96 in https://github.com/vllm-project/vllm/pull/7407
* [Kernel] Flashinfer correctness fix for v0.1.3 by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/7319
* [Core][VLM] Support image embeddings as input by @ywang96 in https://github.com/vllm-project/vllm/pull/6613
* [Frontend] Disallow passing `model` as both argument and option by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7347
* [CI/Build] bump Dockerfile.neuron image base, use public ECR by @dtrifiro in https://github.com/vllm-project/vllm/pull/6832
* [Bugfix] Fix logit soft cap in flash-attn backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7425
* [ci] Entrypoints run upon changes in vllm/ by @khluu in https://github.com/vllm-project/vllm/pull/7423
* [ci] Cancel fastcheck run when PR is marked ready by @khluu in https://github.com/vllm-project/vllm/pull/7427
* [ci] Cancel fastcheck when PR is ready by @khluu in https://github.com/vllm-project/vllm/pull/7433
* [Misc] Use scalar type to dispatch to different `gptq_marlin` kernels by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7323
* [Core] Consolidate `GB` constant and enable float GB arguments by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7416
* [Core/Bugfix] Add FP8 K/V Scale and dtype conversion for prefix/prefill Triton Kernel by @jon-chuang in https://github.com/vllm-project/vllm/pull/7208
* [Bugfix] Handle PackageNotFoundError when checking for xpu version by @sasha0552 in https://github.com/vllm-project/vllm/pull/7398
* [CI/Build] bump minimum cmake version by @dtrifiro in https://github.com/vllm-project/vllm/pull/6999
* [Core] Shut down aDAG workers with clean async llm engine exit by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7224
* [mypy] Misc. typing improvements by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7417
* [Misc] improve logits processors logging message by @aw632 in https://github.com/vllm-project/vllm/pull/7435
* [ci] Remove fast check cancel workflow by @khluu in https://github.com/vllm-project/vllm/pull/7455
* [Bugfix] Fix weight loading for Chameleon when TP>1 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7410
* [hardware] unify usage of is_tpu to current_platform.is_tpu() by @youkaichao in https://github.com/vllm-project/vllm/pull/7102
* [TPU] Suppress import custom_ops warning by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7458
* Revert "[Doc] Update supported_hardware.rst (#7276)" by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7467
* [Frontend][Core] Add plumbing to support audio language models by @petersalas in https://github.com/vllm-project/vllm/pull/7446
* [Misc] Update LM Eval Tolerance by @dsikka in https://github.com/vllm-project/vllm/pull/7473
* [Misc] Update `gptq_marlin` to use new vLLMParameters by @dsikka in https://github.com/vllm-project/vllm/pull/7281
* [Misc] Update Fused MoE weight loading by @dsikka in https://github.com/vllm-project/vllm/pull/7334
* [Misc] Update `awq` and `awq_marlin` to use `vLLMParameters` by @dsikka in https://github.com/vllm-project/vllm/pull/7422
* Announce NVIDIA Meetup by @simon-mo in https://github.com/vllm-project/vllm/pull/7483
* [frontend] spawn engine process from api server process by @youkaichao in https://github.com/vllm-project/vllm/pull/7484
* [Misc] `compressed-tensors` code reuse by @kylesayrs in https://github.com/vllm-project/vllm/pull/7277
* [misc][plugin] add plugin system implementation by @youkaichao in https://github.com/vllm-project/vllm/pull/7426
* [TPU] Support multi-host inference by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7457
* [Bugfix][CI] Import ray under guard by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7486
* [CI/Build]Reduce the time consumption for LoRA tests by @jeejeelee in https://github.com/vllm-project/vllm/pull/7396
* [misc][ci] fix cpu test with plugins by @youkaichao in https://github.com/vllm-project/vllm/pull/7489
* [Bugfix][Docs] Update list of mock imports by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7493
* [doc] update test script to include cudagraph by @youkaichao in https://github.com/vllm-project/vllm/pull/7501
* Fix empty output when temp is too low by @CatherineSue in https://github.com/vllm-project/vllm/pull/2937
* [ci] fix model tests by @youkaichao in https://github.com/vllm-project/vllm/pull/7507
* [Bugfix][Frontend] Disable embedding API for chat models by @QwertyJack in https://github.com/vllm-project/vllm/pull/7504
* [Misc] Deprecation Warning when setting --engine-use-ray by @wallashss in https://github.com/vllm-project/vllm/pull/7424
* [VLM][Core] Support profiling with multiple multi-modal inputs per prompt by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7126
* [core] [3/N] multi-step args and sequence.py by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7452
* [TPU] Set per-rank XLA cache by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7533
* [Misc] Revert `compressed-tensors` code reuse by @kylesayrs in https://github.com/vllm-project/vllm/pull/7521
* llama_index serving integration documentation by @pavanjava in https://github.com/vllm-project/vllm/pull/6973
* [Bugfix][TPU] Correct env variable for XLA cache path by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7544
* [Bugfix] update neuron for version > 0.5.0 by @omrishiv in https://github.com/vllm-project/vllm/pull/7175
* [Misc] Update dockerfile for CPU to cover protobuf installation by @PHILO-HE in https://github.com/vllm-project/vllm/pull/7182
* [Bugfix] Fix default weight loading for scalars by @mgoin in https://github.com/vllm-project/vllm/pull/7534
* [Bugfix][Harmless] Fix hardcoded float16 dtype for model_is_embedding by @mgoin in https://github.com/vllm-project/vllm/pull/7566
* [Misc] Add quantization config support for speculative model. by @ShangmingCai in https://github.com/vllm-project/vllm/pull/7343
* [Feature]: Add OpenAI server prompt_logprobs support #6508 by @gnpinkert in https://github.com/vllm-project/vllm/pull/7453
* [ci/test] rearrange tests and make adag test soft fail by @youkaichao in https://github.com/vllm-project/vllm/pull/7572
* Chat method for offline llm by @nunjunj in https://github.com/vllm-project/vllm/pull/5049
* [CI] Move quantization cpu offload tests out of fastcheck by @mgoin in https://github.com/vllm-project/vllm/pull/7574
* [Misc/Testing] Use `torch.testing.assert_close` by @jon-chuang in https://github.com/vllm-project/vllm/pull/7324
* register custom op for flash attn and use from torch.ops by @youkaichao in https://github.com/vllm-project/vllm/pull/7536
* [Core] Use uvloop with zmq-decoupled front-end by @njhill in https://github.com/vllm-project/vllm/pull/7570
* [CI] Fix crashes of performance benchmark  by @KuntaiDu in https://github.com/vllm-project/vllm/pull/7500
* [Bugfix][Hardware][AMD][Frontend] add quantization param to embedding checking method by @gongdao123 in https://github.com/vllm-project/vllm/pull/7513
* support tqdm in notebooks by @fzyzcjy in https://github.com/vllm-project/vllm/pull/7510
* [Feature][Hardware][Amd] Add fp8 Linear Layer for Rocm by @charlifu in https://github.com/vllm-project/vllm/pull/7210
* [Kernel] W8A16 Int8 inside FusedMoE  by @mzusman in https://github.com/vllm-project/vllm/pull/7415
* [Kernel] Add tuned triton configs for ExpertsInt8 by @mgoin in https://github.com/vllm-project/vllm/pull/7601
* [spec decode] [4/N] Move update_flash_attn_metadata to attn backend by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7571
* [Core] Fix tracking of model forward time to the span traces in case of PP>1 by @sfc-gh-mkeralapura in https://github.com/vllm-project/vllm/pull/7440
* [Doc] Add docs for llmcompressor INT8 and FP8 checkpoints by @mgoin in https://github.com/vllm-project/vllm/pull/7444
* [Doc] Update quantization supported hardware table by @mgoin in https://github.com/vllm-project/vllm/pull/7595
* [Kernel] register punica functions as torch ops by @bnellnm in https://github.com/vllm-project/vllm/pull/7591
* [Kernel][Misc] dynamo support for ScalarType by @bnellnm in https://github.com/vllm-project/vllm/pull/7594
* [Kernel] fix types used in aqlm and ggml kernels to support dynamo by @bnellnm in https://github.com/vllm-project/vllm/pull/7596
* [Model] Align nemotron config with final HF state and fix lm-eval-small by @mgoin in https://github.com/vllm-project/vllm/pull/7611
* [Bugfix] Fix custom_ar support check by @bnellnm in https://github.com/vllm-project/vllm/pull/7617
* .[Build/CI] Enabling passing AMD tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/7610
* [Bugfix] Clear engine reference in AsyncEngineRPCServer by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7618
* [aDAG] Unflake aDAG + PP tests by @rkooo567 in https://github.com/vllm-project/vllm/pull/7600
* [Bugfix] add >= 1.0 constraint for openai dependency by @metasyn in https://github.com/vllm-project/vllm/pull/7612
* [misc] use nvml to get consistent device name by @youkaichao in https://github.com/vllm-project/vllm/pull/7582
* [ci][test] fix engine/logger test by @youkaichao in https://github.com/vllm-project/vllm/pull/7621
* [core][misc] update libcudart finding by @youkaichao in https://github.com/vllm-project/vllm/pull/7620
* [Model] Pipeline parallel support for JAIS by @mrbesher in https://github.com/vllm-project/vllm/pull/7603
* [ci][test] allow longer wait time for api server by @youkaichao in https://github.com/vllm-project/vllm/pull/7629
* [Misc]Fix BitAndBytes exception messages by @jeejeelee in https://github.com/vllm-project/vllm/pull/7626
* [VLM] Refactor `MultiModalConfig` initialization and profiling by @ywang96 in https://github.com/vllm-project/vllm/pull/7530
* [TPU] Skip creating empty tensor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7630
* [TPU] Use mark_dynamic only for dummy run by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7634
* [TPU] Optimize RoPE forward_native2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7636
* [ Bugfix ] Fix Prometheus Metrics With `zeromq` Frontend by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/7279
* [CI/Build] Add text-only test for Qwen models  by @alex-jw-brooks in https://github.com/vllm-project/vllm/pull/7475
* [Misc] Refactor Llama3 RoPE initialization by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7637
* [Core] Optimize SPMD architecture with delta + serialization optimization by @rkooo567 in https://github.com/vllm-project/vllm/pull/7109
* [Core] Use flashinfer sampling kernel when available by @peng1999 in https://github.com/vllm-project/vllm/pull/7137
* fix xpu build by @jikunshang in https://github.com/vllm-project/vllm/pull/7644
* [Misc] Remove Gemma RoPE by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7638
* [MISC] Add prefix cache hit rate to metrics by @comaniac in https://github.com/vllm-project/vllm/pull/7606
* [Bugfix] fix lora_dtype value type in arg_utils.py - part 2 by @c3-ali in https://github.com/vllm-project/vllm/pull/5428
* [core] Multi Step Scheduling by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7000
* [Core] Support tensor parallelism for GGUF quantization by @Isotr0py in https://github.com/vllm-project/vllm/pull/7520
* [Bugfix] Don't disable existing loggers by @a-ys in https://github.com/vllm-project/vllm/pull/7664
* [TPU] Fix redundant input tensor cloning by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7660
* [Bugfix] use StoreBoolean instead of type=bool for --disable-logprobs-during-spec-decoding by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/7665
* [doc] fix doc build error caused by msgspec by @youkaichao in https://github.com/vllm-project/vllm/pull/7659
* [Speculative Decoding] Fixing hidden states handling in batch expansion by @abhigoyal1997 in https://github.com/vllm-project/vllm/pull/7508
* [ci] Install Buildkite test suite analysis by @khluu in https://github.com/vllm-project/vllm/pull/7667
* [Bugfix] support `tie_word_embeddings` for all models by @zijian-hu in https://github.com/vllm-project/vllm/pull/5724
* [CI] Organizing performance benchmark files by @KuntaiDu in https://github.com/vllm-project/vllm/pull/7616
* [misc] add nvidia related library in collect env  by @youkaichao in https://github.com/vllm-project/vllm/pull/7674
* [XPU] fallback to native implementation for xpu custom op by @jianyizh in https://github.com/vllm-project/vllm/pull/7670
* [misc][cuda] add warning for pynvml user by @youkaichao in https://github.com/vllm-project/vllm/pull/7675
* [Core] Refactor executor classes to make it easier to inherit GPUExecutor by @jikunshang in https://github.com/vllm-project/vllm/pull/7673
* [Kernel] (1/N) Machete - Hopper Optimized Mixed Precision Linear Kernel  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7174
* [OpenVINO] Updated documentation by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/7687
* [VLM][Model] Add test for InternViT vision encoder by @Isotr0py in https://github.com/vllm-project/vllm/pull/7409
* [Hardware] [Intel GPU]  refactor xpu worker/executor by @jikunshang in https://github.com/vllm-project/vllm/pull/7686
* [CI/Build] Pin OpenTelemetry versions and make availability errors clearer by @ronensc in https://github.com/vllm-project/vllm/pull/7266
* [Misc] Add jinja2 as an explicit build requirement by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7695
* [Core] Add `AttentionState` abstraction by @Yard1 in https://github.com/vllm-project/vllm/pull/7663
* [Intel GPU] fix xpu not support punica kernel (which use torch.library.custom_op) by @jikunshang in https://github.com/vllm-project/vllm/pull/7685
* [ci][test] adjust max wait time for cpu offloading test by @youkaichao in https://github.com/vllm-project/vllm/pull/7709
* [Core] Pipe `worker_class_fn` argument in Executor by @Yard1 in https://github.com/vllm-project/vllm/pull/7707
* [ci] try to log process using the port to debug the port usage by @youkaichao in https://github.com/vllm-project/vllm/pull/7711
* [Model] Add AWQ quantization support for InternVL2 model by @Isotr0py in https://github.com/vllm-project/vllm/pull/7187
* [Doc] Section for Multimodal Language Models by @ywang96 in https://github.com/vllm-project/vllm/pull/7719
* [mypy] Enable following imports for entrypoints by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7248
* [Bugfix] Mirror jinja2 in pyproject.toml by @sasha0552 in https://github.com/vllm-project/vllm/pull/7723
* [BugFix] Avoid premature async generator exit and raise all exception variations by @njhill in https://github.com/vllm-project/vllm/pull/7698
* [BUG] fix crash on flashinfer backend with cudagraph disabled, when attention group_size not in [1,2,4,8] by @learninmou in https://github.com/vllm-project/vllm/pull/7509
* [Bugfix][Hardware][CPU] Fix `mm_limits` initialization for CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/7735
* [Spec Decoding] Use target model max length as default for draft model by @njhill in https://github.com/vllm-project/vllm/pull/7706
* [Bugfix] chat method add_generation_prompt param by @brian14708 in https://github.com/vllm-project/vllm/pull/7734
* [Bugfix][Frontend] Fix Issues Under High Load With `zeromq` Frontend by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/7394
* [Bugfix] Pass PYTHONPATH from setup.py to CMake by @sasha0552 in https://github.com/vllm-project/vllm/pull/7730
* [multi-step] Raise error if not using async engine by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7703
* [Frontend] Improve Startup Failure UX by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/7716
* [misc] Add Torch profiler support by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7451
* [Model] Add UltravoxModel and UltravoxConfig by @petersalas in https://github.com/vllm-project/vllm/pull/7615
* [ci] [multi-step] narrow multi-step test dependency paths by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/7760
* [Kernel]  Expand MoE weight loading + Add Fused Marlin MoE Kernel by @dsikka in https://github.com/vllm-project/vllm/pull/7527
* [distributed][misc] error on same VLLM_HOST_IP setting by @youkaichao in https://github.com/vllm-project/vllm/pull/7756
* [AMD][CI/Build] Disambiguation of the function call for ROCm 6.2 headers compatibility by @gshtras in https://github.com/vllm-project/vllm/pull/7477
* [Kernel] Replaced `blockReduce[...]` functions with `cub::BlockReduce` by @ProExpertProg in https://github.com/vllm-project/vllm/pull/7233
* [Model] Fix Phi-3.5-vision-instruct 'num_crops' issue by @zifeitong in https://github.com/vllm-project/vllm/pull/7710
* [Bug][Frontend] Improve ZMQ client robustness by @joerunde in https://github.com/vllm-project/vllm/pull/7443
* Revert "[Kernel]  Expand MoE weight loading + Add Fused Marlin MoE Kernel (#7527)" by @mgoin in https://github.com/vllm-project/vllm/pull/7764
* [TPU] Avoid initializing TPU runtime in is_tpu by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7763
* [ci] refine dependency for distributed tests by @youkaichao in https://github.com/vllm-project/vllm/pull/7776
* [Misc] Use torch.compile for GemmaRMSNorm by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7642
* [Speculative Decoding] EAGLE Implementation with Top-1 proposer by @abhigoyal1997 in https://github.com/vllm-project/vllm/pull/6830
* Fix ShardedStateLoader for vllm fp8 quantization by @sfc-gh-zhwang in https://github.com/vllm-project/vllm/pull/7708
* [Bugfix] Don't build machete on cuda <12.0 by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/7757
* [Misc] update fp8 to use `vLLMParameter` by @dsikka in https://github.com/vllm-project/vllm/pull/7437
* [Bugfix] spec decode handle None entries in topk args in create_sequence_group_output by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/7232
* [Misc] Enhance prefix-caching benchmark tool by @Jeffwan in https://github.com/vllm-project/vllm/pull/6568
* [Doc] Fix incorrect docs from #7615 by @petersalas in https://github.com/vllm-project/vllm/pull/7788
* [Bugfix] Use LoadFormat values as choices for `vllm serve --load-format` by @mgoin in https://github.com/vllm-project/vllm/pull/7784
* [ci] Cleanup & refactor Dockerfile to pass different Python versions and sccache bucket via build args by @khluu in https://github.com/vllm-project/vllm/pull/7705
* [Misc] fix typo in triton import warning by @lsy323 in https://github.com/vllm-project/vllm/pull/7794
* [Frontend] error suppression cleanup by @joerunde in https://github.com/vllm-project/vllm/pull/7786
* [Ray backend] Better error when pg topology is bad.  by @rkooo567 in https://github.com/vllm-project/vllm/pull/7584
* [Hardware][Intel GPU] refactor xpu_model_runner, fix xpu tensor parallel by @jikunshang in https://github.com/vllm-project/vllm/pull/7712
* [misc] Add Torch profiler support for CPU-only devices by @DamonFool in https://github.com/vllm-project/vllm/pull/7806
* [BugFix] Fix server crash on empty prompt by @maxdebayser in https://github.com/vllm-project/vllm/pull/7746
* [github][misc] promote asking llm first by @youkaichao in https://github.com/vllm-project/vllm/pull/7809
* [Misc] Update `marlin` to use vLLMParameters by @dsikka in https://github.com/vllm-project/vllm/pull/7803
* Bump version to v0.5.5 by @simon-mo in https://github.com/vllm-project/vllm/pull/7823

## New Contributors
* @jischein made their first contribution in https://github.com/vllm-project/vllm/pull/7129
* @kpapis made their first contribution in https://github.com/vllm-project/vllm/pull/7198
* @xiaobochen123 made their first contribution in https://github.com/vllm-project/vllm/pull/7193
* @Atllkks10 made their first contribution in https://github.com/vllm-project/vllm/pull/7227
* @stas00 made their first contribution in https://github.com/vllm-project/vllm/pull/7243
* @maxdebayser made their first contribution in https://github.com/vllm-project/vllm/pull/7217
* @NiuBlibing made their first contribution in https://github.com/vllm-project/vllm/pull/7288
* @lsy323 made their first contribution in https://github.com/vllm-project/vllm/pull/7005
* @pooyadavoodi made their first contribution in https://github.com/vllm-project/vllm/pull/7132
* @sfc-gh-mkeralapura made their first contribution in https://github.com/vllm-project/vllm/pull/7089
* @jon-chuang made their first contribution in https://github.com/vllm-project/vllm/pull/7208
* @aw632 made their first contribution in https://github.com/vllm-project/vllm/pull/7435
* @petersalas made their first contribution in https://github.com/vllm-project/vllm/pull/7446
* @kylesayrs made their first contribution in https://github.com/vllm-project/vllm/pull/7277
* @QwertyJack made their first contribution in https://github.com/vllm-project/vllm/pull/7504
* @wallashss made their first contribution in https://github.com/vllm-project/vllm/pull/7424
* @pavanjava made their first contribution in https://github.com/vllm-project/vllm/pull/6973
* @PHILO-HE made their first contribution in https://github.com/vllm-project/vllm/pull/7182
* @gnpinkert made their first contribution in https://github.com/vllm-project/vllm/pull/7453
* @gongdao123 made their first contribution in https://github.com/vllm-project/vllm/pull/7513
* @charlifu made their first contribution in https://github.com/vllm-project/vllm/pull/7210
* @metasyn made their first contribution in https://github.com/vllm-project/vllm/pull/7612
* @mrbesher made their first contribution in https://github.com/vllm-project/vllm/pull/7603
* @alex-jw-brooks made their first contribution in https://github.com/vllm-project/vllm/pull/7475
* @a-ys made their first contribution in https://github.com/vllm-project/vllm/pull/7664
* @zijian-hu made their first contribution in https://github.com/vllm-project/vllm/pull/5724
* @jianyizh made their first contribution in https://github.com/vllm-project/vllm/pull/7670
* @learninmou made their first contribution in https://github.com/vllm-project/vllm/pull/7509
* @brian14708 made their first contribution in https://github.com/vllm-project/vllm/pull/7734
* @sfc-gh-zhwang made their first contribution in https://github.com/vllm-project/vllm/pull/7708

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.4...v0.5.5

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.5)

---

## v0.5.4: v0.5.4
**Published:** 2024-08-05

## Highlights

### Model Support
* Enhanced pipeline parallelism support for DeepSeek v2 (#6519), Qwen (#6974), Qwen2 (#6924), and Nemotron (#6863)
* Enhanced vision language model support for InternVL2 (#6514, #7067), BLIP-2 (#5920), MiniCPM-V (#4087, #7122).
* Added H2O Danube3-4b (#6451)
* Added Nemotron models (Nemotron-3, Nemotron-4, Minitron) (#6611)

### Hardware Support
* TPU enhancements: collective communication, TP for async engine, faster compile time (#6891, #6933, #6856, #6813, #5871)
* Intel CPU: enable multiprocessing and tensor parallelism (#6125)

### Performance
We are progressing along our quest to quickly improve performance. Each of the following PRs contributed some improvements, and we anticipate more enhancements in the next release. 

* Separated OpenAI Server's HTTP request handling and model inference loop with `zeromq`. This brought 20% speedup over time to first token and 2x speedup over inter token latency. (#6883)
* Used Python's native array data structure speedup padding. This bring 15% throughput enhancement in large batch size scenarios. (#6779)
* Reduce unnecessary compute when logprobs=None. This reduced latency of get log probs from ~30ms to ~5ms in large batch size scenarios. (#6532)
* Optimize `get_seqs` function, bring 2% throughput enhancements. (#7051)

### Production Features
* Enhancements to speculative decoding: FlashInfer in DraftModelRunner (#6926), observability (#6963), and benchmarks (#6964)
* Refactor the punica kernel based on Triton (#5036)
* Support for guided decoding for offline LLM (#6878)

### Quantization
* Support W4A8 quantization for vllm (#5218)
* Tuned FP8 and INT8 Kernels for Ada Lovelace and SM75 T4 (#6677, #6996, #6848)
* Support reading bitsandbytes pre-quantized model (#5753)


## What's Changed
* [Docs] Announce llama3.1 support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6688
* [doc][distributed] fix doc argument order by @youkaichao in https://github.com/vllm-project/vllm/pull/6691
* [Bugfix] Fix a log error in chunked prefill by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6694
* [BugFix] Fix RoPE error in Llama 3.1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6693
* Bump version to 0.5.3.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/6696
* [Misc] Add ignored layers for `fp8` quantization by @mgoin in https://github.com/vllm-project/vllm/pull/6657
* [Frontend] Add Usage data in each chunk for chat_serving. #6540 by @yecohn in https://github.com/vllm-project/vllm/pull/6652
* [Model] Pipeline Parallel Support for DeepSeek v2 by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6519
* Bump `transformers` version for Llama 3.1 hotfix and patch Chameleon  by @ywang96 in https://github.com/vllm-project/vllm/pull/6690
* [build] relax wheel size limit by @youkaichao in https://github.com/vllm-project/vllm/pull/6704
* [CI] Add smoke test for non-uniform AutoFP8 quantization by @mgoin in https://github.com/vllm-project/vllm/pull/6702
* [Bugfix] StatLoggers: cache spec decode metrics when they get collected. by @tdoublep in https://github.com/vllm-project/vllm/pull/6645
* [bitsandbytes]: support read bnb pre-quantized model by @thesues in https://github.com/vllm-project/vllm/pull/5753
* [Bugfix] fix flashinfer cudagraph capture for PP by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/6708
* [SpecDecoding] Update MLPSpeculator CI tests to use smaller model by @njhill in https://github.com/vllm-project/vllm/pull/6714
* [Bugfix] Fix token padding for chameleon by @ywang96 in https://github.com/vllm-project/vllm/pull/6724
* [Docs][ROCm] Detailed instructions to build from source by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6680
* [Build/CI] Update run-amd-test.sh. Enable Docker Hub login. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/6711
* [Bugfix]fix modelscope compatible issue by @liuyhwangyh in https://github.com/vllm-project/vllm/pull/6730
* Adding f-string to validation error which is missing by @luizanao in https://github.com/vllm-project/vllm/pull/6748
* [Bugfix] Fix speculative decode seeded test by @njhill in https://github.com/vllm-project/vllm/pull/6743
* [Bugfix] Miscalculated latency lead to time_to_first_token_seconds inaccurate. by @AllenDou in https://github.com/vllm-project/vllm/pull/6686
* [Frontend] split run_server into build_server and run_server by @dtrifiro in https://github.com/vllm-project/vllm/pull/6740
* [Kernels] Add fp8 support to `reshape_and_cache_flash` by @Yard1 in https://github.com/vllm-project/vllm/pull/6667
* [Core] Tweaks to model runner/input builder developer APIs by @Yard1 in https://github.com/vllm-project/vllm/pull/6712
* [Bugfix] Bump transformers to 4.43.2 by @mgoin in https://github.com/vllm-project/vllm/pull/6752
* [Doc][AMD][ROCm]Added tips to refer to mi300x tuning guide for mi300x users by @hongxiayang in https://github.com/vllm-project/vllm/pull/6754
* [core][distributed] fix zmq hang by @youkaichao in https://github.com/vllm-project/vllm/pull/6759
* [Frontend] Represent tokens with identifiable strings by @ezliu in https://github.com/vllm-project/vllm/pull/6626
* [Model] Adding support for MiniCPM-V by @HwwwwwwwH in https://github.com/vllm-project/vllm/pull/4087
* [Bugfix] Fix decode tokens w. CUDA graph by @comaniac in https://github.com/vllm-project/vllm/pull/6757
* [Bugfix] Fix awq_marlin and gptq_marlin flags by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/6745
* [Bugfix] Fix encoding_format in examples/openai_embedding_client.py by @CatherineSue in https://github.com/vllm-project/vllm/pull/6755
* [Bugfix] Add image placeholder for OpenAI Compatible Server of MiniCPM-V by @HwwwwwwwH in https://github.com/vllm-project/vllm/pull/6787
* [ Misc ] `fp8-marlin` channelwise via `compressed-tensors` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6524
* [Bugfix] Fix `kv_cache_dtype=fp8` without scales for FP8 checkpoints by @mgoin in https://github.com/vllm-project/vllm/pull/6761
* [Bugfix] Add synchronize to prevent possible data race by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6788
* [Doc] Add documentations for nightly benchmarks by @KuntaiDu in https://github.com/vllm-project/vllm/pull/6412
* [Bugfix] Fix empty (nullptr) channelwise  scales when loading wNa16 using compressed tensors by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/6798
* [doc][distributed] improve multinode serving doc by @youkaichao in https://github.com/vllm-project/vllm/pull/6804
* [Docs] Publish 5th meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6799
* [Core] Fix ray forward_dag error mssg by @rkooo567 in https://github.com/vllm-project/vllm/pull/6792
* [ci][distributed] fix flaky tests by @youkaichao in https://github.com/vllm-project/vllm/pull/6806
* [ci] Mark tensorizer test as soft fail and separate it from grouped test in fast check by @khluu in https://github.com/vllm-project/vllm/pull/6810
* Fix ReplicatedLinear weight loading by @qingquansong in https://github.com/vllm-project/vllm/pull/6793
* [Bugfix] [Easy] Fixed a bug in the multiprocessing GPU executor. by @eaplatanios in https://github.com/vllm-project/vllm/pull/6770
* [Core] Use array to speedup padding by @peng1999 in https://github.com/vllm-project/vllm/pull/6779
* [doc][debugging] add known issues for hangs by @youkaichao in https://github.com/vllm-project/vllm/pull/6816
* [Model] Support Nemotron models (Nemotron-3, Nemotron-4, Minitron) by @mgoin in https://github.com/vllm-project/vllm/pull/6611
* [Bugfix][Kernel] Promote another index to int64_t by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6838
* [Build/CI][ROCm] Minor simplification to Dockerfile.rocm by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6811
* [Misc][TPU] Support TPU in initialize_ray_cluster by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6812
* [Hardware] [Intel] Enable Multiprocessing and tensor parallel in CPU backend and update documentation  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/6125
* [Doc] Add Nemotron to supported model docs by @mgoin in https://github.com/vllm-project/vllm/pull/6843
* [Doc] Update SkyPilot doc for wrong indents and instructions for update service by @Michaelvll in https://github.com/vllm-project/vllm/pull/4283
* Update README.md by @gurpreet-dhami in https://github.com/vllm-project/vllm/pull/6847
* enforce eager mode with bnb quantization temporarily by @chenqianfzh in https://github.com/vllm-project/vllm/pull/6846
* [TPU] Support collective communications in XLA devices by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6813
* [Frontend] Factor out code for running uvicorn by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6828
* [Bug Fix] Illegal memory access, FP8 Llama 3.1 405b  by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/6852
* [Bugfix]: Fix Tensorizer test failures by @sangstar in https://github.com/vllm-project/vllm/pull/6835
* [ROCm] Upgrade PyTorch nightly version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6845
* [Doc] add VLLM_TARGET_DEVICE=neuron to documentation for neuron by @omrishiv in https://github.com/vllm-project/vllm/pull/6844
* [Bugfix][Model] Jamba assertions and no chunked prefill by default for Jamba by @tomeras91 in https://github.com/vllm-project/vllm/pull/6784
* [Model] H2O Danube3-4b by @g-eoj in https://github.com/vllm-project/vllm/pull/6451
* [Hardware][TPU] Implement tensor parallelism with Ray by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5871
* [Doc] Add missing mock import to docs `conf.py` by @hmellor in https://github.com/vllm-project/vllm/pull/6834
* [Bugfix] Use torch.set_num_threads() to configure parallelism in multiproc_gpu_executor by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6802
* [Misc][VLM][Doc] Consolidate offline examples for vision language models by @ywang96 in https://github.com/vllm-project/vllm/pull/6858
* [Bugfix] Fix VLM example typo by @ywang96 in https://github.com/vllm-project/vllm/pull/6859
* [bugfix] make args.stream work by @WrRan in https://github.com/vllm-project/vllm/pull/6831
* [CI/Build][Doc] Update CI and Doc for VLM example changes by @ywang96 in https://github.com/vllm-project/vllm/pull/6860
* [Model] Initial support for BLIP-2 by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5920
* [Docs] Add RunLLM chat widget by @cw75 in https://github.com/vllm-project/vllm/pull/6857
* [TPU] Reduce compilation time & Upgrade PyTorch XLA version  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6856
* [Kernel] Increase precision of GPTQ/AWQ Marlin kernel by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/6795
* Add Nemotron to PP_SUPPORTED_MODELS by @mgoin in https://github.com/vllm-project/vllm/pull/6863
* [Misc] Pass cutlass_fp8_supported correctly in fbgemm_fp8 by @zeyugao in https://github.com/vllm-project/vllm/pull/6871
* [Model] Initialize support for InternVL2 series models by @Isotr0py in https://github.com/vllm-project/vllm/pull/6514
* [Kernel] Tuned FP8 Kernels for Ada Lovelace by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6677
* [Core] Reduce unnecessary compute when logprobs=None by @peng1999 in https://github.com/vllm-project/vllm/pull/6532
* [Kernel] Fix deprecation function warnings squeezellm quant_cuda_kernel by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6901
* [TPU] Add TPU tensor parallelism to async engine by @etwk in https://github.com/vllm-project/vllm/pull/6891
* [Bugfix] Allow vllm to still work if triton is not installed. by @tdoublep in https://github.com/vllm-project/vllm/pull/6786
* [Frontend] New `allowed_token_ids` decoding request parameter by @njhill in https://github.com/vllm-project/vllm/pull/6753
* [Kernel] Remove unused variables in awq/gemm_kernels.cu by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6908
* [ci] GHA workflow to remove ready label upon "/notready" comment by @khluu in https://github.com/vllm-project/vllm/pull/6921
* [Kernel] Fix marlin divide-by-zero warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6904
* [Kernel] Tuned int8 kernels for Ada Lovelace by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6848
* [TPU] Fix greedy decoding by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6933
* [Bugfix] Fix PaliGemma MMP by @ywang96 in https://github.com/vllm-project/vllm/pull/6930
* [Doc] Super tiny fix doc typo by @fzyzcjy in https://github.com/vllm-project/vllm/pull/6949
* [BugFix] Fix use of per-request seed with pipeline parallel by @njhill in https://github.com/vllm-project/vllm/pull/6698
* [Kernel] Squash a few more warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6914
* [OpenVINO] Updated OpenVINO requirements and build docs by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/6948
* [Bugfix] Fix tensorizer memory profiling bug during testing by @sangstar in https://github.com/vllm-project/vllm/pull/6881
* [Kernel] Remove scaled_fp8_quant kernel padding footgun by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6842
* [core][misc] improve free_finished_seq_groups by @youkaichao in https://github.com/vllm-project/vllm/pull/6865
* [Build] Temporarily Disable Kernels and LoRA tests by @simon-mo in https://github.com/vllm-project/vllm/pull/6961
* [Nightly benchmarking suite] Remove pkill python from run benchmark suite by @cadedaniel in https://github.com/vllm-project/vllm/pull/6965
* [CI] [nightly benchmark] Do not re-download sharegpt dataset if exists by @cadedaniel in https://github.com/vllm-project/vllm/pull/6706
* [Speculative decoding] Add serving benchmark for llama3 70b + speculative decoding by @cadedaniel in https://github.com/vllm-project/vllm/pull/6964
* [mypy] Enable following imports for some directories by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6681
* [Bugfix] Fix broadcasting logic for `multi_modal_kwargs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6836
* [CI/Build] Fix mypy errors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6968
* [Bugfix][TPU] Set readonly=True for non-root devices by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6980
* [Bugfix] fix logit processor excceed vocab size issue by @FeiDeng in https://github.com/vllm-project/vllm/pull/6927
* Support W4A8 quantization for vllm by @HandH1998 in https://github.com/vllm-project/vllm/pull/5218
* [Bugfix] Clean up MiniCPM-V by @HwwwwwwwH in https://github.com/vllm-project/vllm/pull/6939
* [Bugfix] Fix feature size calculation for LLaVA-NeXT by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6982
* [Model] use FusedMoE layer in Jamba by @avshalomman in https://github.com/vllm-project/vllm/pull/6935
* [MISC] Introduce pipeline parallelism partition strategies by @comaniac in https://github.com/vllm-project/vllm/pull/6920
* [Bugfix] Support cpu offloading with quant_method.process_weights_after_loading by @mgoin in https://github.com/vllm-project/vllm/pull/6960
* [Kernel] Enable FP8 Cutlass for Ada Lovelace by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6950
* [Kernel] Tuned int8 Cutlass Kernels for SM75 (T4) by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6996
* [Misc] Add compressed-tensors to optimized quant list by @mgoin in https://github.com/vllm-project/vllm/pull/7006
* Revert "[Frontend] Factor out code for running uvicorn" by @simon-mo in https://github.com/vllm-project/vllm/pull/7012
* [Kernel][RFC] Refactor the punica kernel based on Triton by @jeejeelee in https://github.com/vllm-project/vllm/pull/5036
* [Model] Pipeline parallel support for Qwen2 by @xuyi in https://github.com/vllm-project/vllm/pull/6924
* [Bugfix][TPU] Do not use torch.Generator for TPUs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6981
* [Bugfix][Model] Skip loading lm_head weights if using tie_word_embeddings by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6758
* PP comm optimization: replace send with partial send + allgather by @aurickq in https://github.com/vllm-project/vllm/pull/6695
* [Bugfix] Set SamplingParams.max_tokens for OpenAI requests if not provided by user by @zifeitong in https://github.com/vllm-project/vllm/pull/6954
* [core][scheduler] simplify and improve scheduler by @youkaichao in https://github.com/vllm-project/vllm/pull/6867
* [Build/CI] Fixing Docker Hub quota issue. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/7043
* [CI/Build] Update torch to 2.4 by @SageMoore in https://github.com/vllm-project/vllm/pull/6951
* [Bugfix] Fix RMSNorm forward in InternViT attention qk_layernorm by @Isotr0py in https://github.com/vllm-project/vllm/pull/6992
* [CI/Build] Remove sparseml requirement from testing by @mgoin in https://github.com/vllm-project/vllm/pull/7037
* [Bugfix] Lower gemma's unloaded_params exception to warning by @mgoin in https://github.com/vllm-project/vllm/pull/7002
* [Models] Support Qwen model with PP by @andoorve in https://github.com/vllm-project/vllm/pull/6974
* Update run-amd-test.sh by @okakarpa in https://github.com/vllm-project/vllm/pull/7044
* [Misc] Support attention logits soft-capping with flash-attn by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7022
* [CI/Build][Bugfix] Fix CUTLASS header-only line by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/7034
* [Performance] Optimize `get_seqs` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/7051
* [Kernel] Fix input for flashinfer prefill wrapper. by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/7008
* [mypy] Speed up mypy checking by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7056
* [ci][distributed] try to fix pp test by @youkaichao in https://github.com/vllm-project/vllm/pull/7054
* Fix tracing.py by @bong-furiosa in https://github.com/vllm-project/vllm/pull/7065
* [cuda][misc] remove error_on_invalid_device_count_status by @youkaichao in https://github.com/vllm-project/vllm/pull/7069
* [Core] Comment out unused code in sampler by @peng1999 in https://github.com/vllm-project/vllm/pull/7023
* [Hardware][Intel CPU] Update torch 2.4.0 for CPU backend by @DamonFool in https://github.com/vllm-project/vllm/pull/6931
* [ci] set timeout for test_oot_registration.py by @youkaichao in https://github.com/vllm-project/vllm/pull/7082
* [CI/Build] Add support for Python 3.12 by @mgoin in https://github.com/vllm-project/vllm/pull/7035
* [Misc] Disambiguate quantized types via a new ScalarType by @LucasWilkinson in https://github.com/vllm-project/vllm/pull/6396
* [Core] Pipeline parallel with Ray ADAG by @ruisearch42 in https://github.com/vllm-project/vllm/pull/6837
* [Misc] Revive to use loopback address for driver IP by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7091
* [misc] add a flag to enable compile by @youkaichao in https://github.com/vllm-project/vllm/pull/7092
* [ Frontend ] Multiprocessing for OpenAI Server with `zeromq` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6883
* [ci][distributed] shorten wait time if server hangs by @youkaichao in https://github.com/vllm-project/vllm/pull/7098
* [Frontend] Factor out chat message parsing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7055
* [ci][distributed] merge distributed test commands by @youkaichao in https://github.com/vllm-project/vllm/pull/7097
* [ci][distributed] disable ray dag tests by @youkaichao in https://github.com/vllm-project/vllm/pull/7099
* [Model] Refactor and decouple weight loading logic for InternVL2 model by @Isotr0py in https://github.com/vllm-project/vllm/pull/7067
* [Bugfix] Fix block table for seqs that have prefix cache hits by @zachzzc in https://github.com/vllm-project/vllm/pull/7018
* [LoRA]  ReplicatedLinear support LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/7081
* [CI] Temporarily turn off H100 performance benchmark by @KuntaiDu in https://github.com/vllm-project/vllm/pull/7104
* [ci][test] finalize fork_new_process_for_each_test by @youkaichao in https://github.com/vllm-project/vllm/pull/7114
* [Frontend] Warn if user `max_model_len` is greater than derived `max_model_len` by @fialhocoelho in https://github.com/vllm-project/vllm/pull/7080
* Support for guided decoding for offline LLM by @kevinbu233 in https://github.com/vllm-project/vllm/pull/6878
* [misc] add zmq in collect env by @youkaichao in https://github.com/vllm-project/vllm/pull/7119
* [core][misc] simply output processing with shortcut for non-parallel sampling and non-beam search usecase by @youkaichao in https://github.com/vllm-project/vllm/pull/7117
* [Model]Refactor MiniCPMV by @jeejeelee in https://github.com/vllm-project/vllm/pull/7020
* [Bugfix] [SpecDecode] Default speculative_draft_tensor_parallel_size to 1 when using MLPSpeculator by @tdoublep in https://github.com/vllm-project/vllm/pull/7105
* [misc][distributed] improve libcudart.so finding by @youkaichao in https://github.com/vllm-project/vllm/pull/7127
* Clean up remaining Punica C information by @jeejeelee in https://github.com/vllm-project/vllm/pull/7027
* [Model] Add multi-image support for minicpmv offline inference by @HwwwwwwwH in https://github.com/vllm-project/vllm/pull/7122
* [Frontend] Reapply "Factor out code for running uvicorn" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/7095
* [Model] SiglipVisionModel ported from transformers by @ChristopherCho in https://github.com/vllm-project/vllm/pull/6942
* [Speculative decoding] Add periodic log with time spent in proposal/scoring/verification by @cadedaniel in https://github.com/vllm-project/vllm/pull/6963
* [SpecDecode] Support FlashInfer in DraftModelRunner by @bong-furiosa in https://github.com/vllm-project/vllm/pull/6926
* [BugFix] Use IP4 localhost form for zmq bind by @njhill in https://github.com/vllm-project/vllm/pull/7163
* [BugFix] Use args.trust_remote_code by @VastoLorde95 in https://github.com/vllm-project/vllm/pull/7121
* [Misc] Fix typo in GroupCoordinator.recv() by @ruisearch42 in https://github.com/vllm-project/vllm/pull/7167
* [Kernel] Update CUTLASS to 3.5.1 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/7085
* [CI/Build] Suppress divide-by-zero and missing return statement warnings by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/7001
* [Bugfix][CI/Build] Fix CUTLASS FetchContent by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/7171
* bump version to v0.5.4 by @simon-mo in https://github.com/vllm-project/vllm/pull/7139

## New Contributors
* @yecohn made their first contribution in https://github.com/vllm-project/vllm/pull/6652
* @thesues made their first contribution in https://github.com/vllm-project/vllm/pull/5753
* @luizanao made their first contribution in https://github.com/vllm-project/vllm/pull/6748
* @ezliu made their first contribution in https://github.com/vllm-project/vllm/pull/6626
* @HwwwwwwwH made their first contribution in https://github.com/vllm-project/vllm/pull/4087
* @LucasWilkinson made their first contribution in https://github.com/vllm-project/vllm/pull/6798
* @qingquansong made their first contribution in https://github.com/vllm-project/vllm/pull/6793
* @eaplatanios made their first contribution in https://github.com/vllm-project/vllm/pull/6770
* @gurpreet-dhami made their first contribution in https://github.com/vllm-project/vllm/pull/6847
* @omrishiv made their first contribution in https://github.com/vllm-project/vllm/pull/6844
* @cw75 made their first contribution in https://github.com/vllm-project/vllm/pull/6857
* @zeyugao made their first contribution in https://github.com/vllm-project/vllm/pull/6871
* @etwk made their first contribution in https://github.com/vllm-project/vllm/pull/6891
* @fzyzcjy made their first contribution in https://github.com/vllm-project/vllm/pull/6949
* @FeiDeng made their first contribution in https://github.com/vllm-project/vllm/pull/6927
* @HandH1998 made their first contribution in https://github.com/vllm-project/vllm/pull/5218
* @xuyi made their first contribution in https://github.com/vllm-project/vllm/pull/6924
* @bong-furiosa made their first contribution in https://github.com/vllm-project/vllm/pull/7065
* @zachzzc made their first contribution in https://github.com/vllm-project/vllm/pull/7018
* @fialhocoelho made their first contribution in https://github.com/vllm-project/vllm/pull/7080
* @ChristopherCho made their first contribution in https://github.com/vllm-project/vllm/pull/6942
* @VastoLorde95 made their first contribution in https://github.com/vllm-project/vllm/pull/7121

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.3...v0.5.4

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.4)

---

## v0.5.3.post1: v0.5.3.post1
**Published:** 2024-07-23

## Highlights
* We fixed an configuration incompatibility between vLLM (which tested against pre-released version) and the published Meta Llama 3.1 weights (#6693)

## What's Changed
* [Docs] Announce llama3.1 support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6688
* [doc][distributed] fix doc argument order by @youkaichao in https://github.com/vllm-project/vllm/pull/6691
* [Bugfix] Fix a log error in chunked prefill by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6694
* [BugFix] Fix RoPE error in Llama 3.1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6693
* Bump version to 0.5.3.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/6696


**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.3...v0.5.3.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.3.post1)

---

## v0.5.3: v0.5.3
**Published:** 2024-07-23

## Highlights

### Model Support
* vLLM now supports Meta Llama 3.1! Please checkout our blog [here](https://blog.vllm.ai/2024/07/23/llama31.html) for initial details on running the model. 
	* Please checkout [this thread](https://github.com/vllm-project/vllm/issues/6689) for any known issues related to the model. 
	* The model runs on a single 8xH100 or 8xA100 node using FP8 quantization (#6606, #6547, #6487, #6593, #6511, #6515, #6552)
	* The BF16 version of the model should run on multiple nodes using pipeline parallelism ([docs](https://docs.vllm.ai/en/latest/serving/distributed_serving.html)). If you have fast network interconnect, you might want to consider full tensor paralellism as well. (#6599, #6598, #6529, #6569)
	* In order to support long context, a new rope extension method has been added and chunked prefill has been turned on by default for Meta Llama 3.1 series of model. (#6666, #6553, #6673)
* Support Mistral-Nemo (#6548)
* Support Chameleon (#6633, #5770)
* Pipeline parallel support for Mixtral (#6516)

### Hardware Support
* Many enhancements to TPU support. (#6277, #6457, #6506, #6504)

### Performance Enhancements
* Add AWQ support to the Marlin kernel. This brings significant (1.5-2x) perf improvements to existing AWQ models! (#6612)
* Progress towards refactoring for SPMD worker execution. (#6032)
* Progress in improving prepare inputs procedure. (#6164, #6338, #6596)
* Memory optimization for pipeline parallelism. (#6455)


### Production Engine
* Correctness testing for pipeline parallel and CPU offloading (#6410, #6549)
* Support dynamically loading Lora adapter from HuggingFace (#6234)
* Pipeline Parallel using stdlib multiprocessing module (#6130)


### Others
* A CPU offloading implementation, you can now use `--cpu-offload-gb` to control how much memory to "extend" the RAM with. (#6496)
* The new `vllm` CLI is now ready for testing. It comes with three commands: `serve`, `complete`, and `chat`. Feedback and improvements are greatly welcomed! (#6431)
* The wheels now build on Ubuntu 20.04 instead of 22.04. (#6517)



## What's Changed
* [Docs] Add Google Cloud to sponsor list by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6450
* [Misc] Add CustomOp Interface to UnquantizedFusedMoEMethod by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6289
* [CI/Build][TPU] Add TPU CI test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6277
* Pin sphinx-argparse version by @khluu in https://github.com/vllm-project/vllm/pull/6453
* [BugFix][Model] Jamba - Handle aborted requests, Add tests and fix cleanup bug by @mzusman in https://github.com/vllm-project/vllm/pull/6425
* [Bugfix][CI/Build] Test prompt adapters in openai entrypoint tests by @g-eoj in https://github.com/vllm-project/vllm/pull/6419
* [Docs] Announce 5th meetup by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6458
* [CI/Build] vLLM cache directory for images by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6444
* [Frontend] Support for chat completions input in the tokenize endpoint by @sasha0552 in https://github.com/vllm-project/vllm/pull/5923
* [Misc] Fix typos in spec. decode metrics logging. by @tdoublep in https://github.com/vllm-project/vllm/pull/6470
* [Core] Use numpy to speed up padded token processing by @peng1999 in https://github.com/vllm-project/vllm/pull/6442
* [CI/Build] Remove "boardwalk" image asset by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6460
* [doc][misc] remind users to cancel debugging environment variables after debugging by @youkaichao in https://github.com/vllm-project/vllm/pull/6481
* [Hardware][TPU] Support MoE with Pallas GMM kernel  by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6457
* [Doc] Fix the lora adapter path in server startup script by @Jeffwan in https://github.com/vllm-project/vllm/pull/6230
* [Misc] Log spec decode metrics by @comaniac in https://github.com/vllm-project/vllm/pull/6454
* [Kernel][Attention] Separate `Attention.kv_scale` into `k_scale` and `v_scale` by @mgoin in https://github.com/vllm-project/vllm/pull/6081
* [ci][distributed] add pipeline parallel correctness test by @youkaichao in https://github.com/vllm-project/vllm/pull/6410
* [misc][distributed] improve tests by @youkaichao in https://github.com/vllm-project/vllm/pull/6488
* [misc][distributed] add seed to dummy weights by @youkaichao in https://github.com/vllm-project/vllm/pull/6491
* [Distributed][Model] Rank-based Component Creation for Pipeline Parallelism Memory Optimization by @wushidonguc in https://github.com/vllm-project/vllm/pull/6455
* [ROCm] Cleanup Dockerfile and remove outdated patch by @hongxiayang in https://github.com/vllm-project/vllm/pull/6482
* [Misc][Speculative decoding] Typos and typing fixes by @ShangmingCai in https://github.com/vllm-project/vllm/pull/6467
* [Doc][CI/Build] Update docs and tests to use `vllm serve` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6431
* [Bugfix] Fix for multinode crash on 4 PP by @andoorve in https://github.com/vllm-project/vllm/pull/6495
* [TPU] Remove multi-modal args in TPU backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6504
* [Misc] Use `torch.Tensor` for type annotation by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6505
* [Core] Refactor _prepare_model_input_tensors - take 2 by @comaniac in https://github.com/vllm-project/vllm/pull/6164
* [DOC] - Add docker image to Cerebrium Integration by @milo157 in https://github.com/vllm-project/vllm/pull/6510
* [Bugfix] Fix Ray Metrics API usage by @Yard1 in https://github.com/vllm-project/vllm/pull/6354
* [Core] draft_model_runner: Implement prepare_inputs on GPU for advance_step by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/6338
* [ Kernel ] FP8 Dynamic-Per-Token Quant Kernel by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6511
* [Model] Pipeline parallel support for Mixtral by @comaniac in https://github.com/vllm-project/vllm/pull/6516
* [ Kernel ] Fp8 Channelwise Weight Support by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6487
* [core][model] yet another cpu offload implementation by @youkaichao in https://github.com/vllm-project/vllm/pull/6496
* [BugFix] Avoid secondary error in ShmRingBuffer destructor by @njhill in https://github.com/vllm-project/vllm/pull/6530
* [Core] Introduce SPMD worker execution using Ray accelerated DAG by @ruisearch42 in https://github.com/vllm-project/vllm/pull/6032
* [Misc] Minor patch for draft model runner by @comaniac in https://github.com/vllm-project/vllm/pull/6523
* [BugFix][Frontend] Use LoRA tokenizer in OpenAI APIs by @njhill in https://github.com/vllm-project/vllm/pull/6227
* [Bugfix] Update flashinfer.py with PagedAttention forwards - Fixes Gemma2 OpenAI Server Crash by @noamgat in https://github.com/vllm-project/vllm/pull/6501
* [TPU] Refactor TPU worker & model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6506
* [ Misc ] Improve Min Capability Checking in `compressed-tensors` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6522
* [ci] Reword Github bot comment  by @khluu in https://github.com/vllm-project/vllm/pull/6534
* [Model] Support Mistral-Nemo by @mgoin in https://github.com/vllm-project/vllm/pull/6548
* Fix PR comment bot by @khluu in https://github.com/vllm-project/vllm/pull/6554
* [ci][test] add correctness test for cpu offloading by @youkaichao in https://github.com/vllm-project/vllm/pull/6549
* [Kernel] Implement fallback for FP8 channelwise using torch._scaled_mm by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6552
* [CI/Build] Build on Ubuntu 20.04 instead of 22.04 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6517
* Add support for a rope extension method by @simon-mo in https://github.com/vllm-project/vllm/pull/6553
* [Core] Multiprocessing Pipeline Parallel support by @njhill in https://github.com/vllm-project/vllm/pull/6130
* [Bugfix] Make spec. decode respect per-request seed. by @tdoublep in https://github.com/vllm-project/vllm/pull/6034
* [ Misc ] non-uniform quantization via `compressed-tensors` for `Llama` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6515
* [Bugfix][Frontend] Fix missing `/metrics` endpoint by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6463
* [BUGFIX] Raise an error for no draft token case when draft_tp>1 by @wooyeonlee0 in https://github.com/vllm-project/vllm/pull/6369
* [Model] RowParallelLinear: pass bias to quant_method.apply  by @tdoublep in https://github.com/vllm-project/vllm/pull/6327
* [Bugfix][Frontend] remove duplicate init logger by @dtrifiro in https://github.com/vllm-project/vllm/pull/6581
* [Misc] Small perf improvements by @Yard1 in https://github.com/vllm-project/vllm/pull/6520
* [Docs] Update docs for wheel location by @simon-mo in https://github.com/vllm-project/vllm/pull/6580
* [Bugfix] [SpecDecode] AsyncMetricsCollector: update time since last collection by @tdoublep in https://github.com/vllm-project/vllm/pull/6578
* [bugfix][distributed] fix multi-node bug for shared memory by @youkaichao in https://github.com/vllm-project/vllm/pull/6597
* [ Kernel ] Enable Dynamic Per Token `fp8` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6547
* [Docs] Update PP docs by @andoorve in https://github.com/vllm-project/vllm/pull/6598
* [build] add ib so that multi-node support with infiniband can be supported out-of-the-box by @youkaichao in https://github.com/vllm-project/vllm/pull/6599
* [ Kernel ] FP8 Dynamic Per Token Quant - Add scale_ub by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/6593
* [Core] Allow specifying custom Executor by @Yard1 in https://github.com/vllm-project/vllm/pull/6557
* [Bugfix][Core]: Guard for KeyErrors that can occur if a request is aborted with Pipeline Parallel by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6587
* [Misc] Consolidate and optimize logic for building padded tensors by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6541
* [ Misc ] `fbgemm` checkpoints by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6559
* [Bugfix][CI/Build][Hardware][AMD] Fix AMD tests, add HF cache, update CK FA, add partially supported model notes by @mawong-amd in https://github.com/vllm-project/vllm/pull/6543
* [ Kernel ] Enable `fp8-marlin` for `fbgemm-fp8` models by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6606
* [Misc] Fix input_scale typing in w8a8_utils.py by @mgoin in https://github.com/vllm-project/vllm/pull/6579
* [ Bugfix ] Fix AutoFP8 fp8 marlin by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6609
* [Frontend] Move chat utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6602
* [Spec Decode] Disable Log Prob serialization to CPU for spec decoding for both draft and target models. by @sroy745 in https://github.com/vllm-project/vllm/pull/6485
* [Misc] Remove abused noqa by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6619
* [Model] Refactor and decouple phi3v image embedding by @Isotr0py in https://github.com/vllm-project/vllm/pull/6621
* [Kernel][Core] Add AWQ support to the Marlin kernel  by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/6612
* [Model] Initial Support for Chameleon by @ywang96 in https://github.com/vllm-project/vllm/pull/5770
* [Misc] Add a wrapper for torch.inference_mode by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6618
* [Bugfix] Fix `vocab_size` field access in LLaVA models by @jaywonchung in https://github.com/vllm-project/vllm/pull/6624
* [Frontend] Refactor prompt processing by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4028
* [Bugfix][Kernel] Use int64_t for indices in fp8 quant kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6649
* [ci] Use different sccache bucket for CUDA 11.8 wheel build by @khluu in https://github.com/vllm-project/vllm/pull/6656
* [Core] Support dynamically loading Lora adapter from HuggingFace by @Jeffwan in https://github.com/vllm-project/vllm/pull/6234
* [ci][build] add back vim in docker by @youkaichao in https://github.com/vllm-project/vllm/pull/6661
* [Misc] Remove deprecation warning for beam search by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6659
* [Core] Modulize prepare input and attention metadata builder by @comaniac in https://github.com/vllm-project/vllm/pull/6596
* [Bugfix] Fix null `modules_to_not_convert`  in FBGEMM Fp8 quantization by @cli99 in https://github.com/vllm-project/vllm/pull/6665
* [Misc] Enable chunked prefill by default for long context models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6666
* [misc] add start loading models for users information by @youkaichao in https://github.com/vllm-project/vllm/pull/6670
* add tqdm when loading checkpoint shards by @zhaotyer in https://github.com/vllm-project/vllm/pull/6569
* [Misc] Support FP8 kv cache scales from compressed-tensors by @mgoin in https://github.com/vllm-project/vllm/pull/6528
* [doc][distributed] add more doc for setting up multi-node environment by @youkaichao in https://github.com/vllm-project/vllm/pull/6529
* [Misc] Manage HTTP connections in one place by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6600
* [misc] only tqdm for first rank by @youkaichao in https://github.com/vllm-project/vllm/pull/6672
* [VLM][Model] Support image input for Chameleon  by @ywang96 in https://github.com/vllm-project/vllm/pull/6633
* support ignore patterns in model loader by @simon-mo in https://github.com/vllm-project/vllm/pull/6673
* Bump version to v0.5.3 by @simon-mo in https://github.com/vllm-project/vllm/pull/6674

## New Contributors
* @g-eoj made their first contribution in https://github.com/vllm-project/vllm/pull/6419
* @peng1999 made their first contribution in https://github.com/vllm-project/vllm/pull/6442
* @Jeffwan made their first contribution in https://github.com/vllm-project/vllm/pull/6230
* @wushidonguc made their first contribution in https://github.com/vllm-project/vllm/pull/6455
* @ShangmingCai made their first contribution in https://github.com/vllm-project/vllm/pull/6467
* @ruisearch42 made their first contribution in https://github.com/vllm-project/vllm/pull/6032

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.2...v0.5.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.3)

---

## v0.5.2: v0.5.2
**Published:** 2024-07-15

## Major Changes
* â—Planned breaking change â—: we plan to remove beam search (see more in #6226) in the next few releases. This release come with a warning when beam search is enabled for the request. Please voice your concern in the RFC if you do have a valid use case for beam search in vLLM
* The release has moved to a Python version agnostic wheel (#6394). A single wheel can be installed across Python versions vLLM supports.

### Highlights

#### Model Support
* Add PaliGemma (#5189), Fuyu-8B (#3924)
* Support for soft tuned prompts (#4645)
* A [new guide](https://docs.vllm.ai/en/latest/dev/multimodal/adding_multimodal_plugin.html) for adding multi-modal plugins (#6205)

#### Hardware
* AMD: unify CUDA_VISIBLE_DEVICES usage (#6352)

#### Performance
* ZeroMQ fallback for broadcasting large objects (#6183)
* Simplify code to support pipeline parallel (#6406)
* Turn off CUTLASS scaled_mm for Ada Lovelace (#6384)
* Use CUTLASS kernels for the FP8 layers with Bias (#6270)

#### Features
* Enabling bonus token in speculative decoding for KV cache based models (#5765)
* Medusa Implementation with Top-1 proposer (#4978)
* An experimental vLLM CLI for serving and querying OpenAI compatible server (#5090)

#### Others
* Add support for multi-node on CI (#5955)
* Benchmark: add H100 suite (#6047)
* [CI/Build] Add nightly benchmarking for tgi, tensorrt-llm and lmdeploy (#5362)
* Build some nightly wheels (#6380)



## What's Changed
* Update wheel builds to strip debug by @simon-mo in https://github.com/vllm-project/vllm/pull/6161
* Fix release wheel build env var by @simon-mo in https://github.com/vllm-project/vllm/pull/6162
* Move release wheel env var to Dockerfile instead by @simon-mo in https://github.com/vllm-project/vllm/pull/6163
* [Doc] Reorganize Supported Models by Type by @ywang96 in https://github.com/vllm-project/vllm/pull/6167
* [Doc] Move guide for multimodal model and other improvements by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6168
* [Model] Add PaliGemma by @ywang96 in https://github.com/vllm-project/vllm/pull/5189
* add benchmark for fix length input and output by @haichuan1221 in https://github.com/vllm-project/vllm/pull/5857
* [ Misc ] Support Fp8 via `llm-compressor` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6110
* [misc][frontend] log all available endpoints by @youkaichao in https://github.com/vllm-project/vllm/pull/6195
* do not exclude `object` field in CompletionStreamResponse by @kczimm in https://github.com/vllm-project/vllm/pull/6196
* [Bugfix] FIx benchmark args for randomly sampled dataset by @haichuan1221 in https://github.com/vllm-project/vllm/pull/5947
* [Kernel] reloading fused_moe config on the last chunk by @avshalomman in https://github.com/vllm-project/vllm/pull/6210
* [Kernel] Correctly invoke prefill & decode kernels for cross-attention (towards eventual encoder/decoder model support) by @afeldman-nm in https://github.com/vllm-project/vllm/pull/4888
* [Bugfix] use diskcache in outlines _get_guide  #5436  by @ericperfect in https://github.com/vllm-project/vllm/pull/6203
* [Bugfix] Mamba cache Cuda Graph padding by @tomeras91 in https://github.com/vllm-project/vllm/pull/6214
* Add FlashInfer to default Dockerfile by @simon-mo in https://github.com/vllm-project/vllm/pull/6172
* [hardware][cuda] use device id under CUDA_VISIBLE_DEVICES for get_device_capability by @youkaichao in https://github.com/vllm-project/vllm/pull/6216
* [core][distributed] fix ray worker rank assignment by @youkaichao in https://github.com/vllm-project/vllm/pull/6235
* [Bugfix][TPU] Add missing None to model input by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6245
* [Bugfix][TPU] Fix outlines installation in TPU Dockerfile by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6256
* Add support for multi-node on CI by @khluu in https://github.com/vllm-project/vllm/pull/5955
* [CORE] Adding support for insertion of soft-tuned prompts by @SwapnilDreams100 in https://github.com/vllm-project/vllm/pull/4645
* [Docs] Docs update for Pipeline Parallel by @andoorve in https://github.com/vllm-project/vllm/pull/6222
* [Bugfix]fix and needs_scalar_to_array logic check by @qibaoyuan in https://github.com/vllm-project/vllm/pull/6238
* [Speculative Decoding] Medusa Implementation with Top-1 proposer by @abhigoyal1997 in https://github.com/vllm-project/vllm/pull/4978
* [core][distributed] add zmq fallback for broadcasting large objects by @youkaichao in https://github.com/vllm-project/vllm/pull/6183
* [Bugfix][TPU] Add prompt adapter methods to TPUExecutor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6279
* [Doc] Guide for adding multi-modal plugins by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6205
* [Bugfix] Support 2D input shape in MoE layer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6287
* [Bugfix] MLPSpeculator: Use ParallelLMHead in tie_weights=False case. by @tdoublep in https://github.com/vllm-project/vllm/pull/6303
* [CI/Build] Enable mypy typing for remaining folders by @bmuskalla in https://github.com/vllm-project/vllm/pull/6268
* [Bugfix] OpenVINOExecutor abstractmethod error by @park12sj in https://github.com/vllm-project/vllm/pull/6296
* [Speculative Decoding] Enabling bonus token in speculative decoding for KV cache based models by @sroy745 in https://github.com/vllm-project/vllm/pull/5765
* [Bugfix][Neuron] Fix soft prompt method error in NeuronExecutor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6313
* [Doc] Remove comments incorrectly copied from another project by @daquexian in https://github.com/vllm-project/vllm/pull/6286
* [Doc] Update description of vLLM support for CPUs by @DamonFool in https://github.com/vllm-project/vllm/pull/6003
* [BugFix]: set outlines pkg version by @xiangyang-95 in https://github.com/vllm-project/vllm/pull/6262
* [Bugfix] Fix snapshot download in serving benchmark by @ywang96 in https://github.com/vllm-project/vllm/pull/6318
* [Misc] refactor(config): clean up unused code by @aniaan in https://github.com/vllm-project/vllm/pull/6320
* [BugFix]: fix engine timeout due to request abort by @pushan01 in https://github.com/vllm-project/vllm/pull/6255
* [Bugfix] GPTBigCodeForCausalLM: Remove lm_head from supported_lora_modules. by @tdoublep in https://github.com/vllm-project/vllm/pull/6326
* [BugFix] get_and_reset only when scheduler outputs are not empty by @mzusman in https://github.com/vllm-project/vllm/pull/6266
* [ Misc ] Refactor Marlin Python Utilities by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6082
* Benchmark: add H100 suite by @simon-mo in https://github.com/vllm-project/vllm/pull/6047
* [bug fix] Fix llava next feature size calculation. by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/6339
* [doc] update pipeline parallel in readme by @youkaichao in https://github.com/vllm-project/vllm/pull/6347
* [CI/Build] Add nightly benchmarking for tgi, tensorrt-llm and lmdeploy by @KuntaiDu in https://github.com/vllm-project/vllm/pull/5362
* [ BugFix ] Prompt Logprobs Detokenization by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6223
* [Misc] Remove flashinfer warning, add flashinfer tests to CI by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/6351
* [distributed][misc] keep consistent with how pytorch finds libcudart.so by @youkaichao in https://github.com/vllm-project/vllm/pull/6346
* [Bugfix] Fix usage stats logging exception warning with OpenVINO by @helena-intel in https://github.com/vllm-project/vllm/pull/6349
* [Model][Phi3-Small] Remove scipy from blocksparse_attention by @mgoin in https://github.com/vllm-project/vllm/pull/6343
* [CI/Build] (2/2) Switching AMD CI to store images in Docker Hub by @adityagoel14 in https://github.com/vllm-project/vllm/pull/6350
* [ROCm][AMD][Bugfix] unify CUDA_VISIBLE_DEVICES usage in vllm to get device count and fixed navi3x  by @hongxiayang in https://github.com/vllm-project/vllm/pull/6352
* [ Misc ] Remove separate bias add by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6353
* [Misc][Bugfix] Update transformers for tokenizer issue by @ywang96 in https://github.com/vllm-project/vllm/pull/6364
* [ Misc ] Support Models With Bias in `compressed-tensors` integration by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6356
* [Bugfix] Fix dtype mismatch in PaliGemma by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6367
* [Build/CI] Checking/Waiting for the GPU's clean state by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/6379
* [Misc] add fixture to guided processor tests by @kevinbu233 in https://github.com/vllm-project/vllm/pull/6341
* [ci] Add grouped tests & mark tests to run by default for fastcheck pipeline by @khluu in https://github.com/vllm-project/vllm/pull/6365
* [ci] Add GHA workflows to enable full CI run by @khluu in https://github.com/vllm-project/vllm/pull/6381
* [MISC] Upgrade dependency to PyTorch 2.3.1 by @comaniac in https://github.com/vllm-project/vllm/pull/5327
* Build some nightly wheels by default by @simon-mo in https://github.com/vllm-project/vllm/pull/6380
* Fix release-pipeline.yaml by @simon-mo in https://github.com/vllm-project/vllm/pull/6388
* Fix interpolation in release pipeline by @simon-mo in https://github.com/vllm-project/vllm/pull/6389
* Fix release pipeline's -e flag by @simon-mo in https://github.com/vllm-project/vllm/pull/6390
* [Bugfix] Fix illegal memory access in FP8 MoE kernel by @comaniac in https://github.com/vllm-project/vllm/pull/6382
* [Misc] Add generated git commit hash as `vllm.__commit__` by @mgoin in https://github.com/vllm-project/vllm/pull/6386
* Fix release pipeline's dir permission by @simon-mo in https://github.com/vllm-project/vllm/pull/6391
* [Bugfix][TPU] Fix megacore setting for v5e-litepod by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6397
* [ci] Fix wording for GH bot by @khluu in https://github.com/vllm-project/vllm/pull/6398
* [Doc] Fix Typo in Doc by @esaliya in https://github.com/vllm-project/vllm/pull/6392
* [Bugfix] Fix hard-coded value of x in context_attention_fwd by @tdoublep in https://github.com/vllm-project/vllm/pull/6373
* [Docs] Clean up latest news by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6401
* [ci] try to add multi-node tests by @youkaichao in https://github.com/vllm-project/vllm/pull/6280
* Updating LM Format Enforcer version to v10.3 by @noamgat in https://github.com/vllm-project/vllm/pull/6411
* [ Misc ] More Cleanup of Marlin by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6359
* [Misc] Add deprecation warning for beam search by @WoosukKwon in https://github.com/vllm-project/vllm/pull/6402
* [ Misc ] Apply MoE Refactor to Qwen2 + Deepseekv2 To Support Fp8 by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6417
* [Model] Initialize Fuyu-8B support by @Isotr0py in https://github.com/vllm-project/vllm/pull/3924
* Remove unnecessary trailing period in spec_decode.rst by @terrytangyuan in https://github.com/vllm-project/vllm/pull/6405
* [Kernel] Turn off CUTLASS scaled_mm for Ada Lovelace by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6384
* [ci][build] fix commit id by @youkaichao in https://github.com/vllm-project/vllm/pull/6420
* [ Misc ] Enable Quantizing All Layers of DeekSeekv2 by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6423
* [Feature] vLLM CLI for serving and querying OpenAI compatible server by @EthanqX in https://github.com/vllm-project/vllm/pull/5090
* [Doc] xpu backend requires running setvars.sh by @rscohn2 in https://github.com/vllm-project/vllm/pull/6393
* [CI/Build] Cross python wheel by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6394
* [Bugfix] Benchmark serving script used global parameter 'args' in function 'sample_random_requests' by @lxline in https://github.com/vllm-project/vllm/pull/6428
* Report usage for beam search by @simon-mo in https://github.com/vllm-project/vllm/pull/6404
* Add FUNDING.yml by @simon-mo in https://github.com/vllm-project/vllm/pull/6435
* [BugFix] BatchResponseData body should be optional by @zifeitong in https://github.com/vllm-project/vllm/pull/6345
* [Doc] add env docs for flashinfer backend by @DefTruth in https://github.com/vllm-project/vllm/pull/6437
* [core][distributed] simplify code to support pipeline parallel by @youkaichao in https://github.com/vllm-project/vllm/pull/6406
* [Bugfix] Convert image to RGB by default by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6430
* [doc][misc] doc update by @youkaichao in https://github.com/vllm-project/vllm/pull/6439
* [VLM] Minor space optimization for `ClipVisionModel` by @ywang96 in https://github.com/vllm-project/vllm/pull/6436
* [doc][distributed] add suggestion for distributed inference by @youkaichao in https://github.com/vllm-project/vllm/pull/6418
* [Kernel] Use CUTLASS kernels for the FP8 layers with Bias by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/6270
* [Misc] Use 0.0.9 version for flashinfer by @Pernekhan in https://github.com/vllm-project/vllm/pull/6447
* [Bugfix] Add custom Triton cache manager to resolve MoE MP issue  by @tdoublep in https://github.com/vllm-project/vllm/pull/6140
* [Bugfix] use float32 precision in samplers/test_logprobs.py for comparing with HF  by @tdoublep in https://github.com/vllm-project/vllm/pull/6409
* bump version to v0.5.2 by @simon-mo in https://github.com/vllm-project/vllm/pull/6433
* [misc][distributed] fix pp missing layer condition by @youkaichao in https://github.com/vllm-project/vllm/pull/6446

## New Contributors
* @haichuan1221 made their first contribution in https://github.com/vllm-project/vllm/pull/5857
* @kczimm made their first contribution in https://github.com/vllm-project/vllm/pull/6196
* @ericperfect made their first contribution in https://github.com/vllm-project/vllm/pull/6203
* @qibaoyuan made their first contribution in https://github.com/vllm-project/vllm/pull/6238
* @abhigoyal1997 made their first contribution in https://github.com/vllm-project/vllm/pull/4978
* @bmuskalla made their first contribution in https://github.com/vllm-project/vllm/pull/6268
* @park12sj made their first contribution in https://github.com/vllm-project/vllm/pull/6296
* @daquexian made their first contribution in https://github.com/vllm-project/vllm/pull/6286
* @xiangyang-95 made their first contribution in https://github.com/vllm-project/vllm/pull/6262
* @aniaan made their first contribution in https://github.com/vllm-project/vllm/pull/6320
* @pushan01 made their first contribution in https://github.com/vllm-project/vllm/pull/6255
* @helena-intel made their first contribution in https://github.com/vllm-project/vllm/pull/6349
* @adityagoel14 made their first contribution in https://github.com/vllm-project/vllm/pull/6350
* @kevinbu233 made their first contribution in https://github.com/vllm-project/vllm/pull/6341
* @esaliya made their first contribution in https://github.com/vllm-project/vllm/pull/6392
* @EthanqX made their first contribution in https://github.com/vllm-project/vllm/pull/5090
* @rscohn2 made their first contribution in https://github.com/vllm-project/vllm/pull/6393
* @lxline made their first contribution in https://github.com/vllm-project/vllm/pull/6428

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.1...v0.5.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.2)

---

## v0.5.1: v0.5.1
**Published:** 2024-07-05

## Highlights
* vLLM now has pipeline parallelism! (#4412, #5408, #6115, #6120). You can now run the API server with `--pipeline-parallel-size`. This feature is in early stage, please let us know your feedback. 


### Model Support
* Support Gemma 2 (#5908, #6051). Please note that for correctness, Gemma should run with FlashInfer backend which supports logits soft cap. The wheels for FlashInfer can be downloaded [here](https://github.com/flashinfer-ai/flashinfer/releases/tag/v0.0.8)
* Support Jamba (#4115). This is vLLM's first state space model! 
* Support Deepseek-V2 (#4650). Please note that MLA (Multi-head Latent Attention) is not implemented and we are looking for contribution!
* Vision Language Model adding support for Phi3-Vision, dynamic image size, and a registry for processing model inputs (#4986, #5276, #5214)
  * Notably, it has a **breaking change** that all VLM specific arguments are now removed from engine APIs so you no longer need to set it globally via CLI. However, you now only need to pass in `<image>` into the prompt instead of complicated prompt formatting. See more [here](https://docs.vllm.ai/en/latest/models/vlm.html#offline-batched-inference)
  * There is also a new [guide](https://docs.vllm.ai/en/latest/models/enabling_multimodal_inputs.html) on adding VLMs! We would love your contribution for new models! 

### Hardware Support
* Enhancement to TPU support (#5292, #5878, #5850, #5831, #5855)
* OpenVINO backend (#5379)


### Production Service
* Support for sharded tensorized models (#4990)
* Continous streaming of OpenAI response token stats (#5742)


### Performance
* Enhancement in distributed communication via shared memory (#5399)
* Latency enhancement in block manager (#5584)
* Enhancements to `compressed-tensors` supporting Marlin, W4A16 (#5435, #5385)
* Faster FP8 quantize kernel (#5396), FP8 on Ampere (#5975)
* Option to use FlashInfer for prefill, decode, and CUDA Graph for decode (#4628)
* Speculative Decoding
	* MLPSpeculator (#4947, #6050)
	* Typical Acceptance Sampler (#5131, #5348)
* Draft Model Runner (#5799)


### Development Productivity
* Post merge benchmark is now available at perf.vllm.ai! 
* Addition of A100 in CI environment (#5658)
* Step towards nightly wheel publication (#5610)



## What's Changed
* [CI/Build] Add `is_quant_method_supported` to control quantization test configurations by @mgoin in https://github.com/vllm-project/vllm/pull/5253
* Revert "[CI/Build] Add `is_quant_method_supported` to control quantization test configurations" by @simon-mo in https://github.com/vllm-project/vllm/pull/5463
* [CI] Upgrade codespell version. by @rkooo567 in https://github.com/vllm-project/vllm/pull/5381
* [Hardware] Initial TPU integration by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5292
* [Bugfix] Add device assertion to TorchSDPA by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/5402
* [ci] Add AMD, Neuron, Intel tests for AWS CI and turn off default soft fail for GPU tests by @khluu in https://github.com/vllm-project/vllm/pull/5464
* [Kernel] Vectorized FP8 quantize kernel by @comaniac in https://github.com/vllm-project/vllm/pull/5396
* [Bugfix] TYPE_CHECKING for MultiModalData by @kimdwkimdw in https://github.com/vllm-project/vllm/pull/5444
* [Frontend] [Core] Support for sharded tensorized models by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/4990
* [misc] add hint for AttributeError by @youkaichao in https://github.com/vllm-project/vllm/pull/5462
* [Doc] Update debug docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5438
* [Bugfix] Fix typo in scheduler.py (requeset -> request) by @mgoin in https://github.com/vllm-project/vllm/pull/5470
* [Frontend] Add "input speed" to tqdm postfix alongside output speed by @mgoin in https://github.com/vllm-project/vllm/pull/5425
* [Bugfix] Fix wrong multi_modal_input format for CPU runner by @Isotr0py in https://github.com/vllm-project/vllm/pull/5451
* [Core][Distributed] add coordinator to reduce code duplication in tp and pp by @youkaichao in https://github.com/vllm-project/vllm/pull/5293
* [ci] Use sccache to build images by @khluu in https://github.com/vllm-project/vllm/pull/5419
* [Bugfix]if the content is started with ":"(response of ping), client should iâ€¦ by @sywangyi in https://github.com/vllm-project/vllm/pull/5303
* [Kernel] `w4a16` support for `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/5385
* [CI/Build][REDO] Add is_quant_method_supported to control quantization test configurations by @mgoin in https://github.com/vllm-project/vllm/pull/5466
* [Kernel] Tune Qwen2MoE kernel configurations with tp2,4 by @wenyujin333 in https://github.com/vllm-project/vllm/pull/5497
* [Hardware][Intel] Optimize CPU backend and add more performance tips by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/4971
* [Docs] Add 4th meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5509
* [Misc] Add vLLM version getter to utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5098
* [CI/Build] Simplify OpenAI server setup in tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5100
* [Doc] Update LLaVA docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5437
* [Kernel] Factor out epilogues from cutlass kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5391
* [MISC] Remove FP8 warning by @comaniac in https://github.com/vllm-project/vllm/pull/5472
* Seperate dev requirements into lint and test by @Yard1 in https://github.com/vllm-project/vllm/pull/5474
* Revert "[Core] Remove unnecessary copies in flash attn backend" by @Yard1 in https://github.com/vllm-project/vllm/pull/5478
* [misc] fix format.sh by @youkaichao in https://github.com/vllm-project/vllm/pull/5511
* [CI/Build] Disable test_fp8.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5508
* [Kernel] Disable CUTLASS kernels for fp8 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5505
* Add `cuda_device_count_stateless` by @Yard1 in https://github.com/vllm-project/vllm/pull/5473
* [Hardware][Intel] Support CPU inference with AVX2 ISA by @DamonFool in https://github.com/vllm-project/vllm/pull/5452
* [Bugfix]typofix by @AllenDou in https://github.com/vllm-project/vllm/pull/5507
* bump version to v0.5.0.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/5522
* [CI/Build][Misc] Add CI that benchmarks vllm performance on those PRs with `perf-benchmarks` label by @KuntaiDu in https://github.com/vllm-project/vllm/pull/5073
* [CI/Build] Disable LLaVA-NeXT CPU test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5529
* [Kernel] Fix CUTLASS 3.x custom broadcast load epilogue by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5516
* [Misc] Fix arg names by @AllenDou in https://github.com/vllm-project/vllm/pull/5524
* [ Misc ] Rs/compressed tensors cleanup by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5432
* [Kernel] Suppress mma.sp warning on CUDA 12.5 and later by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5401
* [mis] fix flaky test of test_cuda_device_count_stateless by @youkaichao in https://github.com/vllm-project/vllm/pull/5546
* [Core] Remove duplicate processing in async engine by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5525
* [misc][distributed] fix benign error in `is_in_the_same_node` by @youkaichao in https://github.com/vllm-project/vllm/pull/5512
* [Docs] Add ZhenFund as a Sponsor by @simon-mo in https://github.com/vllm-project/vllm/pull/5548
* [Doc] Update documentation on Tensorizer by @sangstar in https://github.com/vllm-project/vllm/pull/5471
* [Bugfix] Enable loading FP8 checkpoints for gpt_bigcode models  by @tdoublep in https://github.com/vllm-project/vllm/pull/5460
* [Bugfix] Fix typo in Pallas backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5558
* [Core][Distributed] improve p2p cache generation by @youkaichao in https://github.com/vllm-project/vllm/pull/5528
* Add ccache to amd by @simon-mo in https://github.com/vllm-project/vllm/pull/5555
* [Core][Bugfix]: fix prefix caching for blockv2 by @leiwen83 in https://github.com/vllm-project/vllm/pull/5364
* [mypy] Enable type checking for test directory by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5017
* [CI/Build] Test both text and token IDs in batched OpenAI Completions API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5568
* [misc] Do not allow to use lora with chunked prefill. by @rkooo567 in https://github.com/vllm-project/vllm/pull/5538
* add gptq_marlin test for bug report https://github.com/vllm-project/vllm/issues/5088 by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/5145
* [BugFix] Don't start a Ray cluster when not using Ray by @njhill in https://github.com/vllm-project/vllm/pull/5570
* [Fix] Correct OpenAI batch response format by @zifeitong in https://github.com/vllm-project/vllm/pull/5554
* Add basic correctness 2 GPU tests to 4 GPU pipeline by @Yard1 in https://github.com/vllm-project/vllm/pull/5518
* [CI][BugFix] Flip is_quant_method_supported condition by @mgoin in https://github.com/vllm-project/vllm/pull/5577
* [build][misc] limit numpy version by @youkaichao in https://github.com/vllm-project/vllm/pull/5582
* [Doc] add debugging tips for crash and multi-node debugging by @youkaichao in https://github.com/vllm-project/vllm/pull/5581
* Fix w8a8 benchmark and add Llama-3-8B by @comaniac in https://github.com/vllm-project/vllm/pull/5562
* [Model] Rename Phi3 rope scaling type by @garg-amit in https://github.com/vllm-project/vllm/pull/5595
* Correct alignment in the seq_len diagram. by @CharlesRiggins in https://github.com/vllm-project/vllm/pull/5592
* [Kernel] `compressed-tensors` marlin 24 support by @dsikka in https://github.com/vllm-project/vllm/pull/5435
* [Misc] use AutoTokenizer for benchmark serving when vLLM not installed by @zhyncs in https://github.com/vllm-project/vllm/pull/5588
* [Hardware][Intel GPU]Add Initial Intel GPU(XPU) inference backend by @jikunshang in https://github.com/vllm-project/vllm/pull/3814
* [CI/BUILD] Support non-AVX512 vLLM building and testing by @DamonFool in https://github.com/vllm-project/vllm/pull/5574
* [CI] Improve the readability of performance benchmarking results and prepare for upcoming performance dashboard by @KuntaiDu in https://github.com/vllm-project/vllm/pull/5571
* [bugfix][distributed] fix 16 gpus local rank arrangement by @youkaichao in https://github.com/vllm-project/vllm/pull/5604
* [Optimization] use a pool to reuse LogicalTokenBlock.token_ids by @youkaichao in https://github.com/vllm-project/vllm/pull/5584
* [Bugfix] Fix KV head calculation for MPT models when using GQA by @bfontain in https://github.com/vllm-project/vllm/pull/5142
* [Fix] Use utf-8 encoding in entrypoints/openai/run_batch.py by @zifeitong in https://github.com/vllm-project/vllm/pull/5606
* [Speculative Decoding 1/2 ] Add typical acceptance sampling as one of the sampling techniques in the verifier by @sroy745 in https://github.com/vllm-project/vllm/pull/5131
* [Model] Initialize Phi-3-vision support by @Isotr0py in https://github.com/vllm-project/vllm/pull/4986
* [Kernel] Add punica dimensions for Granite 13b by @joerunde in https://github.com/vllm-project/vllm/pull/5559
* [misc][typo] fix typo by @youkaichao in https://github.com/vllm-project/vllm/pull/5620
* [Misc] Fix typo by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5618
* [CI] Avoid naming different metrics with the same name in performance benchmark by @KuntaiDu in https://github.com/vllm-project/vllm/pull/5615
* [bugfix][distributed] do not error if two processes do not agree on p2p capability by @youkaichao in https://github.com/vllm-project/vllm/pull/5612
* [Misc] Remove import from transformers logging by @CatherineSue in https://github.com/vllm-project/vllm/pull/5625
* [CI/Build][Misc] Update Pytest Marker for VLMs by @ywang96 in https://github.com/vllm-project/vllm/pull/5623
* [ci] Deprecate original CI template by @khluu in https://github.com/vllm-project/vllm/pull/5624
* [Misc] Add OpenTelemetry support by @ronensc in https://github.com/vllm-project/vllm/pull/4687
* [Misc] Add channel-wise quantization support for w8a8 dynamic per token activation quantization by @dsikka in https://github.com/vllm-project/vllm/pull/5542
* [ci] Setup Release pipeline and build release wheels with cache by @khluu in https://github.com/vllm-project/vllm/pull/5610
* [Model] LoRA support added for command-r by @sergey-tinkoff in https://github.com/vllm-project/vllm/pull/5178
* [Bugfix] Fix for inconsistent behaviour related to sampling and repetition penalties  by @tdoublep in https://github.com/vllm-project/vllm/pull/5639
* [Doc] Added cerebrium as Integration option by @milo157 in https://github.com/vllm-project/vllm/pull/5553
* [Bugfix] Fix CUDA version check for mma warning suppression by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5642
* [Bugfix] Fix w8a8 benchmarks for int8 case by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5643
* [Bugfix] Fix Phi-3 Long RoPE scaling implementation by @ShukantPal in https://github.com/vllm-project/vllm/pull/5628
* [Bugfix] Added test for sampling repetition penalty bug. by @tdoublep in https://github.com/vllm-project/vllm/pull/5659
* [Bugfix][CI/Build][AMD][ROCm]Fixed the cmake build bug which generate garbage on certain devices by @hongxiayang in https://github.com/vllm-project/vllm/pull/5641
* [misc][distributed] use localhost for single-node by @youkaichao in https://github.com/vllm-project/vllm/pull/5619
* [Model] Add FP8 kv cache for Qwen2 by @mgoin in https://github.com/vllm-project/vllm/pull/5656
* [Bugfix] Fix sampling_params passed incorrectly in Phi3v example by @Isotr0py in https://github.com/vllm-project/vllm/pull/5684
* [Misc]Add param max-model-len in benchmark_latency.py by @DearPlanet in https://github.com/vllm-project/vllm/pull/5629
* [CI/Build] Add tqdm to dependencies by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5680
* [ci] Add A100 queue into AWS CI template by @khluu in https://github.com/vllm-project/vllm/pull/5648
* [Frontend][Bugfix] Fix preemption_mode -> preemption-mode for CLI arg in arg_utils.py by @mgoin in https://github.com/vllm-project/vllm/pull/5688
* [ci][distributed] add tests for custom allreduce by @youkaichao in https://github.com/vllm-project/vllm/pull/5689
* [Bugfix] AsyncLLMEngine hangs with asyncio.run by @zifeitong in https://github.com/vllm-project/vllm/pull/5654
* [Doc] Update docker references by @rafvasq in https://github.com/vllm-project/vllm/pull/5614
* [Misc] Add per channel support for static activation quantization; update w8a8 schemes to share base classes by @dsikka in https://github.com/vllm-project/vllm/pull/5650
* [ci] Limit num gpus if specified for A100 by @khluu in https://github.com/vllm-project/vllm/pull/5694
* [Misc] Improve conftest by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5681
* [Bugfix][Doc] FIx Duplicate Explicit Target Name Errors by @ywang96 in https://github.com/vllm-project/vllm/pull/5703
* [Kernel] Update Cutlass int8 kernel configs for SM90 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/5514
* [Model] Port over CLIPVisionModel for VLMs by @ywang96 in https://github.com/vllm-project/vllm/pull/5591
* [Kernel] Update Cutlass int8 kernel configs for SM80 by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/5275
* [Bugfix] Fix the CUDA version check for FP8 support in the CUTLASS kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5715
* [Frontend] Add FlexibleArgumentParser to support both underscore and dash in names by @mgoin in https://github.com/vllm-project/vllm/pull/5718
* [distributed][misc] use fork by default for mp by @youkaichao in https://github.com/vllm-project/vllm/pull/5669
* [Model] MLPSpeculator speculative decoding support by @JRosenkranz in https://github.com/vllm-project/vllm/pull/4947
* [Kernel] Add punica dimension for Qwen2 LoRA by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/5441
* [BugFix] Fix test_phi3v.py by @CatherineSue in https://github.com/vllm-project/vllm/pull/5725
* [Bugfix] Add  fully sharded layer for QKVParallelLinearWithLora by @jeejeelee in https://github.com/vllm-project/vllm/pull/5665
* [Core][Distributed] add shm broadcast by @youkaichao in https://github.com/vllm-project/vllm/pull/5399
* [Kernel][CPU] Add Quick `gelu` to CPU by @ywang96 in https://github.com/vllm-project/vllm/pull/5717
* [Doc] Documentation on supported hardware for quantization methods by @mgoin in https://github.com/vllm-project/vllm/pull/5745
* [BugFix] exclude version 1.15.0 for modelscope by @zhyncs in https://github.com/vllm-project/vllm/pull/5668
* [ci][test] fix ca test in main by @youkaichao in https://github.com/vllm-project/vllm/pull/5746
* [LoRA] Add support for pinning lora adapters in the LRU cache by @rohithkrn in https://github.com/vllm-project/vllm/pull/5603
* [CI][Hardware][Intel GPU] add Intel GPU(XPU) ci pipeline by @jikunshang in https://github.com/vllm-project/vllm/pull/5616
* [Model] Support Qwen-VL and Qwen-VL-Chat models with text-only inputs by @DamonFool in https://github.com/vllm-project/vllm/pull/5710
* [Misc] Remove #4789 workaround left in vllm/entrypoints/openai/run_batch.py by @zifeitong in https://github.com/vllm-project/vllm/pull/5756
* [Bugfix] Fix pin_lora error in TPU executor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5760
* [Docs][TPU] Add installation tip for TPU by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5761
* [core][distributed] improve shared memory broadcast by @youkaichao in https://github.com/vllm-project/vllm/pull/5754
* [BugFix] [Kernel] Add Cutlass2x fallback kernels by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/5744
* [Distributed] Add send and recv helpers by @andoorve in https://github.com/vllm-project/vllm/pull/5719
* [Bugfix] Add phi3v resize for dynamic shape and fix torchvision requirement by @Isotr0py in https://github.com/vllm-project/vllm/pull/5772
* [doc][faq] add warning to download models for every nodes by @youkaichao in https://github.com/vllm-project/vllm/pull/5783
* [Doc] Add "Suggest edit" button to doc pages by @mgoin in https://github.com/vllm-project/vllm/pull/5789
* [Doc] Add Phi-3-medium to list of supported models by @mgoin in https://github.com/vllm-project/vllm/pull/5788
* [Bugfix] Fix FlexibleArgumentParser replaces _ with - for actual args by @CatherineSue in https://github.com/vllm-project/vllm/pull/5795
* [ci] Remove aws template by @khluu in https://github.com/vllm-project/vllm/pull/5757
* [Doc] Add notice about breaking changes to VLMs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5818
* [Speculative Decoding] Support draft model on different tensor-parallel size than target model by @wooyeonlee0 in https://github.com/vllm-project/vllm/pull/5414
* [Misc] Remove useless code in cpu_worker by @DamonFool in https://github.com/vllm-project/vllm/pull/5824
* [Core] Add fault tolerance for `RayTokenizerGroupPool` by @Yard1 in https://github.com/vllm-project/vllm/pull/5748
* [doc][distributed] add both gloo and nccl tests by @youkaichao in https://github.com/vllm-project/vllm/pull/5834
* [CI/Build] Add unit testing for FlexibleArgumentParser by @mgoin in https://github.com/vllm-project/vllm/pull/5798
* [Misc] Update `w4a16` `compressed-tensors` support to include `w8a16` by @dsikka in https://github.com/vllm-project/vllm/pull/5794
* [Hardware][TPU] Refactor TPU backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5831
* [Hardware][AMD][CI/Build][Doc] Upgrade to ROCm 6.1, Dockerfile improvements, test fixes by @mawong-amd in https://github.com/vllm-project/vllm/pull/5422
* [Hardware][TPU] Raise errors for unsupported sampling params by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5850
* [CI/Build] Add E2E tests for MLPSpeculator by @tdoublep in https://github.com/vllm-project/vllm/pull/5791
* [Bugfix] Fix assertion in NeuronExecutor by @aws-patlange in https://github.com/vllm-project/vllm/pull/5841
* [Core] Refactor Worker and ModelRunner to consolidate control plane communication by @stephanie-wang in https://github.com/vllm-project/vllm/pull/5408
* [Misc][Doc] Add Example of using OpenAI Server with VLM by @ywang96 in https://github.com/vllm-project/vllm/pull/5832
* [bugfix][distributed] fix shm broadcast when the queue size is full by @youkaichao in https://github.com/vllm-project/vllm/pull/5801
* [Bugfix] Fix embedding to support 2D inputs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5829
* [Bugfix][TPU] Fix KV cache size calculation by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5860
* [CI/Build] Refactor image test assets by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5821
* [Kernel] Adding bias epilogue support for `cutlass_scaled_mm` by @ProExpertProg in https://github.com/vllm-project/vllm/pull/5560
* [Frontend] Add tokenize/detokenize endpoints by @sasha0552 in https://github.com/vllm-project/vllm/pull/5054
* [Hardware][TPU] Support parallel sampling & Swapping by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5855
* [Bugfix][TPU] Fix CPU cache allocation by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5869
* Support CPU inference with VSX PowerPC ISA by @ChipKerchner in https://github.com/vllm-project/vllm/pull/5652
* [doc] update usage of env var to avoid conflict by @youkaichao in https://github.com/vllm-project/vllm/pull/5873
* [Misc] Add example for LLaVA-NeXT by @ywang96 in https://github.com/vllm-project/vllm/pull/5879
* [BugFix] Fix cuda graph for MLPSpeculator by @njhill in https://github.com/vllm-project/vllm/pull/5875
* [Doc] Add note about context length in Phi-3-Vision example by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5887
* [VLM][Bugfix] Make sure that `multi_modal_kwargs` is broadcasted properly by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/5880
* [Model] Add base class for LoRA-supported models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5018
* [Bugfix] Fix img_sizes Parsing in Phi3-Vision by @ywang96 in https://github.com/vllm-project/vllm/pull/5888
* [CI/Build] [1/3] Reorganize entrypoints tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5526
* [Model][Bugfix] Implicit model flags and reenable Phi-3-Vision by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5896
* [doc][misc] add note for Kubernetes users by @youkaichao in https://github.com/vllm-project/vllm/pull/5916
* [BugFix] Fix `MLPSpeculator` handling of `num_speculative_tokens` by @njhill in https://github.com/vllm-project/vllm/pull/5876
* [BugFix] Fix `min_tokens` behaviour for multiple eos tokens by @njhill in https://github.com/vllm-project/vllm/pull/5849
* [CI/Build] Fix Args for `_get_logits_warper` in Sampler Test by @ywang96 in https://github.com/vllm-project/vllm/pull/5922
* [Model] Add Gemma 2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5908
* [core][misc] remove logical block by @youkaichao in https://github.com/vllm-project/vllm/pull/5882
* [Kernel][ROCm][AMD] fused_moe Triton configs v2 for mi300X by @divakar-amd in https://github.com/vllm-project/vllm/pull/5932
* [Hardware][TPU] Optimize KV cache swapping by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5878
* [VLM][BugFix] Make sure that `multi_modal_kwargs` can broadcast properly with ring buffer. by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/5905
* [Bugfix][Hardware][Intel CPU] Fix unpassed multi_modal_kwargs for CPU runner by @Isotr0py in https://github.com/vllm-project/vllm/pull/5956
* [Core] Registry for processing model inputs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5214
* Unmark fused_moe config json file as executable by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5960
* [Hardware][Intel] OpenVINO vLLM backend by @ilya-lavrenov in https://github.com/vllm-project/vllm/pull/5379
* [Bugfix] Better error message for MLPSpeculator when `num_speculative_tokens` is set too high by @tdoublep in https://github.com/vllm-project/vllm/pull/5894
* [CI/Build] [2/3] Reorganize entrypoints tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5904
* [Distributed] Make it clear that % should not be in tensor dict keys. by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/5927
* [Spec Decode] Introduce DraftModelRunner by @comaniac in https://github.com/vllm-project/vllm/pull/5799
* [Bugfix] Fix compute datatype for cutlass 3.x epilogues by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5931
* [ Misc ] Remove `fp8_shard_indexer` from Col/Row Parallel Linear (Simplify Weight Loading) by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5928
* [ Bugfix ] Enabling Loading Models With Fused QKV/MLP on Disk with FP8 by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5921
* Support Deepseek-V2 by @zwd003 in https://github.com/vllm-project/vllm/pull/4650
* [Bugfix] Only add `Attention.kv_scale` if kv cache quantization is enabled by @mgoin in https://github.com/vllm-project/vllm/pull/5936
* Unmark more files as executable by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5962
* [Bugfix] Fix Engine Failing After Invalid Request - AsyncEngineDeadError by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5963
* [Kernel] Flashinfer for prefill & decode, with Cudagraph support for decode by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/4628
* [Bugfix][TPU] Fix TPU sampler output by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5978
* [Bugfix][TPU] Fix pad slot id by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5977
* [Bugfix] fix missing last itl in openai completions benchmark by @mcalman in https://github.com/vllm-project/vllm/pull/5926
* [Misc] Extend vLLM Metrics logging API by @SolitaryThinker in https://github.com/vllm-project/vllm/pull/5925
* [Kernel] Add punica dimensions for Granite 3b and 8b by @joerunde in https://github.com/vllm-project/vllm/pull/5930
* [Bugfix] Fix precisions in Gemma 1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5913
* [Misc] Update Phi-3-Vision Example by @ywang96 in https://github.com/vllm-project/vllm/pull/5981
* [Bugfix] Support `eos_token_id` from `config.json` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5954
* [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum by @Yard1 in https://github.com/vllm-project/vllm/pull/5974
* [Kernel] Raise an exception in MoE kernel if the batch size is larger then 65k by @comaniac in https://github.com/vllm-project/vllm/pull/5939
* [ CI/Build ] Added E2E Test For Compressed Tensors by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5839
* [CI/Build] Add TP test for vision models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5892
* [ CI/Build ] LM Eval Harness Based CI Testing by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5838
* [Bugfix][CI/Build][Hardware][AMD] Install matching torchvision to fix AMD tests by @mawong-amd in https://github.com/vllm-project/vllm/pull/5949
* [CI/Build] Temporarily Remove Phi3-Vision from TP Test by @ywang96 in https://github.com/vllm-project/vllm/pull/5989
* [CI/Build] Reuse code for checking output consistency by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5988
* [CI/Build] [3/3] Reorganize entrypoints tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5966
* [ci][distributed] fix some cuda init that makes it necessary to use spawn by @youkaichao in https://github.com/vllm-project/vllm/pull/5991
* [Frontend]: Support base64 embedding by @llmpros in https://github.com/vllm-project/vllm/pull/5935
* [Lora] Use safetensor keys instead of adapter_config.json to find unexpected modules.  by @rkooo567 in https://github.com/vllm-project/vllm/pull/5909
* [ CI ] Temporarily Disable Large LM-Eval Tests by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6005
* [Misc] Fix `get_min_capability` by @dsikka in https://github.com/vllm-project/vllm/pull/5971
* [ Misc ] Refactor w8a8 to use `process_weights_after_load` (Simplify Weight Loading) by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5940
* [misc][cuda] use nvml query to avoid accidentally cuda initialization by @youkaichao in https://github.com/vllm-project/vllm/pull/6007
* [Speculative Decoding 2/2 ] Integrate typical acceptance sampler into Spec Decode Worker by @sroy745 in https://github.com/vllm-project/vllm/pull/5348
* [ CI ] Re-enable Large Model LM Eval by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6031
* [doc][misc] remove deprecated api server in doc by @youkaichao in https://github.com/vllm-project/vllm/pull/6037
* [Misc] update benchmark backend for scalellm by @zhyncs in https://github.com/vllm-project/vllm/pull/6018
* [doc][misc] further lower visibility of simple api server by @youkaichao in https://github.com/vllm-project/vllm/pull/6041
* [Bugfix] Use RayActorError for older versions of Ray in  RayTokenizerGroupPool by @Yard1 in https://github.com/vllm-project/vllm/pull/6039
* [Bugfix] adding chunking mechanism to fused_moe to handle large inputs by @avshalomman in https://github.com/vllm-project/vllm/pull/6029
* add FAQ doc under 'serving' by @llmpros in https://github.com/vllm-project/vllm/pull/5946
* [Bugfix][Doc] Fix Doc Formatting by @ywang96 in https://github.com/vllm-project/vllm/pull/6048
* [Bugfix] Add explicit `end_forward` calls to flashinfer by @Yard1 in https://github.com/vllm-project/vllm/pull/6044
* [BugFix] Ensure worker model loop is always stopped at the right time by @njhill in https://github.com/vllm-project/vllm/pull/5987
* [Frontend] Relax api url assertion for openai benchmarking by @jamestwhedbee in https://github.com/vllm-project/vllm/pull/6046
* [Model] Changes to MLPSpeculator to support tie_weights and input_scale by @tdoublep in https://github.com/vllm-project/vllm/pull/5965
* [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/5602
* [Frontend] Add template related params to request by @danieljannai21 in https://github.com/vllm-project/vllm/pull/5709
* [VLM] Remove `image_input_type` from VLM config by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/5852
* [Doc] Reinstate doc dependencies by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6061
* [Speculative Decoding] MLPSpeculator Tensor Parallel support (1/2) by @sirejdua in https://github.com/vllm-project/vllm/pull/6050
* [Core] Pipeline Parallel Support by @andoorve in https://github.com/vllm-project/vllm/pull/4412
* Update conftest.py by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6076
* [ Misc ] Refactor MoE to isolate Fp8 From Mixtral by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5970
* [CORE] Quantized lm-head Framework by @Qubitium in https://github.com/vllm-project/vllm/pull/4442
* [Model] Jamba support by @mzusman in https://github.com/vllm-project/vllm/pull/4115
* [hardware][misc] introduce platform abstraction by @youkaichao in https://github.com/vllm-project/vllm/pull/6080
* [Core] Dynamic image size support for VLMs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5276
* [CI] Fix base url doesn't strip "/" by @rkooo567 in https://github.com/vllm-project/vllm/pull/6087
* [BugFix] Avoid unnecessary Ray import warnings by @njhill in https://github.com/vllm-project/vllm/pull/6079
* [misc][distributed] error on invalid state by @youkaichao in https://github.com/vllm-project/vllm/pull/6092
* [VLM][Frontend] Proper Image Prompt Formatting from OpenAI API by @ywang96 in https://github.com/vllm-project/vllm/pull/6091
* [Doc] Fix Mock Import by @ywang96 in https://github.com/vllm-project/vllm/pull/6094
* [Bugfix] Fix `compute_logits` in Jamba by @ywang96 in https://github.com/vllm-project/vllm/pull/6093
* [Kernel] Expand FP8 support to Ampere GPUs using FP8 Marlin by @mgoin in https://github.com/vllm-project/vllm/pull/5975
* [core][distributed] allow custom allreduce when pipeline parallel size > 1 by @youkaichao in https://github.com/vllm-project/vllm/pull/6117
* [vlm] Remove vision language config. by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/6089
* [ Misc ] Clean Up `CompressedTensorsW8A8` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/6113
* [doc][misc] bump up py version in installation doc by @youkaichao in https://github.com/vllm-project/vllm/pull/6119
* [core][distributed] support layer size undividable by pp size in pipeline parallel inference by @youkaichao in https://github.com/vllm-project/vllm/pull/6115
* [Bugfix] set OMP_NUM_THREADS to 1 by default when using the multiproc_gpu_executor by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/6109
* [Distributed][Core] Support Py39 and Py38 for PP by @andoorve in https://github.com/vllm-project/vllm/pull/6120
* [CI/Build] Cleanup VLM tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6107
* [ROCm][AMD][Model]Adding alibi slopes support in ROCm triton flash attention and naive flash attention by @gshtras in https://github.com/vllm-project/vllm/pull/6043
* [misc][doc] try to add warning for latest html by @youkaichao in https://github.com/vllm-project/vllm/pull/5979
* [Hardware][Intel CPU] Adding intel openmp tunings in Docker file by @zhouyuan in https://github.com/vllm-project/vllm/pull/6008
* [Kernel][Model] logits_soft_cap for Gemma2 with flashinfer by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/6051
* [VLM] Calculate maximum number of multi-modal tokens by model by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6121
* [VLM] Improve consistency between feature size calculation and dummy data for profiling by @ywang96 in https://github.com/vllm-project/vllm/pull/6146
* [VLM] Cleanup validation and update docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/6149
* [Bugfix] Use templated datasource in grafana.json to allow automatic imports by @frittentheke in https://github.com/vllm-project/vllm/pull/6136
* [Frontend] Continuous usage stats in OpenAI completion API by @jvlunteren in https://github.com/vllm-project/vllm/pull/5742
* [Bugfix] Add verbose error if scipy is missing for blocksparse attention by @JGSweets in https://github.com/vllm-project/vllm/pull/5695
* bump version to v0.5.1 by @simon-mo in https://github.com/vllm-project/vllm/pull/6157
* [Docs] Fix readthedocs for tag build by @simon-mo in https://github.com/vllm-project/vllm/pull/6158

## New Contributors
* @kimdwkimdw made their first contribution in https://github.com/vllm-project/vllm/pull/5444
* @sywangyi made their first contribution in https://github.com/vllm-project/vllm/pull/5303
* @garg-amit made their first contribution in https://github.com/vllm-project/vllm/pull/5595
* @CharlesRiggins made their first contribution in https://github.com/vllm-project/vllm/pull/5592
* @zhyncs made their first contribution in https://github.com/vllm-project/vllm/pull/5588
* @bfontain made their first contribution in https://github.com/vllm-project/vllm/pull/5142
* @sroy745 made their first contribution in https://github.com/vllm-project/vllm/pull/5131
* @joerunde made their first contribution in https://github.com/vllm-project/vllm/pull/5559
* @sergey-tinkoff made their first contribution in https://github.com/vllm-project/vllm/pull/5178
* @milo157 made their first contribution in https://github.com/vllm-project/vllm/pull/5553
* @ShukantPal made their first contribution in https://github.com/vllm-project/vllm/pull/5628
* @rafvasq made their first contribution in https://github.com/vllm-project/vllm/pull/5614
* @JRosenkranz made their first contribution in https://github.com/vllm-project/vllm/pull/4947
* @rohithkrn made their first contribution in https://github.com/vllm-project/vllm/pull/5603
* @wooyeonlee0 made their first contribution in https://github.com/vllm-project/vllm/pull/5414
* @aws-patlange made their first contribution in https://github.com/vllm-project/vllm/pull/5841
* @stephanie-wang made their first contribution in https://github.com/vllm-project/vllm/pull/5408
* @ProExpertProg made their first contribution in https://github.com/vllm-project/vllm/pull/5560
* @ChipKerchner made their first contribution in https://github.com/vllm-project/vllm/pull/5652
* @ilya-lavrenov made their first contribution in https://github.com/vllm-project/vllm/pull/5379
* @mcalman made their first contribution in https://github.com/vllm-project/vllm/pull/5926
* @SolitaryThinker made their first contribution in https://github.com/vllm-project/vllm/pull/5925
* @llmpros made their first contribution in https://github.com/vllm-project/vllm/pull/5935
* @avshalomman made their first contribution in https://github.com/vllm-project/vllm/pull/6029
* @danieljannai21 made their first contribution in https://github.com/vllm-project/vllm/pull/5709
* @sirejdua made their first contribution in https://github.com/vllm-project/vllm/pull/6050
* @gshtras made their first contribution in https://github.com/vllm-project/vllm/pull/6043
* @frittentheke made their first contribution in https://github.com/vllm-project/vllm/pull/6136
* @jvlunteren made their first contribution in https://github.com/vllm-project/vllm/pull/5742
* @JGSweets made their first contribution in https://github.com/vllm-project/vllm/pull/5695

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.0...v0.5.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.1)

---

## v0.5.0.post1: v0.5.0.post1
**Published:** 2024-06-14

# Highlights

* Add initial TPU integration (#5292)
* Fix crashes when using FlashAttention backend (#5478)
* Fix issues when using num_devices < num_available_devices (#5473)

## What's Changed
* [CI/Build] Add `is_quant_method_supported` to control quantization test configurations by @mgoin in https://github.com/vllm-project/vllm/pull/5253
* Revert "[CI/Build] Add `is_quant_method_supported` to control quantization test configurations" by @simon-mo in https://github.com/vllm-project/vllm/pull/5463
* [CI] Upgrade codespell version. by @rkooo567 in https://github.com/vllm-project/vllm/pull/5381
* [Hardware] Initial TPU integration by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5292
* [Bugfix] Add device assertion to TorchSDPA by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/5402
* [ci] Add AMD, Neuron, Intel tests for AWS CI and turn off default soft fail for GPU tests by @khluu in https://github.com/vllm-project/vllm/pull/5464
* [Kernel] Vectorized FP8 quantize kernel by @comaniac in https://github.com/vllm-project/vllm/pull/5396
* [Bugfix] TYPE_CHECKING for MultiModalData by @kimdwkimdw in https://github.com/vllm-project/vllm/pull/5444
* [Frontend] [Core] Support for sharded tensorized models by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/4990
* [misc] add hint for AttributeError by @youkaichao in https://github.com/vllm-project/vllm/pull/5462
* [Doc] Update debug docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5438
* [Bugfix] Fix typo in scheduler.py (requeset -> request) by @mgoin in https://github.com/vllm-project/vllm/pull/5470
* [Frontend] Add "input speed" to tqdm postfix alongside output speed by @mgoin in https://github.com/vllm-project/vllm/pull/5425
* [Bugfix] Fix wrong multi_modal_input format for CPU runner by @Isotr0py in https://github.com/vllm-project/vllm/pull/5451
* [Core][Distributed] add coordinator to reduce code duplication in tp and pp by @youkaichao in https://github.com/vllm-project/vllm/pull/5293
* [ci] Use sccache to build images by @khluu in https://github.com/vllm-project/vllm/pull/5419
* [Bugfix]if the content is started with ":"(response of ping), client should iâ€¦ by @sywangyi in https://github.com/vllm-project/vllm/pull/5303
* [Kernel] `w4a16` support for `compressed-tensors` by @dsikka in https://github.com/vllm-project/vllm/pull/5385
* [CI/Build][REDO] Add is_quant_method_supported to control quantization test configurations by @mgoin in https://github.com/vllm-project/vllm/pull/5466
* [Kernel] Tune Qwen2MoE kernel configurations with tp2,4 by @wenyujin333 in https://github.com/vllm-project/vllm/pull/5497
* [Hardware][Intel] Optimize CPU backend and add more performance tips by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/4971
* [Docs] Add 4th meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5509
* [Misc] Add vLLM version getter to utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5098
* [CI/Build] Simplify OpenAI server setup in tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5100
* [Doc] Update LLaVA docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5437
* [Kernel] Factor out epilogues from cutlass kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5391
* [MISC] Remove FP8 warning by @comaniac in https://github.com/vllm-project/vllm/pull/5472
* Seperate dev requirements into lint and test by @Yard1 in https://github.com/vllm-project/vllm/pull/5474
* Revert "[Core] Remove unnecessary copies in flash attn backend" by @Yard1 in https://github.com/vllm-project/vllm/pull/5478
* [misc] fix format.sh by @youkaichao in https://github.com/vllm-project/vllm/pull/5511
* [CI/Build] Disable test_fp8.py by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5508
* [Kernel] Disable CUTLASS kernels for fp8 by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5505
* Add `cuda_device_count_stateless` by @Yard1 in https://github.com/vllm-project/vllm/pull/5473
* [Hardware][Intel] Support CPU inference with AVX2 ISA by @DamonFool in https://github.com/vllm-project/vllm/pull/5452
* [Bugfix]typofix by @AllenDou in https://github.com/vllm-project/vllm/pull/5507
* bump version to v0.5.0.post1 by @simon-mo in https://github.com/vllm-project/vllm/pull/5522

## New Contributors
* @kimdwkimdw made their first contribution in https://github.com/vllm-project/vllm/pull/5444
* @sywangyi made their first contribution in https://github.com/vllm-project/vllm/pull/5303

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.5.0...v0.5.0.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.0.post1)

---

## v0.5.0: v0.5.0
**Published:** 2024-06-11

# Highlights

## Production Features
* [FP8 support](https://docs.vllm.ai/en/stable/quantization/fp8.html) is ready for testing. By quantizing the portion model weights to 8 bit precision float point, the inference speed gets 1.5x boost. Please try it out and let us know your thoughts! (#5352, #5388, #5159, #5238, #5294, #5183, #5144, #5231)
* Add OpenAI [Vision API](https://docs.vllm.ai/en/stable/models/vlm.html) support. Currently only LLaVA and LLaVA-NeXT are supported. We are working on adding more models in the next release. (#5237, #5383, #4199, #5374, #4197)
* [Speculative Decoding](https://docs.vllm.ai/en/stable/models/spec_decode.html) and [Automatic Prefix Caching](https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html) is also ready for testing, we plan to turn them on by default in upcoming releases. (#5400, #5157, #5137, #5324)
* Default to multiprocessing backend for single-node distributed case (#5230)
* Support bitsandbytes quantization and QLoRA (#4776)

## Hardware Support
* Improvements to the Intel CPU CI (#4113, #5241)
* Use TORCH_LIBRARY instead of PYBIND11_MODULE for custom ops (#5047)

## Others
* [Debugging tips](https://docs.vllm.ai/en/stable/getting_started/debugging.html) documentation (#5409, #5430)
* Dynamic Per-Token Activation Quantization (#5037)
* Customizable RoPE theta (#5197)
* Enable passing multiple LoRA adapters at once to generate() (#5300)
* OpenAI `tools` support named functions (#5032)
* Support `stream_options` for OpenAI protocol (#5319, #5135)
* Update Outlines Integration from `FSM` to `Guide` (#4109)

## What's Changed
* [CI/Build] CMakeLists: build all extensions' cmake targets at the same time by @dtrifiro in https://github.com/vllm-project/vllm/pull/5034
* [Kernel] Refactor CUTLASS kernels to always take scales that reside on the GPU by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5137
* [Kernel] Update Cutlass fp8 configs by @varun-sundar-rabindranath in https://github.com/vllm-project/vllm/pull/5144
* [Minor] Fix the path typo in loader.py: save_sharded_states.py -> save_sharded_state.py  by @dashanji in https://github.com/vllm-project/vllm/pull/5151
* [Bugfix] Fix call to init_logger in openai server by @NadavShmayo in https://github.com/vllm-project/vllm/pull/4765
* [Feature][Kernel] Support bitsandbytes quantization and QLoRA by @chenqianfzh in https://github.com/vllm-project/vllm/pull/4776
* [Bugfix] Remove deprecated @abstractproperty by @zhuohan123 in https://github.com/vllm-project/vllm/pull/5174
* [Bugfix]: Fix issues related to prefix caching example (#5177) by @Delviet in https://github.com/vllm-project/vllm/pull/5180
* [BugFix] Prevent `LLM.encode` for non-generation Models  by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5184
* Update test_ignore_eos by @simon-mo in https://github.com/vllm-project/vllm/pull/4898
* [Frontend][OpenAI] Support for returning max_model_len on /v1/models response by @Avinash-Raj in https://github.com/vllm-project/vllm/pull/4643
* [Kernel][ROCm][AMD] enable fused topk_softmax kernel for moe layer by @divakar-amd in https://github.com/vllm-project/vllm/pull/4927
* [Misc] Simplify code and fix type annotations in `conftest.py` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5118
* [Core] Support image processor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4197
* [Core] Remove unnecessary copies in flash attn backend by @Yard1 in https://github.com/vllm-project/vllm/pull/5138
* [Kernel] Pass a device pointer into the quantize kernel for the scales by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5159
* [CI/BUILD] enable intel queue for longer CPU tests by @zhouyuan in https://github.com/vllm-project/vllm/pull/4113
* [Misc]: Implement CPU/GPU swapping in BlockManagerV2 by @Kaiyang-Chen in https://github.com/vllm-project/vllm/pull/3834
* New CI template on AWS stack by @khluu in https://github.com/vllm-project/vllm/pull/5110
* [FRONTEND] OpenAI `tools` support named functions by @br3no in https://github.com/vllm-project/vllm/pull/5032
* [Bugfix] Support `prompt_logprobs==0` by @toslunar in https://github.com/vllm-project/vllm/pull/5217
* [Bugfix] Add warmup for prefix caching example by @zhuohan123 in https://github.com/vllm-project/vllm/pull/5235
* [Kernel] Enhance MoE benchmarking & tuning script by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4921
* [Bugfix]: During testing, use pytest monkeypatch for safely overriding the env var that indicates the vLLM backend by @afeldman-nm in https://github.com/vllm-project/vllm/pull/5210
* [Bugfix] Fix torch.compile() error when using MultiprocessingGPUExecutor by @zifeitong in https://github.com/vllm-project/vllm/pull/5229
* [CI/Build] Add inputs tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5215
* [Bugfix] Fix a bug caused by pip install setuptools>=49.4.0 for CPU backend by @DamonFool in https://github.com/vllm-project/vllm/pull/5249
* [Kernel] Add back batch size 1536 and 3072 to MoE tuning by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5242
* [CI/Build] Simplify model loading for `HfRunner` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5251
* [CI/Build] Reducing CPU CI execution time by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/5241
* [CI] mark AMD test as softfail to prevent blockage by @simon-mo in https://github.com/vllm-project/vllm/pull/5256
* [Misc] Add transformers version to collect_env.py by @mgoin in https://github.com/vllm-project/vllm/pull/5259
* [Misc] update collect env by @youkaichao in https://github.com/vllm-project/vllm/pull/5261
* [Bugfix] Fix prompt_logprobs when SamplingParams.detokenize is set to True by @zifeitong in https://github.com/vllm-project/vllm/pull/5226
* [Misc] Add CustomOp interface for device portability by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5255
* [Misc] Fix docstring of get_attn_backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5271
* [Frontend] OpenAI API server: Add `add_special_tokens` to ChatCompletionRequest (default False) by @tomeras91 in https://github.com/vllm-project/vllm/pull/5278
* [CI] Add nightly benchmarks by @simon-mo in https://github.com/vllm-project/vllm/pull/5260
* [misc] benchmark_serving.py -- add ITL results and tweak TPOT results by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5263
* [Kernel] Add GPU architecture guards to the CUTLASS w8a8 kernels to reduce binary size by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5157
* [Model] Correct Mixtral FP8 checkpoint loading by @comaniac in https://github.com/vllm-project/vllm/pull/5231
* [BugFix] Apply get_cached_tokenizer to the tokenizer setter of LLM by @DriverSong in https://github.com/vllm-project/vllm/pull/5207
* [Kernel] Re-tune Mixtral MoE configurations for FP8 on H100 by @pcmoritz in https://github.com/vllm-project/vllm/pull/5238
* [Docs] Add Sequoia as sponsors by @simon-mo in https://github.com/vllm-project/vllm/pull/5287
* [Speculative Decoding] Add `ProposerWorkerBase` abstract class by @njhill in https://github.com/vllm-project/vllm/pull/5252
* [BugFix] Fix log message about default max model length by @njhill in https://github.com/vllm-project/vllm/pull/5284
* [Bugfix] Make EngineArgs use named arguments for config construction by @mgoin in https://github.com/vllm-project/vllm/pull/5285
* [Bugfix][Frontend/Core] Don't log exception when AsyncLLMEngine gracefully shuts down. by @wuisawesome in https://github.com/vllm-project/vllm/pull/5290
* [Misc] Skip for logits_scale == 1.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5291
* [Docs] Add Ray Summit CFP by @simon-mo in https://github.com/vllm-project/vllm/pull/5295
* [CI] Disable flash_attn backend for spec decode by @simon-mo in https://github.com/vllm-project/vllm/pull/5286
* [Frontend][Core] Update Outlines Integration from `FSM` to `Guide` by @br3no in https://github.com/vllm-project/vllm/pull/4109
* [CI/Build] Update vision tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5307
* Bugfix: fix broken of download models from modelscope by @liuyhwangyh in https://github.com/vllm-project/vllm/pull/5233
* [Kernel] Retune Mixtral 8x22b configs for FP8 on H100 by @pcmoritz in https://github.com/vllm-project/vllm/pull/5294
* [Frontend] enable passing multiple LoRA adapters at once to generate() by @mgoldey in https://github.com/vllm-project/vllm/pull/5300
* [Core] Avoid copying prompt/output tokens if no penalties are used by @Yard1 in https://github.com/vllm-project/vllm/pull/5289
* [Core] Change LoRA embedding sharding to support loading methods by @Yard1 in https://github.com/vllm-project/vllm/pull/5038
* [Misc] Missing error message for custom ops import by @DamonFool in https://github.com/vllm-project/vllm/pull/5282
* [Feature][Frontend]: Add support for `stream_options` in `ChatCompletionRequest` by @Etelis in https://github.com/vllm-project/vllm/pull/5135
* [Misc][Utils] allow get_open_port to be called for multiple times by @youkaichao in https://github.com/vllm-project/vllm/pull/5333
* [Kernel] Switch fp8 layers to use the CUTLASS kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5183
* Remove Ray health check by @Yard1 in https://github.com/vllm-project/vllm/pull/4693
* Addition of lacked ignored_seq_groups in _schedule_chunked_prefill by @JamesLim-sy in https://github.com/vllm-project/vllm/pull/5296
* [Kernel] Dynamic Per-Token Activation Quantization by @dsikka in https://github.com/vllm-project/vllm/pull/5037
* [Frontend] Add OpenAI Vision API Support by @ywang96 in https://github.com/vllm-project/vllm/pull/5237
* [Misc] Remove unused cuda_utils.h in CPU backend by @DamonFool in https://github.com/vllm-project/vllm/pull/5345
* fix DbrxFusedNormAttention missing cache_config by @Calvinnncy97 in https://github.com/vllm-project/vllm/pull/5340
* [Bug Fix] Fix the support check for FP8 CUTLASS  by @cli99 in https://github.com/vllm-project/vllm/pull/5352
* [Misc] Add args for selecting distributed executor to benchmarks by @BKitor in https://github.com/vllm-project/vllm/pull/5335
* [ROCm][AMD] Use pytorch sdpa math backend to do naive attention by @hongxiayang in https://github.com/vllm-project/vllm/pull/4965
* [CI/Test] improve robustness of test by replacing del with context manager (hf_runner) by @youkaichao in https://github.com/vllm-project/vllm/pull/5347
* [CI/Test] improve robustness of test by replacing del with context manager (vllm_runner) by @youkaichao in https://github.com/vllm-project/vllm/pull/5357
* [Misc][Breaking] Change FP8 checkpoint format from act_scale -> input_scale by @mgoin in https://github.com/vllm-project/vllm/pull/5353
* [Core][CUDA Graph] add output buffer for cudagraph to reduce memory footprint by @youkaichao in https://github.com/vllm-project/vllm/pull/5074
* [mis][ci/test] fix flaky test in tests/test_sharded_state_loader.py by @youkaichao in https://github.com/vllm-project/vllm/pull/5361
* [Kernel][Misc] Use TORCH_LIBRARY instead of PYBIND11_MODULE for custom ops by @bnellnm in https://github.com/vllm-project/vllm/pull/5047
* [Bugfix] Fix KeyError: 1 When Using LoRA adapters  by @BlackBird-Coding in https://github.com/vllm-project/vllm/pull/5164
* [Misc] Update to comply with the new `compressed-tensors` config by @dsikka in https://github.com/vllm-project/vllm/pull/5350
* [Frontend][Misc] Enforce Pixel Values as Input Type for VLMs in API Server by @ywang96 in https://github.com/vllm-project/vllm/pull/5374
* [misc][typo] fix typo by @youkaichao in https://github.com/vllm-project/vllm/pull/5372
* [Misc] Improve error message when LoRA parsing fails by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5194
* [Model] Initial support for LLaVA-NeXT by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4199
* [Feature][Frontend]:  Continued `stream_options` implementation also in CompletionRequest by @Etelis in https://github.com/vllm-project/vllm/pull/5319
* [Bugfix] Fix LLaVA-NeXT by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5380
* [ci] Use small_cpu_queue for doc build by @khluu in https://github.com/vllm-project/vllm/pull/5331
* [ci] Mount buildkite agent on Docker container to upload benchmark results by @khluu in https://github.com/vllm-project/vllm/pull/5330
* [Docs] Add Docs on Limitations of VLM Support by @ywang96 in https://github.com/vllm-project/vllm/pull/5383
* [Docs] Alphabetically sort sponsors by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5386
* Bump version to v0.5.0 by @simon-mo in https://github.com/vllm-project/vllm/pull/5384
* [Doc] Add documentation for FP8 W8A8 by @mgoin in https://github.com/vllm-project/vllm/pull/5388
* [ci] Fix Buildkite agent path by @khluu in https://github.com/vllm-project/vllm/pull/5392
* [Misc] Various simplifications and typing fixes by @njhill in https://github.com/vllm-project/vllm/pull/5368
* [Bugfix] OpenAI entrypoint limits logprobs while ignoring server defined --max-logprobs by @maor-ps in https://github.com/vllm-project/vllm/pull/5312
* [Bugfix][Frontend] Cleanup "fix chat logprobs" by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5026
* [Doc] add debugging tips by @youkaichao in https://github.com/vllm-project/vllm/pull/5409
* [Doc][Typo] Fixing Missing Comma by @ywang96 in https://github.com/vllm-project/vllm/pull/5403
* [Misc] Remove VLLM_BUILD_WITH_NEURON env variable by @WoosukKwon in https://github.com/vllm-project/vllm/pull/5389
* [CI] docfix by @rkooo567 in https://github.com/vllm-project/vllm/pull/5410
* [Speculative decoding] Initial spec decode docs by @cadedaniel in https://github.com/vllm-project/vllm/pull/5400
* [Doc] Add an automatic prefix caching section in vllm documentation by @KuntaiDu in https://github.com/vllm-project/vllm/pull/5324
* [Docs] [Spec decode] Fix docs error in code example by @cadedaniel in https://github.com/vllm-project/vllm/pull/5427
* [Bugfix] Fix `MultiprocessingGPUExecutor.check_health` when world_size == 1 by @jsato8094 in https://github.com/vllm-project/vllm/pull/5254
* [Bugfix] fix lora_dtype value type in arg_utils.py by @c3-ali in https://github.com/vllm-project/vllm/pull/5398
* [Frontend] Customizable RoPE theta by @sasha0552 in https://github.com/vllm-project/vllm/pull/5197
* [Core][Distributed] add same-node detection by @youkaichao in https://github.com/vllm-project/vllm/pull/5369
* [Core][Doc] Default to multiprocessing for single-node distributed case by @njhill in https://github.com/vllm-project/vllm/pull/5230
* [Doc] add common case for long waiting time by @youkaichao in https://github.com/vllm-project/vllm/pull/5430

## New Contributors
* @dtrifiro made their first contribution in https://github.com/vllm-project/vllm/pull/5034
* @varun-sundar-rabindranath made their first contribution in https://github.com/vllm-project/vllm/pull/5144
* @dashanji made their first contribution in https://github.com/vllm-project/vllm/pull/5151
* @chenqianfzh made their first contribution in https://github.com/vllm-project/vllm/pull/4776
* @Delviet made their first contribution in https://github.com/vllm-project/vllm/pull/5180
* @Avinash-Raj made their first contribution in https://github.com/vllm-project/vllm/pull/4643
* @zhouyuan made their first contribution in https://github.com/vllm-project/vllm/pull/4113
* @Kaiyang-Chen made their first contribution in https://github.com/vllm-project/vllm/pull/3834
* @khluu made their first contribution in https://github.com/vllm-project/vllm/pull/5110
* @toslunar made their first contribution in https://github.com/vllm-project/vllm/pull/5217
* @DamonFool made their first contribution in https://github.com/vllm-project/vllm/pull/5249
* @tomeras91 made their first contribution in https://github.com/vllm-project/vllm/pull/5278
* @DriverSong made their first contribution in https://github.com/vllm-project/vllm/pull/5207
* @mgoldey made their first contribution in https://github.com/vllm-project/vllm/pull/5300
* @JamesLim-sy made their first contribution in https://github.com/vllm-project/vllm/pull/5296
* @Calvinnncy97 made their first contribution in https://github.com/vllm-project/vllm/pull/5340
* @cli99 made their first contribution in https://github.com/vllm-project/vllm/pull/5352
* @BKitor made their first contribution in https://github.com/vllm-project/vllm/pull/5335
* @BlackBird-Coding made their first contribution in https://github.com/vllm-project/vllm/pull/5164
* @maor-ps made their first contribution in https://github.com/vllm-project/vllm/pull/5312
* @c3-ali made their first contribution in https://github.com/vllm-project/vllm/pull/5398

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.4.3...v0.5.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.5.0)

---

## v0.4.3: v0.4.3
**Published:** 2024-06-01

## Highlights 
### Model Support
#### LLM
* Added support for Falcon (#5069)
* Added support for IBM Granite Code models (#4636)
* Added blocksparse flash attention kernel and Phi-3-Small model (#4799)
* Added Snowflake arctic model implementation (#4652, #4889, #4690)
* Supported Dynamic RoPE scaling (#4638)
* Supported for long context lora (#4787)

#### Embedding Models
* Intial support for Embedding API with e5-mistral-7b-instruct (#3734)
* Cross-attention KV caching and memory-management towards encoder-decoder model support (#4837)

#### Vision Language Model
* Add base class for vision-language models (#4809)
* Consolidate prompt arguments to LLM engines (#4328)
* LLaVA model refactor (#4910)

### Hardware Support
#### AMD
* Add fused_moe Triton configs (#4951)
* Add support for Punica kernels (#3140)
* Extending the set of AMD tests with Regression, Basic Correctness, Distributed, Engine, Llava Tests (#4797)

### Production Engine
#### Batch API
* Support OpenAI batch file format (#4794)

#### Making Ray Optional
* Add `MultiprocessingGPUExecutor` (#4539)
* Eliminate parallel worker per-step task scheduling overhead (#4894)

#### Automatic Prefix Caching
* Accelerating the hashing function by avoiding deep copies (#4696)

#### Speculative Decoding
* CUDA graph support (#4295)
* Enable TP>1 speculative decoding (#4840)
* Improve n-gram efficiency (#4724)

### Performance Optimization

#### Quantization
* Add GPTQ Marlin 2:4 sparse structured support (#4790)
* Initial Activation Quantization Support (#4525)
* Marlin prefill performance improvement (about better on average) (#4983)
* Automatically Detect SparseML models (#5119)

#### Better Attention Kernel
* Use flash-attn for decoding (#3648)

#### FP8
* Improve FP8 linear layer performance (#4691)
* Add w8a8 CUTLASS kernels (#4749)
* Support for CUTLASS kernels in CUDA graphs (#4954)
* Load FP8 kv-cache scaling factors from checkpoints (#4893)
* Make static FP8 scaling more robust (#4570)
* Refactor FP8 kv-cache with NVIDIA float8_e4m3 support (#4535)

#### Optimize Distributed Communication
* change python dict to pytorch tensor (#4607)
* change python dict to pytorch tensor for blocks to swap (#4659)
* improve paccess check (#4992)
* remove vllm-nccl (#5091)
* support both cpu and device tensor in broadcast tensor dict (#4660)

### Extensible Architecture

#### Pipeline Parallelism
* refactor custom allreduce to support multiple tp groups (#4754)
* refactor pynccl to hold multiple communicators (#4591)
* Support PP PyNCCL Groups (#4988)




## What's Changed
* Disable cuda version check in vllm-openai image by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/4530
* [Bugfix] Fix `asyncio.Task` not being subscriptable by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4623
* [CI] use ccache actions properly in release workflow by @simon-mo in https://github.com/vllm-project/vllm/pull/4629
* [CI] Add retry for agent lost by @cadedaniel in https://github.com/vllm-project/vllm/pull/4633
* Update lm-format-enforcer to 0.10.1 by @noamgat in https://github.com/vllm-project/vllm/pull/4631
* [Kernel] Make static FP8 scaling more robust by @pcmoritz in https://github.com/vllm-project/vllm/pull/4570
* [Core][Optimization] change python dict to pytorch tensor by @youkaichao in https://github.com/vllm-project/vllm/pull/4607
* [Build/CI] Fixing 'docker run' to re-enable AMD CI tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/4642
* [Bugfix] Fixed error in slice_lora_b for MergedQKVParallelLinearWithLora by @FurtherAI in https://github.com/vllm-project/vllm/pull/4609
* [Core][Optimization] change copy-on-write from dict[int, list] to list by @youkaichao in https://github.com/vllm-project/vllm/pull/4648
* [Bug fix][Core] fixup ngram not setup correctly by @leiwen83 in https://github.com/vllm-project/vllm/pull/4551
* [Core][Distributed] support both cpu and device tensor in broadcast tensor dict by @youkaichao in https://github.com/vllm-project/vllm/pull/4660
* [Core] Optimize sampler get_logprobs by @rkooo567 in https://github.com/vllm-project/vllm/pull/4594
* [CI] Make mistral tests pass by @rkooo567 in https://github.com/vllm-project/vllm/pull/4596
* [Bugfix][Kernel] allow non-power-of-2 for prefix prefill with alibi  by @DefTruth in https://github.com/vllm-project/vllm/pull/4573
* [Misc] Add `get_name` method to attention backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4685
* [Core] Faster startup for LoRA enabled models by @Yard1 in https://github.com/vllm-project/vllm/pull/4634
* [Core][Optimization] change python dict to pytorch tensor for blocks to swap by @youkaichao in https://github.com/vllm-project/vllm/pull/4659
* [CI/Test] fix swap test for multi gpu by @youkaichao in https://github.com/vllm-project/vllm/pull/4689
* [Misc] Use vllm-flash-attn instead of flash-attn by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4686
* [Dynamic Spec Decoding] Auto-disable by the running queue size by @comaniac in https://github.com/vllm-project/vllm/pull/4592
* [Speculative decoding] [Bugfix] Fix overallocation in ngram + spec logprobs by @cadedaniel in https://github.com/vllm-project/vllm/pull/4672
* [Bugfix] Fine-tune gptq_marlin configs to be more similar to marlin by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4626
* [Frontend] add tok/s speed metric to llm class when using tqdm by @MahmoudAshraf97 in https://github.com/vllm-project/vllm/pull/4400
* [Frontend] Move async logic outside of constructor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4674
* [Misc] Remove unnecessary ModelRunner imports by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4703
* [Misc] Set block size at initialization & Fix test_model_runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4705
* [ROCm] Add support for Punica kernels on AMD GPUs by @kliuae in https://github.com/vllm-project/vllm/pull/3140
* [Bugfix] Fix CLI arguments in OpenAI server docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4709
* [Bugfix] Update grafana.json by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4711
* [Bugfix] Add logs for all model dtype casting by @mgoin in https://github.com/vllm-project/vllm/pull/4717
* [Model] Snowflake arctic model implementation by @sfc-gh-hazhang in https://github.com/vllm-project/vllm/pull/4652
* [Kernel] [FP8] Improve FP8 linear layer performance by @pcmoritz in https://github.com/vllm-project/vllm/pull/4691
* [Kernel] Refactor FP8 kv-cache with NVIDIA float8_e4m3 support by @comaniac in https://github.com/vllm-project/vllm/pull/4535
* [Core][Distributed] refactor pynccl to hold multiple communicators by @youkaichao in https://github.com/vllm-project/vllm/pull/4591
* [Misc] Keep only one implementation of the create_dummy_prompt function. by @AllenDou in https://github.com/vllm-project/vllm/pull/4716
* chunked-prefill-doc-syntax by @simon-mo in https://github.com/vllm-project/vllm/pull/4603
* [Core]fix type annotation for `swap_blocks` by @jikunshang in https://github.com/vllm-project/vllm/pull/4726
* [Misc] Apply a couple g++ cleanups by @stevegrubb in https://github.com/vllm-project/vllm/pull/4719
* [Core] Fix circular reference which leaked llm instance in local dev env by @rkooo567 in https://github.com/vllm-project/vllm/pull/4737
* [Bugfix] Fix CLI arguments in OpenAI server docs by @AllenDou in https://github.com/vllm-project/vllm/pull/4729
* [Speculative decoding] CUDA graph support by @heeju-kim2 in https://github.com/vllm-project/vllm/pull/4295
* [CI] Nits for bad initialization of SeqGroup in testing by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4748
* [Core][Test] fix function name typo in custom allreduce by @youkaichao in https://github.com/vllm-project/vllm/pull/4750
* [Model][Misc] Add e5-mistral-7b-instruct and Embedding API by @CatherineSue in https://github.com/vllm-project/vllm/pull/3734
* [Model] Add support for IBM Granite Code models by @yikangshen in https://github.com/vllm-project/vllm/pull/4636
* [CI/Build] Tweak Marlin Nondeterminism Issues In CI by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4713
* [CORE] Improvement in ranks code by @SwapnilDreams100 in https://github.com/vllm-project/vllm/pull/4718
* [Core][Distributed] refactor custom allreduce to support multiple tp groups by @youkaichao in https://github.com/vllm-project/vllm/pull/4754
* [CI/Build] Move `test_utils.py` to `tests/utils.py` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4425
* [Scheduler] Warning upon preemption and Swapping by @rkooo567 in https://github.com/vllm-project/vllm/pull/4647
* [Misc] Enhance attention selector by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4751
* [Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 by @sangstar in https://github.com/vllm-project/vllm/pull/4208
* [Speculative decoding] Improve n-gram efficiency by @comaniac in https://github.com/vllm-project/vllm/pull/4724
* [Kernel] Use flash-attn for decoding by @skrider in https://github.com/vllm-project/vllm/pull/3648
* [Bugfix] Fix dynamic FP8 quantization for Mixtral by @pcmoritz in https://github.com/vllm-project/vllm/pull/4793
* [Doc] Shorten README by removing supported model list by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4796
* [Doc] Add API reference for offline inference by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4710
* [Doc] Add meetups to the doc by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4798
* [Core][Hash][Automatic Prefix caching] Accelerating the hashing function by avoiding deep copies by @KuntaiDu in https://github.com/vllm-project/vllm/pull/4696
* [Bugfix][Doc] Fix CI failure in docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4804
* [Core] Add MultiprocessingGPUExecutor by @njhill in https://github.com/vllm-project/vllm/pull/4539
* Add 4th meetup announcement to readme by @simon-mo in https://github.com/vllm-project/vllm/pull/4817
* Revert "[Kernel] Use flash-attn for decoding (#3648)" by @rkooo567 in https://github.com/vllm-project/vllm/pull/4820
* [Core][2/N] Model runner refactoring part 2. Combine prepare prefill / decode to a single API by @rkooo567 in https://github.com/vllm-project/vllm/pull/4681
* [CI/Build] Further decouple HuggingFace implementation from ours during tests by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4166
* [Bugfix] Properly set distributed_executor_backend in ParallelConfig by @zifeitong in https://github.com/vllm-project/vllm/pull/4816
* [Doc] Highlight the fourth meetup in the README by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4842
* [Frontend] Re-enable custom roles in Chat Completions API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4758
* [Frontend] Support OpenAI batch file format by @wuisawesome in https://github.com/vllm-project/vllm/pull/4794
* [Core] Implement sharded state loader by @aurickq in https://github.com/vllm-project/vllm/pull/4690
* [Speculative decoding][Re-take] Enable TP>1 speculative decoding by @comaniac in https://github.com/vllm-project/vllm/pull/4840
* Add marlin unit tests and marlin benchmark script by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4815
* [Kernel] add bfloat16 support for gptq marlin kernel by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/4788
* [docs] Fix typo in examples filename openi -> openai by @wuisawesome in https://github.com/vllm-project/vllm/pull/4864
* [Frontend] Separate OpenAI Batch Runner usage from API Server by @wuisawesome in https://github.com/vllm-project/vllm/pull/4851
* [Bugfix] Bypass authorization API token for preflight requests by @dulacp in https://github.com/vllm-project/vllm/pull/4862
* Add GPTQ Marlin 2:4 sparse structured support by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4790
* Add JSON output support for benchmark_latency and benchmark_throughput by @simon-mo in https://github.com/vllm-project/vllm/pull/4848
* [ROCm][AMD][Bugfix] adding a missing triton autotune config by @hongxiayang in https://github.com/vllm-project/vllm/pull/4845
* [Core][Distributed] remove graph mode  function by @youkaichao in https://github.com/vllm-project/vllm/pull/4818
* [Misc] remove old comments by @youkaichao in https://github.com/vllm-project/vllm/pull/4866
* [Kernel] Add punica dimension for Qwen1.5-32B LoRA by @Silencioo in https://github.com/vllm-project/vllm/pull/4850
* [Kernel] Add w8a8 CUTLASS kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/4749
* [Bugfix] Fix FP8 KV cache support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4869
* Support to serve vLLM on Kubernetes with LWS by @kerthcet in https://github.com/vllm-project/vllm/pull/4829
* [Frontend] OpenAI API server: Do not add bos token by default when encoding by @bofenghuang in https://github.com/vllm-project/vllm/pull/4688
* [Build/CI] Extending the set of AMD tests with Regression, Basic Correctness, Distributed, Engine, Llava Tests by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/4797
* [Bugfix] fix rope error when load models with different dtypes  by @jinzhen-lin in https://github.com/vllm-project/vllm/pull/4835
* Sync huggingface modifications of qwen Moe model by @eigen2017 in https://github.com/vllm-project/vllm/pull/4774
* [Doc] Update Ray Data distributed offline inference example by @Yard1 in https://github.com/vllm-project/vllm/pull/4871
* [Bugfix] Relax tiktoken to >= 0.6.0 by @mgoin in https://github.com/vllm-project/vllm/pull/4890
* [ROCm][Hardware][AMD] Adding Navi21 to fallback to naive attention if Triton is not used by @alexeykondrat in https://github.com/vllm-project/vllm/pull/4658
* [Lora] Support long context lora  by @rkooo567 in https://github.com/vllm-project/vllm/pull/4787
* [Bugfix][Model] Add base class for vision-language models by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4809
* [Kernel] Add marlin_24 unit tests by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4901
* [Kernel] Add flash-attn back by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4907
* [Model] LLaVA model refactor by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4910
* Remove marlin warning by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4918
* [Bugfix]: Fix communication Timeout error in safety-constrained distributed System by @ZwwWayne in https://github.com/vllm-project/vllm/pull/4914
* [Build/CI] Enabling AMD Entrypoints Test by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/4834
* [Bugfix] Fix dummy weight for fp8 by @mzusman in https://github.com/vllm-project/vllm/pull/4916
* [Core] Sharded State Loader download from HF by @aurickq in https://github.com/vllm-project/vllm/pull/4889
* [Doc]Add documentation to benchmarking script when running TGI by @KuntaiDu in https://github.com/vllm-project/vllm/pull/4920
* [Core] Fix scheduler considering "no LoRA" as "LoRA" by @Yard1 in https://github.com/vllm-project/vllm/pull/4897
* [Model] add rope_scaling support for qwen2 by @hzhwcmhf in https://github.com/vllm-project/vllm/pull/4930
* [Model] Add Phi-2 LoRA support by @Isotr0py in https://github.com/vllm-project/vllm/pull/4886
* [Docs] Add acknowledgment for sponsors by @simon-mo in https://github.com/vllm-project/vllm/pull/4925
* [CI/Build] Codespell ignore `build/` directory by @mgoin in https://github.com/vllm-project/vllm/pull/4945
* [Bugfix] Fix flag name for  `max_seq_len_to_capture` by @kerthcet in https://github.com/vllm-project/vllm/pull/4935
* [Bugfix][Kernel] Add head size check for attention backend selection by @Isotr0py in https://github.com/vllm-project/vllm/pull/4944
* [Frontend] Dynamic RoPE scaling by @sasha0552 in https://github.com/vllm-project/vllm/pull/4638
* [CI/Build] Enforce style for C++ and CUDA code with `clang-format` by @mgoin in https://github.com/vllm-project/vllm/pull/4722
* [misc] remove comments that were supposed to be removed by @rkooo567 in https://github.com/vllm-project/vllm/pull/4977
* [Kernel] Fixup for CUTLASS kernels in CUDA graphs by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/4954
* [Misc] Load FP8 kv-cache scaling factors from checkpoints by @comaniac in https://github.com/vllm-project/vllm/pull/4893
* [Model] LoRA gptbigcode implementation by @raywanb in https://github.com/vllm-project/vllm/pull/3949
* [Core] Eliminate parallel worker per-step task scheduling overhead by @njhill in https://github.com/vllm-project/vllm/pull/4894
* [Minor] Fix small typo in llama.py: QKVParallelLinear -> QuantizationConfig by @pcmoritz in https://github.com/vllm-project/vllm/pull/4991
* [Misc] Take user preference in attention selector by @comaniac in https://github.com/vllm-project/vllm/pull/4960
* Marlin 24 prefill performance improvement (about 25% better on average) by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/4983
* [Bugfix] Update Dockerfile.cpu to fix NameError: name 'vllm_ops' is not defined by @LetianLee in https://github.com/vllm-project/vllm/pull/5009
* [Core][1/N] Support PP PyNCCL Groups by @andoorve in https://github.com/vllm-project/vllm/pull/4988
* [Kernel] Initial Activation Quantization Support by @dsikka in https://github.com/vllm-project/vllm/pull/4525
* [Core]: Option To Use Prompt Token Ids Inside Logits Processor by @kezouke in https://github.com/vllm-project/vllm/pull/4985
* [Doc] add ccache guide in doc by @youkaichao in https://github.com/vllm-project/vllm/pull/5012
* [Bugfix] Fix Mistral v0.3 Weight Loading by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5005
* [Core][Bugfix]: fix prefix caching for blockv2 by @leiwen83 in https://github.com/vllm-project/vllm/pull/4764
* [Kernel][Backend][Model] Blocksparse flash attention kernel and Phi-3-Small model by @linxihui in https://github.com/vllm-project/vllm/pull/4799
* [Misc] add logging level env var by @youkaichao in https://github.com/vllm-project/vllm/pull/5045
* [Dynamic Spec Decoding] Minor fix for disabling speculative decoding by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/5000
* [Misc] Make Serving Benchmark More User-friendly by @ywang96 in https://github.com/vllm-project/vllm/pull/5044
* [Bugfix / Core] Prefix Caching Guards (merged with main) by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4846
* [Core] Allow AQLM on Pascal by @sasha0552 in https://github.com/vllm-project/vllm/pull/5058
* [Model] Add support for falcon-11B by @Isotr0py in https://github.com/vllm-project/vllm/pull/5069
* [Core] Sliding window for block manager v2 by @mmoskal in https://github.com/vllm-project/vllm/pull/4545
* [BugFix] Fix Embedding Models with TP>1 by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5075
* [Kernel][ROCm][AMD] Add fused_moe Triton configs for MI300X by @divakar-amd in https://github.com/vllm-project/vllm/pull/4951
* [Docs] Add Dropbox as sponsors by @simon-mo in https://github.com/vllm-project/vllm/pull/5089
* [Core] Consolidate prompt arguments to LLM engines by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4328
* [Bugfix] Remove the last EOS token unless explicitly specified by @jsato8094 in https://github.com/vllm-project/vllm/pull/5077
* [Misc] add gpu_memory_utilization arg by @pandyamarut in https://github.com/vllm-project/vllm/pull/5079
* [Core][Optimization] remove vllm-nccl by @youkaichao in https://github.com/vllm-project/vllm/pull/5091
* [Bugfix] Fix arguments passed to `Sequence` in stop checker test by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5092
* [Core][Distributed] improve p2p access check by @youkaichao in https://github.com/vllm-project/vllm/pull/4992
* [Core] Cross-attention KV caching and memory-management (towards eventual encoder/decoder model support) by @afeldman-nm in https://github.com/vllm-project/vllm/pull/4837
* [Doc]Replace deprecated flag in readme by @ronensc in https://github.com/vllm-project/vllm/pull/4526
* [Bugfix][CI/Build] Fix test and improve code for `merge_async_iterators` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5096
* [Bugfix][CI/Build] Fix codespell failing to skip files in `git diff` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5097
* [Core] Avoid the need to pass `None` values to `Sequence.inputs` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5099
* [Bugfix] logprobs is not compatible with the OpenAI spec #4795 by @Etelis in https://github.com/vllm-project/vllm/pull/5031
* [Doc][Build] update after removing vllm-nccl by @youkaichao in https://github.com/vllm-project/vllm/pull/5103
* [Bugfix] gptq_marlin: Ensure g_idx_sort_indices is not a Parameter by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/5108
* [CI/Build] Docker cleanup functionality for amd servers  by @okakarpa in https://github.com/vllm-project/vllm/pull/5112
* [BUGFIX] [FRONTEND] Correct chat logprobs by @br3no in https://github.com/vllm-project/vllm/pull/5029
* [Bugfix] Automatically Detect SparseML models by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5119
* [CI/Build] increase wheel size limit to 200 MB by @youkaichao in https://github.com/vllm-project/vllm/pull/5130
* [Misc] remove duplicate definition of `seq_lens_tensor` in model_runner.py by @ita9naiwa in https://github.com/vllm-project/vllm/pull/5129
* [Doc] Use intersphinx and update entrypoints docs by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/5125
* add doc about serving option on dstack by @deep-diver in https://github.com/vllm-project/vllm/pull/3074
* Bump version to v0.4.3 by @simon-mo in https://github.com/vllm-project/vllm/pull/5046
* [Build] Disable sm_90a in cu11 by @simon-mo in https://github.com/vllm-project/vllm/pull/5141
* [Bugfix] Avoid Warnings in SparseML Activation Quantization by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/5120
* [Kernel] Marlin_24: Ensure the mma.sp instruction is using the ::ordered_metadata modifier (introduced with PTX 8.5) by @alexm-neuralmagic in https://github.com/vllm-project/vllm/pull/5136
* [Model] Support MAP-NEO model by @xingweiqu in https://github.com/vllm-project/vllm/pull/5081
* Revert "[Kernel] Marlin_24: Ensure the mma.sp instruction is using the ::ordered_metadata modifier (introduced with PTX 8.5)" by @simon-mo in https://github.com/vllm-project/vllm/pull/5149
* [Misc]: optimize eager mode host time by @functionxu123 in https://github.com/vllm-project/vllm/pull/4196
* [Model] Enable FP8 QKV in MoE and refine kernel tuning script by @comaniac in https://github.com/vllm-project/vllm/pull/5039
* [Doc] Add checkmark for GPTBigCodeForCausalLM LoRA support by @njhill in https://github.com/vllm-project/vllm/pull/5171
* [Build] Guard against older CUDA versions when building CUTLASS 3.x kernels by @tlrmchlsmth in https://github.com/vllm-project/vllm/pull/5168

## New Contributors
* @MahmoudAshraf97 made their first contribution in https://github.com/vllm-project/vllm/pull/4400
* @sfc-gh-hazhang made their first contribution in https://github.com/vllm-project/vllm/pull/4652
* @stevegrubb made their first contribution in https://github.com/vllm-project/vllm/pull/4719
* @heeju-kim2 made their first contribution in https://github.com/vllm-project/vllm/pull/4295
* @yikangshen made their first contribution in https://github.com/vllm-project/vllm/pull/4636
* @KuntaiDu made their first contribution in https://github.com/vllm-project/vllm/pull/4696
* @wuisawesome made their first contribution in https://github.com/vllm-project/vllm/pull/4794
* @aurickq made their first contribution in https://github.com/vllm-project/vllm/pull/4690
* @jinzhen-lin made their first contribution in https://github.com/vllm-project/vllm/pull/4788
* @dulacp made their first contribution in https://github.com/vllm-project/vllm/pull/4862
* @Silencioo made their first contribution in https://github.com/vllm-project/vllm/pull/4850
* @tlrmchlsmth made their first contribution in https://github.com/vllm-project/vllm/pull/4749
* @kerthcet made their first contribution in https://github.com/vllm-project/vllm/pull/4829
* @bofenghuang made their first contribution in https://github.com/vllm-project/vllm/pull/4688
* @eigen2017 made their first contribution in https://github.com/vllm-project/vllm/pull/4774
* @alexeykondrat made their first contribution in https://github.com/vllm-project/vllm/pull/4658
* @ZwwWayne made their first contribution in https://github.com/vllm-project/vllm/pull/4914
* @mzusman made their first contribution in https://github.com/vllm-project/vllm/pull/4916
* @hzhwcmhf made their first contribution in https://github.com/vllm-project/vllm/pull/4930
* @raywanb made their first contribution in https://github.com/vllm-project/vllm/pull/3949
* @LetianLee made their first contribution in https://github.com/vllm-project/vllm/pull/5009
* @dsikka made their first contribution in https://github.com/vllm-project/vllm/pull/4525
* @kezouke made their first contribution in https://github.com/vllm-project/vllm/pull/4985
* @linxihui made their first contribution in https://github.com/vllm-project/vllm/pull/4799
* @divakar-amd made their first contribution in https://github.com/vllm-project/vllm/pull/4951
* @pandyamarut made their first contribution in https://github.com/vllm-project/vllm/pull/5079
* @afeldman-nm made their first contribution in https://github.com/vllm-project/vllm/pull/4837
* @Etelis made their first contribution in https://github.com/vllm-project/vllm/pull/5031
* @okakarpa made their first contribution in https://github.com/vllm-project/vllm/pull/5112
* @deep-diver made their first contribution in https://github.com/vllm-project/vllm/pull/3074
* @xingweiqu made their first contribution in https://github.com/vllm-project/vllm/pull/5081
* @functionxu123 made their first contribution in https://github.com/vllm-project/vllm/pull/4196

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.4.2...v0.4.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.4.3)

---

## v0.4.2: v0.4.2
**Published:** 2024-05-05

## Highlights
### Features
* [Chunked prefill is ready for testing](https://docs.vllm.ai/en/latest/models/performance.html#chunked-prefill)! It improves inter-token latency in high load scenario by chunking the prompt processing and priortizes decode (#4580)
* Speculative decoding functionalities: logprobs (#4378), ngram (#4237)
* Support FlashInfer as attention backend (#4353)

### Models and Enhancements
* Add support for Phi-3-mini (#4298, #4372, #4380)
* Add more histogram metrics (#2764, #4523)
* Full tensor parallelism for LoRA layers (#3524)
* Expanding Marlin kernel to support all GPTQ models (#3922, #4466, #4533)

### Dependency Upgrade
* Upgrade to `torch==2.3.0` (#4454)
* Upgrade to `tensorizer==2.9.0` (#4467)
* Expansion of AMD test suite (#4267)

### Progress and Dev Experience
* Centralize and document all environment variables (#4548, #4574)
* Progress towards fully typed codebase (#4337, #4427, #4555, #4450)
* Progress towards pipeline parallelism (#4512, #4444, #4566)
* Progress towards multiprocessing based executors (#4348, #4402, #4419)
* Progress towards FP8 support (#4343, #4332, 4527)


## What's Changed
* [Core][Distributed] use existing torch.cuda.device context manager by @youkaichao in https://github.com/vllm-project/vllm/pull/4318
* [Misc] Update ShareGPT Dataset Sampling in Serving Benchmark by @ywang96 in https://github.com/vllm-project/vllm/pull/4279
* [Bugfix] Fix marlin kernel crash on H100 by @alexm-nm in https://github.com/vllm-project/vllm/pull/4218
* [Doc] Add note for docker user  by @youkaichao in https://github.com/vllm-project/vllm/pull/4340
* [Misc] Use public API in benchmark_throughput by @zifeitong in https://github.com/vllm-project/vllm/pull/4300
* [Model] Adds Phi-3 support by @caiom in https://github.com/vllm-project/vllm/pull/4298
* [Core] Move ray_utils.py from `engine` to `executor` package by @njhill in https://github.com/vllm-project/vllm/pull/4347
* [Bugfix][Model] Refactor OLMo model to support new HF format in transformers 4.40.0 by @Isotr0py in https://github.com/vllm-project/vllm/pull/4324
* [CI/Build] Adding functionality to reset the node's GPUs before processing. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/4213
* [Doc] README Phi-3 name fix. by @caiom in https://github.com/vllm-project/vllm/pull/4372
* [Core]refactor aqlm quant ops  by @jikunshang in https://github.com/vllm-project/vllm/pull/4351
* [Mypy] Typing lora folder by @rkooo567 in https://github.com/vllm-project/vllm/pull/4337
* [Misc] Optimize flash attention backend log  by @esmeetu in https://github.com/vllm-project/vllm/pull/4368
* [Core] Add `shutdown()` method to `ExecutorBase` by @njhill in https://github.com/vllm-project/vllm/pull/4349
* [Core] Move function tracing setup to util function by @njhill in https://github.com/vllm-project/vllm/pull/4352
* [ROCm][Hardware][AMD][Doc] Documentation update for ROCm by @hongxiayang in https://github.com/vllm-project/vllm/pull/4376
* [Bugfix] Fix parameter name in `get_tokenizer` by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4107
* [Frontend] Add --log-level option to api server by @normster in https://github.com/vllm-project/vllm/pull/4377
* [CI] Disable non-lazy string operation on logging by @rkooo567 in https://github.com/vllm-project/vllm/pull/4326
* [Core] Refactoring sampler and support prompt logprob for chunked prefill  by @rkooo567 in https://github.com/vllm-project/vllm/pull/4309
* [Misc][Refactor] Generalize linear_method to be quant_method by @comaniac in https://github.com/vllm-project/vllm/pull/4373
* [Misc] add RFC issue template by @youkaichao in https://github.com/vllm-project/vllm/pull/4401
* [Core] Introduce `DistributedGPUExecutor` abstract class by @njhill in https://github.com/vllm-project/vllm/pull/4348
* [Kernel] Optimize FP8 support for MoE kernel / Mixtral via static scales by @pcmoritz in https://github.com/vllm-project/vllm/pull/4343
* [Frontend][Bugfix] Disallow extra fields in OpenAI API by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4355
* [Misc] Fix logger format typo by @esmeetu in https://github.com/vllm-project/vllm/pull/4396
* [ROCm][Hardware][AMD] Enable group query attention for triton FA by @hongxiayang in https://github.com/vllm-project/vllm/pull/4406
* [Kernel] Full Tensor Parallelism for LoRA Layers by @FurtherAI in https://github.com/vllm-project/vllm/pull/3524
* [Model] Phi-3 4k sliding window temp. fix by @caiom in https://github.com/vllm-project/vllm/pull/4380
* [Bugfix][Core] Fix get decoding config from ray by @esmeetu in https://github.com/vllm-project/vllm/pull/4335
* [Bugfix] Abort requests when the connection to /v1/completions is interrupted by @chestnut-Q in https://github.com/vllm-project/vllm/pull/4363
* [BugFix] Fix `min_tokens` when `eos_token_id` is None by @njhill in https://github.com/vllm-project/vllm/pull/4389
* âœ¨ support local cache for models by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/4374
* [BugFix] Fix return type of executor execute_model methods by @njhill in https://github.com/vllm-project/vllm/pull/4402
* [BugFix] Resolved Issues For LinearMethod --> QuantConfig by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4418
* [Misc] fix typo in llm_engine init logging by @DefTruth in https://github.com/vllm-project/vllm/pull/4428
* Add more Prometheus metrics by @ronensc in https://github.com/vllm-project/vllm/pull/2764
* [CI] clean docker cache for neuron by @simon-mo in https://github.com/vllm-project/vllm/pull/4441
* [mypy][5/N] Support all typing on model executor by @rkooo567 in https://github.com/vllm-project/vllm/pull/4427
* [Kernel] Marlin Expansion: Support AutoGPTQ Models with Marlin by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3922
* [CI] hotfix: soft fail neuron test by @simon-mo in https://github.com/vllm-project/vllm/pull/4458
* [Core][Distributed] use cpu group to broadcast metadata in cpu by @youkaichao in https://github.com/vllm-project/vllm/pull/4444
* [Misc] Upgrade to `torch==2.3.0` by @mgoin in https://github.com/vllm-project/vllm/pull/4454
* [Bugfix][Kernel] Fix compute_type for MoE kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4463
* [Core]Refactor gptq_marlin ops by @jikunshang in https://github.com/vllm-project/vllm/pull/4466
* [BugFix] fix num_lookahead_slots missing in async executor by @leiwen83 in https://github.com/vllm-project/vllm/pull/4165
* [Doc] add visualization for multi-stage dockerfile by @prashantgupta24 in https://github.com/vllm-project/vllm/pull/4456
* [Kernel] Support Fp8 Checkpoints (Dynamic + Static) by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4332
* [Frontend] Support complex message content for chat completions endpoint by @fgreinacher in https://github.com/vllm-project/vllm/pull/3467
* [Frontend] [Core] Tensorizer: support dynamic `num_readers`, update version by @alpayariyak in https://github.com/vllm-project/vllm/pull/4467
* [Bugfix][Minor] Make ignore_eos effective  by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/4468
* fix_tokenizer_snapshot_download_bug by @kingljl in https://github.com/vllm-project/vllm/pull/4493
* Unable to find Punica extension issue during source code installation by @kingljl in https://github.com/vllm-project/vllm/pull/4494
* [Core] Centralize GPU Worker construction by @njhill in https://github.com/vllm-project/vllm/pull/4419
* [Misc][Typo] type annotation fix by @HarryWu99 in https://github.com/vllm-project/vllm/pull/4495
* [Misc] fix typo in block manager by @Juelianqvq in https://github.com/vllm-project/vllm/pull/4453
* Allow user to define whitespace pattern for outlines by @robcaulk in https://github.com/vllm-project/vllm/pull/4305
* [Misc]Add customized information for models by @jeejeelee in https://github.com/vllm-project/vllm/pull/4132
* [Test] Add ignore_eos test  by @rkooo567 in https://github.com/vllm-project/vllm/pull/4519
* [Bugfix] Fix the fp8 kv_cache check error that occurs when failing to obtain the CUDA version. by @AnyISalIn in https://github.com/vllm-project/vllm/pull/4173
* [Bugfix] Fix 307 Redirect for `/metrics` by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4523
* [Doc] update(example model): for OpenAI compatible serving by @fpaupier in https://github.com/vllm-project/vllm/pull/4503
* [Bugfix] Use random seed if seed is -1 by @sasha0552 in https://github.com/vllm-project/vllm/pull/4531
* [CI/Build][Bugfix] VLLM_USE_PRECOMPILED should skip compilation by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/4534
* [Speculative decoding] Add ngram prompt lookup decoding by @leiwen83 in https://github.com/vllm-project/vllm/pull/4237
* [Core] Enable prefix caching with block manager v2 enabled by @leiwen83 in https://github.com/vllm-project/vllm/pull/4142
* [Core] Add `multiproc_worker_utils` for multiprocessing-based workers by @njhill in https://github.com/vllm-project/vllm/pull/4357
* [Kernel] Update fused_moe tuning script for FP8 by @pcmoritz in https://github.com/vllm-project/vllm/pull/4457
* [Bugfix] Add validation for seed by @sasha0552 in https://github.com/vllm-project/vllm/pull/4529
* [Bugfix][Core] Fix and refactor logging stats by @esmeetu in https://github.com/vllm-project/vllm/pull/4336
* [Core][Distributed] fix pynccl del error by @youkaichao in https://github.com/vllm-project/vllm/pull/4508
* [Misc] Remove Mixtral device="cuda" declarations by @pcmoritz in https://github.com/vllm-project/vllm/pull/4543
* [Misc] Fix expert_ids shape in MoE by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4517
* [MISC] Rework logger to enable pythonic custom logging configuration to be provided by @tdg5 in https://github.com/vllm-project/vllm/pull/4273
* [Bug fix][Core] assert num_new_tokens == 1 fails when SamplingParams.n is not 1 and max_tokens is large & Add tests for preemption by @rkooo567 in https://github.com/vllm-project/vllm/pull/4451
* [CI]Add regression tests to ensure the async engine generates metrics by @ronensc in https://github.com/vllm-project/vllm/pull/4524
* [mypy][6/N] Fix all the core subdirectory typing by @rkooo567 in https://github.com/vllm-project/vllm/pull/4450
* [Core][Distributed] enable multiple tp group by @youkaichao in https://github.com/vllm-project/vllm/pull/4512
* [Kernel] Support running GPTQ 8-bit models in Marlin by @alexm-nm in https://github.com/vllm-project/vllm/pull/4533
* [mypy][7/N] Cover all directories by @rkooo567 in https://github.com/vllm-project/vllm/pull/4555
* [Misc] Exclude the `tests` directory from being packaged by @itechbear in https://github.com/vllm-project/vllm/pull/4552
* [BugFix] Include target-device specific requirements.txt in sdist by @markmc in https://github.com/vllm-project/vllm/pull/4559
* [Misc] centralize all usage of environment variables by @youkaichao in https://github.com/vllm-project/vllm/pull/4548
* [kernel] fix sliding window in prefix prefill Triton kernel by @mmoskal in https://github.com/vllm-project/vllm/pull/4405
* [CI/Build] AMD CI pipeline with extended set of tests. by @Alexei-V-Ivanov-AMD in https://github.com/vllm-project/vllm/pull/4267
* [Core] Ignore infeasible swap requests. by @rkooo567 in https://github.com/vllm-project/vllm/pull/4557
* [Core][Distributed] enable allreduce for multiple tp groups by @youkaichao in https://github.com/vllm-project/vllm/pull/4566
* [BugFix] Prevent the task of `_force_log` from being garbage collected by @Atry in https://github.com/vllm-project/vllm/pull/4567
* [Misc] remove chunk detected debug logs by @DefTruth in https://github.com/vllm-project/vllm/pull/4571
* [Doc] add env vars to the doc by @youkaichao in https://github.com/vllm-project/vllm/pull/4572
* [Core][Model runner refactoring 1/N] Refactor attn metadata term by @rkooo567 in https://github.com/vllm-project/vllm/pull/4518
* [Bugfix] Allow "None" or "" to be passed to CLI for string args that default to None by @mgoin in https://github.com/vllm-project/vllm/pull/4586
* Fix/async chat serving by @schoennenbeck in https://github.com/vllm-project/vllm/pull/2727
* [Kernel] Use flashinfer for decoding by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/4353
* [Speculative decoding] Support target-model logprobs by @cadedaniel in https://github.com/vllm-project/vllm/pull/4378
* [Misc] add installation time env vars by @youkaichao in https://github.com/vllm-project/vllm/pull/4574
* [Misc][Refactor] Introduce ExecuteModelData by @comaniac in https://github.com/vllm-project/vllm/pull/4540
* [Doc] Chunked Prefill Documentation by @rkooo567 in https://github.com/vllm-project/vllm/pull/4580
* [Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) by @mgoin in https://github.com/vllm-project/vllm/pull/4527
* [CI] check size of the wheels by @simon-mo in https://github.com/vllm-project/vllm/pull/4319
* [Bugfix] Fix inappropriate content of model_name tag in Prometheus metrics by @DearPlanet in https://github.com/vllm-project/vllm/pull/3937
* bump version to v0.4.2 by @simon-mo in https://github.com/vllm-project/vllm/pull/4600
* [CI] Reduce wheel size by not shipping debug symbols by @simon-mo in https://github.com/vllm-project/vllm/pull/4602

## New Contributors
* @zifeitong made their first contribution in https://github.com/vllm-project/vllm/pull/4300
* @caiom made their first contribution in https://github.com/vllm-project/vllm/pull/4298
* @Alexei-V-Ivanov-AMD made their first contribution in https://github.com/vllm-project/vllm/pull/4213
* @normster made their first contribution in https://github.com/vllm-project/vllm/pull/4377
* @FurtherAI made their first contribution in https://github.com/vllm-project/vllm/pull/3524
* @chestnut-Q made their first contribution in https://github.com/vllm-project/vllm/pull/4363
* @prashantgupta24 made their first contribution in https://github.com/vllm-project/vllm/pull/4374
* @fgreinacher made their first contribution in https://github.com/vllm-project/vllm/pull/3467
* @alpayariyak made their first contribution in https://github.com/vllm-project/vllm/pull/4467
* @HarryWu99 made their first contribution in https://github.com/vllm-project/vllm/pull/4495
* @Juelianqvq made their first contribution in https://github.com/vllm-project/vllm/pull/4453
* @robcaulk made their first contribution in https://github.com/vllm-project/vllm/pull/4305
* @AnyISalIn made their first contribution in https://github.com/vllm-project/vllm/pull/4173
* @sasha0552 made their first contribution in https://github.com/vllm-project/vllm/pull/4531
* @tdg5 made their first contribution in https://github.com/vllm-project/vllm/pull/4273
* @itechbear made their first contribution in https://github.com/vllm-project/vllm/pull/4552
* @markmc made their first contribution in https://github.com/vllm-project/vllm/pull/4559
* @Atry made their first contribution in https://github.com/vllm-project/vllm/pull/4567
* @schoennenbeck made their first contribution in https://github.com/vllm-project/vllm/pull/2727
* @DearPlanet made their first contribution in https://github.com/vllm-project/vllm/pull/3937

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.4.1...v0.4.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.4.2)

---

## v0.4.1: v0.4.1
**Published:** 2024-04-24

## Highlights

Features
* Support and enhance CommandR+ (#3829), minicpm (#3893), Meta Llama 3 (#4175, #4182), Mixtral 8x22b (#4073, #4002)
* Support private model registration, and updating our support policy (#3871, 3948)
* Support PyTorch 2.2.1 and Triton 2.2.0 (#4061, #4079, #3805, #3904, #4271)
* Add option for using LM Format Enforcer for guided decoding (#3868)
* Add option for optionally initialize tokenizer and detokenizer (#3748)
* Add option for load model using `tensorizer` (#3476)

Enhancements
* vLLM is now mostly type checked by `mypy` (#3816, #4006, #4161, #4043)
* Progress towards chunked prefill scheduler (#3550, #3853, #4280, #3884)
* Progress towards speculative decoding (#3250, #3706, #3894)
* Initial support with dynamic per-tensor scaling via FP8 (#4118)

Hardwares
* Intel CPU inference backend is added (#3993, #3634)
* AMD backend is enhanced with Triton kernel and e4m3fn KV cache (#3643, #3290)

## What's Changed
* [Kernel] Layernorm performance optimization by @mawong-amd in https://github.com/vllm-project/vllm/pull/3662
* [Doc] Update installation doc for build from source and explain the dependency on torch/cuda version by @youkaichao in https://github.com/vllm-project/vllm/pull/3746
* [CI/Build] Make Marlin Tests Green by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3753
* [Misc] Minor fixes in requirements.txt by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3769
* [Misc] Some minor simplifications to detokenization logic by @njhill in https://github.com/vllm-project/vllm/pull/3670
* [Misc] Fix Benchmark TTFT Calculation for Chat Completions by @ywang96 in https://github.com/vllm-project/vllm/pull/3768
* [Speculative decoding 4/9] Lookahead scheduling for speculative decoding by @cadedaniel in https://github.com/vllm-project/vllm/pull/3250
* [Misc] Add support for new autogptq checkpoint_format by @Qubitium in https://github.com/vllm-project/vllm/pull/3689
* [Misc] [CI/Build] Speed up block manager CPU-only unit tests ~10x by opting-out of GPU cleanup by @cadedaniel in https://github.com/vllm-project/vllm/pull/3783
* [Hardware][Intel] Add CPU inference backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/3634
* [HotFix] [CI/Build] Minor fix for CPU backend CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/3787
* [Frontend][Bugfix] allow using the default middleware with a root path by @A-Mahla in https://github.com/vllm-project/vllm/pull/3788
* [Doc] Fix vLLMEngine Doc Page by @ywang96 in https://github.com/vllm-project/vllm/pull/3791
* [CI/Build] fix TORCH_CUDA_ARCH_LIST in wheel build by @youkaichao in https://github.com/vllm-project/vllm/pull/3801
* Fix crash when try torch.cuda.set_device in worker by @leiwen83 in https://github.com/vllm-project/vllm/pull/3770
* [Bugfix] Add `__init__.py` files for `vllm/core/block/` and `vllm/spec_decode/` by @mgoin in https://github.com/vllm-project/vllm/pull/3798
* [CI/Build] 0.4.0.post1, fix sm 7.0/7.5 binary by @youkaichao in https://github.com/vllm-project/vllm/pull/3803
* [Speculative decoding] Adding configuration object for speculative decoding by @cadedaniel in https://github.com/vllm-project/vllm/pull/3706
* [BugFix] Use different mechanism to get vllm version in `is_cpu()` by @njhill in https://github.com/vllm-project/vllm/pull/3804
* [Doc] Update README.md by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3806
* [Doc] Update contribution guidelines for better onboarding by @michaelfeil in https://github.com/vllm-project/vllm/pull/3819
* [3/N] Refactor scheduler for chunked prefill scheduling by @rkooo567 in https://github.com/vllm-project/vllm/pull/3550
* Enable scaled FP8 (e4m3fn) KV cache on ROCm (AMD GPU) by @AdrianAbeyta in https://github.com/vllm-project/vllm/pull/3290
* [Misc] Publish 3rd meetup slides by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3835
* Fixes the argument for local_tokenizer_group by @sighingnow in https://github.com/vllm-project/vllm/pull/3754
* [Core] Enable hf_transfer by default if available by @michaelfeil in https://github.com/vllm-project/vllm/pull/3817
* [Bugfix] Add kv_scale input parameter to CPU backend by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3840
* [Core] [Frontend] Make detokenization optional by @mgerstgrasser in https://github.com/vllm-project/vllm/pull/3749
* [Bugfix] Fix args in benchmark_serving by @CatherineSue in https://github.com/vllm-project/vllm/pull/3836
* [Benchmark] Refactor sample_requests in benchmark_throughput by @gty111 in https://github.com/vllm-project/vllm/pull/3613
* [Core] manage nccl via a pypi package & upgrade to pt 2.2.1 by @youkaichao in https://github.com/vllm-project/vllm/pull/3805
* [Hardware][CPU] Update cpu torch to match default of 2.2.1 by @mgoin in https://github.com/vllm-project/vllm/pull/3854
* [Model] Cohere CommandR+ by @saurabhdash2512 in https://github.com/vllm-project/vllm/pull/3829
* [Core] improve robustness of pynccl by @youkaichao in https://github.com/vllm-project/vllm/pull/3860
* [Doc]Add asynchronous engine arguments to documentation. by @SeanGallen in https://github.com/vllm-project/vllm/pull/3810
* [CI/Build] fix pip cache with vllm_nccl & refactor dockerfile to build wheels by @youkaichao in https://github.com/vllm-project/vllm/pull/3859
* [Misc] Add pytest marker to opt-out of global test cleanup by @cadedaniel in https://github.com/vllm-project/vllm/pull/3863
* [Misc] Fix linter issues in examples/fp8/quantizer/quantize.py by @cadedaniel in https://github.com/vllm-project/vllm/pull/3864
* [Bugfix] Fixing requirements.txt by @noamgat in https://github.com/vllm-project/vllm/pull/3865
* [Misc] Define common requirements by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3841
* Add option to completion API to truncate prompt tokens by @tdoublep in https://github.com/vllm-project/vllm/pull/3144
* [Chunked Prefill][4/n] Chunked prefill scheduler. by @rkooo567 in https://github.com/vllm-project/vllm/pull/3853
* [Bugfix] Fix incorrect output on OLMo models in Tensor Parallelism by @Isotr0py in https://github.com/vllm-project/vllm/pull/3869
* [CI/Benchmark] add more iteration and use multiple percentiles for robust latency benchmark by @youkaichao in https://github.com/vllm-project/vllm/pull/3889
* [Core] enable out-of-tree model register by @youkaichao in https://github.com/vllm-project/vllm/pull/3871
* [WIP][Core] latency optimization by @youkaichao in https://github.com/vllm-project/vllm/pull/3890
* [Bugfix] Fix Llava inference with Tensor Parallelism. by @Isotr0py in https://github.com/vllm-project/vllm/pull/3883
* [Model] add minicpm by @SUDA-HLT-ywfang in https://github.com/vllm-project/vllm/pull/3893
* [Bugfix] Added Command-R GPTQ support by @egortolmachev in https://github.com/vllm-project/vllm/pull/3849
* [Bugfix] Enable Proper `attention_bias` Usage in Llama Model Configuration by @Ki6an in https://github.com/vllm-project/vllm/pull/3767
* [Hotfix][CI/Build][Kernel] CUDA 11.8 does not support layernorm optimizations by @mawong-amd in https://github.com/vllm-project/vllm/pull/3782
* [BugFix][Model] Fix commandr RoPE max_position_embeddings by @esmeetu in https://github.com/vllm-project/vllm/pull/3919
* [Core] separate distributed_init from worker by @youkaichao in https://github.com/vllm-project/vllm/pull/3904
* [Misc] [Core] Implement RFC "Augment BaseExecutor interfaces to enable hardware-agnostic speculative decoding" by @cadedaniel in https://github.com/vllm-project/vllm/pull/3837
* [Bugfix] Fix KeyError on loading GPT-NeoX by @jsato8094 in https://github.com/vllm-project/vllm/pull/3925
* [ROCm][Hardware][AMD] Use Triton Kernel for default FA on ROCm by @jpvillam-amd in https://github.com/vllm-project/vllm/pull/3643
* [Misc] Avoid loading incorrect LoRA config by @jeejeelee in https://github.com/vllm-project/vllm/pull/3777
* [Benchmark] Add cpu options to bench scripts by @PZD-CHINA in https://github.com/vllm-project/vllm/pull/3915
* [Bugfix]  fix utils.py/merge_dict func TypeError: 'type' object is not subscriptable by @zhaotyer in https://github.com/vllm-project/vllm/pull/3955
* [Bugfix] Fix logits processor when prompt_logprobs is not None by @huyiwen in https://github.com/vllm-project/vllm/pull/3899
* [Bugfix] handle prompt_logprobs in _apply_min_tokens_penalty by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/3876
* [Bugfix][ROCm] Add numba to Dockerfile.rocm by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3962
* [Model][AMD] ROCm support for 256 head dims for Gemma by @jamestwhedbee in https://github.com/vllm-project/vllm/pull/3972
* [Doc] Add doc to state our model support policy by @youkaichao in https://github.com/vllm-project/vllm/pull/3948
* [Bugfix] Remove key sorting for `guided_json` parameter in OpenAi compatible Server by @dmarasco in https://github.com/vllm-project/vllm/pull/3945
* [Doc] Fix getting stared to use publicly available model by @fpaupier in https://github.com/vllm-project/vllm/pull/3963
* [Bugfix] handle hf_config with architectures == None by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/3982
* [WIP][Core][Refactor] move vllm/model_executor/parallel_utils into vllm/distributed and vllm/device_communicators by @youkaichao in https://github.com/vllm-project/vllm/pull/3950
* [Core][5/N] Fully working chunked prefill e2e by @rkooo567 in https://github.com/vllm-project/vllm/pull/3884
* [Core][Model] Use torch.compile to accelerate layernorm in commandr by @youkaichao in https://github.com/vllm-project/vllm/pull/3985
* [Test] Add xformer and flash attn tests by @rkooo567 in https://github.com/vllm-project/vllm/pull/3961
* [Misc] refactor ops and cache_ops layer  by @jikunshang in https://github.com/vllm-project/vllm/pull/3913
* [Doc][Installation] delete python setup.py develop by @youkaichao in https://github.com/vllm-project/vllm/pull/3989
* [Kernel] Fused MoE Config for Mixtral 8x22 by @ywang96 in https://github.com/vllm-project/vllm/pull/4002
* fix-bgmv-kernel-640 by @kingljl in https://github.com/vllm-project/vllm/pull/4007
* [Hardware][Intel] Isolate CPUModelRunner and ModelRunner for better maintenance by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/3824
* [Core] Set `linear_weights` directly on the layer by @Yard1 in https://github.com/vllm-project/vllm/pull/3977
* [Core][Distributed] make init_distributed_environment compatible with init_process_group by @youkaichao in https://github.com/vllm-project/vllm/pull/4014
* Fix echo/logprob OpenAI completion bug by @dylanwhawk in https://github.com/vllm-project/vllm/pull/3441
* [Kernel] Add extra punica sizes to support bigger vocabs by @Yard1 in https://github.com/vllm-project/vllm/pull/4015
* [BugFix] Fix handling of stop strings and stop token ids by @njhill in https://github.com/vllm-project/vllm/pull/3672
* [Doc] Add typing hints / mypy types cleanup by @michaelfeil in https://github.com/vllm-project/vllm/pull/3816
* [Core] Support LoRA on quantized models by @jeejeelee in https://github.com/vllm-project/vllm/pull/4012
* [Frontend][Core] Move `merge_async_iterators` to utils by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4026
* [Test] Test multiple attn backend for chunked prefill.  by @rkooo567 in https://github.com/vllm-project/vllm/pull/4023
* [Bugfix] fix type hint for py 3.8 by @youkaichao in https://github.com/vllm-project/vllm/pull/4036
* [Misc] Fix typo in scheduler.py by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4022
* [mypy] Add mypy type annotation part 1 by @rkooo567 in https://github.com/vllm-project/vllm/pull/4006
* [Core] fix custom allreduce default value by @youkaichao in https://github.com/vllm-project/vllm/pull/4040
* Fix triton compilation issue by @Bellk17 in https://github.com/vllm-project/vllm/pull/3984
* [Bugfix] Fix LoRA bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/4032
* [CI/Test] expand ruff and yapf for all supported python version by @youkaichao in https://github.com/vllm-project/vllm/pull/4037
* [Bugfix] More type hint fixes for py 3.8 by @dylanwhawk in https://github.com/vllm-project/vllm/pull/4039
* [Core][Distributed] improve logging for init dist by @youkaichao in https://github.com/vllm-project/vllm/pull/4042
* [Bugfix] fix_log_time_in_metrics by @zspo in https://github.com/vllm-project/vllm/pull/4050
* [Bugfix] fix_small_bug_in_neuron_executor by @zspo in https://github.com/vllm-project/vllm/pull/4051
* [Kernel] Add punica dimension for Baichuan-13B by @jeejeelee in https://github.com/vllm-project/vllm/pull/4053
* [Frontend] [Core] feat: Add model loading using `tensorizer` by @sangstar in https://github.com/vllm-project/vllm/pull/3476
* [Core] avoid too many cuda context by caching p2p test by @youkaichao in https://github.com/vllm-project/vllm/pull/4021
* [BugFix] Fix tensorizer extra in setup.py by @njhill in https://github.com/vllm-project/vllm/pull/4072
* [Docs] document that mixtral 8x22b is supported by @simon-mo in https://github.com/vllm-project/vllm/pull/4073
* [Misc] Upgrade triton to 2.2.0 by @esmeetu in https://github.com/vllm-project/vllm/pull/4061
* [Bugfix] Fix filelock version requirement by @zhuohan123 in https://github.com/vllm-project/vllm/pull/4075
* [Misc][Minor] Fix CPU block num log in CPUExecutor. by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/4088
* [Core] Simplifications to executor classes by @njhill in https://github.com/vllm-project/vllm/pull/4071
* [Doc] Add better clarity for tensorizer usage by @sangstar in https://github.com/vllm-project/vllm/pull/4090
* [Bugfix] Fix ray workers profiling with nsight  by @rickyyx in https://github.com/vllm-project/vllm/pull/4095
* [Typing] Fix Sequence type GenericAlias only available after Python 3.9. by @rkooo567 in https://github.com/vllm-project/vllm/pull/4092
* [Core] Fix engine-use-ray broken  by @rkooo567 in https://github.com/vllm-project/vllm/pull/4105
* LM Format Enforcer Guided Decoding Support by @noamgat in https://github.com/vllm-project/vllm/pull/3868
* [Core] Refactor model loading code by @Yard1 in https://github.com/vllm-project/vllm/pull/4097
* [Speculative decoding 6/9] Integrate speculative decoding with LLMEngine  by @cadedaniel in https://github.com/vllm-project/vllm/pull/3894
* [Misc] [CI] Fix CI failure caught after merge by @cadedaniel in https://github.com/vllm-project/vllm/pull/4126
* [CI] Move CPU/AMD tests to after wait by @cadedaniel in https://github.com/vllm-project/vllm/pull/4123
* [Core] replace narrow-usage RayWorkerVllm to general WorkerWrapper to reduce code duplication by @youkaichao in https://github.com/vllm-project/vllm/pull/4024
* [Bugfix] fix output parsing error for trtllm backend by @elinx in https://github.com/vllm-project/vllm/pull/4137
* [Kernel] Add punica dimension for Swallow-MS-7B LoRA by @ucciicci in https://github.com/vllm-project/vllm/pull/4134
* [Typing] Mypy typing part 2 by @rkooo567 in https://github.com/vllm-project/vllm/pull/4043
* [Core] Add integrity check during initialization; add test for it by @youkaichao in https://github.com/vllm-project/vllm/pull/4155
* Allow model to be served under multiple names by @hmellor in https://github.com/vllm-project/vllm/pull/2894
* [Bugfix] Get available quantization methods from quantization registry by @mgoin in https://github.com/vllm-project/vllm/pull/4098
* [Bugfix][Kernel] allow non-power-of-two head sizes in prefix prefill by @mmoskal in https://github.com/vllm-project/vllm/pull/4128
* [Docs] document that Meta Llama 3 is supported by @simon-mo in https://github.com/vllm-project/vllm/pull/4175
* [Bugfix] Support logprobs when using guided_json and other constrained decoding fields by @jamestwhedbee in https://github.com/vllm-project/vllm/pull/4149
* [Misc] Bump transformers to latest version by @njhill in https://github.com/vllm-project/vllm/pull/4176
* [CI/CD] add neuron docker and ci test scripts by @liangfu in https://github.com/vllm-project/vllm/pull/3571
* [Bugfix] Fix CustomAllreduce pcie nvlink topology detection (#3974) by @agt in https://github.com/vllm-project/vllm/pull/4159
* [Core] add an option to log every function call to for debugging hang/crash in distributed inference by @youkaichao in https://github.com/vllm-project/vllm/pull/4079
* Support eos_token_id from generation_config.json by @simon-mo in https://github.com/vllm-project/vllm/pull/4182
* [Bugfix] Fix LoRA loading check by @jeejeelee in https://github.com/vllm-project/vllm/pull/4138
* Bump version of 0.4.1 by @simon-mo in https://github.com/vllm-project/vllm/pull/4177
* [Misc] fix docstrings by @UranusSeven in https://github.com/vllm-project/vllm/pull/4191
* [Bugfix][Core] Restore logging of stats in the async engine by @ronensc in https://github.com/vllm-project/vllm/pull/4150
* [Misc] add nccl in collect env by @youkaichao in https://github.com/vllm-project/vllm/pull/4211
* Pass `tokenizer_revision` when getting tokenizer in openai serving by @chiragjn in https://github.com/vllm-project/vllm/pull/4214
* [Bugfix] Add fix for JSON whitespace by @ayusher in https://github.com/vllm-project/vllm/pull/4189
* Fix missing docs and out of sync `EngineArgs` by @hmellor in https://github.com/vllm-project/vllm/pull/4219
* [Kernel][FP8] Initial support with dynamic per-tensor scaling by @comaniac in https://github.com/vllm-project/vllm/pull/4118
* [Frontend] multiple sampling params support  by @nunjunj in https://github.com/vllm-project/vllm/pull/3570
* Updating lm-format-enforcer version and adding links to decoding libraries in docs by @noamgat in https://github.com/vllm-project/vllm/pull/4222
* Don't show default value for flags in `EngineArgs` by @hmellor in https://github.com/vllm-project/vllm/pull/4223
* [Doc]: Update the page of adding new models by @YeFD in https://github.com/vllm-project/vllm/pull/4236
* Make initialization of tokenizer and detokenizer optional by @GeauxEric in https://github.com/vllm-project/vllm/pull/3748
* [AMD][Hardware][Misc][Bugfix] xformer cleanup and light navi logic and CI fixes and refactoring by @hongxiayang in https://github.com/vllm-project/vllm/pull/4129
* [Core][Distributed] fix _is_full_nvlink detection by @youkaichao in https://github.com/vllm-project/vllm/pull/4233
* [Misc] Add vision language model support to CPU backend by @Isotr0py in https://github.com/vllm-project/vllm/pull/3968
* [Bugfix] Fix type annotations in CPU model runner by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4256
* [Frontend] Enable support for CPU backend in AsyncLLMEngine. by @sighingnow in https://github.com/vllm-project/vllm/pull/3993
* [Bugfix] Ensure download_weights_from_hf(..) inside loader is using the revision parameter by @alexm-nm in https://github.com/vllm-project/vllm/pull/4217
* Add example scripts to documentation by @hmellor in https://github.com/vllm-project/vllm/pull/4225
* [Core] Scheduler perf fix by @rkooo567 in https://github.com/vllm-project/vllm/pull/4270
* [Doc] Update the SkyPilot doc with serving and Llama-3 by @Michaelvll in https://github.com/vllm-project/vllm/pull/4276
* [Core][Distributed] use absolute path for library file by @youkaichao in https://github.com/vllm-project/vllm/pull/4271
* Fix `autodoc` directives by @hmellor in https://github.com/vllm-project/vllm/pull/4272
* [Mypy] Part 3 fix typing for nested directories for most of directory by @rkooo567 in https://github.com/vllm-project/vllm/pull/4161
* [Core] Some simplification of WorkerWrapper changes by @njhill in https://github.com/vllm-project/vllm/pull/4183
* [Core] Scheduling optimization 2 by @rkooo567 in https://github.com/vllm-project/vllm/pull/4280
* [Speculative decoding 7/9] Speculative decoding end-to-end correctness tests. by @cadedaniel in https://github.com/vllm-project/vllm/pull/3951
* [Bugfix] Fixing max token error message for openai compatible server by @jgordley in https://github.com/vllm-project/vllm/pull/4016
* [Bugfix] Add init_cached_hf_modules to RayWorkerWrapper by @DefTruth in https://github.com/vllm-project/vllm/pull/4286
* [Core][Logging] Add last frame information for better debugging by @youkaichao in https://github.com/vllm-project/vllm/pull/4278
* [CI] Add ccache for wheel builds job by @simon-mo in https://github.com/vllm-project/vllm/pull/4281
* AQLM CUDA support by @jaemzfleming in https://github.com/vllm-project/vllm/pull/3287
* [Bugfix][Frontend] Raise exception when file-like chat template fails to be opened by @DarkLight1337 in https://github.com/vllm-project/vllm/pull/4292
* [Kernel] FP8 support for MoE kernel / Mixtral by @pcmoritz in https://github.com/vllm-project/vllm/pull/4244
* [Bugfix] fixed fp8 conflict with aqlm by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/4307
* [Core][Distributed] use cpu/gloo to initialize pynccl by @youkaichao in https://github.com/vllm-project/vllm/pull/4248
* [CI][Build] change pynvml to nvidia-ml-py by @youkaichao in https://github.com/vllm-project/vllm/pull/4302
* [Misc] Reduce supported Punica dtypes by @WoosukKwon in https://github.com/vllm-project/vllm/pull/4304

## New Contributors
* @mawong-amd made their first contribution in https://github.com/vllm-project/vllm/pull/3662
* @Qubitium made their first contribution in https://github.com/vllm-project/vllm/pull/3689
* @bigPYJ1151 made their first contribution in https://github.com/vllm-project/vllm/pull/3634
* @A-Mahla made their first contribution in https://github.com/vllm-project/vllm/pull/3788
* @AdrianAbeyta made their first contribution in https://github.com/vllm-project/vllm/pull/3290
* @mgerstgrasser made their first contribution in https://github.com/vllm-project/vllm/pull/3749
* @CatherineSue made their first contribution in https://github.com/vllm-project/vllm/pull/3836
* @saurabhdash2512 made their first contribution in https://github.com/vllm-project/vllm/pull/3829
* @SeanGallen made their first contribution in https://github.com/vllm-project/vllm/pull/3810
* @SUDA-HLT-ywfang made their first contribution in https://github.com/vllm-project/vllm/pull/3893
* @egortolmachev made their first contribution in https://github.com/vllm-project/vllm/pull/3849
* @Ki6an made their first contribution in https://github.com/vllm-project/vllm/pull/3767
* @jsato8094 made their first contribution in https://github.com/vllm-project/vllm/pull/3925
* @jpvillam-amd made their first contribution in https://github.com/vllm-project/vllm/pull/3643
* @PZD-CHINA made their first contribution in https://github.com/vllm-project/vllm/pull/3915
* @zhaotyer made their first contribution in https://github.com/vllm-project/vllm/pull/3955
* @huyiwen made their first contribution in https://github.com/vllm-project/vllm/pull/3899
* @dmarasco made their first contribution in https://github.com/vllm-project/vllm/pull/3945
* @fpaupier made their first contribution in https://github.com/vllm-project/vllm/pull/3963
* @kingljl made their first contribution in https://github.com/vllm-project/vllm/pull/4007
* @DarkLight1337 made their first contribution in https://github.com/vllm-project/vllm/pull/4026
* @Bellk17 made their first contribution in https://github.com/vllm-project/vllm/pull/3984
* @sangstar made their first contribution in https://github.com/vllm-project/vllm/pull/3476
* @rickyyx made their first contribution in https://github.com/vllm-project/vllm/pull/4095
* @elinx made their first contribution in https://github.com/vllm-project/vllm/pull/4137
* @ucciicci made their first contribution in https://github.com/vllm-project/vllm/pull/4134
* @mmoskal made their first contribution in https://github.com/vllm-project/vllm/pull/4128
* @agt made their first contribution in https://github.com/vllm-project/vllm/pull/4159
* @ayusher made their first contribution in https://github.com/vllm-project/vllm/pull/4189
* @nunjunj made their first contribution in https://github.com/vllm-project/vllm/pull/3570
* @YeFD made their first contribution in https://github.com/vllm-project/vllm/pull/4236
* @GeauxEric made their first contribution in https://github.com/vllm-project/vllm/pull/3748
* @alexm-nm made their first contribution in https://github.com/vllm-project/vllm/pull/4217
* @jgordley made their first contribution in https://github.com/vllm-project/vllm/pull/4016
* @DefTruth made their first contribution in https://github.com/vllm-project/vllm/pull/4286
* @jaemzfleming made their first contribution in https://github.com/vllm-project/vllm/pull/3287

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.4.0...v0.4.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.4.1)

---

## v0.4.0.post1: v0.4.0.post1, restore sm70/75 support
**Published:** 2024-04-02

## Highlight

v0.4.0 lacks support for sm70/75 support. We did a hotfix for it.

## What's Changed
* [Kernel] Layernorm performance optimization by @mawong-amd in https://github.com/vllm-project/vllm/pull/3662
* [Doc] Update installation doc for build from source and explain the dependency on torch/cuda version by @youkaichao in https://github.com/vllm-project/vllm/pull/3746
* [CI/Build] Make Marlin Tests Green by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3753
* [Misc] Minor fixes in requirements.txt by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3769
* [Misc] Some minor simplifications to detokenization logic by @njhill in https://github.com/vllm-project/vllm/pull/3670
* [Misc] Fix Benchmark TTFT Calculation for Chat Completions by @ywang96 in https://github.com/vllm-project/vllm/pull/3768
* [Speculative decoding 4/9] Lookahead scheduling for speculative decoding by @cadedaniel in https://github.com/vllm-project/vllm/pull/3250
* [Misc] Add support for new autogptq checkpoint_format by @Qubitium in https://github.com/vllm-project/vllm/pull/3689
* [Misc] [CI/Build] Speed up block manager CPU-only unit tests ~10x by opting-out of GPU cleanup by @cadedaniel in https://github.com/vllm-project/vllm/pull/3783
* [Hardware][Intel] Add CPU inference backend by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/3634
* [HotFix] [CI/Build] Minor fix for CPU backend CI by @bigPYJ1151 in https://github.com/vllm-project/vllm/pull/3787
* [Frontend][Bugfix] allow using the default middleware with a root path by @A-Mahla in https://github.com/vllm-project/vllm/pull/3788
* [Doc] Fix vLLMEngine Doc Page by @ywang96 in https://github.com/vllm-project/vllm/pull/3791
* [CI/Build] fix TORCH_CUDA_ARCH_LIST in wheel build by @youkaichao in https://github.com/vllm-project/vllm/pull/3801
* Fix crash when try torch.cuda.set_device in worker by @leiwen83 in https://github.com/vllm-project/vllm/pull/3770
* [Bugfix] Add `__init__.py` files for `vllm/core/block/` and `vllm/spec_decode/` by @mgoin in https://github.com/vllm-project/vllm/pull/3798
* [CI/Build] 0.4.0.post1, fix sm 7.0/7.5 binary by @youkaichao in https://github.com/vllm-project/vllm/pull/3803

## New Contributors
* @mawong-amd made their first contribution in https://github.com/vllm-project/vllm/pull/3662
* @Qubitium made their first contribution in https://github.com/vllm-project/vllm/pull/3689
* @bigPYJ1151 made their first contribution in https://github.com/vllm-project/vllm/pull/3634
* @A-Mahla made their first contribution in https://github.com/vllm-project/vllm/pull/3788

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.4.0...v0.4.0.post1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.4.0.post1)

---

## v0.4.0: v0.4.0
**Published:** 2024-03-30

## Major changes
### Models
* New models: Command+R(#3433), Qwen2 MoE(#3346), DBRX(#3660), XVerse (#3610), Jais (#3183).
* New vision language model: LLaVA (#3042)

### Production features
* Automatic prefix caching (#2762, #3703) supporting long system prompt to be automatically cached across requests. Use the flag `--enable-prefix-caching` to turn it on.
* Support `json_object` in OpenAI server for arbitrary JSON, `--use-delay` flag to improve time to first token across many requests, and `min_tokens` to EOS suppression.
* Progress in chunked prefill scheduler (#3236, #3538), and speculative decoding (#3103).
* Custom all reduce kernel has been re-enabled after more robustness fixes.
* Replaced cupy dependency due to its bugs.

### Hardware
* Improved Neuron support for AWS Inferentia.
* CMake based build system for extensibility.

### Ecosystem
* Extensive serving benchmark refactoring (#3277)
* Usage statistics collection (#2852)

## What's Changed
* allow user chose log level by --log-level instead of fixed 'info'. by @AllenDou in https://github.com/vllm-project/vllm/pull/3109
* Reorder kv dtype check to avoid nvcc not found error on AMD platform by @cloudhan in https://github.com/vllm-project/vllm/pull/3104
* Add Automatic Prefix Caching by @SageMoore in https://github.com/vllm-project/vllm/pull/2762
* Add vLLM version info to logs and openai API server by @jasonacox in https://github.com/vllm-project/vllm/pull/3161
* [FIX] Fix styles in automatic prefix caching & add a automatic prefix caching benchmark by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3158
* Make it easy to profile workers with nsight by @pcmoritz in https://github.com/vllm-project/vllm/pull/3162
* [DOC] add setup document to support neuron backend by @liangfu in https://github.com/vllm-project/vllm/pull/2777
* [Minor Fix] Remove unused code in benchmark_prefix_caching.py by @gty111 in https://github.com/vllm-project/vllm/pull/3171
* Add document for vllm paged attention kernel. by @pian13131 in https://github.com/vllm-project/vllm/pull/2978
* enable --gpu-memory-utilization in benchmark_throughput.py by @AllenDou in https://github.com/vllm-project/vllm/pull/3175
* [Minor fix] The domain dns.google may cause a socket.gaierror exception by @ttbachyinsda in https://github.com/vllm-project/vllm/pull/3176
* Push logprob generation to LLMEngine by @Yard1 in https://github.com/vllm-project/vllm/pull/3065
* Add health check, make async Engine more robust by @Yard1 in https://github.com/vllm-project/vllm/pull/3015
* Fix the openai benchmarking requests to work with latest OpenAI apis by @wangchen615 in https://github.com/vllm-project/vllm/pull/2992
* [ROCm] enable cupy in order to enable  cudagraph mode for AMD GPUs by @hongxiayang in https://github.com/vllm-project/vllm/pull/3123
* Store `eos_token_id` in `Sequence` for easy access by @njhill in https://github.com/vllm-project/vllm/pull/3166
* [Fix] Avoid pickling entire LLMEngine for Ray workers by @njhill in https://github.com/vllm-project/vllm/pull/3207
* [Tests] Add block manager and scheduler tests by @rkooo567 in https://github.com/vllm-project/vllm/pull/3108
* [Testing] Fix core tests by @cadedaniel in https://github.com/vllm-project/vllm/pull/3224
* A simple addition of `dynamic_ncols=True` by @chujiezheng in https://github.com/vllm-project/vllm/pull/3242
* Add GPTQ support for Gemma by @TechxGenus in https://github.com/vllm-project/vllm/pull/3200
* Update requirements-dev.txt to include package for benchmarking scripts. by @wangchen615 in https://github.com/vllm-project/vllm/pull/3181
* Separate attention backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3005
* Measure model memory usage by @mgoin in https://github.com/vllm-project/vllm/pull/3120
* Possible fix for conflict between Automated Prefix Caching (#2762) and multi-LoRA support (#1804) by @jacobthebanana in https://github.com/vllm-project/vllm/pull/3263
* Fix auto prefix bug by @ElizaWszola in https://github.com/vllm-project/vllm/pull/3239
* Connect engine healthcheck to openai server by @njhill in https://github.com/vllm-project/vllm/pull/3260
* Feature add lora support for Qwen2 by @whyiug in https://github.com/vllm-project/vllm/pull/3177
* [Minor Fix] Fix comments in benchmark_serving by @gty111 in https://github.com/vllm-project/vllm/pull/3252
* [Docs] Fix Unmocked Imports by @ywang96 in https://github.com/vllm-project/vllm/pull/3275
* [FIX] Make `flash_attn` optional by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3269
* Move model filelocks from `/tmp/` to `~/.cache/vllm/locks/` dir by @mgoin in https://github.com/vllm-project/vllm/pull/3241
* [FIX] Fix prefix test error on main by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3286
* [Speculative decoding 3/9] Worker which speculates, scores, and applies rejection sampling by @cadedaniel in https://github.com/vllm-project/vllm/pull/3103
* Enhance lora tests with more layer and rank variations by @tterrysun in https://github.com/vllm-project/vllm/pull/3243
* [ROCM] Fix blockReduceSum to use correct warp counts for ROCm and CUDA by @dllehr-amd in https://github.com/vllm-project/vllm/pull/3262
* [BugFix] Fix get tokenizer when using ray  by @esmeetu in https://github.com/vllm-project/vllm/pull/3301
* [Fix] Fix best_of behavior when n=1 by @njhill in https://github.com/vllm-project/vllm/pull/3298
* Re-enable the 80 char line width limit by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3305
* [docs] Add LoRA support information for models by @pcmoritz in https://github.com/vllm-project/vllm/pull/3299
* Add distributed model executor abstraction by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3191
* [ROCm] Fix warp and lane calculation in blockReduceSum by @kliuae in https://github.com/vllm-project/vllm/pull/3321
* Support Mistral Model Inference with transformers-neuronx by @DAIZHENWEI in https://github.com/vllm-project/vllm/pull/3153
* docs: Add BentoML deployment doc by @Sherlock113 in https://github.com/vllm-project/vllm/pull/3336
* Fixes #1556 double free by @br3no in https://github.com/vllm-project/vllm/pull/3347
* Add kernel for GeGLU with approximate GELU by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3337
* [Fix] fix quantization arg when using marlin by @DreamTeamWangbowen in https://github.com/vllm-project/vllm/pull/3319
* add hf_transfer to requirements.txt by @RonanKMcGovern in https://github.com/vllm-project/vllm/pull/3031
* fix bias in if, ambiguous by @hliuca in https://github.com/vllm-project/vllm/pull/3259
* [Minor Fix] Use cupy-cuda11x in CUDA 11.8 build by @chenxu2048 in https://github.com/vllm-project/vllm/pull/3256
* Add missing kernel for CodeLlama-34B on A/H100 (no tensor parallelism) when using Multi-LoRA. by @orsharir in https://github.com/vllm-project/vllm/pull/3350
* Add batched RoPE kernel by @tterrysun in https://github.com/vllm-project/vllm/pull/3095
* Fix lint by @Yard1 in https://github.com/vllm-project/vllm/pull/3388
* [FIX] Simpler fix for async engine running on ray by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3371
* [Hotfix] [Debug] test_openai_server.py::test_guided_regex_completion by @simon-mo in https://github.com/vllm-project/vllm/pull/3383
* allow user to chose which vllm's merics to display in grafana by @AllenDou in https://github.com/vllm-project/vllm/pull/3393
* [Kernel] change benchmark script so that result can be directly used; tune moe kernel in A100/H100 with tp=2,4,8 by @youkaichao in https://github.com/vllm-project/vllm/pull/3389
* Install `flash_attn` in Docker image by @tdoublep in https://github.com/vllm-project/vllm/pull/3396
* Add args for mTLS support by @declark1 in https://github.com/vllm-project/vllm/pull/3410
* [issue templates] add some issue templates by @youkaichao in https://github.com/vllm-project/vllm/pull/3412
* Fix assertion failure in Qwen 1.5 with prefix caching enabled by @chenxu2048 in https://github.com/vllm-project/vllm/pull/3373
* fix marlin config repr by @qeternity in https://github.com/vllm-project/vllm/pull/3414
* Feature: dynamic shared mem moe_align_block_size_kernel by @akhoroshev in https://github.com/vllm-project/vllm/pull/3376
* [Misc] add HOST_IP env var by @youkaichao in https://github.com/vllm-project/vllm/pull/3419
* Add chat templates for Falcon by @Dinghow in https://github.com/vllm-project/vllm/pull/3420
* Add chat templates for ChatGLM by @Dinghow in https://github.com/vllm-project/vllm/pull/3418
* Fix `dist.broadcast` stall without group argument by @GindaChen in https://github.com/vllm-project/vllm/pull/3408
* Fix tie_word_embeddings for Qwen2. by @fyabc in https://github.com/vllm-project/vllm/pull/3344
* [Fix] Add args for mTLS support by @declark1 in https://github.com/vllm-project/vllm/pull/3430
* Fixes the misuse/mixuse of time.time()/time.monotonic() by @sighingnow in https://github.com/vllm-project/vllm/pull/3220
* [Misc] add error message in non linux platform by @youkaichao in https://github.com/vllm-project/vllm/pull/3438
* Fix issue templates by @hmellor in https://github.com/vllm-project/vllm/pull/3436
* fix document error for value and v_vec illustration by @laneeeee in https://github.com/vllm-project/vllm/pull/3421
* Asynchronous tokenization by @Yard1 in https://github.com/vllm-project/vllm/pull/2879
* Removed Extraneous Print Message From OAI Server by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3440
* [Misc] PR templates by @youkaichao in https://github.com/vllm-project/vllm/pull/3413
* Fixes the incorrect argument in the prefix-prefill test cases by @sighingnow in https://github.com/vllm-project/vllm/pull/3246
* Replace `lstrip()` with `removeprefix()` to fix Ruff linter warning by @ronensc in https://github.com/vllm-project/vllm/pull/2958
* Fix Baichuan chat template by @Dinghow in https://github.com/vllm-project/vllm/pull/3340
* [Misc] fix line length for entire codebase by @simon-mo in https://github.com/vllm-project/vllm/pull/3444
* Support arbitrary json_object in OpenAI and Context Free Grammar by @simon-mo in https://github.com/vllm-project/vllm/pull/3211
* Fix setup.py neuron-ls issue by @simon-mo in https://github.com/vllm-project/vllm/pull/2671
* [Misc] Define from_dict and to_dict in InputMetadata by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3452
* [CI] Shard tests for LoRA and Kernels to speed up by @simon-mo in https://github.com/vllm-project/vllm/pull/3445
* [Bugfix] Make moe_align_block_size AMD-compatible by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3470
* CI: Add ROCm Docker Build by @simon-mo in https://github.com/vllm-project/vllm/pull/2886
* [Testing] Add test_config.py to CI by @cadedaniel in https://github.com/vllm-project/vllm/pull/3437
* [CI/Build] Fix Bad Import In Test by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/3473
* [Misc] Fix PR Template by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3478
* Cmake based build system by @bnellnm in https://github.com/vllm-project/vllm/pull/2830
* [Core] Zero-copy asdict for InputMetadata by @Yard1 in https://github.com/vllm-project/vllm/pull/3475
* [Misc] Update README for the Third vLLM Meetup by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3479
* [Core] Cache some utils by @Yard1 in https://github.com/vllm-project/vllm/pull/3474
* [Core] print error before deadlock by @youkaichao in https://github.com/vllm-project/vllm/pull/3459
* [Doc] Add docs about OpenAI compatible server by @simon-mo in https://github.com/vllm-project/vllm/pull/3288
* [BugFix] Avoid initializing CUDA too early by @njhill in https://github.com/vllm-project/vllm/pull/3487
* Update dockerfile with ModelScope support by @ifsheldon in https://github.com/vllm-project/vllm/pull/3429
* [Doc] minor fix to neuron-installation.rst by @jimburtoft in https://github.com/vllm-project/vllm/pull/3505
* Revert "[Core] Cache some utils" by @simon-mo in https://github.com/vllm-project/vllm/pull/3507
* [Doc] minor fix of spelling in amd-installation.rst by @jimburtoft in https://github.com/vllm-project/vllm/pull/3506
* Use lru_cache for some environment detection utils by @simon-mo in https://github.com/vllm-project/vllm/pull/3508
* [PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled by @ElizaWszola in https://github.com/vllm-project/vllm/pull/3357
* [Core] Add generic typing to `LRUCache` by @njhill in https://github.com/vllm-project/vllm/pull/3511
* [Misc] Remove cache stream and cache events by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3461
* Abort when nvcc command is not found in the PATH by @AllenDou in https://github.com/vllm-project/vllm/pull/3527
* Check for _is_cuda() in compute_num_jobs by @bnellnm in https://github.com/vllm-project/vllm/pull/3481
* [Bugfix] Fix ROCm support in CMakeLists.txt by @jamestwhedbee in https://github.com/vllm-project/vllm/pull/3534
* [1/n] Triton sampling kernel by @Yard1 in https://github.com/vllm-project/vllm/pull/3186
* [1/n][Chunked Prefill] Refactor input query shapes by @rkooo567 in https://github.com/vllm-project/vllm/pull/3236
* Migrate `logits` computation and gather to `model_runner` by @esmeetu in https://github.com/vllm-project/vllm/pull/3233
* [BugFix] Hot fix in setup.py for neuron build by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3537
* [PREFIX CACHING FOLLOW UP] OrderedDict-based evictor by @ElizaWszola in https://github.com/vllm-project/vllm/pull/3431
* Fix 1D query issue from `_prune_hidden_states` by @rkooo567 in https://github.com/vllm-project/vllm/pull/3539
* [ðŸš€ Ready to be merged] Added support for Jais models by @grandiose-pizza in https://github.com/vllm-project/vllm/pull/3183
* [Misc][Log] Add log for tokenizer length not equal to vocabulary size by @esmeetu in https://github.com/vllm-project/vllm/pull/3500
* [Misc] Bump up transformers to v4.39.0 & Remove StarCoder2Config by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3551
* [BugFix] gemma loading after quantization or LoRA. by @taeminlee in https://github.com/vllm-project/vllm/pull/3553
* [Bugfix][Model] Fix Qwen2 by @esmeetu in https://github.com/vllm-project/vllm/pull/3554
* [Hardware][Neuron] Refactor neuron support by @zhuohan123 in https://github.com/vllm-project/vllm/pull/3471
* Some fixes for custom allreduce kernels by @hanzhi713 in https://github.com/vllm-project/vllm/pull/2760
* Dynamic scheduler delay to improve ITL performance  by @tdoublep in https://github.com/vllm-project/vllm/pull/3279
* [Core] Improve detokenization performance for prefill by @Yard1 in https://github.com/vllm-project/vllm/pull/3469
* [Bugfix] use SoftLockFile instead of LockFile by @kota-iizuka in https://github.com/vllm-project/vllm/pull/3578
* [Misc] Fix BLOOM copyright notice by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3591
* [Misc] Bump transformers version by @ywang96 in https://github.com/vllm-project/vllm/pull/3592
* [BugFix] Fix Falcon tied embeddings by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3590
* [BugFix] 1D query fix for MoE models by @njhill in https://github.com/vllm-project/vllm/pull/3597
* [CI] typo fix: is_hip --> is_hip() by @youkaichao in https://github.com/vllm-project/vllm/pull/3595
* [CI/Build] respect the common environment variable MAX_JOBS by @youkaichao in https://github.com/vllm-project/vllm/pull/3600
* [CI/Build] fix flaky test by @youkaichao in https://github.com/vllm-project/vllm/pull/3602
* [BugFix] minor fix: method typo in `rotary_embedding.py` file, get_device() -> device by @jikunshang in https://github.com/vllm-project/vllm/pull/3604
* [Bugfix] Revert "[Bugfix] use SoftLockFile instead of LockFile (#3578)" by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3599
* [Model] Add starcoder2 awq support by @shaonianyr in https://github.com/vllm-project/vllm/pull/3569
* [Core] Refactor Attention Take 2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3462
* [Bugfix] fix automatic prefix args and add log info by @gty111 in https://github.com/vllm-project/vllm/pull/3608
* [CI] Try introducing isort.  by @rkooo567 in https://github.com/vllm-project/vllm/pull/3495
* [Core] Adding token ranks along with logprobs by @SwapnilDreams100 in https://github.com/vllm-project/vllm/pull/3516
* feat: implement the min_tokens sampling parameter by @tjohnson31415 in https://github.com/vllm-project/vllm/pull/3124
* [Bugfix] API stream returning two stops by @dylanwhawk in https://github.com/vllm-project/vllm/pull/3450
* hotfix isort on logprobs ranks pr by @simon-mo in https://github.com/vllm-project/vllm/pull/3622
* [Feature] Add vision language model support. by @xwjiang2010 in https://github.com/vllm-project/vllm/pull/3042
* Optimize `_get_ranks` in Sampler by @Yard1 in https://github.com/vllm-project/vllm/pull/3623
* [Misc] Include matched stop string/token in responses by @njhill in https://github.com/vllm-project/vllm/pull/2976
* Enable more models to  inference based on LoRA by @jeejeelee in https://github.com/vllm-project/vllm/pull/3382
* [Bugfix] Fix ipv6 address parsing bug by @liiliiliil in https://github.com/vllm-project/vllm/pull/3641
* [BugFix] Fix ipv4 address parsing regression by @njhill in https://github.com/vllm-project/vllm/pull/3645
* [Kernel] support non-zero cuda devices  in punica kernels by @jeejeelee in https://github.com/vllm-project/vllm/pull/3636
* [Doc]add lora support by @jeejeelee in https://github.com/vllm-project/vllm/pull/3649
* [Misc] Minor fix in KVCache type by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3652
* [Core] remove cupy dependency by @youkaichao in https://github.com/vllm-project/vllm/pull/3625
* [Bugfix] More faithful implementation of Gemma by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3653
* [Bugfix] [Hotfix] fix nccl library name by @youkaichao in https://github.com/vllm-project/vllm/pull/3661
* [Model] Add support for DBRX by @megha95 in https://github.com/vllm-project/vllm/pull/3660
* [Misc] add the "download-dir" option to the latency/throughput benchmarks by @AmadeusChan in https://github.com/vllm-project/vllm/pull/3621
* feat(benchmarks): Add Prefix Caching Benchmark to Serving Benchmark by @ywang96 in https://github.com/vllm-project/vllm/pull/3277
* Add support for Cohere's Command-R model by @zeppombal in https://github.com/vllm-project/vllm/pull/3433
* [Docs] Add Command-R to supported models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3669
* [Model] Fix and clean commandr by @esmeetu in https://github.com/vllm-project/vllm/pull/3671
* [Model] Add support for xverse  by @hxer7963 in https://github.com/vllm-project/vllm/pull/3610
* [CI/Build] update default number of jobs and nvcc threads to avoid overloading the system by @youkaichao in https://github.com/vllm-project/vllm/pull/3675
* [Kernel] Add Triton MoE kernel configs for DBRX + A100 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3679
* [Core] [Bugfix] Refactor block manager subsystem for better testability by @cadedaniel in https://github.com/vllm-project/vllm/pull/3492
* [Model] Add support for Qwen2MoeModel by @wenyujin333 in https://github.com/vllm-project/vllm/pull/3346
* [Kernel] DBRX Triton MoE kernel H100 by @ywang96 in https://github.com/vllm-project/vllm/pull/3692
* [2/N] Chunked prefill data update by @rkooo567 in https://github.com/vllm-project/vllm/pull/3538
* [Bugfix] Update neuron_executor.py to add optional vision_language_config. by @adamrb in https://github.com/vllm-project/vllm/pull/3695
* fix benchmark format reporting in buildkite by @simon-mo in https://github.com/vllm-project/vllm/pull/3693
* [CI] Add test case to run examples scripts by @simon-mo in https://github.com/vllm-project/vllm/pull/3638
* [Core] Support multi-node inference(eager and cuda graph) by @esmeetu in https://github.com/vllm-project/vllm/pull/3686
* [Kernel] Add MoE Triton kernel configs for A100 40GB by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3700
* [Bugfix] Set enable_prefix_caching=True in prefix caching example by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3703
* fix logging msg for block manager by @simon-mo in https://github.com/vllm-project/vllm/pull/3701
* [Core] fix del of communicator by @youkaichao in https://github.com/vllm-project/vllm/pull/3702
* [Benchmark] Change mii to use persistent deployment and support tensor parallel by @IKACE in https://github.com/vllm-project/vllm/pull/3628
* bump version to v0.4.0 by @simon-mo in https://github.com/vllm-project/vllm/pull/3705
* Revert "bump version to v0.4.0" by @youkaichao in https://github.com/vllm-project/vllm/pull/3708
* [Test] Make model tests run again and remove --forked from pytest by @rkooo567 in https://github.com/vllm-project/vllm/pull/3631
* [Misc] Minor type annotation fix by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3716
* [Core][Test] move local_rank to the last arg with default value to keep api compatible by @youkaichao in https://github.com/vllm-project/vllm/pull/3711
* add ccache to docker build image by @simon-mo in https://github.com/vllm-project/vllm/pull/3704
* Usage Stats Collection by @yhu422 in https://github.com/vllm-project/vllm/pull/2852
* [BugFix] Fix tokenizer out of vocab size by @esmeetu in https://github.com/vllm-project/vllm/pull/3685
* [BugFix][Frontend] Fix completion logprobs=0 error by @esmeetu in https://github.com/vllm-project/vllm/pull/3731
* [Bugfix] Command-R Max Model Length by @ywang96 in https://github.com/vllm-project/vllm/pull/3727
* bump version to v0.4.0 by @simon-mo in https://github.com/vllm-project/vllm/pull/3712
* [ROCm][Bugfix] Fixed several bugs related to rccl path and attention selector logic by @hongxiayang in https://github.com/vllm-project/vllm/pull/3699
* usage lib get version another way by @simon-mo in https://github.com/vllm-project/vllm/pull/3735
* [BugFix] Use consistent logger everywhere by @njhill in https://github.com/vllm-project/vllm/pull/3738
* [Core][Bugfix] cache len of tokenizer by @youkaichao in https://github.com/vllm-project/vllm/pull/3741
* Fix build when nvtools is missing by @bnellnm in https://github.com/vllm-project/vllm/pull/3698
* CMake build elf without PTX by @simon-mo in https://github.com/vllm-project/vllm/pull/3739

## New Contributors
* @cloudhan made their first contribution in https://github.com/vllm-project/vllm/pull/3104
* @SageMoore made their first contribution in https://github.com/vllm-project/vllm/pull/2762
* @jasonacox made their first contribution in https://github.com/vllm-project/vllm/pull/3161
* @gty111 made their first contribution in https://github.com/vllm-project/vllm/pull/3171
* @pian13131 made their first contribution in https://github.com/vllm-project/vllm/pull/2978
* @ttbachyinsda made their first contribution in https://github.com/vllm-project/vllm/pull/3176
* @wangchen615 made their first contribution in https://github.com/vllm-project/vllm/pull/2992
* @chujiezheng made their first contribution in https://github.com/vllm-project/vllm/pull/3242
* @TechxGenus made their first contribution in https://github.com/vllm-project/vllm/pull/3200
* @mgoin made their first contribution in https://github.com/vllm-project/vllm/pull/3120
* @jacobthebanana made their first contribution in https://github.com/vllm-project/vllm/pull/3263
* @ElizaWszola made their first contribution in https://github.com/vllm-project/vllm/pull/3239
* @DAIZHENWEI made their first contribution in https://github.com/vllm-project/vllm/pull/3153
* @Sherlock113 made their first contribution in https://github.com/vllm-project/vllm/pull/3336
* @br3no made their first contribution in https://github.com/vllm-project/vllm/pull/3347
* @DreamTeamWangbowen made their first contribution in https://github.com/vllm-project/vllm/pull/3319
* @RonanKMcGovern made their first contribution in https://github.com/vllm-project/vllm/pull/3031
* @hliuca made their first contribution in https://github.com/vllm-project/vllm/pull/3259
* @orsharir made their first contribution in https://github.com/vllm-project/vllm/pull/3350
* @youkaichao made their first contribution in https://github.com/vllm-project/vllm/pull/3389
* @tdoublep made their first contribution in https://github.com/vllm-project/vllm/pull/3396
* @declark1 made their first contribution in https://github.com/vllm-project/vllm/pull/3410
* @qeternity made their first contribution in https://github.com/vllm-project/vllm/pull/3414
* @akhoroshev made their first contribution in https://github.com/vllm-project/vllm/pull/3376
* @Dinghow made their first contribution in https://github.com/vllm-project/vllm/pull/3420
* @fyabc made their first contribution in https://github.com/vllm-project/vllm/pull/3344
* @laneeeee made their first contribution in https://github.com/vllm-project/vllm/pull/3421
* @bnellnm made their first contribution in https://github.com/vllm-project/vllm/pull/2830
* @ifsheldon made their first contribution in https://github.com/vllm-project/vllm/pull/3429
* @jimburtoft made their first contribution in https://github.com/vllm-project/vllm/pull/3505
* @grandiose-pizza made their first contribution in https://github.com/vllm-project/vllm/pull/3183
* @taeminlee made their first contribution in https://github.com/vllm-project/vllm/pull/3553
* @kota-iizuka made their first contribution in https://github.com/vllm-project/vllm/pull/3578
* @shaonianyr made their first contribution in https://github.com/vllm-project/vllm/pull/3569
* @SwapnilDreams100 made their first contribution in https://github.com/vllm-project/vllm/pull/3516
* @tjohnson31415 made their first contribution in https://github.com/vllm-project/vllm/pull/3124
* @xwjiang2010 made their first contribution in https://github.com/vllm-project/vllm/pull/3042
* @liiliiliil made their first contribution in https://github.com/vllm-project/vllm/pull/3641
* @AmadeusChan made their first contribution in https://github.com/vllm-project/vllm/pull/3621
* @zeppombal made their first contribution in https://github.com/vllm-project/vllm/pull/3433
* @hxer7963 made their first contribution in https://github.com/vllm-project/vllm/pull/3610
* @wenyujin333 made their first contribution in https://github.com/vllm-project/vllm/pull/3346
* @adamrb made their first contribution in https://github.com/vllm-project/vllm/pull/3695
* @IKACE made their first contribution in https://github.com/vllm-project/vllm/pull/3628
* @yhu422 made their first contribution in https://github.com/vllm-project/vllm/pull/2852

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.3.3...v0.4.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.4.0)

---

## v0.3.3: v0.3.3
**Published:** 2024-03-01

## Major changes

* StarCoder2 support
* Performance optimization and LoRA support for Gemma
* 2/3/8-bit GPTQ support
* Integrate Marlin Kernels for Int4 GPTQ inference
* Performance optimization for MoE kernel
* [Experimental] AWS Inferentia2 support
* [Experimental] Structured output (JSON, Regex) in OpenAI Server

## What's Changed
* Update a comment in `benchmark_serving.py` by @ronensc in https://github.com/vllm-project/vllm/pull/2934
* Added early stopping to completion APIs by @Maxusmusti in https://github.com/vllm-project/vllm/pull/2939
* Migrate MistralForCausalLM to LlamaForCausalLM by @esmeetu in https://github.com/vllm-project/vllm/pull/2868
* Use Llama RMSNorm for Gemma by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2974
* chore(vllm): codespell for spell checking  by @mspronesti in https://github.com/vllm-project/vllm/pull/2820
* Optimize GeGLU layer in Gemma by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2975
* [FIX] Fix issue #2904 by @44670 in https://github.com/vllm-project/vllm/pull/2983
* Remove Flash Attention in test env by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2982
* Include tokens from prompt phase in `counter_generation_tokens` by @ronensc in https://github.com/vllm-project/vllm/pull/2802
* Fix nvcc not found in vllm-openai image by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/2781
* [Fix] Fix assertion on Mistral YaRN model len by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2984
* Port metrics from `aioprometheus` to `prometheus_client` by @hmellor in https://github.com/vllm-project/vllm/pull/2730
* Add LogProbs for Chat Completions in OpenAI by @jlcmoore in https://github.com/vllm-project/vllm/pull/2918
* Optimized fused MoE Kernel, take 2 by @pcmoritz in https://github.com/vllm-project/vllm/pull/2979
* [Minor] Remove gather_cached_kv kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3043
* [Minor] Remove unused config file by @esmeetu in https://github.com/vllm-project/vllm/pull/3039
* Fix using CuPy for eager mode by @esmeetu in https://github.com/vllm-project/vllm/pull/3037
* Fix stablelm by @esmeetu in https://github.com/vllm-project/vllm/pull/3038
* Support Orion model by @dachengai in https://github.com/vllm-project/vllm/pull/2539
* fix `get_ip` error in pure ipv6 environment by @Jingru in https://github.com/vllm-project/vllm/pull/2931
* [Minor] Fix type annotation in fused moe by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3045
* Support logit bias for OpenAI API by @dylanwhawk in https://github.com/vllm-project/vllm/pull/3027
* [Minor] Fix StableLMEpochForCausalLM -> StableLmForCausalLM by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3046
* Enables GQA support in the prefix prefill kernels by @sighingnow in https://github.com/vllm-project/vllm/pull/3007
* multi-lora documentation fix by @ElefHead in https://github.com/vllm-project/vllm/pull/3064
* Restrict prometheus_client >= 0.18.0 to prevent errors when importing pkgs by @AllenDou in https://github.com/vllm-project/vllm/pull/3070
* Support inference with transformers-neuronx by @liangfu in https://github.com/vllm-project/vllm/pull/2569
* Add LoRA support for Gemma by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3050
* Add Support for 2/3/8-bit GPTQ Quantization Models by @chu-tianxiang in https://github.com/vllm-project/vllm/pull/2330
* Fix: `AttributeError` in OpenAI-compatible server by @jaywonchung in https://github.com/vllm-project/vllm/pull/3018
* add cache_config's info to prometheus metrics. by @AllenDou in https://github.com/vllm-project/vllm/pull/3100
* Support starcoder2 architecture by @sh0416 in https://github.com/vllm-project/vllm/pull/3089
* Fix building from source on WSL by @aliencaocao in https://github.com/vllm-project/vllm/pull/3112
* [Fix] Don't deep-copy LogitsProcessors when copying SamplingParams by @njhill in https://github.com/vllm-project/vllm/pull/3099
* Add guided decoding for OpenAI API server by @felixzhu555 in https://github.com/vllm-project/vllm/pull/2819
* Fix: Output text is always truncated in some models by @HyperdriveHustle in https://github.com/vllm-project/vllm/pull/3016
* Remove exclude_unset in streaming response by @sh0416 in https://github.com/vllm-project/vllm/pull/3143
* docs: Add tutorial on deploying vLLM model with KServe by @terrytangyuan in https://github.com/vllm-project/vllm/pull/2586
* fix relative import path of protocol.py by @Huarong in https://github.com/vllm-project/vllm/pull/3134
* Integrate Marlin Kernels for Int4 GPTQ inference by @robertgshaw2-neuralmagic in https://github.com/vllm-project/vllm/pull/2497
* Bump up to v0.3.3 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/3129

## New Contributors
* @Maxusmusti made their first contribution in https://github.com/vllm-project/vllm/pull/2939
* @44670 made their first contribution in https://github.com/vllm-project/vllm/pull/2983
* @jlcmoore made their first contribution in https://github.com/vllm-project/vllm/pull/2918
* @dachengai made their first contribution in https://github.com/vllm-project/vllm/pull/2539
* @dylanwhawk made their first contribution in https://github.com/vllm-project/vllm/pull/3027
* @ElefHead made their first contribution in https://github.com/vllm-project/vllm/pull/3064
* @AllenDou made their first contribution in https://github.com/vllm-project/vllm/pull/3070
* @jaywonchung made their first contribution in https://github.com/vllm-project/vllm/pull/3018
* @sh0416 made their first contribution in https://github.com/vllm-project/vllm/pull/3089
* @aliencaocao made their first contribution in https://github.com/vllm-project/vllm/pull/3112
* @felixzhu555 made their first contribution in https://github.com/vllm-project/vllm/pull/2819
* @HyperdriveHustle made their first contribution in https://github.com/vllm-project/vllm/pull/3016
* @terrytangyuan made their first contribution in https://github.com/vllm-project/vllm/pull/2586
* @Huarong made their first contribution in https://github.com/vllm-project/vllm/pull/3134

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.3.2...v0.3.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.3.3)

---

## v0.3.2: v0.3.2
**Published:** 2024-02-21

## Major Changes

This version adds support for the OLMo and Gemma Model, as well as `seed` parameter. 

## What's Changed
* Defensively copy `sampling_params` by @njhill in https://github.com/vllm-project/vllm/pull/2881
* multi-LoRA as extra models in OpenAI server by @jvmncs in https://github.com/vllm-project/vllm/pull/2775
* Add code-revision config argument for Hugging Face Hub by @mbm-ai in https://github.com/vllm-project/vllm/pull/2892
* [Minor] Small fix to make distributed init logic in worker looks cleaner by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2905
* [Test] Add basic correctness test by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2908
* Support OLMo models. by @Isotr0py in https://github.com/vllm-project/vllm/pull/2832
* Add warning to prevent changes to benchmark api server by @simon-mo in https://github.com/vllm-project/vllm/pull/2858
* Fix `vllm:prompt_tokens_total` metric calculation by @ronensc in https://github.com/vllm-project/vllm/pull/2869
* [ROCm] include gfx908 as supported by @jamestwhedbee in https://github.com/vllm-project/vllm/pull/2792
* [FIX] Fix beam search test by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2930
* Make vLLM logging formatting optional by @Yard1 in https://github.com/vllm-project/vllm/pull/2877
* Add metrics to RequestOutput by @Yard1 in https://github.com/vllm-project/vllm/pull/2876
* Add Gemma model by @xiangxu-google in https://github.com/vllm-project/vllm/pull/2964
* Upgrade transformers to v4.38.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2965
* [FIX] Add Gemma model to the doc by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2966
* [ROCm] Upgrade transformers to v4.38.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2967
* Support per-request seed by @njhill in https://github.com/vllm-project/vllm/pull/2514
* Bump up version to v0.3.2 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2968

## New Contributors
* @jvmncs made their first contribution in https://github.com/vllm-project/vllm/pull/2775
* @mbm-ai made their first contribution in https://github.com/vllm-project/vllm/pull/2892
* @Isotr0py made their first contribution in https://github.com/vllm-project/vllm/pull/2832
* @jamestwhedbee made their first contribution in https://github.com/vllm-project/vllm/pull/2792

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.3.1...v0.3.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.3.2)

---

## v0.3.1: v0.3.1
**Published:** 2024-02-16

## Major Changes

This version fixes the following major bugs:
* Memory leak with distributed execution. (Solved by using CuPY for collective communication).
* Support for Python 3.8.

Also with many smaller bug fixes listed below.

## What's Changed
* Fixes assertion failure in prefix caching: the lora index mapping should respect `prefix_len`. by @sighingnow in https://github.com/vllm-project/vllm/pull/2688
* fix some bugs about parameter description by @zspo in https://github.com/vllm-project/vllm/pull/2689
* [Minor] Fix test_cache.py CI test failure by @pcmoritz in https://github.com/vllm-project/vllm/pull/2684
* Add unit test for Mixtral MoE layer by @pcmoritz in https://github.com/vllm-project/vllm/pull/2677
* Refactor Prometheus and Add Request Level Metrics by @rib-2 in https://github.com/vllm-project/vllm/pull/2316
* Add Internlm2 by @Leymore in https://github.com/vllm-project/vllm/pull/2666
* Fix compile error when using rocm by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/2648
* fix python 3.8 syntax by @simon-mo in https://github.com/vllm-project/vllm/pull/2716
* Update README for meetup slides by @simon-mo in https://github.com/vllm-project/vllm/pull/2718
* Use revision when downloading the quantization config file by @Pernekhan in https://github.com/vllm-project/vllm/pull/2697
* remove hardcoded `device="cuda" ` to support more device by @jikunshang in https://github.com/vllm-project/vllm/pull/2503
* fix length_penalty default value to 1.0 by @zspo in https://github.com/vllm-project/vllm/pull/2667
* Add one example to run batch inference distributed on Ray by @c21 in https://github.com/vllm-project/vllm/pull/2696
* docs: update langchain serving instructions by @mspronesti in https://github.com/vllm-project/vllm/pull/2736
* Set&Get llm internal tokenizer instead of the TokenizerGroup by @dancingpipi in https://github.com/vllm-project/vllm/pull/2741
* Remove eos tokens from output by default by @zcnrex in https://github.com/vllm-project/vllm/pull/2611
* add requirement: triton >= 2.1.0 by @whyiug in https://github.com/vllm-project/vllm/pull/2746
* [Minor] Fix benchmark_latency by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2765
* [ROCm] Fix some kernels failed unit tests by @hongxiayang in https://github.com/vllm-project/vllm/pull/2498
* Set local logging level via env variable by @gardberg in https://github.com/vllm-project/vllm/pull/2774
* [ROCm] Fixup arch checks for ROCM by @dllehr-amd in https://github.com/vllm-project/vllm/pull/2627
* Add fused top-K softmax kernel for MoE by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2769
* fix issue when model parameter is not a model id but path of the model. by @liuyhwangyh in https://github.com/vllm-project/vllm/pull/2489
* [Minor] More fix of test_cache.py CI test failure by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/2750
* [ROCm] Fix build problem resulted from previous commit related to FP8 kv-cache support  by @hongxiayang in https://github.com/vllm-project/vllm/pull/2790
* Add documentation on how to do incremental builds by @pcmoritz in https://github.com/vllm-project/vllm/pull/2796
* [Ray] Integration compiled DAG off by default by @rkooo567 in https://github.com/vllm-project/vllm/pull/2471
* Disable custom all reduce by default by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2808
* [ROCm] support Radeonâ„¢ 7900 series (gfx1100) without using flash-attention by @hongxiayang in https://github.com/vllm-project/vllm/pull/2768
* Add documentation section about LoRA by @pcmoritz in https://github.com/vllm-project/vllm/pull/2834
* Refactor 2 awq gemm kernels into m16nXk32 by @zcnrex in https://github.com/vllm-project/vllm/pull/2723
* Serving Benchmark Refactoring by @ywang96 in https://github.com/vllm-project/vllm/pull/2433
* [CI] Ensure documentation build is checked in CI by @simon-mo in https://github.com/vllm-project/vllm/pull/2842
* Refactor llama family models by @esmeetu in https://github.com/vllm-project/vllm/pull/2637
* Revert "Refactor llama family models" by @pcmoritz in https://github.com/vllm-project/vllm/pull/2851
* Use CuPy for CUDA graphs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2811
* Remove Yi model definition, please use `LlamaForCausalLM` instead by @pcmoritz in https://github.com/vllm-project/vllm/pull/2854
* Add LoRA support for Mixtral by @tterrysun in https://github.com/vllm-project/vllm/pull/2831
* Migrate InternLMForCausalLM to LlamaForCausalLM by @pcmoritz in https://github.com/vllm-project/vllm/pull/2860
* Fix internlm after https://github.com/vllm-project/vllm/pull/2860 by @pcmoritz in https://github.com/vllm-project/vllm/pull/2861
* [Fix] Fix memory profiling when GPU is used by multiple processes by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2863
* Fix docker python version by @NikolaBorisov in https://github.com/vllm-project/vllm/pull/2845
* Migrate AquilaForCausalLM to LlamaForCausalLM by @esmeetu in https://github.com/vllm-project/vllm/pull/2867
* Don't use cupy NCCL for AMD backends by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2855
* Align LoRA code between Mistral and Mixtral (fixes #2875) by @pcmoritz in https://github.com/vllm-project/vllm/pull/2880
* [BugFix] Fix GC bug for `LLM` class by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2882
* Fix decilm.py by @pcmoritz in https://github.com/vllm-project/vllm/pull/2883
* [ROCm] Dockerfile fix for flash-attention build by @hongxiayang in https://github.com/vllm-project/vllm/pull/2885
* Prefix Caching- fix t4 triton error by @caoshiyi in https://github.com/vllm-project/vllm/pull/2517
* Bump up to v0.3.1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2887

## New Contributors
* @sighingnow made their first contribution in https://github.com/vllm-project/vllm/pull/2688
* @rib-2 made their first contribution in https://github.com/vllm-project/vllm/pull/2316
* @Leymore made their first contribution in https://github.com/vllm-project/vllm/pull/2666
* @Pernekhan made their first contribution in https://github.com/vllm-project/vllm/pull/2697
* @jikunshang made their first contribution in https://github.com/vllm-project/vllm/pull/2503
* @c21 made their first contribution in https://github.com/vllm-project/vllm/pull/2696
* @zcnrex made their first contribution in https://github.com/vllm-project/vllm/pull/2611
* @whyiug made their first contribution in https://github.com/vllm-project/vllm/pull/2746
* @gardberg made their first contribution in https://github.com/vllm-project/vllm/pull/2774
* @dllehr-amd made their first contribution in https://github.com/vllm-project/vllm/pull/2627
* @rkooo567 made their first contribution in https://github.com/vllm-project/vllm/pull/2471
* @ywang96 made their first contribution in https://github.com/vllm-project/vllm/pull/2433
* @tterrysun made their first contribution in https://github.com/vllm-project/vllm/pull/2831

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.3.0...v0.3.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.3.1)

---

## v0.3.0: v0.3.0
**Published:** 2024-01-31

## Major Changes
- Experimental multi-lora support 
- Experimental prefix caching support
- FP8 KV Cache support
- Optimized MoE performance and Deepseek MoE support
- CI tested PRs
- Support batch completion in server

## What's Changed
* Miner fix of type hint by @beginlner in https://github.com/vllm-project/vllm/pull/2340
* Build docker image with shared objects from "build" step by @payoto in https://github.com/vllm-project/vllm/pull/2237
* Ensure metrics are logged regardless of requests by @ichernev in https://github.com/vllm-project/vllm/pull/2347
* Changed scheduler to use deques instead of lists by @NadavShmayo in https://github.com/vllm-project/vllm/pull/2290
* Fix eager mode performance by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2377
* [Minor] Remove unused code in attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2384
* Add baichuan chat template jinjia file by @EvilPsyCHo in https://github.com/vllm-project/vllm/pull/2390
* [Speculative decoding 1/9] Optimized rejection sampler by @cadedaniel in https://github.com/vllm-project/vllm/pull/2336
* Fix ipv4 ipv6 dualstack by @yunfeng-scale in https://github.com/vllm-project/vllm/pull/2408
* [Minor] Rename phi_1_5 to phi by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2385
* [DOC] Add additional comments for LLMEngine and AsyncLLMEngine by @litone01 in https://github.com/vllm-project/vllm/pull/1011
* [Minor] Fix the format in quick start guide related to Model Scope by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2425
* Add gradio chatbot for openai webserver by @arkohut in https://github.com/vllm-project/vllm/pull/2307
* [BUG] RuntimeError: deque mutated during iteration in abort_seq_group by @chenxu2048 in https://github.com/vllm-project/vllm/pull/2371
* Allow setting fastapi root_path argument by @chiragjn in https://github.com/vllm-project/vllm/pull/2341
* Address Phi modeling update 2 by @huiwy in https://github.com/vllm-project/vllm/pull/2428
* Update a more user-friendly error message, offering more considerate advice for beginners, when using V100 GPU #1901 by @chuanzhubin in https://github.com/vllm-project/vllm/pull/2374
* Update quickstart.rst with small clarifying change (fix typo) by @nautsimon in https://github.com/vllm-project/vllm/pull/2369
* Aligning `top_p` and `top_k` Sampling by @chenxu2048 in https://github.com/vllm-project/vllm/pull/1885
* [Minor] Fix err msg by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2431
* [Minor] Optimize cuda graph memory usage by @esmeetu in https://github.com/vllm-project/vllm/pull/2437
* [CI] Add Buildkite by @simon-mo in https://github.com/vllm-project/vllm/pull/2355
* Announce the second vLLM meetup by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2444
* Allow buildkite to retry build on agent lost by @simon-mo in https://github.com/vllm-project/vllm/pull/2446
* Fix weigit loading for GQA with TP by @zhangch9 in https://github.com/vllm-project/vllm/pull/2379
* CI: make sure benchmark script exit on error by @simon-mo in https://github.com/vllm-project/vllm/pull/2449
* ci: retry on build failure as well by @simon-mo in https://github.com/vllm-project/vllm/pull/2457
* Add StableLM3B model by @ita9naiwa in https://github.com/vllm-project/vllm/pull/2372
* OpenAI refactoring by @FlorianJoncour in https://github.com/vllm-project/vllm/pull/2360
* [Experimental] Prefix Caching Support by @caoshiyi in https://github.com/vllm-project/vllm/pull/1669
* fix stablelm.py tensor-parallel-size bug by @YingchaoX in https://github.com/vllm-project/vllm/pull/2482
* Minor fix in prefill cache example by @JasonZhu1313 in https://github.com/vllm-project/vllm/pull/2494
* fix: fix some args desc by @zspo in https://github.com/vllm-project/vllm/pull/2487
* [Neuron] Add an option to build with neuron by @liangfu in https://github.com/vllm-project/vllm/pull/2065
* Don't download both safetensor and bin files. by @NikolaBorisov in https://github.com/vllm-project/vllm/pull/2480
* [BugFix] Fix abort_seq_group by @beginlner in https://github.com/vllm-project/vllm/pull/2463
* refactor completion api for readability by @simon-mo in https://github.com/vllm-project/vllm/pull/2499
* Support OpenAI API server in `benchmark_serving.py` by @hmellor in https://github.com/vllm-project/vllm/pull/2172
* Simplify broadcast logic for control messages by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2501
* [Bugfix] fix load local safetensors model by @esmeetu in https://github.com/vllm-project/vllm/pull/2512
* Add benchmark serving to CI by @simon-mo in https://github.com/vllm-project/vllm/pull/2505
* Add `group` as an argument in broadcast ops by @GindaChen in https://github.com/vllm-project/vllm/pull/2522
* [Fix] Keep `scheduler.running` as deque by @njhill in https://github.com/vllm-project/vllm/pull/2523
* migrate pydantic from v1 to v2 by @joennlae in https://github.com/vllm-project/vllm/pull/2531
* [Speculative decoding 2/9] Multi-step worker for draft model by @cadedaniel in https://github.com/vllm-project/vllm/pull/2424
* Fix "Port could not be cast to integer value as <function get_open_port>" by @pcmoritz in https://github.com/vllm-project/vllm/pull/2545
* Add qwen2 by @JustinLin610 in https://github.com/vllm-project/vllm/pull/2495
* Fix progress bar and allow HTTPS in `benchmark_serving.py` by @hmellor in https://github.com/vllm-project/vllm/pull/2552
* Add a 1-line docstring to explain why calling context_attention_fwd twice in test_prefix_prefill.py by @JasonZhu1313 in https://github.com/vllm-project/vllm/pull/2553
* [Feature] Simple API token authentication by @taisazero in https://github.com/vllm-project/vllm/pull/1106
* Add multi-LoRA support by @Yard1 in https://github.com/vllm-project/vllm/pull/1804
* lint: format all python file instead of just source code by @simon-mo in https://github.com/vllm-project/vllm/pull/2567
* [Bugfix] fix crash if max_tokens=None by @NikolaBorisov in https://github.com/vllm-project/vllm/pull/2570
* Added `include_stop_str_in_output` and `length_penalty` parameters to OpenAI API by @galatolofederico in https://github.com/vllm-project/vllm/pull/2562
* [Doc] Fix the syntax error in the doc of supported_models. by @keli-wen in https://github.com/vllm-project/vllm/pull/2584
* Support Batch Completion in Server by @simon-mo in https://github.com/vllm-project/vllm/pull/2529
* fix names and license by @JustinLin610 in https://github.com/vllm-project/vllm/pull/2589
* [Fix] Use a correct device when creating OptionalCUDAGuard by @sh1ng in https://github.com/vllm-project/vllm/pull/2583
* [ROCm] add support to ROCm 6.0 and MI300 by @hongxiayang in https://github.com/vllm-project/vllm/pull/2274
* Support for Stable LM 2 by @dakotamahan-stability in https://github.com/vllm-project/vllm/pull/2598
* Don't build punica kernels by default by @pcmoritz in https://github.com/vllm-project/vllm/pull/2605
* AWQ: Up to 2.66x higher throughput by @casper-hansen in https://github.com/vllm-project/vllm/pull/2566
* Use head_dim in config if exists by @xiangxu-google in https://github.com/vllm-project/vllm/pull/2622
* Custom all reduce kernels by @hanzhi713 in https://github.com/vllm-project/vllm/pull/2192
* [Minor] Fix warning on Ray dependencies by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2630
* Speed up Punica compilation by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2632
* Small async_llm_engine refactor by @andoorve in https://github.com/vllm-project/vllm/pull/2618
* Update Ray version requirements by @simon-mo in https://github.com/vllm-project/vllm/pull/2636
* Support FP8-E5M2 KV Cache by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/2279
* Fix error when tp > 1 by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/2644
* No repeated IPC open by @hanzhi713 in https://github.com/vllm-project/vllm/pull/2642
* ROCm: Allow setting compilation target by @rlrs in https://github.com/vllm-project/vllm/pull/2581
* DeepseekMoE support with Fused MoE kernel by @zwd003 in https://github.com/vllm-project/vllm/pull/2453
* Fused MOE for Mixtral by @pcmoritz in https://github.com/vllm-project/vllm/pull/2542
* Fix 'Actor methods cannot be called directly' when using `--engine-use-ray` by @HermitSun in https://github.com/vllm-project/vllm/pull/2664
* Add swap_blocks unit tests by @sh1ng in https://github.com/vllm-project/vllm/pull/2616
* Fix a small typo (tenosr -> tensor) by @pcmoritz in https://github.com/vllm-project/vllm/pull/2672
* [Minor] Fix false warning when TP=1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2674
* Add quantized mixtral support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2673
* Bump up version to v0.3.0 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2656

## New Contributors
* @payoto made their first contribution in https://github.com/vllm-project/vllm/pull/2237
* @NadavShmayo made their first contribution in https://github.com/vllm-project/vllm/pull/2290
* @EvilPsyCHo made their first contribution in https://github.com/vllm-project/vllm/pull/2390
* @litone01 made their first contribution in https://github.com/vllm-project/vllm/pull/1011
* @arkohut made their first contribution in https://github.com/vllm-project/vllm/pull/2307
* @chiragjn made their first contribution in https://github.com/vllm-project/vllm/pull/2341
* @huiwy made their first contribution in https://github.com/vllm-project/vllm/pull/2428
* @chuanzhubin made their first contribution in https://github.com/vllm-project/vllm/pull/2374
* @nautsimon made their first contribution in https://github.com/vllm-project/vllm/pull/2369
* @zhangch9 made their first contribution in https://github.com/vllm-project/vllm/pull/2379
* @ita9naiwa made their first contribution in https://github.com/vllm-project/vllm/pull/2372
* @caoshiyi made their first contribution in https://github.com/vllm-project/vllm/pull/1669
* @YingchaoX made their first contribution in https://github.com/vllm-project/vllm/pull/2482
* @JasonZhu1313 made their first contribution in https://github.com/vllm-project/vllm/pull/2494
* @zspo made their first contribution in https://github.com/vllm-project/vllm/pull/2487
* @liangfu made their first contribution in https://github.com/vllm-project/vllm/pull/2065
* @NikolaBorisov made their first contribution in https://github.com/vllm-project/vllm/pull/2480
* @GindaChen made their first contribution in https://github.com/vllm-project/vllm/pull/2522
* @njhill made their first contribution in https://github.com/vllm-project/vllm/pull/2523
* @joennlae made their first contribution in https://github.com/vllm-project/vllm/pull/2531
* @pcmoritz made their first contribution in https://github.com/vllm-project/vllm/pull/2545
* @JustinLin610 made their first contribution in https://github.com/vllm-project/vllm/pull/2495
* @taisazero made their first contribution in https://github.com/vllm-project/vllm/pull/1106
* @galatolofederico made their first contribution in https://github.com/vllm-project/vllm/pull/2562
* @keli-wen made their first contribution in https://github.com/vllm-project/vllm/pull/2584
* @sh1ng made their first contribution in https://github.com/vllm-project/vllm/pull/2583
* @hongxiayang made their first contribution in https://github.com/vllm-project/vllm/pull/2274
* @dakotamahan-stability made their first contribution in https://github.com/vllm-project/vllm/pull/2598
* @xiangxu-google made their first contribution in https://github.com/vllm-project/vllm/pull/2622
* @andoorve made their first contribution in https://github.com/vllm-project/vllm/pull/2618
* @rlrs made their first contribution in https://github.com/vllm-project/vllm/pull/2581
* @zwd003 made their first contribution in https://github.com/vllm-project/vllm/pull/2453

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.7...v0.3.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.3.0)

---

## v0.2.7: v0.2.7
**Published:** 2024-01-04

## Major Changes

* Up to 70% throughput improvement for distributed inference by removing serialization/deserialization overheads
* Fix tensor parallelism support for Mixtral + GPTQ/AWQ

## What's Changed
* Minor fix for gpu-memory-utilization description by @SuhongMoon in https://github.com/vllm-project/vllm/pull/2162
* [BugFix] Raise error when max_model_len is larger than KV cache size by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2163
* [BugFix] Fix RoPE kernel on long sequences by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2164
* Add SSL arguments to API servers by @HMellor in https://github.com/vllm-project/vllm/pull/2109
* typo fix by @oushu1zhangxiangxuan1 in https://github.com/vllm-project/vllm/pull/2166
* [ROCm] Fixes for GPTQ on ROCm by @kliuae in https://github.com/vllm-project/vllm/pull/2180
* Update Help Text for --gpu-memory-utilization Argument by @SuhongMoon in https://github.com/vllm-project/vllm/pull/2183
* [Minor] Add warning on CUDA graph memory usage by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2182
* Added DeciLM-7b and DeciLM-7b-instruct by @avideci in https://github.com/vllm-project/vllm/pull/2062
* [BugFix] Fix weight loading for Mixtral with TP by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2208
* Make _prepare_sample non blocking and pin memory of CPU input buffers by @hanzhi713 in https://github.com/vllm-project/vllm/pull/2207
* Remove Sampler copy stream by @Yard1 in https://github.com/vllm-project/vllm/pull/2209
* Fix a broken link by @ronensc in https://github.com/vllm-project/vllm/pull/2222
* Disable Ray usage stats collection by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2206
* [BugFix] Fix recovery logic for sequence group by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2186
* Update installation instructions to include CUDA 11.8 xFormers by @skt7 in https://github.com/vllm-project/vllm/pull/2246
* Add "About" Heading to README.md by @blueceiling in https://github.com/vllm-project/vllm/pull/2260
* [BUGFIX] Do not return ignored sentences twice in async llm engine by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2258
* [BUGFIX] Fix API server test by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2270
* [BUGFIX] Fix the path of test prompts by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2273
* [BUGFIX] Fix communication test by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2285
* Add support GPT-NeoX Models without attention biases by @dalgarak in https://github.com/vllm-project/vllm/pull/2301
* [FIX] Fix kernel bug by @jeejeelee in https://github.com/vllm-project/vllm/pull/1959
* fix typo and remove unused code by @esmeetu in https://github.com/vllm-project/vllm/pull/2305
* Enable CUDA graph for GPTQ & SqueezeLLM by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2318
* Fix Gradio example: remove deprecated parameter `concurrency_count` by @ronensc in https://github.com/vllm-project/vllm/pull/2315
* Use NCCL instead of ray for control-plane communication to remove serialization overhead by @zhuohan123 in https://github.com/vllm-project/vllm/pull/2221
* Remove unused const TIMEOUT_TO_PREVENT_DEADLOCK by @ronensc in https://github.com/vllm-project/vllm/pull/2321
* [Minor] Revert the changes in test_cache by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2335
* Bump up to v0.2.7 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2337

## New Contributors
* @SuhongMoon made their first contribution in https://github.com/vllm-project/vllm/pull/2162
* @HMellor made their first contribution in https://github.com/vllm-project/vllm/pull/2109
* @oushu1zhangxiangxuan1 made their first contribution in https://github.com/vllm-project/vllm/pull/2166
* @kliuae made their first contribution in https://github.com/vllm-project/vllm/pull/2180
* @avideci made their first contribution in https://github.com/vllm-project/vllm/pull/2062
* @hanzhi713 made their first contribution in https://github.com/vllm-project/vllm/pull/2207
* @ronensc made their first contribution in https://github.com/vllm-project/vllm/pull/2222
* @skt7 made their first contribution in https://github.com/vllm-project/vllm/pull/2246
* @blueceiling made their first contribution in https://github.com/vllm-project/vllm/pull/2260
* @dalgarak made their first contribution in https://github.com/vllm-project/vllm/pull/2301

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.6...v0.2.7

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.7)

---

## v0.2.6: v0.2.6
**Published:** 2023-12-17

## Major changes
* Fast model execution with CUDA/HIP graph
* W4A16 GPTQ support (thanks to @chu-tianxiang)
* Fix memory profiling with tensor parallelism
* Fix *.bin weight loading for Mixtral models

## What's Changed
* Fix typing in generate function for AsyncLLMEngine & add toml to requirements-dev by @mezuzza in https://github.com/vllm-project/vllm/pull/2100
* Fix Dockerfile.rocm by @tjtanaa in https://github.com/vllm-project/vllm/pull/2101
* avoid multiple redefinition by @MitchellX in https://github.com/vllm-project/vllm/pull/1817
* Add a flag to include stop string in output text by @yunfeng-scale in https://github.com/vllm-project/vllm/pull/1976
* Add GPTQ support by @chu-tianxiang in https://github.com/vllm-project/vllm/pull/916
* [Docs] Add quantization support to docs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2135
* [ROCm] Temporarily remove GPTQ ROCm support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2138
* simplify loading weights logic by @esmeetu in https://github.com/vllm-project/vllm/pull/2133
* Optimize model execution with CUDA graph by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1926
* [Minor] Delete Llama tokenizer warnings by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2146
* Fix all-reduce memory usage by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2151
* Pin PyTorch & xformers versions by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2155
* Remove dependency on CuPy by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2152
* [Docs] Add CUDA graph support to docs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2148
* Temporarily enforce eager mode for GPTQ models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2154
* [Minor] Add more detailed explanation on `quantization` argument by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2145
* [Minor] Fix xformers version by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2158
* [Minor] Add Phi 2 to supported models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2159
* Make sampler less blocking by @Yard1 in https://github.com/vllm-project/vllm/pull/1889
* [Minor] Fix a typo in .pt weight support by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2160
* Disable CUDA graph for SqueezeLLM by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2161
* Bump up to v0.2.6 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2157

## New Contributors
* @mezuzza made their first contribution in https://github.com/vllm-project/vllm/pull/2100
* @MitchellX made their first contribution in https://github.com/vllm-project/vllm/pull/1817

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.5...v0.2.6

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.6)

---

## v0.2.5: v0.2.5
**Published:** 2023-12-14

## Major changes

* Optimize Mixtral performance with expert parallelism (thanks to @Yard1)
* [BugFix] Fix input positions for long context with sliding window

## What's Changed
* Update Dockerfile to support Mixtral by @simon-mo in https://github.com/vllm-project/vllm/pull/2027
* Remove python 3.10 requirement by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2040
* [CI/CD] Upgrade PyTorch version to v2.1.1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2045
* Upgrade transformers version to 4.36.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2046
* Remove einops from dependencies by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2049
* gqa added to mpt attn by @megha95 in https://github.com/vllm-project/vllm/pull/1938
* Update Dockerfile to build Megablocks by @simon-mo in https://github.com/vllm-project/vllm/pull/2042
* Fix peak memory profiling by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2031
* Implement lazy model loader by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2044
* [ROCm] Upgrade xformers version dependency for ROCm; update documentations by @tjtanaa in https://github.com/vllm-project/vllm/pull/2079
* Update installation instruction for CUDA 11.8 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2086
* [Docs] Add notes on ROCm-supported models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2087
* [BugFix] Fix input positions for long context with sliding window by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2088
* Mixtral expert parallelism by @Yard1 in https://github.com/vllm-project/vllm/pull/2090
* Bump up to v0.2.5 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2095


**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.4...v0.2.5

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.5)

---

## v0.2.4: v0.2.4
**Published:** 2023-12-11

## Major changes

* Mixtral model support (officially from @mistralai)
* AMD GPU support (collaboration with @embeddedllm)

## What's Changed
* add custom server params by @esmeetu in https://github.com/vllm-project/vllm/pull/1868
* support ChatGLMForConditionalGeneration by @dancingpipi in https://github.com/vllm-project/vllm/pull/1932
* Save pytorch profiler output for latency benchmark by @Yard1 in https://github.com/vllm-project/vllm/pull/1871
* Fix typo in adding_model.rst by @petergtz in https://github.com/vllm-project/vllm/pull/1947
* Make InternLM follow `rope_scaling` in `config.json` by @theFool32 in https://github.com/vllm-project/vllm/pull/1956
* Fix quickstart.rst example by @gottlike in https://github.com/vllm-project/vllm/pull/1964
* Adding number of nvcc_threads during build as envar by @AguirreNicolas in https://github.com/vllm-project/vllm/pull/1893
* fix typo in getenv call by @dskhudia in https://github.com/vllm-project/vllm/pull/1972
* [Continuation] Merge EmbeddedLLM/vllm-rocm into vLLM main by @tjtanaa in https://github.com/vllm-project/vllm/pull/1836
* Fix Baichuan2-7B-Chat by @firebook in https://github.com/vllm-project/vllm/pull/1987
* [Docker] Add cuda arch list as build option by @simon-mo in https://github.com/vllm-project/vllm/pull/1950
* Fix for KeyError on Loading LLaMA by @imgaojun in https://github.com/vllm-project/vllm/pull/1978
* [Minor] Fix code style for baichuan by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2003
* Fix OpenAI server completion_tokens referenced before assignment by @js8544 in https://github.com/vllm-project/vllm/pull/1996
* [Minor] Add comment on skipping rope caches by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2004
* Replace head_mapping params with num_kv_heads to attention kernel. by @wbn03 in https://github.com/vllm-project/vllm/pull/1997
* Fix completion API echo and logprob combo by @simon-mo in https://github.com/vllm-project/vllm/pull/1992
* Mixtral 8x7B support by @pierrestock in https://github.com/vllm-project/vllm/pull/2011
* Minor fixes for Mixtral by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2015
* Change load format for Mixtral by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2028
* Update run_on_sky.rst by @eltociear in https://github.com/vllm-project/vllm/pull/2025
* Update requirements.txt for mixtral by @0-hero in https://github.com/vllm-project/vllm/pull/2029
* Revert #2029 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2030
* [Minor] Fix latency benchmark script by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2035
* [Minor] Fix type annotation in Mixtral by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2036
* Update README.md to add megablocks requirement for mixtral by @0-hero in https://github.com/vllm-project/vllm/pull/2033
* [Minor] Fix import error msg for megablocks by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2038
* Bump up to v0.2.4 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/2034

## New Contributors
* @dancingpipi made their first contribution in https://github.com/vllm-project/vllm/pull/1932
* @petergtz made their first contribution in https://github.com/vllm-project/vllm/pull/1947
* @theFool32 made their first contribution in https://github.com/vllm-project/vllm/pull/1956
* @gottlike made their first contribution in https://github.com/vllm-project/vllm/pull/1964
* @AguirreNicolas made their first contribution in https://github.com/vllm-project/vllm/pull/1893
* @dskhudia made their first contribution in https://github.com/vllm-project/vllm/pull/1972
* @tjtanaa made their first contribution in https://github.com/vllm-project/vllm/pull/1836
* @firebook made their first contribution in https://github.com/vllm-project/vllm/pull/1987
* @imgaojun made their first contribution in https://github.com/vllm-project/vllm/pull/1978
* @js8544 made their first contribution in https://github.com/vllm-project/vllm/pull/1996
* @wbn03 made their first contribution in https://github.com/vllm-project/vllm/pull/1997
* @pierrestock made their first contribution in https://github.com/vllm-project/vllm/pull/2011
* @0-hero made their first contribution in https://github.com/vllm-project/vllm/pull/2029

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.3...v0.2.4

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.4)

---

## v0.2.3: v0.2.3
**Published:** 2023-12-03

## Major changes

* Refactoring on Worker, InputMetadata, and Attention
* Fix TP support for AWQ models
* Support Prometheus metrics
* Fix Baichuan & Baichuan 2

## What's Changed
* Add instructions to install vllm+cu118 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1717
* Documentation about official docker image by @simon-mo in https://github.com/vllm-project/vllm/pull/1709
* Fix the code block's format in deploying_with_docker page by @HermitSun in https://github.com/vllm-project/vllm/pull/1722
* Migrate linter from `pylint` to `ruff` by @simon-mo in https://github.com/vllm-project/vllm/pull/1665
* [FIX] Update the doc link in README.md by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1730
* [BugFix] Fix a bug in loading safetensors by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1732
* Fix hanging in the scheduler caused by long prompts by @chenxu2048 in https://github.com/vllm-project/vllm/pull/1534
* [Fix] Fix bugs in scheduler by @linotfan in https://github.com/vllm-project/vllm/pull/1727
* Rewrite torch.repeat_interleave to remove cpu synchronization by @beginlner in https://github.com/vllm-project/vllm/pull/1599
* fix RAM OOM when load large models in tensor parallel mode. by @boydfd in https://github.com/vllm-project/vllm/pull/1395
* [BugFix] Fix TP support for AWQ by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1731
* [FIX] Fix the case when `input_is_parallel=False` for `ScaledActivation` by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1737
* Add stop_token_ids in SamplingParams.__repr__ by @chenxu2048 in https://github.com/vllm-project/vllm/pull/1745
* [DOCS]Â Add engine args documentation by @casper-hansen in https://github.com/vllm-project/vllm/pull/1741
* Set top_p=0 and top_k=-1 in greedy sampling by @beginlner in https://github.com/vllm-project/vllm/pull/1748
* Fix repetition penalty aligned with huggingface by @beginlner in https://github.com/vllm-project/vllm/pull/1577
* [build] Avoid building too many extensions by @ymwangg in https://github.com/vllm-project/vllm/pull/1624
* [Minor] Fix model docstrings by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1764
* Added echo function to OpenAI API server. by @wanmok in https://github.com/vllm-project/vllm/pull/1504
* Init model on GPU to reduce CPU memory footprint by @beginlner in https://github.com/vllm-project/vllm/pull/1796
* Correct comments in parallel_state.py by @explainerauthors in https://github.com/vllm-project/vllm/pull/1818
* Fix OPT weight loading by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1819
* [FIX] Fix class naming by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1803
* Move the definition of BlockTable a few lines above so we could use it in BlockAllocator by @explainerauthors in https://github.com/vllm-project/vllm/pull/1791
* [FIX] Fix formatting error in main branch by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1822
* [Fix] Fix RoPE in ChatGLM-32K by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1841
* Better integration with Ray Serve by @FlorianJoncour in https://github.com/vllm-project/vllm/pull/1821
* Refactor Attention by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1840
* [Docs] Add information about using shared memory in docker by @simon-mo in https://github.com/vllm-project/vllm/pull/1845
* Disable Logs Requests should Disable Logging of requests. by @MichaelMcCulloch in https://github.com/vllm-project/vllm/pull/1779
* Refactor worker & InputMetadata by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1843
* Avoid multiple instantiations of the RoPE class by @jeejeeli in https://github.com/vllm-project/vllm/pull/1828
* [FIX] Fix docker build error (#1831) by @allenhaozi in https://github.com/vllm-project/vllm/pull/1832
* Add profile option to latency benchmark by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1839
* Remove `max_num_seqs` in latency benchmark by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1855
* Support max-model-len argument for throughput benchmark by @aisensiy in https://github.com/vllm-project/vllm/pull/1858
* Fix rope cache key error by @esmeetu in https://github.com/vllm-project/vllm/pull/1867
* docs: add instructions for Langchain by @mspronesti in https://github.com/vllm-project/vllm/pull/1162
* Support chat template and `echo` for chat API by @Tostino in https://github.com/vllm-project/vllm/pull/1756
* Fix Baichuan tokenizer error by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1874
* Add weight normalization for Baichuan 2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1876
* Fix the typo in SamplingParams' docstring. by @xukp20 in https://github.com/vllm-project/vllm/pull/1886
* [Docs] Update the AWQ documentation to highlight performance issue by @simon-mo in https://github.com/vllm-project/vllm/pull/1883
* Fix the broken sampler tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1896
* Add Production Metrics in Prometheus format by @simon-mo in https://github.com/vllm-project/vllm/pull/1890
* Add PyTorch-native implementation of custom layers by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1898
* Fix broken worker test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1900
* chore(examples-docs): upgrade to OpenAI V1  by @mspronesti in https://github.com/vllm-project/vllm/pull/1785
* Fix num_gpus when TP > 1 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1852
* Bump up to v0.2.3 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1903

## New Contributors
* @boydfd made their first contribution in https://github.com/vllm-project/vllm/pull/1395
* @explainerauthors made their first contribution in https://github.com/vllm-project/vllm/pull/1818
* @FlorianJoncour made their first contribution in https://github.com/vllm-project/vllm/pull/1821
* @MichaelMcCulloch made their first contribution in https://github.com/vllm-project/vllm/pull/1779
* @jeejeeli made their first contribution in https://github.com/vllm-project/vllm/pull/1828
* @allenhaozi made their first contribution in https://github.com/vllm-project/vllm/pull/1832
* @aisensiy made their first contribution in https://github.com/vllm-project/vllm/pull/1858
* @xukp20 made their first contribution in https://github.com/vllm-project/vllm/pull/1886

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.2...v0.2.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.3)

---

## v0.2.2: v0.2.2
**Published:** 2023-11-19

## Major changes

* Bump up to PyTorch v2.1 + CUDA 12.1 ([vLLM+CUDA 11.8 is also provided](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#install-with-pip))
* Extensive refactoring for better tensor parallelism & quantization support
* New models: Yi, ChatGLM, Phi
* Changes in scheduler: from 1D flattened input tensor to 2D tensor
* AWQ support for all models
* Added LogitsProcessor API
* Preliminary support for SqueezeLLM

## What's Changed
* Change scheduler & input tensor shape by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1381
* Add Mistral 7B to `test_models` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1366
* fix typo by @WrRan in https://github.com/vllm-project/vllm/pull/1383
* Fix TP bug by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1389
* Fix type hints by @lxrite in https://github.com/vllm-project/vllm/pull/1427
* remove useless statements by @WrRan in https://github.com/vllm-project/vllm/pull/1408
* Pin dependency versions by @thiagosalvatore in https://github.com/vllm-project/vllm/pull/1429
* SqueezeLLM Support by @chooper1 in https://github.com/vllm-project/vllm/pull/1326
* aquila model add rope_scaling by @Sanster in https://github.com/vllm-project/vllm/pull/1457
* fix: don't skip first special token. by @gesanqiu in https://github.com/vllm-project/vllm/pull/1497
* Support repetition_penalty by @beginlner in https://github.com/vllm-project/vllm/pull/1424
* Fix bias in InternLM by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1501
* Delay GPU->CPU sync in sampling by @Yard1 in https://github.com/vllm-project/vllm/pull/1337
* Refactor LLMEngine demo script for clarity and modularity by @iongpt in https://github.com/vllm-project/vllm/pull/1413
* Fix logging issues by @Tostino in https://github.com/vllm-project/vllm/pull/1494
* Add py.typed so consumers of vLLM can get type checking by @jroesch in https://github.com/vllm-project/vllm/pull/1509
* vLLM always places spaces between special tokens by @blahblahasdf in https://github.com/vllm-project/vllm/pull/1373
* [Fix] Fix duplicated logging messages by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1524
* Add dockerfile by @skrider in https://github.com/vllm-project/vllm/pull/1350
* Fix integer overflows in attention & cache ops by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1514
* [Small] Formatter only checks lints in changed files by @cadedaniel in https://github.com/vllm-project/vllm/pull/1528
* Add `MptForCausalLM` key in model_loader by @wenfeiy-db in https://github.com/vllm-project/vllm/pull/1526
* [BugFix] Fix a bug when engine_use_ray=True and worker_use_ray=False and TP>1 by @beginlner in https://github.com/vllm-project/vllm/pull/1531
* Adding a health endpoint by @Fluder-Paradyne in https://github.com/vllm-project/vllm/pull/1540
* Remove `MPTConfig` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1529
* Force paged attention v2 for long contexts by @Yard1 in https://github.com/vllm-project/vllm/pull/1510
* docs: add description by @lots-o in https://github.com/vllm-project/vllm/pull/1553
* Added logits processor API to sampling params by @noamgat in https://github.com/vllm-project/vllm/pull/1469
* YaRN support implementation by @Yard1 in https://github.com/vllm-project/vllm/pull/1264
* Add Quantization and AutoAWQ to docs by @casper-hansen in https://github.com/vllm-project/vllm/pull/1235
* Support Yi model by @esmeetu in https://github.com/vllm-project/vllm/pull/1567
* ChatGLM2 Support by @GoHomeToMacDonal in https://github.com/vllm-project/vllm/pull/1261
* Upgrade to CUDA 12 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1527
* [Worker] Fix input_metadata.selected_token_indices in worker by @ymwangg in https://github.com/vllm-project/vllm/pull/1546
* Build CUDA11.8 wheels for release by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1596
* Add Yi model to quantization support by @forpanyang in https://github.com/vllm-project/vllm/pull/1600
* Dockerfile: Upgrade Cuda to 12.1 by @GhaziSyed in https://github.com/vllm-project/vllm/pull/1609
* config parser: add ChatGLM2 seq_length to `_get_and_verify_max_len` by @irasin in https://github.com/vllm-project/vllm/pull/1617
* Fix cpu heavy code in async function _AsyncLLMEngine._run_workers_async by @dominik-schwabe in https://github.com/vllm-project/vllm/pull/1628
* Fix #1474 - gptj AssertionError : assert param_slice.shape == loaded_weight.shape by @lihuahua123 in https://github.com/vllm-project/vllm/pull/1631
* [Minor] Move RoPE selection logic to `get_rope` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1633
* Add DeepSpeed MII backend to benchmark script by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1649
* TP/quantization/weight loading refactor part 2 - Refactor quantized linear logic and extend quantization support to all models by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1622
* Remove `MptConfig`   by @megha95 in https://github.com/vllm-project/vllm/pull/1668
* feat(config): support parsing torch.dtype by @aarnphm in https://github.com/vllm-project/vllm/pull/1641
* Fix loading error when safetensors contains empty tensor by @twaka in https://github.com/vllm-project/vllm/pull/1687
* [Minor] Fix duplication of ignored seq group in engine step by @simon-mo in https://github.com/vllm-project/vllm/pull/1666
* [models] Microsoft Phi 1.5 by @maximzubkov in https://github.com/vllm-project/vllm/pull/1664
* [Fix] Update Supported Models List by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1690
* Return usage for openai requests by @ichernev in https://github.com/vllm-project/vllm/pull/1663
* [Fix] Fix comm test by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1691
* Update the adding-model doc according to the new refactor by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1692
* Add 'not' to this annotation: "#FIXME(woosuk): Do not use internal method" by @linotfan in https://github.com/vllm-project/vllm/pull/1704
* Support Min P Sampler by @esmeetu in https://github.com/vllm-project/vllm/pull/1642
* Read quantization_config in hf config by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1695
* Support download models from www.modelscope.cn by @liuyhwangyh in https://github.com/vllm-project/vllm/pull/1588
* follow up of #1687 when safetensors model contains 0-rank tensors by @twaka in https://github.com/vllm-project/vllm/pull/1696
* Add AWQ support for all models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1714
* Support fused add rmsnorm for LLaMA by @beginlner in https://github.com/vllm-project/vllm/pull/1667
* [Fix] Fix warning msg on quantization by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1715
* Bump up the version to v0.2.2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1689

## New Contributors
* @lxrite made their first contribution in https://github.com/vllm-project/vllm/pull/1427
* @thiagosalvatore made their first contribution in https://github.com/vllm-project/vllm/pull/1429
* @chooper1 made their first contribution in https://github.com/vllm-project/vllm/pull/1326
* @beginlner made their first contribution in https://github.com/vllm-project/vllm/pull/1424
* @iongpt made their first contribution in https://github.com/vllm-project/vllm/pull/1413
* @Tostino made their first contribution in https://github.com/vllm-project/vllm/pull/1494
* @jroesch made their first contribution in https://github.com/vllm-project/vllm/pull/1509
* @skrider made their first contribution in https://github.com/vllm-project/vllm/pull/1350
* @cadedaniel made their first contribution in https://github.com/vllm-project/vllm/pull/1528
* @wenfeiy-db made their first contribution in https://github.com/vllm-project/vllm/pull/1526
* @Fluder-Paradyne made their first contribution in https://github.com/vllm-project/vllm/pull/1540
* @lots-o made their first contribution in https://github.com/vllm-project/vllm/pull/1553
* @noamgat made their first contribution in https://github.com/vllm-project/vllm/pull/1469
* @casper-hansen made their first contribution in https://github.com/vllm-project/vllm/pull/1235
* @GoHomeToMacDonal made their first contribution in https://github.com/vllm-project/vllm/pull/1261
* @ymwangg made their first contribution in https://github.com/vllm-project/vllm/pull/1546
* @forpanyang made their first contribution in https://github.com/vllm-project/vllm/pull/1600
* @GhaziSyed made their first contribution in https://github.com/vllm-project/vllm/pull/1609
* @irasin made their first contribution in https://github.com/vllm-project/vllm/pull/1617
* @dominik-schwabe made their first contribution in https://github.com/vllm-project/vllm/pull/1628
* @lihuahua123 made their first contribution in https://github.com/vllm-project/vllm/pull/1631
* @megha95 made their first contribution in https://github.com/vllm-project/vllm/pull/1668
* @aarnphm made their first contribution in https://github.com/vllm-project/vllm/pull/1641
* @simon-mo made their first contribution in https://github.com/vllm-project/vllm/pull/1666
* @maximzubkov made their first contribution in https://github.com/vllm-project/vllm/pull/1664
* @ichernev made their first contribution in https://github.com/vllm-project/vllm/pull/1663
* @linotfan made their first contribution in https://github.com/vllm-project/vllm/pull/1704
* @liuyhwangyh made their first contribution in https://github.com/vllm-project/vllm/pull/1588

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.1...v0.2.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.2)

---

## v0.2.1.post1: v0.2.1.post1
**Published:** 2023-10-17

This is an emergency release to fix a bug on tensor parallelism support.

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.1.post1)

---

## v0.2.1: v0.2.1
**Published:** 2023-10-16

## Major Changes

* PagedAttention V2 kernel: Up to 20% end-to-end latency reduction
* Support log probabilities for prompt tokens
* AWQ support for Mistral 7B

## What's Changed
* fixing typo in `tiiuae/falcon-rw-7b` model name by @0ssamaak0 in https://github.com/vllm-project/vllm/pull/1226
* Added `dtype` arg to benchmarks by @kg6-sleipnir in https://github.com/vllm-project/vllm/pull/1228
* fix vulnerable memory modification to gpu shared memory by @soundOfDestiny in https://github.com/vllm-project/vllm/pull/1241
* support sharding llama2-70b on more than 8 GPUs by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1209
* [Minor] Fix type annotations by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1238
* TP/quantization/weight loading refactor part 1 - Simplify parallel linear logic by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1181
* add support for tokenizer revision by @cassanof in https://github.com/vllm-project/vllm/pull/1163
* Use monotonic time where appropriate by @Yard1 in https://github.com/vllm-project/vllm/pull/1249
* API server support ipv4 / ipv6 dualstack by @yunfeng-scale in https://github.com/vllm-project/vllm/pull/1288
* Move bfloat16 check to worker by @Yard1 in https://github.com/vllm-project/vllm/pull/1259
* [FIX] Explain why the finished_reason of ignored sequences are length by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1289
* Update README.md by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1292
* [Minor] Fix comment in mistral.py by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1303
* lock torch version to 2.0.1 when build for #1283 by @yanxiyue in https://github.com/vllm-project/vllm/pull/1290
* minor update by @WrRan in https://github.com/vllm-project/vllm/pull/1311
* change the timing of sorting logits by @yhlskt23 in https://github.com/vllm-project/vllm/pull/1309
* workaround of AWQ for Turing GPUs by @twaka in https://github.com/vllm-project/vllm/pull/1252
* Fix overflow in awq kernel by @chu-tianxiang in https://github.com/vllm-project/vllm/pull/1295
* Update model_loader.py by @AmaleshV in https://github.com/vllm-project/vllm/pull/1278
* Add blacklist for model checkpoint by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1325
* Update README.md Aquila2. by @ftgreat in https://github.com/vllm-project/vllm/pull/1331
* Improve detokenization performance by @Yard1 in https://github.com/vllm-project/vllm/pull/1338
* Bump up transformers version & Remove MistralConfig by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1254
* Fix the issue for AquilaChat2-* models by @lu-wang-dl in https://github.com/vllm-project/vllm/pull/1339
* Fix error message on `TORCH_CUDA_ARCH_LIST` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1239
* Minor fix on AWQ kernel launch by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1356
* Implement PagedAttention V2 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1348
* Implement prompt logprobs & Batched topk for computing logprobs by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1328
* Fix PyTorch version to 2.0.1 in workflow by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1377
* Fix PyTorch index URL in workflow by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1378
* Fix sampler test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1379
* Bump up the version to v0.2.1 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1355

## New Contributors
* @0ssamaak0 made their first contribution in https://github.com/vllm-project/vllm/pull/1226
* @kg6-sleipnir made their first contribution in https://github.com/vllm-project/vllm/pull/1228
* @soundOfDestiny made their first contribution in https://github.com/vllm-project/vllm/pull/1241
* @cassanof made their first contribution in https://github.com/vllm-project/vllm/pull/1163
* @yunfeng-scale made their first contribution in https://github.com/vllm-project/vllm/pull/1288
* @yanxiyue made their first contribution in https://github.com/vllm-project/vllm/pull/1290
* @yhlskt23 made their first contribution in https://github.com/vllm-project/vllm/pull/1309
* @chu-tianxiang made their first contribution in https://github.com/vllm-project/vllm/pull/1295
* @AmaleshV made their first contribution in https://github.com/vllm-project/vllm/pull/1278
* @lu-wang-dl made their first contribution in https://github.com/vllm-project/vllm/pull/1339

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.2.0...v0.2.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.1)

---

## v0.2.0: v0.2.0
**Published:** 2023-09-28

## Major changes

* Up to 60% performance improvement by optimizing de-tokenization and sampler
* Initial support for AWQ (performance not optimized)
* Support for RoPE scaling and LongChat
* Support for Mistral-7B
* Many bug fixes

## What's Changed
* add option to shorten prompt print in log by @leiwen83 in https://github.com/vllm-project/vllm/pull/991
* Make `max_model_len` configurable by @Yard1 in https://github.com/vllm-project/vllm/pull/972
* Fix typo in README.md by @eltociear in https://github.com/vllm-project/vllm/pull/1033
* Use TGI-like incremental detokenization by @Yard1 in https://github.com/vllm-project/vllm/pull/984
* Add Model Revision Support in https://github.com/vllm-project/vllm/pull/1014
* [FIX] Minor bug fixes by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1035
* Announce paper release by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1036
* Fix detokenization leaving special tokens by @Yard1 in https://github.com/vllm-project/vllm/pull/1044
* Add pandas to requirements.txt by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1047
* OpenAI-Server: Only fail if logit_bias has actual values by @LLukas22 in https://github.com/vllm-project/vllm/pull/1045
* Fix warning message on LLaMA FastTokenizer by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1037
* Abort when coroutine is cancelled by @rucyang in https://github.com/vllm-project/vllm/pull/1020
* Implement AWQ quantization support for LLaMA by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1032
* Remove AsyncLLMEngine busy loop, shield background task by @Yard1 in https://github.com/vllm-project/vllm/pull/1059
* Fix hanging when prompt exceeds limit by @chenxu2048 in https://github.com/vllm-project/vllm/pull/1029
* [FIX] Don't initialize parameter by default by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1067
* added support for quantize on LLM module by @orellavie1212 in https://github.com/vllm-project/vllm/pull/1080
* align llm_engine and async_engine step method. by @esmeetu in https://github.com/vllm-project/vllm/pull/1081
* Fix get_max_num_running_seqs for waiting and swapped seq groups by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1068
* Add safetensors support for quantized models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1073
* Add minimum capability requirement for AWQ by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1064
* [Community] Add vLLM Discord server by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1086
* Add pyarrow to dependencies & Print warning on Ray import error by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1094
* Add gpu_memory_utilization and swap_space to LLM by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1090
* Add documentation to Triton server tutorial by @tanmayv25 in https://github.com/vllm-project/vllm/pull/983
* rope_theta and max_position_embeddings from config by @Yard1 in https://github.com/vllm-project/vllm/pull/1096
* Replace torch.cuda.DtypeTensor with torch.tensor by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1123
* Add float16 and float32 to dtype choices by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1115
* clean api code, remove redundant background task. by @esmeetu in https://github.com/vllm-project/vllm/pull/1102
* feat: support stop_token_ids parameter. by @gesanqiu in https://github.com/vllm-project/vllm/pull/1097
* Use `--ipc=host` in `docker run` for distributed inference by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1125
* Docs: Fix broken link to openai example by @nkpz in https://github.com/vllm-project/vllm/pull/1145
* Announce the First vLLM Meetup by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1148
* [Sampler] Vectorized sampling (simplified) by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1048
* [FIX] Simplify sampler logic by @zhuohan123 in https://github.com/vllm-project/vllm/pull/1156
* Fix config for Falcon by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1164
* Align `max_tokens` behavior with openai by @HermitSun in https://github.com/vllm-project/vllm/pull/852
* [Setup] Enable `TORCH_CUDA_ARCH_LIST` for selecting target GPUs by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1074
* Add comments on RoPE initialization by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1176
* Allocate more shared memory to attention kernel by @Yard1 in https://github.com/vllm-project/vllm/pull/1154
* Support Longchat by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/555
* fix typo (?) by @WrRan in https://github.com/vllm-project/vllm/pull/1184
* fix qwen-14b model by @Sanster in https://github.com/vllm-project/vllm/pull/1173
* Automatically set `max_num_batched_tokens` by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1198
* Use standard extras for `uvicorn` by @danilopeixoto in https://github.com/vllm-project/vllm/pull/1166
* Keep special sampling params by @blahblahasdf in https://github.com/vllm-project/vllm/pull/1186
* qwen add rope_scaling by @Sanster in https://github.com/vllm-project/vllm/pull/1210
* [Mistral] Mistral-7B-v0.1 support by @Bam4d in https://github.com/vllm-project/vllm/pull/1196
* Fix Mistral model by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1220
* [Fix] Remove false assertion by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1222
* Add Mistral to supported model list by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1221
* Fix OOM in attention kernel test by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1223
* Provide default max model length by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1224
* Bump up the version to v0.2.0 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1212

## New Contributors
* @leiwen83 made their first contribution in https://github.com/vllm-project/vllm/pull/991
* @LLukas22 made their first contribution in https://github.com/vllm-project/vllm/pull/1045
* @rucyang made their first contribution in https://github.com/vllm-project/vllm/pull/1020
* @chenxu2048 made their first contribution in https://github.com/vllm-project/vllm/pull/1029
* @orellavie1212 made their first contribution in https://github.com/vllm-project/vllm/pull/1080
* @tanmayv25 made their first contribution in https://github.com/vllm-project/vllm/pull/983
* @nkpz made their first contribution in https://github.com/vllm-project/vllm/pull/1145
* @WrRan made their first contribution in https://github.com/vllm-project/vllm/pull/1184
* @danilopeixoto made their first contribution in https://github.com/vllm-project/vllm/pull/1166
* @blahblahasdf made their first contribution in https://github.com/vllm-project/vllm/pull/1186
* @Bam4d made their first contribution in https://github.com/vllm-project/vllm/pull/1196

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.7...v0.2.0

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.2.0)

---

## v0.1.7: v0.1.7
**Published:** 2023-09-11

A minor release to fix the bugs in ALiBi, Falcon-40B, and Code Llama.

## What's Changed
* fix "tansformers_module" ModuleNotFoundError when load model with `trust_remote_code=True` by @Jingru in https://github.com/vllm-project/vllm/pull/871
* Fix wrong dtype in PagedAttentionWithALiBi bias by @Yard1 in https://github.com/vllm-project/vllm/pull/996
* fix: CUDA error when inferencing with Falcon-40B base model by @kyujin-cho in https://github.com/vllm-project/vllm/pull/992
* [Docs] Update installation page by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1005
* Update setup.py by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1006
* Use FP32 in RoPE initialization by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1004
* Bump up the version to v0.1.7 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1013

## New Contributors
* @Jingru made their first contribution in https://github.com/vllm-project/vllm/pull/871
* @kyujin-cho made their first contribution in https://github.com/vllm-project/vllm/pull/992

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.6...v0.1.7

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.7)

---

## v0.1.6: v0.1.6
**Published:** 2023-09-08

**Note:** This is an emergency release to revert a breaking API change that can make many existing codes using AsyncLLMServer not work.

## What's Changed
* faster startup of vLLM  by @ri938 in https://github.com/vllm-project/vllm/pull/982
* Start background task in `AsyncLLMEngine.generate` by @Yard1 in https://github.com/vllm-project/vllm/pull/988
* Bump up the version to v0.1.6 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/989

## New Contributors
* @ri938 made their first contribution in https://github.com/vllm-project/vllm/pull/982

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.5...v0.1.6

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.6)

---

## v0.1.5: v0.1.5
**Published:** 2023-09-07

## Major Changes
* Align beam search with `hf_model.generate`.
* Stablelize AsyncLLMEngine with a background engine loop.
* Add support for CodeLLaMA.
* Add many model correctness tests.
* Many other correctness fixes.

## What's Changed
* Add support for CodeLlama by @Yard1 in https://github.com/vllm-project/vllm/pull/854
* [Fix] Fix a condition for ignored sequences by @zhuohan123 in https://github.com/vllm-project/vllm/pull/867
* use flash-attn via xformers by @tmm1 in https://github.com/vllm-project/vllm/pull/877
* Enable request body OpenAPI spec for OpenAI endpoints by @Peilun-Li in https://github.com/vllm-project/vllm/pull/865
* Accelerate LLaMA model loading by @JF-D in https://github.com/vllm-project/vllm/pull/234
* Improve _prune_hidden_states micro-benchmark by @tmm1 in https://github.com/vllm-project/vllm/pull/707
* fix: bug fix when penalties are negative by @pfldy2850 in https://github.com/vllm-project/vllm/pull/913
* [Docs] Minor fixes in supported models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/920
* Fix README.md Link by @zhuohan123 in https://github.com/vllm-project/vllm/pull/927
* Add tests for models by @WoosukKwon in https://github.com/vllm-project/vllm/pull/922
* Avoid compiling kernels for double data type by @WoosukKwon in https://github.com/vllm-project/vllm/pull/933
* [BugFix] Fix NaN errors in paged attention kernel by @WoosukKwon in https://github.com/vllm-project/vllm/pull/936
* Refactor AsyncLLMEngine by @Yard1 in https://github.com/vllm-project/vllm/pull/880
* Only emit warning about internal tokenizer if it isn't being used by @nelson-liu in https://github.com/vllm-project/vllm/pull/939
* Align vLLM's beam search implementation with HF generate by @zhuohan123 in https://github.com/vllm-project/vllm/pull/857
* Initialize AsyncLLMEngine bg loop correctly by @Yard1 in https://github.com/vllm-project/vllm/pull/943
* FIx vLLM cannot launch by @HermitSun in https://github.com/vllm-project/vllm/pull/948
* Clean up kernel unit tests by @WoosukKwon in https://github.com/vllm-project/vllm/pull/938
* Use queue for finished requests by @Yard1 in https://github.com/vllm-project/vllm/pull/957
* [BugFix] Implement RoPE for GPT-J by @WoosukKwon in https://github.com/vllm-project/vllm/pull/941
* Set torch default dtype in a context manager by @Yard1 in https://github.com/vllm-project/vllm/pull/971
* Bump up transformers version in requirements.txt by @WoosukKwon in https://github.com/vllm-project/vllm/pull/976
* Make `AsyncLLMEngine` more robust & fix batched abort by @Yard1 in https://github.com/vllm-project/vllm/pull/969
* Enable safetensors loading for all models by @zhuohan123 in https://github.com/vllm-project/vllm/pull/974
* [FIX] Fix Alibi implementation in PagedAttention kernel by @zhuohan123 in https://github.com/vllm-project/vllm/pull/945
* Bump up the version to v0.1.5 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/944

## New Contributors
* @tmm1 made their first contribution in https://github.com/vllm-project/vllm/pull/877
* @Peilun-Li made their first contribution in https://github.com/vllm-project/vllm/pull/865
* @JF-D made their first contribution in https://github.com/vllm-project/vllm/pull/234
* @pfldy2850 made their first contribution in https://github.com/vllm-project/vllm/pull/913
* @nelson-liu made their first contribution in https://github.com/vllm-project/vllm/pull/939

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.4...v0.1.5

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.5)

---

## v0.1.4: vLLM v0.1.4
**Published:** 2023-08-25

## Major changes

* From now on, vLLM is published with pre-built CUDA binaries. Users don't have to compile the vLLM's CUDA kernels on their machine.
* New models: InternLM, Qwen, Aquila.
* Optimizing CUDA kernels for paged attention and GELU.
* Many bug fixes.

## What's Changed
* Fix gibberish outputs of GPT-BigCode-based models by @HermitSun in https://github.com/vllm-project/vllm/pull/676
* [OPTIMIZATION] Optimizes the single_query_cached_kv_attention kernel by @naed90 in https://github.com/vllm-project/vllm/pull/420
* add QWen-7b support by @Sanster in https://github.com/vllm-project/vllm/pull/685
* add internlm model by @gqjia in https://github.com/vllm-project/vllm/pull/528
* Check the max prompt length for the OpenAI completions API by @nicobasile in https://github.com/vllm-project/vllm/pull/472
* [Fix] unwantted bias in InternLM Model by @wangruohui in https://github.com/vllm-project/vllm/pull/740
* Supports tokens and arrays of tokens as inputs to the OpenAI completion API by @wanmok in https://github.com/vllm-project/vllm/pull/715
* Fix baichuan doc style by @UranusSeven in https://github.com/vllm-project/vllm/pull/748
* Fix typo in tokenizer.py by @eltociear in https://github.com/vllm-project/vllm/pull/750
* Align with huggingface Top K sampling by @Abraham-Xu in https://github.com/vllm-project/vllm/pull/753
* explicitly del state by @cauyxy in https://github.com/vllm-project/vllm/pull/784
* Fix typo in sampling_params.py by @wangcx18 in https://github.com/vllm-project/vllm/pull/788
* [Feature | CI] Added a github action to build wheels by @Danielkinz in https://github.com/vllm-project/vllm/pull/746
* set default coompute capability according to cuda version by @zxdvd in https://github.com/vllm-project/vllm/pull/773
* Fix mqa is false case in gpt_bigcode by @zhaoyang-star in https://github.com/vllm-project/vllm/pull/806
* Add support for aquila by @shunxing1234 in https://github.com/vllm-project/vllm/pull/663
* Update Supported Model List by @zhuohan123 in https://github.com/vllm-project/vllm/pull/825
* Fix 'GPTBigCodeForCausalLM' object has no attribute 'tensor_model_parallel_world_size' by @HermitSun in https://github.com/vllm-project/vllm/pull/827
* Add compute capability 8.9 to default targets by @WoosukKwon in https://github.com/vllm-project/vllm/pull/829
* Implement approximate GELU kernels by @WoosukKwon in https://github.com/vllm-project/vllm/pull/828
* Fix typo of Aquila in README.md by @ftgreat in https://github.com/vllm-project/vllm/pull/836
* Fix for breaking changes in xformers 0.0.21 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/834
* Clean up code by @wenjun93 in https://github.com/vllm-project/vllm/pull/844
* Set replacement=True in torch.multinomial by @WoosukKwon in https://github.com/vllm-project/vllm/pull/858
* Bump up the version to v0.1.4 by @WoosukKwon in https://github.com/vllm-project/vllm/pull/846

## New Contributors
* @naed90 made their first contribution in https://github.com/vllm-project/vllm/pull/420
* @gqjia made their first contribution in https://github.com/vllm-project/vllm/pull/528
* @nicobasile made their first contribution in https://github.com/vllm-project/vllm/pull/472
* @wanmok made their first contribution in https://github.com/vllm-project/vllm/pull/715
* @UranusSeven made their first contribution in https://github.com/vllm-project/vllm/pull/748
* @eltociear made their first contribution in https://github.com/vllm-project/vllm/pull/750
* @Abraham-Xu made their first contribution in https://github.com/vllm-project/vllm/pull/753
* @cauyxy made their first contribution in https://github.com/vllm-project/vllm/pull/784
* @wangcx18 made their first contribution in https://github.com/vllm-project/vllm/pull/788
* @Danielkinz made their first contribution in https://github.com/vllm-project/vllm/pull/746
* @zhaoyang-star made their first contribution in https://github.com/vllm-project/vllm/pull/806
* @shunxing1234 made their first contribution in https://github.com/vllm-project/vllm/pull/663
* @ftgreat made their first contribution in https://github.com/vllm-project/vllm/pull/836
* @wenjun93 made their first contribution in https://github.com/vllm-project/vllm/pull/844

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.3...v0.1.4

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.4)

---

## v0.1.3: vLLM v0.1.3
**Published:** 2023-08-02

## What's Changed

### Major changes

* More model support: LLaMA 2, Falcon, GPT-J, Baichuan, etc.
* Efficient support for MQA and GQA.
* Changes in the scheduling algorithm: vLLM now uses a TGI-style continuous batching.
* And many bug fixes.

### All changes

* fix: only response [DONE] once when streaming response. by @gesanqiu in https://github.com/vllm-project/vllm/pull/378
* [Fix] Change /generate response-type to json for non-streaming by @nicolasf in https://github.com/vllm-project/vllm/pull/374
* Add trust-remote-code flag to handle remote tokenizers by @codethazine in https://github.com/vllm-project/vllm/pull/364
* avoid python list copy in sequence initialization by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/401
* [Fix] Sort LLM outputs by request ID before return by @WoosukKwon in https://github.com/vllm-project/vllm/pull/402
* Add trust_remote_code arg to get_config by @WoosukKwon in https://github.com/vllm-project/vllm/pull/405
* Don't try to load training_args.bin by @lpfhs in https://github.com/vllm-project/vllm/pull/373
* [Model] Add support for GPT-J by @AndreSlavescu in https://github.com/vllm-project/vllm/pull/226
* fix: freeze pydantic to v1 by @kemingy in https://github.com/vllm-project/vllm/pull/429
* Fix handling of special tokens in decoding. by @xcnick in https://github.com/vllm-project/vllm/pull/418
* add vocab padding for LLama(Support WizardLM) by @esmeetu in https://github.com/vllm-project/vllm/pull/411
* Fix the `KeyError` when loading bloom-based models by @HermitSun in https://github.com/vllm-project/vllm/pull/441
* Optimize MQA Kernel by @zhuohan123 in https://github.com/vllm-project/vllm/pull/452
* Offload port selection to OS by @zhangir-azerbayev in https://github.com/vllm-project/vllm/pull/467
* [Doc] Add doc for running vLLM on the cloud by @Michaelvll in https://github.com/vllm-project/vllm/pull/426
* [Fix] Fix the condition of max_seq_len by @zhuohan123 in https://github.com/vllm-project/vllm/pull/477
* Add support for baichuan by @codethazine in https://github.com/vllm-project/vllm/pull/365
* fix max seq len by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/489
* Fixed old name reference for max_seq_len by @MoeedDar in https://github.com/vllm-project/vllm/pull/498
* hotfix attn alibi wo head mapping by @Oliver-ss in https://github.com/vllm-project/vllm/pull/496
* fix(ray_utils): ignore re-init error by @mspronesti in https://github.com/vllm-project/vllm/pull/465
* Support `trust_remote_code` in benchmark by @wangruohui in https://github.com/vllm-project/vllm/pull/518
* fix: enable trust-remote-code in api server & benchmark. by @gesanqiu in https://github.com/vllm-project/vllm/pull/509
* Ray placement group support by @Yard1 in https://github.com/vllm-project/vllm/pull/397
* Fix bad assert in initialize_cluster if PG already exists by @Yard1 in https://github.com/vllm-project/vllm/pull/526
* Add support for LLaMA-2 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/505
* GPTJConfig has no attribute rotary.  by @leegohi04517 in https://github.com/vllm-project/vllm/pull/532
* [Fix] Fix GPTBigcoder for distributed execution by @zhuohan123 in https://github.com/vllm-project/vllm/pull/503
* Fix paged attention testing. by @shanshanpt in https://github.com/vllm-project/vllm/pull/495
* fixed tensor parallel is not defined by @MoeedDar in https://github.com/vllm-project/vllm/pull/564
* Add Baichuan-7B to README by @zhuohan123 in https://github.com/vllm-project/vllm/pull/494
* [Fix] Add chat completion Example and simplify dependencies by @zhuohan123 in https://github.com/vllm-project/vllm/pull/576
* [Fix] Add model sequence length into model config by @zhuohan123 in https://github.com/vllm-project/vllm/pull/575
* [Fix] fix import error of RayWorker (#604) by @zxdvd in https://github.com/vllm-project/vllm/pull/605
* fix ModuleNotFoundError by @mklf in https://github.com/vllm-project/vllm/pull/599
* [Doc] Change old max_seq_len to max_model_len in docs by @SiriusNEO in https://github.com/vllm-project/vllm/pull/622
* fix biachuan-7b tp by @Sanster in https://github.com/vllm-project/vllm/pull/598
* [Model] support baichuan-13b based on baichuan-7b by @Oliver-ss in https://github.com/vllm-project/vllm/pull/643
* Fix log message in scheduler by @LiuXiaoxuanPKU in https://github.com/vllm-project/vllm/pull/652
* Add Falcon support (new) by @zhuohan123 in https://github.com/vllm-project/vllm/pull/592
* [BUG FIX] upgrade fschat version to 0.2.23 by @YHPeter in https://github.com/vllm-project/vllm/pull/650
* Refactor scheduler by @WoosukKwon in https://github.com/vllm-project/vllm/pull/658
* [Doc] Add Baichuan 13B to supported models by @zhuohan123 in https://github.com/vllm-project/vllm/pull/656
* Bump up version to 0.1.3 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/657

## New Contributors
* @nicolasf made their first contribution in https://github.com/vllm-project/vllm/pull/374
* @codethazine made their first contribution in https://github.com/vllm-project/vllm/pull/364
* @lpfhs made their first contribution in https://github.com/vllm-project/vllm/pull/373
* @AndreSlavescu made their first contribution in https://github.com/vllm-project/vllm/pull/226
* @kemingy made their first contribution in https://github.com/vllm-project/vllm/pull/429
* @xcnick made their first contribution in https://github.com/vllm-project/vllm/pull/418
* @esmeetu made their first contribution in https://github.com/vllm-project/vllm/pull/411
* @HermitSun made their first contribution in https://github.com/vllm-project/vllm/pull/441
* @zhangir-azerbayev made their first contribution in https://github.com/vllm-project/vllm/pull/467
* @MoeedDar made their first contribution in https://github.com/vllm-project/vllm/pull/498
* @Oliver-ss made their first contribution in https://github.com/vllm-project/vllm/pull/496
* @mspronesti made their first contribution in https://github.com/vllm-project/vllm/pull/465
* @wangruohui made their first contribution in https://github.com/vllm-project/vllm/pull/518
* @Yard1 made their first contribution in https://github.com/vllm-project/vllm/pull/397
* @leegohi04517 made their first contribution in https://github.com/vllm-project/vllm/pull/532
* @shanshanpt made their first contribution in https://github.com/vllm-project/vllm/pull/495
* @zxdvd made their first contribution in https://github.com/vllm-project/vllm/pull/605
* @mklf made their first contribution in https://github.com/vllm-project/vllm/pull/599
* @SiriusNEO made their first contribution in https://github.com/vllm-project/vllm/pull/622
* @Sanster made their first contribution in https://github.com/vllm-project/vllm/pull/598
* @YHPeter made their first contribution in https://github.com/vllm-project/vllm/pull/650

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.2...v0.1.3

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.3)

---

## v0.1.2: vLLM v0.1.2
**Published:** 2023-07-05

## What's Changed

- Initial support for GPTBigCode 
- Support for MPT and BLOOM
- Custom tokenizer 
- ChatCompletion endpoint in OpenAI demo server
- Code format
- Various bug fixes and improvements 
- Documentation improvement 

## Contributors

Thanks to the following amazing people who contributed to this release:

@michaelfeil @WoosukKwon @metacryptom @merrymercy @BasicCoder @zhuohan123 @twaka @comaniac @neubig @JRC1995 @LiuXiaoxuanPKU @bm777 @Michaelvll @gesanqiu @ironpinguin @coolcloudcol @akxxsb

**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.1...v0.1.2

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.2)

---

## v0.1.1: vLLM v0.1.1 (Patch)
**Published:** 2023-06-22

## What's Changed
* Fix Ray node resources error by @zhuohan123 in https://github.com/vllm-project/vllm/pull/193
* [Bugfix] Fix a bug in RequestOutput.finished by @WoosukKwon in https://github.com/vllm-project/vllm/pull/202
* [Fix] Better error message when there is OOM during cache initialization by @zhuohan123 in https://github.com/vllm-project/vllm/pull/203
* Bump up version to 0.1.1 by @zhuohan123 in https://github.com/vllm-project/vllm/pull/204


**Full Changelog**: https://github.com/vllm-project/vllm/compare/v0.1.0...v0.1.1

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.1)

---

## v0.1.0: vLLM v0.1.0
**Published:** 2023-06-20

# The first official release of vLLM!

See our [README](https://github.com/vllm-project/vllm) for details.

# Thanks

Thanks @WoosukKwon @zhuohan123 @suquark for their contributions.

[View on GitHub](https://github.com/vllm-project/vllm/releases/tag/v0.1.0)

---

