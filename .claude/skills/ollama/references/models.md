# Ollama - Models

**Pages:** 15

---

## Pull a model

**URL:** llms-txt#pull-a-model

Source: https://docs.ollama.com/api/pull

openapi.yaml post /api/pull

---

## Push a model

**URL:** llms-txt#push-a-model

Source: https://docs.ollama.com/api/push

openapi.yaml post /api/push

---

## List running models

**URL:** llms-txt#list-running-models

Source: https://docs.ollama.com/api/ps

openapi.yaml get /api/ps
Retrieve a list of models that are currently running

---

## OpenAI compatibility

**URL:** llms-txt#openai-compatibility

**Contents:**
- Usage
  - OpenAI Python library

Source: https://docs.ollama.com/api/openai-compatibility

Ollama provides compatibility with parts of the [OpenAI API](https://platform.openai.com/docs/api-reference) to help connect existing applications to Ollama.

### OpenAI Python library

#### Structured outputs

```python  theme={"system"}
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")

**Examples:**

Example 1 (unknown):
```unknown
#### Structured outputs
```

---

## Copy a model

**URL:** llms-txt#copy-a-model

Source: https://docs.ollama.com/api/copy

openapi.yaml post /api/copy

---

## CLI Reference

**URL:** llms-txt#cli-reference

**Contents:**
  - Run a model
  - Generate embeddings
  - Download a model
  - Remove a model
  - List models
  - Sign in to Ollama
  - Sign out of Ollama
  - Create a customized model
  - List running models
  - Stop a running model

Source: https://docs.ollama.com/cli

For multiline input, you can wrap text with `"""`:

#### Multimodal models

### Generate embeddings

Output is a JSON array:

### Sign in to Ollama

### Sign out of Ollama

### Create a customized model

First, create a `Modelfile`

Then run `ollama create`:

### List running models

### Stop a running model

To view a list of environment variables that can be set run `ollama serve --help`

**Examples:**

Example 1 (unknown):
```unknown
ollama run gemma3
```

Example 2 (unknown):
```unknown
>>> """Hello,
... world!
... """
I'm a basic program that prints the famous "Hello, world!" message to the console.
```

Example 3 (unknown):
```unknown
ollama run gemma3 "What's in this image? /Users/jmorgan/Desktop/smile.png"
```

Example 4 (unknown):
```unknown
ollama run embeddinggemma "Hello world"
```

---

## Modelfile Reference

**URL:** llms-txt#modelfile-reference

**Contents:**
- Table of Contents
- Format

Source: https://docs.ollama.com/modelfile

A Modelfile is the blueprint to create and share customized models using Ollama.

* [Format](#format)
* [Examples](#examples)
* [Instructions](#instructions)
  * [FROM (Required)](#from-required)
    * [Build from existing model](#build-from-existing-model)
    * [Build from a Safetensors model](#build-from-a-safetensors-model)
    * [Build from a GGUF file](#build-from-a-gguf-file)
  * [PARAMETER](#parameter)
    * [Valid Parameters and Values](#valid-parameters-and-values)
  * [TEMPLATE](#template)
    * [Template Variables](#template-variables)
  * [SYSTEM](#system)
  * [ADAPTER](#adapter)
  * [LICENSE](#license)
  * [MESSAGE](#message)
* [Notes](#notes)

The format of the `Modelfile`:

---

## Create a model

**URL:** llms-txt#create-a-model

Source: https://docs.ollama.com/api/create

openapi.yaml post /api/create

---

## Delete a model

**URL:** llms-txt#delete-a-model

Source: https://docs.ollama.com/api/delete

openapi.yaml delete /api/delete

---

## Show model details

**URL:** llms-txt#show-model-details

Source: https://docs.ollama.com/api-reference/show-model-details

openapi.yaml post /api/show

---

## To build a new Modelfile based on this one, replace the FROM line with:

**URL:** llms-txt#to-build-a-new-modelfile-based-on-this-one,-replace-the-from-line-with:

---

## List models

**URL:** llms-txt#list-models

Source: https://docs.ollama.com/api/tags

openapi.yaml get /api/tags
Fetch a list of models and their details

---

## sets a custom system message to specify the behavior of the chat assistant

**URL:** llms-txt#sets-a-custom-system-message-to-specify-the-behavior-of-the-chat-assistant

SYSTEM You are Mario from super mario bros, acting as an assistant.
shell  theme={"system"}
ollama show --modelfile llama3.2

**Examples:**

Example 1 (unknown):
```unknown
To use this:

1. Save it as a file (e.g. `Modelfile`)
2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`
3. `ollama run choose-a-model-name`
4. Start using the model!

To view the Modelfile of a given model, use the `ollama show --modelfile` command.
```

Example 2 (unknown):
```unknown

```

---

## Modelfile generated by "ollama show"

**URL:** llms-txt#modelfile-generated-by-"ollama-show"

---

## Importing a Model

**URL:** llms-txt#importing-a-model

**Contents:**
- Table of Contents
- Importing a fine tuned adapter from Safetensors weights
- Importing a model from Safetensors weights
- Importing a GGUF based model or adapter
- Quantizing a Model
  - Supported Quantizations
- Sharing your model on ollama.com

Source: https://docs.ollama.com/import

* [Importing a Safetensors adapter](#Importing-a-fine-tuned-adapter-from-Safetensors-weights)
* [Importing a Safetensors model](#Importing-a-model-from-Safetensors-weights)
* [Importing a GGUF file](#Importing-a-GGUF-based-model-or-adapter)
* [Sharing models on ollama.com](#Sharing-your-model-on-ollamacom)

## Importing a fine tuned adapter from Safetensors weights

First, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:

Make sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.

Now run `ollama create` from the directory where the `Modelfile` was created:

Lastly, test the model:

Ollama supports importing adapters based on several different model architectures including:

* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
* Mistral (including Mistral 1, Mistral 2, and Mixtral); and
* Gemma (including Gemma 1 and Gemma 2)

You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:

* Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)
* [Unsloth](https://github.com/unslothai/unsloth)
* [MLX](https://github.com/ml-explore/mlx)

## Importing a model from Safetensors weights

First, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:

If you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.

Now run the `ollama create` command from the directory where you created the `Modelfile`:

Lastly, test the model:

Ollama supports importing models for several different architectures including:

* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
* Mistral (including Mistral 1, Mistral 2, and Mixtral);
* Gemma (including Gemma 1 and Gemma 2); and
* Phi3

This includes importing foundation models as well as any fine tuned models which have been *fused* with a foundation model.

## Importing a GGUF based model or adapter

If you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:

* converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp;
* converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or
* downloading a model or adapter from a place such as HuggingFace

To import a GGUF model, create a `Modelfile` containing:

For a GGUF adapter, create the `Modelfile` with:

When importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:

* a model from Ollama
* a GGUF file
* a Safetensors based model

Once you have created your `Modelfile`, use the `ollama create` command to build the model.

## Quantizing a Model

Quantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.

Ollama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.

First, create a Modelfile with the FP16 or FP32 based model you wish to quantize.

Use `ollama create` to then create the quantized model.

### Supported Quantizations

* `q4_0`
* `q4_1`
* `q5_0`
* `q5_1`
* `q8_0`

#### K-means Quantizations

* `q3_K_S`
* `q3_K_M`
* `q3_K_L`
* `q4_K_S`
* `q4_K_M`
* `q5_K_S`
* `q5_K_M`
* `q6_K`

## Sharing your model on ollama.com

You can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.

First, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.

<img src="https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=d99f1340e6cfd85d36d49a444491cc63" alt="Sign-Up" width="40%" data-og-width="756" data-og-height="1192" data-path="images/signup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=8546e600b6c2b29ca91b23d837e9dc94 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=786fba46e20fe8f9c2675abb620c7643 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=06b9d9837022e7dbe9f9005f6f977eca 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ff9cc91091ffad52f8fb30a990d3089 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=727fbd87da3a076b45794ea248f1afd3 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=2ff3ce5e5197144725e860513cfe59e8 2500w" />

The `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.

Now that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.

Follow the directions on the page to determine where your Ollama Public Key is located.

<img src="https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ced4d97ecf6b115219f929a4914205e" alt="Ollama Keys" width="80%" data-og-width="1200" data-og-height="893" data-path="images/ollama-keys.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=e3c7925a5a4f5dadafcf3908df55b97c 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=66bb814e45ec58e6b15a4be890c6fec8 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=44af18318e8469b719218a46305361d6 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=46d0cfa93938c243b8e41a169ceedb1f 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=c235d8ed66c24171946e376eb5e6663e 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=4844902d0289c2154b65d3d0339e9934 2500w" />

Click on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.

To push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy
your model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).

Once your model has been pushed, other users can pull and run it by using the command:

**Examples:**

Example 1 (unknown):
```unknown
Make sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.

Now run `ollama create` from the directory where the `Modelfile` was created:
```

Example 2 (unknown):
```unknown
Lastly, test the model:
```

Example 3 (unknown):
```unknown
Ollama supports importing adapters based on several different model architectures including:

* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
* Mistral (including Mistral 1, Mistral 2, and Mixtral); and
* Gemma (including Gemma 1 and Gemma 2)

You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:

* Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)
* [Unsloth](https://github.com/unslothai/unsloth)
* [MLX](https://github.com/ml-explore/mlx)

## Importing a model from Safetensors weights

First, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:
```

Example 4 (unknown):
```unknown
If you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.

Now run the `ollama create` command from the directory where you created the `Modelfile`:
```

---
