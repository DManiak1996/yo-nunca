{
  "title": "Importing a Model",
  "content": "Source: https://docs.ollama.com/import\n\n* [Importing a Safetensors adapter](#Importing-a-fine-tuned-adapter-from-Safetensors-weights)\n* [Importing a Safetensors model](#Importing-a-model-from-Safetensors-weights)\n* [Importing a GGUF file](#Importing-a-GGUF-based-model-or-adapter)\n* [Sharing models on ollama.com](#Sharing-your-model-on-ollamacom)\n\n## Importing a fine tuned adapter from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:\n\nMake sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.\n\nNow run `ollama create` from the directory where the `Modelfile` was created:\n\nLastly, test the model:\n\nOllama supports importing adapters based on several different model architectures including:\n\n* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n* Mistral (including Mistral 1, Mistral 2, and Mixtral); and\n* Gemma (including Gemma 1 and Gemma 2)\n\nYou can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:\n\n* Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)\n* [Unsloth](https://github.com/unslothai/unsloth)\n* [MLX](https://github.com/ml-explore/mlx)\n\n## Importing a model from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:\n\nIf you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.\n\nNow run the `ollama create` command from the directory where you created the `Modelfile`:\n\nLastly, test the model:\n\nOllama supports importing models for several different architectures including:\n\n* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n* Mistral (including Mistral 1, Mistral 2, and Mixtral);\n* Gemma (including Gemma 1 and Gemma 2); and\n* Phi3\n\nThis includes importing foundation models as well as any fine tuned models which have been *fused* with a foundation model.\n\n## Importing a GGUF based model or adapter\n\nIf you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:\n\n* converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp;\n* converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or\n* downloading a model or adapter from a place such as HuggingFace\n\nTo import a GGUF model, create a `Modelfile` containing:\n\nFor a GGUF adapter, create the `Modelfile` with:\n\nWhen importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:\n\n* a model from Ollama\n* a GGUF file\n* a Safetensors based model\n\nOnce you have created your `Modelfile`, use the `ollama create` command to build the model.\n\n## Quantizing a Model\n\nQuantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.\n\nOllama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.\n\nFirst, create a Modelfile with the FP16 or FP32 based model you wish to quantize.\n\nUse `ollama create` to then create the quantized model.\n\n### Supported Quantizations\n\n* `q4_0`\n* `q4_1`\n* `q5_0`\n* `q5_1`\n* `q8_0`\n\n#### K-means Quantizations\n\n* `q3_K_S`\n* `q3_K_M`\n* `q3_K_L`\n* `q4_K_S`\n* `q4_K_M`\n* `q5_K_S`\n* `q5_K_M`\n* `q6_K`\n\n## Sharing your model on ollama.com\n\nYou can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.\n\nFirst, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.\n\n<img src=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=d99f1340e6cfd85d36d49a444491cc63\" alt=\"Sign-Up\" width=\"40%\" data-og-width=\"756\" data-og-height=\"1192\" data-path=\"images/signup.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=8546e600b6c2b29ca91b23d837e9dc94 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=786fba46e20fe8f9c2675abb620c7643 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=06b9d9837022e7dbe9f9005f6f977eca 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ff9cc91091ffad52f8fb30a990d3089 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=727fbd87da3a076b45794ea248f1afd3 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=2ff3ce5e5197144725e860513cfe59e8 2500w\" />\n\nThe `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.\n\nNow that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.\n\nFollow the directions on the page to determine where your Ollama Public Key is located.\n\n<img src=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ced4d97ecf6b115219f929a4914205e\" alt=\"Ollama Keys\" width=\"80%\" data-og-width=\"1200\" data-og-height=\"893\" data-path=\"images/ollama-keys.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=e3c7925a5a4f5dadafcf3908df55b97c 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=66bb814e45ec58e6b15a4be890c6fec8 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=44af18318e8469b719218a46305361d6 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=46d0cfa93938c243b8e41a169ceedb1f 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=c235d8ed66c24171946e376eb5e6663e 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=4844902d0289c2154b65d3d0339e9934 2500w\" />\n\nClick on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.\n\nTo push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy\nyour model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).\n\nOnce your model has been pushed, other users can pull and run it by using the command:",
  "code_samples": [
    {
      "code": "Make sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.\n\nNow run `ollama create` from the directory where the `Modelfile` was created:",
      "language": "unknown"
    },
    {
      "code": "Lastly, test the model:",
      "language": "unknown"
    },
    {
      "code": "Ollama supports importing adapters based on several different model architectures including:\n\n* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n* Mistral (including Mistral 1, Mistral 2, and Mixtral); and\n* Gemma (including Gemma 1 and Gemma 2)\n\nYou can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:\n\n* Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)\n* [Unsloth](https://github.com/unslothai/unsloth)\n* [MLX](https://github.com/ml-explore/mlx)\n\n## Importing a model from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:",
      "language": "unknown"
    },
    {
      "code": "If you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.\n\nNow run the `ollama create` command from the directory where you created the `Modelfile`:",
      "language": "unknown"
    },
    {
      "code": "Lastly, test the model:",
      "language": "unknown"
    },
    {
      "code": "Ollama supports importing models for several different architectures including:\n\n* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n* Mistral (including Mistral 1, Mistral 2, and Mixtral);\n* Gemma (including Gemma 1 and Gemma 2); and\n* Phi3\n\nThis includes importing foundation models as well as any fine tuned models which have been *fused* with a foundation model.\n\n## Importing a GGUF based model or adapter\n\nIf you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:\n\n* converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp;\n* converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or\n* downloading a model or adapter from a place such as HuggingFace\n\nTo import a GGUF model, create a `Modelfile` containing:",
      "language": "unknown"
    },
    {
      "code": "For a GGUF adapter, create the `Modelfile` with:",
      "language": "unknown"
    },
    {
      "code": "When importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:\n\n* a model from Ollama\n* a GGUF file\n* a Safetensors based model\n\nOnce you have created your `Modelfile`, use the `ollama create` command to build the model.",
      "language": "unknown"
    },
    {
      "code": "## Quantizing a Model\n\nQuantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.\n\nOllama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.\n\nFirst, create a Modelfile with the FP16 or FP32 based model you wish to quantize.",
      "language": "unknown"
    },
    {
      "code": "Use `ollama create` to then create the quantized model.",
      "language": "unknown"
    },
    {
      "code": "### Supported Quantizations\n\n* `q4_0`\n* `q4_1`\n* `q5_0`\n* `q5_1`\n* `q8_0`\n\n#### K-means Quantizations\n\n* `q3_K_S`\n* `q3_K_M`\n* `q3_K_L`\n* `q4_K_S`\n* `q4_K_M`\n* `q5_K_S`\n* `q5_K_M`\n* `q6_K`\n\n## Sharing your model on ollama.com\n\nYou can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.\n\nFirst, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.\n\n<img src=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=d99f1340e6cfd85d36d49a444491cc63\" alt=\"Sign-Up\" width=\"40%\" data-og-width=\"756\" data-og-height=\"1192\" data-path=\"images/signup.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=8546e600b6c2b29ca91b23d837e9dc94 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=786fba46e20fe8f9c2675abb620c7643 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=06b9d9837022e7dbe9f9005f6f977eca 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ff9cc91091ffad52f8fb30a990d3089 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=727fbd87da3a076b45794ea248f1afd3 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/signup.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=2ff3ce5e5197144725e860513cfe59e8 2500w\" />\n\nThe `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.\n\nNow that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.\n\nFollow the directions on the page to determine where your Ollama Public Key is located.\n\n<img src=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=7ced4d97ecf6b115219f929a4914205e\" alt=\"Ollama Keys\" width=\"80%\" data-og-width=\"1200\" data-og-height=\"893\" data-path=\"images/ollama-keys.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=280&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=e3c7925a5a4f5dadafcf3908df55b97c 280w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=560&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=66bb814e45ec58e6b15a4be890c6fec8 560w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=840&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=44af18318e8469b719218a46305361d6 840w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1100&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=46d0cfa93938c243b8e41a169ceedb1f 1100w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=1650&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=c235d8ed66c24171946e376eb5e6663e 1650w, https://mintcdn.com/ollama-9269c548/uieua2DvLKVQ74Ga/images/ollama-keys.png?w=2500&fit=max&auto=format&n=uieua2DvLKVQ74Ga&q=85&s=4844902d0289c2154b65d3d0339e9934 2500w\" />\n\nClick on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.\n\nTo push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy\nyour model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).",
      "language": "unknown"
    },
    {
      "code": "Once your model has been pushed, other users can pull and run it by using the command:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Table of Contents",
      "id": "table-of-contents"
    },
    {
      "level": "h2",
      "text": "Importing a fine tuned adapter from Safetensors weights",
      "id": "importing-a-fine-tuned-adapter-from-safetensors-weights"
    },
    {
      "level": "h2",
      "text": "Importing a model from Safetensors weights",
      "id": "importing-a-model-from-safetensors-weights"
    },
    {
      "level": "h2",
      "text": "Importing a GGUF based model or adapter",
      "id": "importing-a-gguf-based-model-or-adapter"
    },
    {
      "level": "h2",
      "text": "Quantizing a Model",
      "id": "quantizing-a-model"
    },
    {
      "level": "h3",
      "text": "Supported Quantizations",
      "id": "supported-quantizations"
    },
    {
      "level": "h2",
      "text": "Sharing your model on ollama.com",
      "id": "sharing-your-model-on-ollama.com"
    }
  ],
  "url": "llms-txt#importing-a-model",
  "links": []
}