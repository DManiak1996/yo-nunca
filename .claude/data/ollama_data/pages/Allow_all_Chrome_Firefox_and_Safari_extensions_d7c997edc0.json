{
  "title": "Allow all Chrome, Firefox, and Safari extensions",
  "content": "OLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve\nshell  theme={\"system\"}\ncurl http://localhost:11434/api/generate -d '{\"model\": \"mistral\"}'\nshell  theme={\"system\"}\ncurl http://localhost:11434/api/chat -d '{\"model\": \"mistral\"}'\nshell  theme={\"system\"}\nollama run llama3.2 \"\"\nshell  theme={\"system\"}\nollama stop llama3.2\nshell  theme={\"system\"}\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": -1}'\nshell  theme={\"system\"}\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\nshell  theme={\"system\"}\nollama signin\n```\n\n* **Manually copy & paste** the key on the **Ollama Keys** page:\n  [https://ollama.com/settings/keys](https://ollama.com/settings/keys)\n\n### Where the Ollama Public Key lives\n\n| OS      | Path to `id_ed25519.pub`                     |\n| :------ | :------------------------------------------- |\n| macOS   | `~/.ollama/id_ed25519.pub`                   |\n| Linux   | `/usr/share/ollama/.ollama/id_ed25519.pub`   |\n| Windows | `C:\\Users\\<username>\\.ollama\\id_ed25519.pub` |\n\n<Note>\n  Replace \\<username> with your actual Windows user name.\n</Note>",
  "code_samples": [
    {
      "code": "Refer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## Where are models stored?\n\n* macOS: `~/.ollama/models`\n* Linux: `/usr/share/ollama/.ollama/models`\n* Windows: `C:\\Users\\%username%\\.ollama\\models`\n\n### How do I set them to a different location?\n\nIf a different directory needs to be used, set the environment variable `OLLAMA_MODELS` to the chosen directory.\n\n<Note>\n  On Linux using the standard installer, the `ollama` user needs read and write access to the specified directory. To assign the directory to the `ollama` user run `sudo chown -R ollama:ollama <directory>`.\n</Note>\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## How can I use Ollama in Visual Studio Code?\n\nThere is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of [extensions & plugins](https://github.com/ollama/ollama#extensions--plugins) at the bottom of the main repository readme.\n\n## How do I use Ollama with GPU acceleration in Docker?\n\nThe Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit). See [ollama/ollama](https://hub.docker.com/r/ollama/ollama) for more details.\n\nGPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.\n\n## Why is networking slow in WSL2 on Windows 10?\n\nThis can impact both installing Ollama, as well as downloading models.\n\nOpen `Control Panel > Networking and Internet > View network status and tasks` and click on `Change adapter settings` on the left panel. Find the `vEthernel (WSL)` adapter, right click and select `Properties`.\nClick on `Configure` and open the `Advanced` tab. Search through each of the properties until you find `Large Send Offload Version 2 (IPv4)` and `Large Send Offload Version 2 (IPv6)`. *Disable* both of these\nproperties.\n\n## How can I preload a model into Ollama to get faster response times?\n\nIf you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the `/api/generate` and `/api/chat` API endpoints.\n\nTo preload the mistral model using the generate endpoint, use:",
      "language": "unknown"
    },
    {
      "code": "To use the chat completions endpoint, use:",
      "language": "unknown"
    },
    {
      "code": "To preload a model using the CLI, use the command:",
      "language": "unknown"
    },
    {
      "code": "## How do I keep a model loaded in memory or make it unload immediately?\n\nBy default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the `ollama stop` command:",
      "language": "unknown"
    },
    {
      "code": "If you're using the API, use the `keep_alive` parameter with the `/api/generate` and `/api/chat` endpoints to set the amount of time that a model stays in memory. The `keep_alive` parameter can be set to:\n\n* a duration string (such as \"10m\" or \"24h\")\n* a number in seconds (such as 3600)\n* any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\")\n* '0' which will unload the model immediately after generating a response\n\nFor example, to preload a model and leave it in memory use:",
      "language": "unknown"
    },
    {
      "code": "To unload the model and free up memory use:",
      "language": "unknown"
    },
    {
      "code": "Alternatively, you can change the amount of time all models are loaded into memory by setting the `OLLAMA_KEEP_ALIVE` environment variable when starting the Ollama server. The `OLLAMA_KEEP_ALIVE` variable uses the same parameter types as the `keep_alive` parameter types mentioned above. Refer to the section explaining [how to configure the Ollama server](#how-do-i-configure-ollama-server) to correctly set the environment variable.\n\nThe `keep_alive` API parameter with the `/api/generate` and `/api/chat` API endpoints will override the `OLLAMA_KEEP_ALIVE` setting.\n\n## How do I manage the maximum number of requests the Ollama server can queue?\n\nIf too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded. You can adjust how many requests may be queue by setting `OLLAMA_MAX_QUEUE`.\n\n## How does Ollama handle concurrent requests?\n\nOllama supports two levels of concurrent processing. If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time. For a given model, if there is sufficient available memory when the model is loaded, it is configured to allow parallel request processing.\n\nIf there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded. As prior models become idle, one or more will be unloaded to make room for the new model. Queued requests will be processed in order. When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.\n\nParallel request processing for a given model results in increasing the context size by the number of parallel requests. For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.\n\nThe following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:\n\n* `OLLAMA_MAX_LOADED_MODELS` - The maximum number of models that can be loaded concurrently provided they fit in available memory. The default is 3 \\* the number of GPUs or 3 for CPU inference.\n* `OLLAMA_NUM_PARALLEL` - The maximum number of parallel requests each model will process at the same time. The default will auto-select either 4 or 1 based on available memory.\n* `OLLAMA_MAX_QUEUE` - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512\n\nNote: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting. Once ROCm v6.2 is available, Windows Radeon will follow the defaults above. You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.\n\n## How does Ollama load models on multiple GPUs?\n\nWhen loading a new model, Ollama evaluates the required VRAM for the model against what is currently available. If the model will entirely fit on any single GPU, Ollama will load the model on that GPU. This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference. If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.\n\n## How can I enable Flash Attention?\n\nFlash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows. To enable Flash Attention, set the `OLLAMA_FLASH_ATTENTION` environment variable to `1` when starting the Ollama server.\n\n## How can I set the quantization type for the K/V cache?\n\nThe K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.\n\nTo use quantized K/V cache with Ollama you can set the following environment variable:\n\n* `OLLAMA_KV_CACHE_TYPE` - The quantization type for the K/V cache. Default is `f16`.\n\n<Note>\n  Currently this is a global option - meaning all models will run with the\n  specified quantization type.\n</Note>\n\nThe currently available K/V cache quantization types are:\n\n* `f16` - high precision and memory usage (default).\n* `q8_0` - 8-bit quantization, uses approximately 1/2 the memory of `f16` with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).\n* `q4_0` - 4-bit quantization, uses approximately 1/4 the memory of `f16` with a small-medium loss in precision that may be more noticeable at higher context sizes.\n\nHow much the cache quantization impacts the model's response quality will depend on the model and the task. Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.\n\nYou may need to experiment with different quantization types to find the best balance between memory usage and quality.\n\n## Where can I find my Ollama Public Key?\n\nYour **Ollama Public Key** is the public part of the key pair that lets your local Ollama instance talk to [ollama.com](https://ollama.com).\n\nYou'll need it to:\n\n* Push models to Ollama\n* Pull private models from Ollama to your machine\n* Run models hosted in [Ollama Cloud](https://ollama.com/cloud)\n\n### How to Add the Key\n\n* **Sign-in via the Settings page** in the **Mac** and **Windows App**\n\n* **Signâ€‘in via CLI**",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Where are models stored?",
      "id": "where-are-models-stored?"
    },
    {
      "level": "h3",
      "text": "How do I set them to a different location?",
      "id": "how-do-i-set-them-to-a-different-location?"
    },
    {
      "level": "h2",
      "text": "How can I use Ollama in Visual Studio Code?",
      "id": "how-can-i-use-ollama-in-visual-studio-code?"
    },
    {
      "level": "h2",
      "text": "How do I use Ollama with GPU acceleration in Docker?",
      "id": "how-do-i-use-ollama-with-gpu-acceleration-in-docker?"
    },
    {
      "level": "h2",
      "text": "Why is networking slow in WSL2 on Windows 10?",
      "id": "why-is-networking-slow-in-wsl2-on-windows-10?"
    },
    {
      "level": "h2",
      "text": "How can I preload a model into Ollama to get faster response times?",
      "id": "how-can-i-preload-a-model-into-ollama-to-get-faster-response-times?"
    },
    {
      "level": "h2",
      "text": "How do I keep a model loaded in memory or make it unload immediately?",
      "id": "how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately?"
    },
    {
      "level": "h2",
      "text": "How do I manage the maximum number of requests the Ollama server can queue?",
      "id": "how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue?"
    },
    {
      "level": "h2",
      "text": "How does Ollama handle concurrent requests?",
      "id": "how-does-ollama-handle-concurrent-requests?"
    },
    {
      "level": "h2",
      "text": "How does Ollama load models on multiple GPUs?",
      "id": "how-does-ollama-load-models-on-multiple-gpus?"
    },
    {
      "level": "h2",
      "text": "How can I enable Flash Attention?",
      "id": "how-can-i-enable-flash-attention?"
    },
    {
      "level": "h2",
      "text": "How can I set the quantization type for the K/V cache?",
      "id": "how-can-i-set-the-quantization-type-for-the-k/v-cache?"
    },
    {
      "level": "h2",
      "text": "Where can I find my Ollama Public Key?",
      "id": "where-can-i-find-my-ollama-public-key?"
    },
    {
      "level": "h3",
      "text": "How to Add the Key",
      "id": "how-to-add-the-key"
    },
    {
      "level": "h3",
      "text": "Where the Ollama Public Key lives",
      "id": "where-the-ollama-public-key-lives"
    }
  ],
  "url": "llms-txt#allow-all-chrome,-firefox,-and-safari-extensions",
  "links": []
}