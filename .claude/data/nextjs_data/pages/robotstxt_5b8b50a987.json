{
  "url": "https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots",
  "title": "robots.txt",
  "content": "Add or generate a robots.txt file that matches the Robots Exclusion Standard in the root of app directory to tell search engine crawlers which URLs they can access on your site.\n\nAdd a robots.js or robots.ts file that returns a Robots object.\n\nGood to know: robots.js is a special Route Handlers that is cached by default unless it uses a Dynamic API or dynamic config option.\n\nYou can customise how individual search engine bots crawl your site by passing an array of user agents to the rules property. For example:",
  "headings": [
    {
      "level": "h1",
      "text": "robots.txt",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Static robots.txt",
      "id": "static-robotstxt"
    },
    {
      "level": "h2",
      "text": "Generate a Robots file",
      "id": "generate-a-robots-file"
    },
    {
      "level": "h3",
      "text": "Customizing specific user agents",
      "id": "customizing-specific-user-agents"
    },
    {
      "level": "h3",
      "text": "Robots object",
      "id": "robots-object"
    },
    {
      "level": "h2",
      "text": "Version History",
      "id": "version-history"
    }
  ],
  "code_samples": [
    {
      "code": "User-Agent: *\nAllow: /\nDisallow: /private/\n\nSitemap: https://acme.com/sitemap.xml",
      "language": "unknown"
    },
    {
      "code": "import type { MetadataRoute } from 'next'\n \nexport default function robots(): MetadataRoute.Robots {\n  return {\n    rules: {\n      userAgent: '*',\n      allow: '/',\n      disallow: '/private/',\n    },\n    sitemap: 'https://acme.com/sitemap.xml',\n  }\n}",
      "language": "python"
    },
    {
      "code": "User-Agent: *\nAllow: /\nDisallow: /private/\n\nSitemap: https://acme.com/sitemap.xml",
      "language": "unknown"
    },
    {
      "code": "import type { MetadataRoute } from 'next'\n \nexport default function robots(): MetadataRoute.Robots {\n  return {\n    rules: [\n      {\n        userAgent: 'Googlebot',\n        allow: ['/'],\n        disallow: '/private/',\n      },\n      {\n        userAgent: ['Applebot', 'Bingbot'],\n        disallow: ['/'],\n      },\n    ],\n    sitemap: 'https://acme.com/sitemap.xml',\n  }\n}",
      "language": "python"
    },
    {
      "code": "User-Agent: Googlebot\nAllow: /\nDisallow: /private/\n\nUser-Agent: Applebot\nDisallow: /\n\nUser-Agent: Bingbot\nDisallow: /\n\nSitemap: https://acme.com/sitemap.xml",
      "language": "unknown"
    },
    {
      "code": "type Robots = {\n  rules:\n    | {\n        userAgent?: string | string[]\n        allow?: string | string[]\n        disallow?: string | string[]\n        crawlDelay?: number\n      }\n    | Array<{\n        userAgent: string | string[]\n        allow?: string | string[]\n        disallow?: string | string[]\n        crawlDelay?: number\n      }>\n  sitemap?: string | string[]\n  host?: string\n}",
      "language": "unknown"
    }
  ],
  "patterns": [
    {
      "description": "File-system conventionsMetadata Filesrobots.txtCopy pagerobots.txtAdd or generate a robots.txt file that matches the Robots Exclusion Standard in the root of app directory to tell search engine crawlers which URLs they can access on your site. Static robots.txt app/robots.txtUser-Agent: * Allow: / Disallow: /private/ Sitemap: https://acme.com/sitemap.xml Generate a Robots file Add a robots.js or robots.ts file that returns a Robots object. Good to know: robots.js is a special Route Handlers that is cached by default unless it uses a Dynamic API or dynamic config option. app/robots.tsTypeScriptJavaScriptTypeScriptimport type { MetadataRoute } from 'next' export default function robots(): MetadataRoute.Robots { return { rules: { userAgent: '*', allow: '/', disallow: '/private/', }, sitemap: 'https://acme.com/sitemap.xml', } } Output: User-Agent: * Allow: / Disallow: /private/ Sitemap: https://acme.com/sitemap.xml Customizing specific user agents You can customise how individual search engine bots crawl your site by passing an array of user agents to the rules property. For example: app/robots.tsTypeScriptJavaScriptTypeScriptimport type { MetadataRoute } from 'next' export default function robots(): MetadataRoute.Robots { return { rules: [ { userAgent: 'Googlebot', allow: ['/'], disallow: '/private/', }, { userAgent: ['Applebot', 'Bingbot'], disallow: ['/'], }, ], sitemap: 'https://acme.com/sitemap.xml', } } Output: User-Agent: Googlebot Allow: / Disallow: /private/ User-Agent: Applebot Disallow: / User-Agent: Bingbot Disallow: / Sitemap: https://acme.com/sitemap.xml Robots object type Robots = { rules: | { userAgent?: string | string[] allow?: string | string[] disallow?: string | string[] crawlDelay?: number } | Array<{ userAgent: string | string[] allow?: string | string[] disallow?: string | string[] crawlDelay?: number }> sitemap?: string | string[] host?: string } Version History VersionChangesv13.3.0robots introduced.",
      "code": "robots.txt"
    },
    {
      "description": "robots.txtAdd or generate a robots.txt file that matches the Robots Exclusion Standard in the root of app directory to tell search engine crawlers which URLs they can access on your site. Static robots.txt app/robots.txtUser-Agent: * Allow: / Disallow: /private/ Sitemap: https://acme.com/sitemap.xml Generate a Robots file Add a robots.js or robots.ts file that returns a Robots object. Good to know: robots.js is a special Route Handlers that is cached by default unless it uses a Dynamic API or dynamic config option. app/robots.tsTypeScriptJavaScriptTypeScriptimport type { MetadataRoute } from 'next' export default function robots(): MetadataRoute.Robots { return { rules: { userAgent: '*', allow: '/', disallow: '/private/', }, sitemap: 'https://acme.com/sitemap.xml', } } Output: User-Agent: * Allow: / Disallow: /private/ Sitemap: https://acme.com/sitemap.xml Customizing specific user agents You can customise how individual search engine bots crawl your site by passing an array of user agents to the rules property. For example: app/robots.tsTypeScriptJavaScriptTypeScriptimport type { MetadataRoute } from 'next' export default function robots(): MetadataRoute.Robots { return { rules: [ { userAgent: 'Googlebot', allow: ['/'], disallow: '/private/', }, { userAgent: ['Applebot', 'Bingbot'], disallow: ['/'], }, ], sitemap: 'https://acme.com/sitemap.xml', } } Output: User-Agent: Googlebot Allow: / Disallow: /private/ User-Agent: Applebot Disallow: / User-Agent: Bingbot Disallow: / Sitemap: https://acme.com/sitemap.xml Robots object type Robots = { rules: | { userAgent?: string | string[] allow?: string | string[] disallow?: string | string[] crawlDelay?: number } | Array<{ userAgent: string | string[] allow?: string | string[] disallow?: string | string[] crawlDelay?: number }> sitemap?: string | string[] host?: string } Version History VersionChangesv13.3.0robots introduced.",
      "code": "robots.txt"
    },
    {
      "description": "You can customise how individual search engine bots crawl your site by passing an array of user agents to the rules property. For example:",
      "code": "rules"
    }
  ],
  "links": [
    "https://nextjs.org/docs/app/api-reference/file-conventions",
    "https://nextjs.org/docs/app/api-reference/file-conventions/metadata",
    "https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots",
    "https://nextjs.org/docs/app/guides/caching",
    "https://nextjs.org/docs/app/api-reference/file-conventions/metadata/opengraph-image",
    "https://nextjs.org/docs/app/api-reference/file-conventions/metadata/sitemap"
  ]
}