{
  "title": "Metric-based evaluations",
  "content": "Available on Pro and Enterprise plans\n\nMetric-based evaluation is available on Pro and Enterprise plans. Registered community and Starter plan users can also use it for a single workflow.\n\n### What are metric-based evaluations?\n\nOnce your workflow is ready for deployment, you often want to test it on more examples than [when you were building it](../light-evaluations/).\n\nFor example, when production executions start to turn up edge cases, you want to add them to your test dataset so that you can make sure they're covered.\n\nFor large datasets like the ones built from production data, it can be hard to get a sense of performance just by eyeballing the results. Instead, you must measure performance. Metric-based evaluations can assign one or more scores to each test run, which you can compare to previous runs. Individual scores get rolled up to measure performance on the whole dataset.\n\nThis feature allows you to run evaluations that calculate metrics, track how those metrics change between runs and drill down into the reasons for those changes.\n\nMetrics can be deterministic functions (such as the distance between two strings) or you can calculate them using AI. Metrics often involve checking how far away the output is from a *reference output* (also called ground truth). To do so, the dataset must contain that reference output. Some evaluations don't need this reference output though (for example, checking text for sentiment or toxicity).\n\nCredentials for Google Sheets\n\nEvaluations use data tables or Google Sheets to store the test dataset. To use Google Sheets as a dataset source, configure a [Google Sheets credential](../../../integrations/builtin/credentials/google/).\n\n1. Set up [light evaluation](../light-evaluations/)\n1. Add metrics to workflow\n1. Run evaluation and view results\n\n### 1. Set up light evaluation\n\nFollow the [setup instructions](../light-evaluations/) to create a dataset and wire it up to your workflow, writing outputs back to the dataset.\n\nThe following steps use the same support ticket classification workflow from the light evaluation docs:\n\n### 2. Add metrics to workflow\n\nMetrics are dimensions used to score the output of your workflow. They often compare the actual workflow output with a reference output. It's common to use AI to calculate metrics, although it's sometimes possible to just use code. In n8n, metrics are always numbers.\n\nYou need to add the logic to calculate the metrics for your workflow, at a point after it has produced the outputs. You can add any reference outputs your metric uses as a column in your dataset. This makes sure they it will be available in the workflow, since they will be output by the evaluation trigger.\n\nUse the **Set Metrics** operation to calculate:\n\n- **Correctness (AI-based)**: Whether the answer's meaning is consistent with a supplied reference answer. Uses a scale of 1 to 5, with 5 being the best.\n- **Helpfulness (AI-based)**: Whether the response answers the given query. Uses a scale of 1 to 5, with 5 being the best.\n- **String Similarity**: How close the answer is to the reference answer, measured character-by-character (edit distance). Returns a score between 0 and 1.\n- **Categorization**: Whether the answer is an exact match with the reference answer. Returns 1 when matching and 0 otherwise.\n- **Tools Used**: Whether the execution used tools or not. Returns a score between 0 and 1.\n\nYou can also add custom metrics. Just calculate the metrics within the workflow and then map them into an Evaluation node. Use the **Set Metrics** operation and choose **Custom Metrics** as the Metric. You can then set the names and values for the metrics you want to return.\n\n- [RAG document relevance](https://n8n.io/workflows/4273): when working with a vector database, whether the documents retrieved are relevant to the question.\n\nCalculating metrics can add latency and cost, so you may only want to do it when running an evaluation and avoid it when making a production execution. You can do this by putting the metric logic after a ['check if evaluating' operation](../../../integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#check-if-evaluating).\n\n### 3. Run evaluation and view results\n\nSwitch to the **Evaluations** tab on your workflow and click the **Run evaluation** button. An evaluation will start. Once the evaluation has finished, it will display a summary score for each metric.\n\nYou can see the results for each test case by clicking on the test run row. Clicking on an individual test case will open the execution that produced it (in a new tab).",
  "code_samples": [],
  "headings": [
    {
      "level": "h3",
      "text": "What are metric-based evaluations?",
      "id": "what-are-metric-based-evaluations?"
    },
    {
      "level": "h2",
      "text": "How it works",
      "id": "how-it-works"
    },
    {
      "level": "h3",
      "text": "1. Set up light evaluation",
      "id": "1.-set-up-light-evaluation"
    },
    {
      "level": "h3",
      "text": "2. Add metrics to workflow",
      "id": "2.-add-metrics-to-workflow"
    },
    {
      "level": "h3",
      "text": "3. Run evaluation and view results",
      "id": "3.-run-evaluation-and-view-results"
    }
  ],
  "url": "llms-txt#metric-based-evaluations",
  "links": []
}