{
  "title": "Ollama credentials",
  "content": "You can use these credentials to authenticate the following nodes:\n\n- [Ollama](../../cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/)\n- [Chat Ollama](../../cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/)\n- [Embeddings Ollama](../../cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsollama/)\n\nCreate and run an [Ollama](https://ollama.com/) instance with one user. Refer to the Ollama [Quick Start](https://github.com/ollama/ollama/blob/main/README.md#quickstart) for more information.\n\n## Supported authentication methods\n\nRefer to [Ollama's API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) for more information about the service.\n\nView n8n's [Advanced AI](../../../../advanced-ai/) documentation.\n\n## Using instance URL\n\nTo configure this credential, you'll need:\n\n- The **Base URL** of your Ollama instance or remote authenticated Ollama instances.\n- (Optional) The **API Key** for Bearer token authentication if connecting to a remote, authenticated proxy.\n\nThe default **Base URL** is `http://localhost:11434`, but if you've set the `OLLAMA_HOST` environment variable, enter that value. If you have issues connecting to a local n8n server, try `127.0.0.1` instead of `localhost`.\n\nIf you're connecting to Ollama through authenticated proxy services (such as [Open WebUI](https://docs.openwebui.com/getting-started/api-endpoints/#-ollama-api-proxy-support)) you must include an API key. If you don't need authentication, leave this field empty. When provided, the API key is sent as a Bearer token in the `Authorization` header of the request to the Ollama API.\n\nRefer to [How do I configure Ollama server?](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server) for more information.\n\n### Ollama and self-hosted n8n\n\nIf you're self-hosting n8n on the same machine as Ollama, you may run into issues if they're running in different containers.\n\nFor this setup, open a specific port for n8n to communicate with Ollama by setting the `OLLAMA_ORIGINS` variable or adjusting `OLLAMA_HOST` to an address the other container can access.\n\nRefer to Ollama's [How can I allow additional web origins to access Ollama?](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-allow-additional-web-origins-to-access-ollama) for more information.",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "Supported authentication methods",
      "id": "supported-authentication-methods"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Using instance URL",
      "id": "using-instance-url"
    },
    {
      "level": "h3",
      "text": "Ollama and self-hosted n8n",
      "id": "ollama-and-self-hosted-n8n"
    }
  ],
  "url": "llms-txt#ollama-credentials",
  "links": []
}