{
  "title": "Light evaluations",
  "content": "Available on registered community and paid plans\n\nLight evaluations are available to registered community users and on all paid plans.\n\n## What are light evaluations?\n\nWhen building your workflow, you often want to test it with a handful of examples to get a sense of how it performs and make improvements. At this stage of workflow development, looking over workflow outputs for each example is often enough. The benefits of setting up more [formal scoring or metrics](../metric-based-evaluations/) don't yet justify the effort.\n\nLight evaluation allows you to run the examples in a test dataset through your workflow one-by-one, writing the outputs back to your dataset. You can then examine those outputs next to each other, and visually compare them to the expected outputs (if you have them).\n\nCredentials for Google Sheets\n\nEvaluations use data tables or Google Sheets to store the test dataset. To use Google Sheets as a dataset source, configure a [Google Sheets credential](../../../integrations/builtin/credentials/google/).\n\nLight evaluations take place in the 'Editor' tab of your workflow, although you’ll find instructions on how to set it up in the 'Evaluations' tab.\n\n1. Create a dataset\n1. Wire the dataset up to the workflow\n1. Write workflow outputs back to dataset\n1. Run evaluation\n\nThe following explanation will use a sample workflow that assigns a category and priority to incoming support tickets.\n\n### 1. Create a dataset\n\nCreate a data table or Google Sheet with a handful of examples for your workflow. Your dataset should contain columns for:\n\n- The workflow input\n- (Optional) The expected or correct workflow output\n- The actual output\n\nLeave the actual output column or columns blank, since you'll be filling them during the evaluation.\n\nA [sample dataset](https://docs.google.com/spreadsheets/d/1uuPS5cHtSNZ6HNLOi75A2m8nVWZrdBZ_Ivf58osDAS8/edit?gid=294497137#gid=294497137) for the support ticket classification workflow.\n\n### 2. Wire the dataset up to your workflow\n\n#### Insert an evaluation trigger to pull in your dataset\n\nEach time the [evaluation trigger](../../../integrations/builtin/core-nodes/n8n-nodes-base.evaluationtrigger/) runs, it will output a single item representing one row of your dataset.\n\nClicking the 'Evaluate all' button to the left of the evaluation trigger will run your workflow multiple times in sequence, once for each row in your dataset. This is a special behavior of the evaluation trigger.\n\nWhile wiring the trigger up, you often only want to run it once. You can do this by either:\n\n- Setting the trigger's 'Max rows to process' to 1\n- Clicking on the 'Execute node' button on the trigger (rather than the 'Evaluate all' button)\n\n#### Wire the trigger up to your workflow\n\nYou can now connect the evaluation trigger to the rest of your workflow and reference the data that it outputs. At a minimum, you need to use the dataset’s input column(s) later in the workflow.\n\nIf you have multiple triggers in your workflow you will need to [merge their branches together](../tips-and-common-issues/#combining-multiple-triggers).\n\nThe support ticket classification workflow with the evaluation trigger added in and wired up.\n\n### 3. Write workflow outputs back to dataset\n\nTo populate the output column(s) of your dataset when the evaluation runs:\n\n- Insert the 'Set outputs' action of the [evaluation node](../../../integrations/builtin/core-nodes/n8n-nodes-base.evaluation/)\n- Wire it up to your workflow at a point after it has produced the outputs you're evaluating\n- In the node's parameters, map the workflow outputs into the correct dataset column\n\nThe support ticket classification workflow with the 'set outputs' node added in and wired up.\n\n### 4. Run evaluation\n\nClick on the **Execute workflow** button to the left of the evaluation trigger. The workflow will execute multiple times, once for each row of the dataset:\n\nReview the outputs of each execution in the data table or Google Sheet, and examine the execution details using the workflow's 'executions' tab if you need to.\n\nOnce your dataset grows past a handful of examples, consider [metric-based evaluation](../metric-based-evaluations/) to get a numerical view of performance. See also [tips and common issues](../tips-and-common-issues/).",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "What are light evaluations?",
      "id": "what-are-light-evaluations?"
    },
    {
      "level": "h2",
      "text": "How it works",
      "id": "how-it-works"
    },
    {
      "level": "h3",
      "text": "1. Create a dataset",
      "id": "1.-create-a-dataset"
    },
    {
      "level": "h3",
      "text": "2. Wire the dataset up to your workflow",
      "id": "2.-wire-the-dataset-up-to-your-workflow"
    },
    {
      "level": "h3",
      "text": "3. Write workflow outputs back to dataset",
      "id": "3.-write-workflow-outputs-back-to-dataset"
    },
    {
      "level": "h3",
      "text": "4. Run evaluation",
      "id": "4.-run-evaluation"
    }
  ],
  "url": "llms-txt#light-evaluations",
  "links": []
}