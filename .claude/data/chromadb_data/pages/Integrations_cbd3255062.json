{
  "title": "Integrations",
  "content": "- [Integrations](https://docs.trychroma.com/integrations/chroma-integrations.md): The page covers Chroma's integration capabilities, detailing both embedding and framework integrations with various AI tools and libraries, along with their compatibility in Python and Typescript/JavaScript.\n- [Baseten](https://docs.trychroma.com/integrations/embedding-models/baseten.md): The page provides information on Baseten, a model inference provider, and guides users on integrating Baseten's embedding models with Chroma, including setup instructions for using the `openai` Python package.\n- [Cloudflare Workers AI](https://docs.trychroma.com/integrations/embedding-models/cloudflare-workers-ai.md): The page provides information on using Chroma's wrapper for Cloudflare Workers AI embedding models, including setup instructions, code examples in Python and TypeScript, and requirements for an API key and Cloudflare account.\n- [Cohere](https://docs.trychroma.com/integrations/embedding-models/cohere.md): The page provides an overview of Chroma's integration with Cohere's embedding API, including setup instructions and code examples for using the API in both Python and TypeScript, covering multilingual and multimodal model examples.\n- [Google Gemini](https://docs.trychroma.com/integrations/embedding-models/google-gemini.md): The page provides instructions for using Chroma's wrapper for Google's Generative AI embedding API, including setup and usage examples in Python and TypeScript.\n- [Hugging Face Server](https://docs.trychroma.com/integrations/embedding-models/hugging-face-server.md): The page explains how to set up and use the Hugging Face Text Embedding Server with Chroma, providing commands and code examples for both Python and TypeScript, as well as information on configuring server-side models and authentication with API keys.\n- [Hugging Face](https://docs.trychroma.com/integrations/embedding-models/hugging-face.md): The page explains how to use Chroma's wrapper for Hugging Face's embedding API, requiring an API key and allowing for optional model selection.\n- [Instructor](https://docs.trychroma.com/integrations/embedding-models/instructor.md): The page describes the use of the instructor-embeddings library for generating embeddings with Chroma, highlighting installation instructions, model options, and device compatibility.\n- [JinaAI](https://docs.trychroma.com/integrations/embedding-models/jina-ai.md): The page describes how to use Chroma's wrapper for JinaAI's embedding API, detailing code examples in Python and TypeScript, usage of late chunking in Jina-embeddings-v3, and the task parameter for optimizing different embedding uses.\n- [Mistral](https://docs.trychroma.com/integrations/embedding-models/mistral.md): The page describes how to use Chroma's wrapper around Mistral's embedding API, providing instructions for implementation in both Python and TypeScript.\n- [Ollama](https://docs.trychroma.com/integrations/embedding-models/ollama.md): The page explains how to use Chroma's wrapper around Ollama's embeddings API to generate embeddings for documents using the `OllamaEmbeddingFunction` in both Python and TypeScript.\n- [OpenAI](https://docs.trychroma.com/integrations/embedding-models/openai.md): The page describes Chroma's wrapper around OpenAI's embedding API, detailing how to use various OpenAI embedding models with Python and TypeScript, and providing information on obtaining and configuring the necessary API keys and settings.\n- [Roboflow](https://docs.trychroma.com/integrations/embedding-models/roboflow.md): The page explains how to use Roboflow Inference with Chroma to compute multi-modal text and image embeddings using the `RoboflowEmbeddingFunction` class, both via the Roboflow cloud and locally on your hardware.\n- [Together AI](https://docs.trychroma.com/integrations/embedding-models/together-ai.md): The page explains how to use Chroma's wrapper for Together AI embedding models, including setup requirements, example code in Python and TypeScript, and configuration details for model name and API key integration.\n- [VoyageAI](https://docs.trychroma.com/integrations/embedding-models/voyageai.md): The page describes how to use Chroma's wrapper for VoyageAI's embedding API in both Python and TypeScript, requiring an API key and illustrating usage with multilingual examples.\n- [Anthropic MCP Integration](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp.md): The page provides detailed instructions on integrating Anthropic's Claude AI with Chroma's vector database through the Model Context Protocol (MCP), including setup, configuration, and examples of using Chroma for persistent memory and knowledge management.\n- [Braintrust](https://docs.trychroma.com/integrations/frameworks/braintrust.md): The page introduces Braintrust as an enterprise-grade stack for building AI products, detailing its integration with Chroma and providing a sample Python script for evaluations using Braintrust.\n- [DeepEval](https://docs.trychroma.com/integrations/frameworks/deepeval.md): The page describes DeepEval, an open-source LLM evaluation framework, including installation, preparation, evaluation metrics, and optimization for Chroma retrievers in RAG systems.\n- [Haystack](https://docs.trychroma.com/integrations/frameworks/haystack.md): The page describes the integration of the open-source Haystack LLM framework with Chroma, detailing installation instructions and providing code examples for using Chroma with Haystack to write documents into a ChromaDocumentStore and build retrieval-augmented generation (RAG) pipelines.\n- [Langchain](https://docs.trychroma.com/integrations/frameworks/langchain.md): The page provides resources and links for integrating Langchain with Chroma in both Python and JavaScript, including tutorials, demos, and documentation.\n- [LlamaIndex](https://docs.trychroma.com/integrations/frameworks/llamaindex.md): The page provides links and information related to using LlamaIndex as a vector store in Chroma, including a documentation page, demo, and Chroma Loader on Llamahub.\n- [OpenLIT](https://docs.trychroma.com/integrations/frameworks/openlit.md): The page introduces OpenLIT, an LLM Application Observability tool with OpenTelemetry auto-instrumentation for Chroma, and provides a guide for installing and initializing OpenLIT, as well as visualizing LLM Observability data.\n- [OpenLLMetry](https://docs.trychroma.com/integrations/frameworks/openllmetry.md): The page introduces OpenLLMetry, an observability tool for systems using Chroma, and provides installation and configuration instructions.\n- [Streamlit](https://docs.trychroma.com/integrations/frameworks/streamlit.md): The page introduces Streamlit and describes how to use it with Chroma, highlighting installation, main benefits, a simple example, resources, and tutorials for integrating Chroma with Streamlit.\n- [VoltAgent](https://docs.trychroma.com/integrations/frameworks/voltagent.md): The page provides an overview of using VoltAgent, a TypeScript framework for AI agents, with Chroma for semantic search, including setup instructions, environment configuration, and example usage patterns for retrieval-augmented generation systems.",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#integrations",
  "links": []
}